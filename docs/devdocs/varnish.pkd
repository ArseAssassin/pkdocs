---
name: Varnish
slug: varnish
text_format: markdown
generator: src:devdocs
generator_command: src:devdocs
version: ''
copyright: |-
  Copyright © 2006 Verdens Gang AS
  Copyright © 2006–2020 Varnish Software AS
  Licensed under the BSD-2-Clause License.
  https://varnish-cache.org/docs/7.4/index.html
homepage: https://varnish-cache.org/

---
- name: Achieving a high hitrate
  id: users-guide/increasing-your-hitrate
  summary: Now that Varnish is up and running you can access your web application through Varnish
  description: "# Achieving a high hitrate\n\nNow that Varnish is up and running you can access your web application through Varnish. Unless your application is specifically written to work behind a web accelerator you’ll probably need to do some changes to either the configuration or the application in order to get a high hitrate in Varnish.\n\nVarnish will not cache your data unless it’s absolutely sure it is safe to do so. So, for you to understand how Varnish decides if and how to cache a page, we’ll guide you through a couple of tools that you should find useful to understand what is happening in your Varnish setup.\n\nNote that you need a tool to see the HTTP headers that fly between Varnish and the backend. On the Varnish server, the easiest way to do this is to use [varnishlog](../reference/varnishlog#varnishlog-1) and [varnishtop](../reference/varnishtop#varnishtop-1) but sometimes a client-side tool makes sense. Here are the ones we commonly use.\n\n## Tool: varnishtop\n\nYou can use varnishtop to identify what URLs are hitting the backend the most. `varnishtop -i BereqURL` is an essential command, showing you the top requests Varnish is sending to the backend. You can see some other examples of [varnishtop](../reference/varnishtop#varnishtop-1) usage in [Statistics](operation-statistics#users-guide-statistics).\n\n## Tool: varnishlog\n\nWhen you have identified an URL which is frequently sent to the backend you can use [varnishlog](../reference/varnishlog#varnishlog-1) to have a look at the request. `varnishlog -q 'ReqURL ~ \"^/foo/bar\"'` will show you the requests coming from the client matching `/foo/bar`.\n\nFor more information on how [varnishlog](../reference/varnishlog#varnishlog-1) works please see [Logging in Varnish](operation-logging#users-guide-logging) or then man page.\n\n## Tool: lwp-request\n\n`lwp-request` is tool that is a part of The World-Wide Web library for Perl. It’s a couple of really basic programs that can execute an HTTP request and show you the result. We mostly use the two programs, `GET` and `HEAD`.\n\nvg.no was the first site to use Varnish and the people running Varnish there are quite clueful. So it’s interesting to look at their HTTP Headers. Let’s send a GET request for their home page:\n\n``` python\n$ GET -H 'Host: www.vg.no' -Used http://vg.no/\nGET http://vg.no/\nHost: www.vg.no\nUser-Agent: lwp-request/5.834 libwww-perl/5.834\n\n200 OK\nCache-Control: must-revalidate\nRefresh: 600\nTitle: VG Nett - Forsiden - VG Nett\nX-Age: 463\nX-Cache: HIT\nX-Rick-Would-Never: Let you down\nX-VG-Jobb: http://www.finn.no/finn/job/fulltime/result?keyword=vg+multimedia Merk:HeaderNinja\nX-VG-Korken: http://www.youtube.com/watch?v=Fcj8CnD5188\nX-VG-WebCache: joanie\nX-VG-WebServer: leon\n```\n\nOK. Lets look at what `GET` does. `GET` usually sends off HTTP 0.9 requests, which lack the ‘Host’ header. So we add a ‘Host’ header with the ‘-H’ option. ‘-U’ print request headers, ‘-s’ prints response status, ‘-e’ prints response headers and ‘-d’ discards the actual content. We don’t really care about the content, only the headers.\n\nAs you can see, VG adds quite a bit of information in their headers. Some of the headers, like the ‘X-Rick-Would-Never’ are specific to vg.no and their somewhat odd sense of humour. Others, like the ‘X-VG-Webcache’ are for debugging purposes.\n\nSo, to check whether a site sets cookies for a specific URL, just do:\n\n``` python\nGET -Used http://example.com/ |grep ^Set-Cookie\n```\n\n## Tool: Live HTTP Headers\n\nThere is also a plugin for Firefox called `Live HTTP Headers`. This plugin can show you what headers are being sent and received. `Live HTTP Headers` can be found at [https://addons.mozilla.org/en-US/firefox/addon/3829/](https://addons.mozilla.org/en-US/firefox/addon/3829/) or by googling “Live HTTP Headers”.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/increasing-your-hitrate.html](https://varnish-cache.org/docs/7.4/users-guide/increasing-your-hitrate.html)"
- name: ACLs
  id: users-guide/vcl-example-acls
  summary: You create a named access control list with the acl keyword
  description: "# ACLs\n\nYou create a named access control list with the `acl` keyword. You can match the IP address of the client against an ACL with the match operator.:\n\n``` python\n# Who is allowed to purge....\nacl local {\n    \"localhost\";\n    \"192.168.1.0\"/24; /* and everyone on the local network */\n    ! \"192.168.1.23\"; /* except for the dialin router */\n}\n\nsub vcl_recv {\n  if (req.method == \"PURGE\") {\n    if (client.ip ~ local) {\n       return(purge);\n    } else {\n       return(synth(403, \"Access denied.\"));\n    }\n  }\n}\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-example-acls.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-example-acls.html)"
- name: Adding WebSockets support
  id: users-guide/vcl-example-websockets
  summary: WebSockets is a technology for creating a bidirectional stream-based channel over HTTP
  description: "# Adding WebSockets support\n\nWebSockets is a technology for creating a bidirectional stream-based channel over HTTP.\n\nTo run WebSockets through Varnish you need to pipe the request and copy the Upgrade and Connection headers as follows:\n\n``` python\nsub vcl_recv {\n    if (req.http.upgrade ~ \"(?i)websocket\") {\n        return (pipe);\n    }\n}\n\nsub vcl_pipe {\n    if (req.http.upgrade) {\n        set bereq.http.upgrade = req.http.upgrade;\n        set bereq.http.connection = req.http.connection;\n    }\n}\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-example-websockets.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-example-websockets.html)"
- name: Altering the backend response
  id: users-guide/vcl-example-manipulating-responses
  summary: We also remove any Set-Cookie headers in order to avoid creation of a hit-for-miss object
  description: "# Altering the backend response\n\nHere we override the TTL of a object coming from the backend if it matches certain criteria:\n\n``` python\nsub vcl_backend_response {\n   if (bereq.url ~ \"\\.(png|gif|jpg)$\") {\n     unset beresp.http.set-cookie;\n     set beresp.ttl = 1h;\n  }\n}\n```\n\nWe also remove any Set-Cookie headers in order to avoid creation of a `hit-for-miss` object. See [VCL Actions](../reference/vcl-step#vcl-actions).\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-example-manipulating-responses.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-example-manipulating-responses.html)"
- name: Backend servers
  id: users-guide/vcl-backends
  summary: Varnish has a concept of “backend” or “origin” servers
  description: "# Backend servers\n\nVarnish has a concept of “backend” or “origin” servers. A backend server is the server providing the content Varnish will accelerate.\n\nOur first task is to tell Varnish where it can find its backends. Start your favorite text editor and open the relevant VCL file.\n\nSomewhere in the top there will be a section that looks a bit like this.:\n\n``` python\n# backend default {\n#     .host = \"127.0.0.1\";\n#     .port = \"8080\";\n# }\n```\n\nWe remove the comment markings in this text stanza making the it look like.:\n\n``` python\nbackend default {\n    .host = \"127.0.0.1\";\n    .port = \"8080\";\n}\n```\n\nNow, this piece of configuration defines a backend in Varnish called *default*. When Varnish needs to get content from this backend it will connect to port 8080 on localhost (127.0.0.1).\n\nVarnish can have several backends defined you can even join several backends together into clusters of backends for load balancing purposes.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-backends.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-backends.html)"
- name: Backend servers
  id: tutorial/backend_servers
  summary: Varnish has a concept of backend or origin servers
  description: "# Backend servers\n\nVarnish has a concept of `backend` or origin servers. A backend server is the server providing the content Varnish will accelerate via the cache.\n\nOur first task is to tell Varnish where it can find its content. Start your favorite text editor and open the Varnish default configuration file. If you installed from source this is `/usr/local/etc/varnish/default.vcl`, if you installed from a package it is probably `/etc/varnish/default.vcl`.\n\nIf you’ve been following the tutorial there is probably a section of the configuration that looks like this:\n\n``` python\nvcl 4.0;\n\nbackend default {\n    .host = \"www.varnish-cache.org\";\n    .port = \"80\";\n}\n```\n\nThis means we set up a backend in Varnish that fetches content from the host www.varnish-cache.org on port 80.\n\nSince you probably don’t want to be mirroring varnish-cache.org we need to get Varnish to fetch content from your own origin server. We’ve already bound Varnish to the public port 80 on the server so now we need to tie it to the origin.\n\nFor this example, let’s pretend the origin server is running on localhost, port 8080.:\n\n``` python\nvcl 4.0;\n\nbackend default {\n  .host = \"127.0.0.1\";\n  .port = \"8080\";\n}\n```\n\nVarnish can have several backends defined and can even join several backends together into clusters of backends for load balancing purposes, having Varnish pick one backend based on different algorithms.\n\nNext, let’s have a look at some of what makes Varnish unique and what you can do with it.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/tutorial/backend_servers.html](https://varnish-cache.org/docs/7.4/tutorial/backend_servers.html)"
- name: Built-in VCL
  id: users-guide/vcl-built-in-code
  summary: Whenever a VCL program is loaded, the built-in VCL is appended to it
  description: "# Built-in VCL\n\nWhenever a VCL program is loaded, the built-in VCL is appended to it. The vcl built-in subs ([VCL Steps](../reference/vcl-step#id7)) have a special property, they can appear multiple times and the result is concatenation of all built-in subroutines.\n\nFor example, let’s take the following snippet:\n\n``` python\nsub vcl_recv {\n    # loaded code for vcl_recv\n}\n```\n\nThe effective VCL that is supplied to the compiler looks like:\n\n``` python\nsub vcl_recv {\n    # loaded code for vcl_recv\n    # built-in code for vcl_recv\n}\n```\n\nThis is how it is guaranteed that all [Varnish Processing States](../reference/states#reference-states) have at least one `return (<action>)`.\n\nIt is generally recommended not to invariably return from loaded code to let Varnish execute the built-in code, because the built-in code provides essentially a sensible default behavior for an HTTP cache.\n\n## Built-in subroutines split\n\nIt might however not always be practical that the built-in VCL rules take effect at the very end of a state, so some subroutines like `vcl_recv` are split into multiple calls to other subroutines.\n\nBy convention, those assistant subroutines are named after the variable they operate on, like `req` or `beresp`. This allows for instance to circumvent default behavior.\n\nFor example, `vcl_recv` in the built-in VCL prevents caching when clients have a cookie. If you can trust your backend to always specify whether a response is cacheable or not regardless of whether the request contained a cookie you can do this:\n\n``` python\nsub vcl_req_cookie {\n    return;\n}\n```\n\nWith this, all other default behaviors from the built-in `vcl_recv` are executed and only cookie handling is affected.\n\nAnother example is how the built-in `vcl_backend_response` treats a negative TTL as a signal not to cache. It’s a historical mechanism to mark a response as uncacheable, but only if the built-in `vcl_backend_response` is not circumvented by a `return (<action>)`.\n\nHowever, in a multi-tier architecture where a backend might be another Varnish server, you might want to cache stale responses to allow the delivery of graced objects and enable revalidation on the next fetch. This can be done with the following snippet:\n\n``` python\nsub vcl_beresp_stale {\n    if (beresp.ttl + beresp.grace > 0s) {\n        return;\n    }\n}\n```\n\nThis granularity, and the general goal of the built-in subroutines split is to allow to circumvent a specific aspect of the default rules without giving the entire logic up.\n\n## Built-in VCL reference\n\nA copy of the `builtin.vcl` file might be provided with your Varnish installation but [varnishd](../reference/varnishd#varnishd-1) is the reference to determine the code that is appended to any loaded VCL.\n\nThe VCL compilation happens in two passes:\n\n- the first one compiles the built-in VCL only,\n- and the second pass compiles the concatenation of the loaded and built-in VCLs.\n\nAny VCL subroutine present in the built-in VCL can be extended, in which case the loaded VCL code will be executed before the built-in code.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-built-in-code.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-built-in-code.html)"
- name: Bundling VMODs with the Varnish distribution
  id: dev-guide/policy_vmods
  summary: Decisions about whether to add a new Varnish module (VMOD) to those bundled with Varnish are guided by these criteria
  description: "# Bundling VMODs with the Varnish distribution\n\nDecisions about whether to add a new Varnish module (VMOD) to those bundled with Varnish are guided by these criteria.\n\n- The VMOD is known to be in widespread use and in high demand for common use cases.\n\n- Or, if the VMOD is relatively new, it provides compelling features that the developer group agrees will be a valuable enhancement for the project.\n\n- The VMOD does not create dependencies on additional external libraries. VMODs that are “glue” for a library come from third parties.\n\n  - We don’t want to add new burdens of dependency and compatibility to the project.\n  - We don’t want to force Varnish deployments to install more than admins explicitly choose to install.\n\n- The VMOD code follows project conventions (passes make distcheck, follows source code style, and so forth).\n\n  - A pull request can demonstrate that this is the case (after any necessary fixups).\n\n- The developer group commits to maintaining the code for the long run (so there will have to be a consensus that we’re comfortable with it).\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/dev-guide/policy_vmods.html](https://varnish-cache.org/docs/7.4/dev-guide/policy_vmods.html)"
- name: CLI - bossing Varnish around
  id: users-guide/run_cli
  summary: If you want to run varnishadm from a remote system, we recommend you use ssh into the system where varnishd runs
  description: "# CLI - bossing Varnish around\n\nOnce `varnishd` is started, you can control it using the `varnishadm` program and the command line interface:\n\n``` python\nvarnishadm help\n```\n\nIf you want to run `varnishadm` from a remote system, we recommend you use `ssh` into the system where `varnishd` runs. (But see also: [Local and remote CLI connections](../reference/cli_protocol#ref-remote-cli))\n\nYou can SSH into the `varnishd` computer and run `varnishadm`:\n\n``` python\nssh $hostname varnishadm help\n```\n\nIf you give no command arguments, `varnishadm` runs in interactive mode with command-completion, command-history and other comforts:\n\n``` text\ncritter phk> ./varnishadm\n200\n-----------------------------\nVarnish Cache CLI 1.0\n-----------------------------\nFreeBSD,13.0-CURRENT,amd64,-jnone,-sdefault,-sdefault,-hcritbit\nvarnish-trunk revision 2bd5d2adfc407216ebaa653fae882d3c8d47f5e1\n\nType 'help' for command list.\nType 'quit' to close CLI session.\nType 'start' to launch worker process.\n\nvarnish>\n```\n\nThe CLI always returns a three digit status code to tell how things went.\n\n200 and 201 means *OK*, anything else means that some kind of trouble prevented the execution of the command.\n\n(If you get 201, it means that the output was truncated, See the [cli_limit](../reference/varnishd#ref-param-cli-limit) parameter.)\n\nWhen commands are given as arguments to `varnishadm`, a status different than 200 or 201 will cause it to exit with status 1 and print the status code on standard error.\n\n## What can you do with the CLI\n\nFrom the CLI you can:\n\n- load/use/discard VCL programs\n- ban (invalidate) cache content\n- change parameters\n- start/stop worker process\n\nWe will discuss each of these briefly below.\n\n### Load, use and discard VCL programs\n\nAll caching and policy decisions are made by VCL programs.\n\nYou can have multiple VCL programs loaded, but one of them is designated the “active” VCL program, and this is where all new requests start out.\n\nTo load new VCL program:\n\n``` python\nvarnish> vcl.load some_name some_filename\n```\n\nLoading will read the VCL program from the file, and compile it. If the compilation fails, you will get an error messages:\n\n``` text\n.../mask is not numeric.\n('input' Line 4 Pos 17)\n                \"192.168.2.0/24x\",\n----------------#################-\n\nRunning VCC-compiler failed, exit 1\nVCL compilation failed\n```\n\nIf compilation succeeds, the VCL program is loaded, and you can now make it the active VCL, whenever you feel like it:\n\n``` python\nvarnish> vcl.use some_name\n```\n\nIf you find out that was a really bad idea, you can switch back to the previous VCL program again:\n\n``` python\nvarnish> vcl.use old_name\n```\n\nThe switch is instantaneous, all new requests will start using the VCL you activated right away. The requests currently being processed complete using whatever VCL they started with.\n\nWe highly recommend you design an emergency-VCL, and always keep it loaded, so it can be activated with\n\n``` python\nvcl.use emergency\n```\n\n### Ban cache content\n\nVarnish offers “purges” to remove things from cache, but that requires you to know exactly what they are.\n\nSometimes it is useful to be able to throw things out of cache without having an exact list of what to throw out.\n\nImagine for instance that the company logo changed and now you need Varnish to stop serving the old logo out of the cache:\n\n``` text\nvarnish> ban req.url ~ \"logo.*[.]png\"\n```\n\nshould do that, and yes, that is a regular expression.\n\nWe call this “banning” because the objects are still in the cache, but they are now banned from delivery, while all the rest of the cache is unaffected.\n\nEven when you want to throw out *all* the cached content, banning is both faster and less disruptive that a restart:\n\n``` python\nvarnish> ban obj.http.date ~ .*\n```\n\n### Change parameters\n\nParameters can be set on the command line with the ‘-p’ argument, but almost all parameters can be examined and changed on the fly from the CLI:\n\n``` text\nvarnish> param.show prefer_ipv6\n200\nprefer_ipv6         off [bool]\n                    Default is off\n                    Prefer IPv6 address when connecting to backends\n                    which have both IPv4 and IPv6 addresses.\n\nvarnish> param.set prefer_ipv6 true\n200\n```\n\nIn general it is not a good idea to modify parameters unless you have a good reason, such as performance tuning or security configuration.\n\nMost parameters will take effect instantly, or with a short delay, but a few of them requires you to restart the child process before they take effect. This is always mentioned in the description of the parameter.\n\n### Starting and stopping the worker process\n\nIn general you should just leave the worker process running, but if you need to stop and/or start it, the obvious commands work:\n\n``` python\nvarnish> stop\n```\n\nand:\n\n``` python\nvarnish> start\n```\n\nIf you start `varnishd` with the ‘-d’ (debugging) argument, you will always need to start the child process explicitly.\n\nShould the child process die, the master process will automatically restart it, but you can disable that with the [auto_restart](../reference/varnishd#ref-param-auto-restart) parameter.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/run_cli.html](https://varnish-cache.org/docs/7.4/users-guide/run_cli.html)"
- name: Compiling Varnish from source
  id: installation/install_source
  summary: Alternatively, if you want to hack on Varnish, you should clone our git repository by doing
  description: "# Compiling Varnish from source\n\nIf there are no binary packages available for your system, or if you want to compile Varnish from source for other reasons, follow these steps:\n\n## Getting hold of the source\n\nDownload the appropriate release tarball, which you can find on [https://varnish-cache.org/releases/](https://varnish-cache.org/releases/) .\n\nAlternatively, if you want to hack on Varnish, you should clone our git repository by doing.\n\n`git clone https://github.com/varnishcache/varnish-cache`\n\n## Build dependencies on FreeBSD\n\nTo get the dependencies required to build varnish from source you can either:\n\n``` python\npkg install automake pkgconf py36-sphinx py36-docutils pcre2 libtool\n```\n\nAnd optionally, to be able to run all the testcases:\n\n``` python\npkg install haproxy nghttp2 vttest\n```\n\nOr if you want the built from sources:\n\n``` python\ncd /usr/ports/www/varnish6\nmake depends clean\n```\n\nThen continue [Compiling Varnish](#compiling-varnish)\n\n## Build dependencies on Debian / Ubuntu\n\nIn order to build Varnish from source you need a number of packages installed. On a Debian or Ubuntu system, use this command to install them (replace `sudo apt-get install` if needed):\n\n``` python\nsudo apt-get install \\\n    make \\\n    automake \\\n    autotools-dev \\\n    libedit-dev \\\n    libjemalloc-dev \\\n    libncurses-dev \\\n    libpcre2-dev \\\n    libtool \\\n    pkg-config \\\n    python3-docutils \\\n    python3-sphinx \\\n    cpio\n```\n\nOptionally, to rebuild the svg files:\n\n``` python\nsudo apt-get install graphviz\n```\n\nRecommended, in particular if you plan on building custom vmods:\n\n``` python\nsudo apt-get install autoconf-archive\n```\n\nOptionally, to pull from a repository:\n\n``` python\nsudo apt-get install git\n```\n\nThen continue [Compiling Varnish](#compiling-varnish)\n\n## Build dependencies on Red Hat / CentOS\n\nin the following shell commands, replace `sudo yum install` if needed.\n\nInstall sphinx\n\n- On Red Hat / CentOS 8, sphinx is not included in the default repositories, so execute these steps to include it from the powertools repository:\n\n  ``` python\n  sudo dnf install -y 'dnf-command(config-manager)'\n  sudo yum config-manager --set-enabled powertools\n  sudo yum install -y diffutils python3-sphinx\n  ```\n\n- On Red Hat / CentOS \\<= 7, install sphinx:\n\n  ``` python\n  sudo yum install -y python-sphinx\n  ```\n\nThe following step should conclude installation of the required packages:\n\n``` python\nyum install -y \\\n      make \\\n      autoconf \\\n      automake \\\n      jemalloc-devel \\\n      libedit-devel \\\n      libtool \\\n      libunwind-devel \\\n      ncurses-devel \\\n      pcre2-devel \\\n      pkgconfig \\\n      python3-docutils \\\n      cpio\n```\n\nOptionally, to rebuild the svg files:\n\n``` python\nyum install graphviz\n```\n\nOptionally, to pull from a repository:\n\n``` python\nyum install git\n```\n\nThen continue [Compiling Varnish](#compiling-varnish)\n\n## Build dependencies on MacOS\n\nTo compile varnish on MacOS, these steps should install the required dependencies:\n\n- Install `xcode` via the App Store\n\n- Install dependencies via `brew`:\n\n  ``` python\n  brew install \\\n      autoconf \\\n      automake \\\n      pkg-config \\\n      libtool \\\n      docutils \\\n      sphinx-doc\n  ```\n\n- Add sphinx to PATH as advised by the installer:\n\n  ``` python\n  PATH=\"/usr/local/opt/sphinx-doc/bin:$PATH\"\n  ```\n\nThen continue [Compiling Varnish](#compiling-varnish)\n\n## Build dependencies on Alpine Linux\n\nAs of Alpine 3, these steps should install the required dependencies:\n\n- Add the [Alpine Community Repository](https://wiki.alpinelinux.org/wiki/Enable_Community_Repository)\n\n- Install dependencies:\n\n  ``` python\n  apk add -q \\\n      autoconf \\\n      automake \\\n      build-base \\\n      ca-certificates \\\n      cpio \\\n      gzip \\\n      libedit-dev \\\n      libtool \\\n      libunwind-dev \\\n      linux-headers \\\n      pcre2-dev \\\n      py-docutils \\\n      py3-sphinx \\\n      tar \\\n      sudo\n  ```\n\nOptionally, to rebuild the svg files:\n\n``` python\napk add -q graphviz\n```\n\nOptionally, to pull from a repository:\n\n``` python\napk add -q git\n```\n\nThen continue [Compiling Varnish](#compiling-varnish), using the `--with-unwind` `configure` option.\n\n## Build dependencies on a SmartOS Zone\n\nAs of SmartOS pkgsrc 2019Q4, install the following packages:\n\n``` python\npkgin in autoconf automake editline libtool ncurses \\\n         pcre2 python37 py37-sphinx py37-docutils gmake gcc8 pkg-config\n```\n\n*Note:* you will probably need to add `/opt/local/gcc8/bin` to `PATH` in order to have `gcc` available.\n\nOptionally, to rebuild the svg files:\n\n``` python\npkgin in graphviz\n```\n\nOptionally, to pull from a repository:\n\n``` python\npkgin in git\n```\n\n## Building on Solaris and other Solaris-ish OSes\n\nBuilding with gcc should be straight forward, as long as the above requirements are installed.\n\nBy convention, consider installing Varnish under `/opt/local` using:\n\n``` python\n./configure \\\n        --prefix=/opt/local \\\n        --mandir=/opt/local/man\n```\n\nAlternatively, building with Solaris Studio 12.4 should work considering the following recommendations:\n\n- have GNU `nm` in `$PATH` before Solaris `nm`\n\n- Provide compiler flags for `configure` to include paths under which dependencies are installed. Example for `/opt/local`:\n\n  ``` python\n  ./configure \\\n          --prefix=/opt/local \\\n          --mandir=/opt/local/man \\\n          CPPFLAGS=\"-I/opt/local/include\" \\\n          CFLAGS=\"-m64\" \\\n          LDFLAGS=\"-L/opt/local/lib -R/opt/local/lib\"\n  ```\n\n## Compiling Varnish\n\nThe configuration will need the dependencies above satisfied. Once that is taken care of:\n\n``` python\ncd varnish-cache\nsh autogen.sh\nsh configure\nmake\n```\n\nThe `configure` script takes some arguments, but more likely than not you can forget about that for now, almost everything in Varnish can be tweaked with run time parameters.\n\nBefore you install, you may want to run the test suite, make a cup of tea while it runs, it usually takes a couple of minutes:\n\n``` python\nmake check\n```\n\nDon’t worry if one or two tests fail. Some of the tests are a bit too timing sensitive (Please tell us which so we can fix them). However, if a lot of them fail, and in particular if the `b00000.vtc` test fails, something is horribly wrong. You will get nowhere without figuring this one out.\n\n## Installing\n\nAnd finally, the true test of a brave heart: `sudo make install`\n\nVarnish will now be installed in `/usr/local`. The `varnishd` binary is in `/usr/local/sbin/varnishd`. To make sure that the necessary links and caches of the most recent shared libraries are found, run `sudo ldconfig`.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/installation/install_source.html](https://varnish-cache.org/docs/7.4/installation/install_source.html)"
- name: Compression
  id: users-guide/compression
  summary: In Varnish 3.0 we introduced native support for compression, using gzip encoding
  description: "# Compression\n\nIn Varnish 3.0 we introduced native support for compression, using gzip encoding. *Before* 3.0, Varnish would never compress objects.\n\nIn Varnish 4.0 compression defaults to “on”, meaning that it tries to be smart and do the sensible thing.\n\nIf you don’t want Varnish tampering with the encoding you can disable compression all together by setting the parameter `http_gzip_support` to false. Please see man [varnishd](../reference/varnishd#varnishd-1) for details.\n\n## Default behaviour\n\nThe default behaviour is active when the `http_gzip_support` parameter is set to “on” and neither `beresp.do_gzip` nor `beresp.do_gunzip` are used in VCL.\n\nUnless returning from `vcl_recv` with `pipe` or `pass`, Varnish modifies `req.http.Accept-Encoding`: if the client supports gzip `req.http.Accept-Encoding` is set to “gzip”, otherwise the header is removed.\n\nUnless the request is a `pass`, Varnish sets `bereq.http.Accept-Encoding` to “gzip” before `vcl_backend_fetch` runs, so the header can be changed in VCL.\n\nIf the server responds with gzip’ed content it will be stored in memory in its compressed form and `Accept-Encoding` will be added to the `Vary` header.\n\nTo clients supporting gzip, compressed content is delivered unmodified.\n\nFor clients not supporting gzip, compressed content gets decompressed on the fly while delivering it. The `Content-Encoding` response header gets removed and any `Etag` gets weakened (by prepending “W/”).\n\nFor Vary Lookups, `Accept-Encoding` is ignored.\n\n## Compressing content if backends don’t\n\nYou can tell Varnish to compress content before storing it in cache in `vcl_backend_response` by setting `beresp.do_gzip` to “true”, like this:\n\n``` python\nsub vcl_backend_response {\n    if (beresp.http.content-type ~ \"text\") {\n        set beresp.do_gzip = true;\n    }\n}\n```\n\nWith `beresp.do_gzip` set to “true”, Varnish will make the following changes to the headers of the resulting object before inserting it in the cache:\n\n- set `obj.http.Content-Encoding` to “gzip”\n- add “Accept-Encoding” to `obj.http.Vary`, unless already present\n- weaken any `Etag` (by prepending “W/”)\n\nGenerally, Varnish doesn’t use much CPU so it might make more sense to have Varnish spend CPU cycles compressing content than doing it in your web- or application servers, which are more likely to be CPU-bound.\n\nPlease make sure that you don’t try to compress content that is uncompressable, like JPG, GIF and MP3 files. You’ll only waste CPU cycles.\n\n## Uncompressing content before entering the cache\n\nYou can also uncompress content before storing it in cache by setting `beresp.do_gunzip` to “true”. One use case for this feature is to work around badly configured backends uselessly compressing already compressed content like JPG images (but fixing the misbehaving backend is always the better option).\n\nWith `beresp.do_gunzip` set to “true”, Varnish will make the following changes to the headers of the resulting object before inserting it in the cache:\n\n- remove `obj.http.Content-Encoding`\n- weaken any `Etag` (by prepending “W/”)\n\n## GZIP and ESI\n\nIf you are using Edge Side Includes (ESI) you’ll be happy to note that ESI and GZIP work together really well. Varnish will magically decompress the content to do the ESI-processing, then recompress it for efficient storage and delivery.\n\n## Turning off gzip support\n\nWhen the `http_gzip_support` parameter is set to “off”, Varnish does not do any of the header alterations documented above, handles `Vary: Accept-Encoding` like it would for any other `Vary` value and ignores `beresp.do_gzip` and `beresp.do_gunzip`.\n\n## A random outburst\n\nPoul-Henning Kamp has written [How GZIP, and GZIP+ESI works in Varnish](https://varnish-cache.org/docs/7.4/phk/gzip.html#phk-gzip) which talks a bit more about how the implementation works.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/compression.html](https://varnish-cache.org/docs/7.4/users-guide/compression.html)"
- name: Content composition with Edge Side Includes
  id: users-guide/esi
  summary: Varnish can create web pages by assembling different pages, called fragments, together into one page
  description: "# Content composition with Edge Side Includes\n\nVarnish can create web pages by assembling different pages, called `fragments`, together into one page. These `fragments` can have individual cache policies. If you have a web site with a list showing the five most popular articles on your site, this list can probably be cached as a `fragment` and included in all the other pages.\n\nUsed properly this strategy can dramatically increase your hit rate and reduce the load on your servers.\n\nIn Varnish we’ve only implemented a small subset of ESI, because most of the rest of the ESI specifications facilities are easier and better done with VCL:\n\n``` python\nesi:include\nesi:remove\n<!--esi ...-->\n```\n\nContent substitution based on variables and cookies is not implemented.\n\nVarnish will not process ESI instructions in HTML comments.\n\n## Example: esi:include\n\nLets see an example how this could be used. This simple cgi script outputs the date:\n\n``` python\n#!/bin/sh\n\necho 'Content-type: text/html'\necho ''\ndate \"+%Y-%m-%d %H:%M\"\n```\n\nNow, lets have an HTML file that has an ESI include statement:\n\n``` python\n<HTML>\n<BODY>\nThe time is: <esi:include src=\"/cgi-bin/date.cgi\"/>\nat this very moment.\n</BODY>\n</HTML>\n```\n\nFor ESI to work you need to activate ESI processing in VCL, like this:\n\n``` python\nsub vcl_backend_response {\n    if (bereq.url == \"/test.html\") {\n       set beresp.do_esi = true; // Do ESI processing\n       set beresp.ttl = 24 h;    // Sets the TTL on the HTML above\n    } elseif (bereq.url == \"/cgi-bin/date.cgi\") {\n       set beresp.ttl = 1m;      // Sets a one minute TTL on\n                                 // the included object\n    }\n}\n```\n\nNote that `set beresp.do_esi = true;` is not required, and should be avoided, for the included fragments, unless they also contains `<ESI::include …/>` instructions.\n\n## Example: esi:remove and \\<!–esi … –\\>\n\nThe `<esi:remove>` and `<!–esi … –>` constructs can be used to present appropriate content whether or not ESI is available, for example you can include content when ESI is available or link to it when it is not. ESI processors will remove the start (“\\<!–esi”) and the end (”–\\>”) when the page is processed, while still processing the contents. If the page is not processed, it will remain intact, becoming a HTML/XML comment tag. ESI processors will remove `<esi:remove>` tags and all content contained in them, allowing you to only render the content when the page is not being ESI-processed. For example:\n\n``` python\n<esi:remove>\n  <a href=\"http://www.example.com/LICENSE\">The license</a>\n</esi:remove>\n<!--esi\n<p>The full text of the license:</p>\n<esi:include src=\"http://example.com/LICENSE\" />\n-->\n```\n\n## What happens when it fails ?\n\nBy default, the fragments must have `resp.status` 200 or 206 or their inclusion will cause the parent request to abort.\n\nLikewise, if the fragment is a streaming fetch, and that fetch fails, the parent request aborts.\n\nIf you include synthetic fragments, that is fragments created in `vcl_backend_error{}` or `vcl_synth{}`, you must set `(be)resp.status` to 200 before `return(deliver);`\n\nWe say “abort” rather than “fail”, because by the time Varnish starts inserting the fragments, the HTTP response header has long since been sent, and it is no longer possible to change the parent requests’s `resp.status` to a 5xx, so the only way to signal that something is amiss, is to close the connection.\n\nHowever, it is possible to allow individual `<ESI:include…` to continue in case of failures, by setting:\n\n``` python\nparam.set feature +esi_include_onerror\n```\n\nand tagging those specific includes:\n\n``` python\n<ESI:include src=\"…\" onerror=\"continue\"/>\n```\n\n## Can an ESI fragment also use ESI-includes ?\n\nYes, but the depth is limited by the `max_esi_depth` parameter in order to prevent infinite recursion.\n\n## Doing ESI on JSON and other non-XML’ish content\n\nVarnish will peek at the first byte of an object and if it is not a “\\<” Varnish assumes you didn’t really mean to ESI process it. You can disable this check by:\n\n``` python\nparam.set feature +esi_disable_xml_check\n```\n\n## Ignoring BOM in ESI objects\n\nIf you backend spits out a Unicode Byte-Order-Mark as the first bytes of the response, the “\\<” check will fail unless you set:\n\n``` python\nparam.set feature +esi_remove_bom\n```\n\n## ESI on invalid XML\n\nThe ESI parser expects the XML to be reasonably well formed, but this may fail if you are ESI including non-XML files. You can make the ESI parser disregard anything but ESI tags by setting:\n\n``` python\nparam.set feature +esi_ignore_other_elements\n```\n\n## ESI includes with HTTPS protocol\n\nIf ESI:include tags specify HTTPS protocol, it will be ignored by default, because Varnish has no way to fetch it with encryption. If you want Varnish to fetch them like it does anything else, set:\n\n``` python\nparam.set feature +esi_ignore_https\n```\n\n## ESI on partial responses (206)\n\nVarnish supports range requests, but in general partial responses make no sense in an ESI context.\n\nIf you really know what you are doing, change the 206 to a 200:\n\n``` python\nsub vcl_backend_response {\n    if (beresp.status == 206 && beresp.http.secret == \"swordfish\") {\n        set beresp.do_esi = True;\n        set beresp.status = 200;\n    }\n}\n```\n\n## ESI and return(vcl(…))\n\nIf the original client request switched to a different VCL using `return(vcl(...))` in `vcl_recv`, any esi:include-requests will still start out in the same VCL as the original did, *not* in the one it switched to.\n\n## ESI and gzip compression\n\nVarnish’s ESI implementation handles gzip compression automatically, no matter how it is mixed: The parent request can be compressed or uncompressed and the fragments can be compressed or uncompressed, it all works out.\n\nVarnish does this compressing all parts of ESI responses separately, and stitching them together on the fly during delivery, which has a negative impact on compression ratio.\n\nWhen you `set beresp.do_esi = True;` on a gzip’ed response, it will be uncompressed and recompressed part-wise during the fetch.\n\nThe part-wise compression reduces the opportunities for removing redundancy, because back-references in the gzip data stream cannot point outside it’s own part.\n\nThe other case where compression ratio is impacted, is if an uncompressed fragment is inserted into a compressed response.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/esi.html](https://varnish-cache.org/docs/7.4/users-guide/esi.html)"
- name: Device detection
  id: users-guide/devicedetection
  summary: Device detection is figuring out what kind of content to serve to a client based on the User-Agent string supplied in a request
  description: "# Device detection\n\nDevice detection is figuring out what kind of content to serve to a client based on the User-Agent string supplied in a request.\n\nUse cases for this are for example to send size reduced files to mobile clients with small screens and on high latency networks, or to provide a streaming video codec that the client understands.\n\nThere are a couple of typical strategies to use for this type of scenario: 1) Redirect to another URL. 2) Use a different backend for the special client. 3) Change the backend request so that the backend sends tailored content.\n\nTo perhaps make the strategies easier to understand, we, in this context, assume that the `req.http.X-UA-Device` header is present and unique per client class.\n\nSetting this header can be as simple as:\n\n``` python\nsub vcl_recv {\n    if (req.http.User-Agent ~ \"(?i)iphone\" {\n        set req.http.X-UA-Device = \"mobile-iphone\";\n    }\n}\n```\n\nThere are different commercial and free offerings in doing grouping and identifying clients in further detail. For a basic and community based regular expression set, see [https://github.com/varnishcache/varnish-devicedetect/](https://github.com/varnishcache/varnish-devicedetect/).\n\n## Serve the different content on the same URL\n\nThe tricks involved are: 1. Detect the client (pretty simple, just include `devicedetect.vcl` and call it). 2. Figure out how to signal the backend the client class. This includes for example setting a header, changing a header or even changing the backend request URL. 3. Modify any response from the backend to add missing ‘Vary’ headers, so Varnish’ internal handling of this kicks in. 4. Modify output sent to the client so any caches outside our control don’t serve the wrong content.\n\nAll this needs to be done while still making sure that we only get one cached object per URL per device class.\n\n### Example 1: Send HTTP header to backend\n\nThe basic case is that Varnish adds the ‘X-UA-Device’ HTTP header on the backend requests, and the backend mentions in the response ‘Vary’ header that the content is dependent on this header.\n\nEverything works out of the box from Varnish’ perspective.\n\nVCL:\n\n``` python\nsub vcl_recv {\n    # call some detection engine that set req.http.X-UA-Device\n}\n# req.http.X-UA-Device is copied by Varnish into bereq.http.X-UA-Device\n\n# so, this is a bit counterintuitive. The backend creates content based on\n# the normalized User-Agent, but we use Vary on X-UA-Device so Varnish will\n# use the same cached object for all U-As that map to the same X-UA-Device.\n#\n# If the backend does not mention in Vary that it has crafted special\n# content based on the User-Agent (==X-UA-Device), add it.\n# If your backend does set Vary: User-Agent, you may have to remove that here.\nsub vcl_backend_response {\n    if (bereq.http.X-UA-Device) {\n        if (!beresp.http.Vary) { # no Vary at all\n            set beresp.http.Vary = \"X-UA-Device\";\n        } elseif (beresp.http.Vary !~ \"X-UA-Device\") { # add to existing Vary\n            set beresp.http.Vary = beresp.http.Vary + \", X-UA-Device\";\n        }\n    }\n    # comment this out if you don't want the client to know your\n    # classification\n    set beresp.http.X-UA-Device = bereq.http.X-UA-Device;\n}\n\n# to keep any caches in the wild from serving wrong content to client #2\n# behind them, we need to transform the Vary on the way out.\nsub vcl_deliver {\n    if ((req.http.X-UA-Device) && (resp.http.Vary)) {\n        set resp.http.Vary = regsub(resp.http.Vary, \"X-UA-Device\", \"User-Agent\");\n    }\n}\n```\n\n### Example 2: Normalize the User-Agent string\n\nAnother way of signalling the device type is to override or normalize the ‘User-Agent’ header sent to the backend.\n\nFor example:\n\n``` python\nUser-Agent: Mozilla/5.0 (Linux; U; Android 2.2; nb-no; HTC Desire Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\n```\n\nbecomes:\n\n``` python\nUser-Agent: mobile-android\n```\n\nwhen seen by the backend.\n\nThis works if you don’t need the original header for anything on the backend. A possible use for this is for CGI scripts where only a small set of predefined headers are (by default) available for the script.\n\nVCL:\n\n``` python\nsub vcl_recv {\n    # call some detection engine that set req.http.X-UA-Device\n}\n\n# override the header before it is sent to the backend\nsub vcl_miss { if (req.http.X-UA-Device) { set req.http.User-Agent = req.http.X-UA-Device; } }\nsub vcl_pass { if (req.http.X-UA-Device) { set req.http.User-Agent = req.http.X-UA-Device; } }\n\n# standard Vary handling code from previous examples.\nsub vcl_backend_response {\n    if (bereq.http.X-UA-Device) {\n        if (!beresp.http.Vary) { # no Vary at all\n            set beresp.http.Vary = \"X-UA-Device\";\n        } elseif (beresp.http.Vary !~ \"X-UA-Device\") { # add to existing Vary\n            set beresp.http.Vary = beresp.http.Vary + \", X-UA-Device\";\n        }\n    }\n    set beresp.http.X-UA-Device = bereq.http.X-UA-Device;\n}\nsub vcl_deliver {\n    if ((req.http.X-UA-Device) && (resp.http.Vary)) {\n        set resp.http.Vary = regsub(resp.http.Vary, \"X-UA-Device\", \"User-Agent\");\n    }\n}\n```\n\n### Example 3: Add the device class as a GET query parameter\n\nIf everything else fails, you can add the device type as a GET argument.\n\n[http://example.com/article/1234.html](http://example.com/article/1234.html) –\\> [http://example.com/article/1234.html?devicetype=mobile-iphone](http://example.com/article/1234.html?devicetype=mobile-iphone)\n\nThe client itself does not see this classification, only the backend request is changed.\n\nVCL:\n\n``` python\nsub vcl_recv {\n    # call some detection engine that set req.http.X-UA-Device\n}\n\nsub append_ua {\n    if ((req.http.X-UA-Device) && (req.method == \"GET\")) {\n        # if there are existing GET arguments;\n        if (req.url ~ \"\\?\") {\n            set req.http.X-get-devicetype = \"&devicetype=\" + req.http.X-UA-Device;\n        } else {\n            set req.http.X-get-devicetype = \"?devicetype=\" + req.http.X-UA-Device;\n        }\n        set req.url = req.url + req.http.X-get-devicetype;\n        unset req.http.X-get-devicetype;\n    }\n}\n\n# do this after vcl_hash, so all Vary-ants can be purged in one go. (avoid ban()ing)\nsub vcl_miss { call append_ua; }\nsub vcl_pass { call append_ua; }\n\n# Handle redirects, otherwise standard Vary handling code from previous\n# examples.\nsub vcl_backend_response {\n    if (bereq.http.X-UA-Device) {\n        if (!beresp.http.Vary) { # no Vary at all\n            set beresp.http.Vary = \"X-UA-Device\";\n        } elseif (beresp.http.Vary !~ \"X-UA-Device\") { # add to existing Vary\n            set beresp.http.Vary = beresp.http.Vary + \", X-UA-Device\";\n        }\n\n        # if the backend returns a redirect (think missing trailing slash),\n        # we will potentially show the extra address to the client. we\n        # don't want that.  if the backend reorders the get parameters, you\n        # may need to be smarter here. (? and & ordering)\n\n        if (beresp.status == 301 || beresp.status == 302 || beresp.status == 303) {\n            set beresp.http.location = regsub(beresp.http.location, \"[?&]devicetype=.*$\", \"\");\n        }\n    }\n    set beresp.http.X-UA-Device = bereq.http.X-UA-Device;\n}\nsub vcl_deliver {\n    if ((req.http.X-UA-Device) && (resp.http.Vary)) {\n        set resp.http.Vary = regsub(resp.http.Vary, \"X-UA-Device\", \"User-Agent\");\n    }\n}\n```\n\n## Different backend for mobile clients\n\nIf you have a different backend that serves pages for mobile clients, or any special needs in VCL, you can use the ‘X-UA-Device’ header like this:\n\n``` python\nbackend mobile {\n    .host = \"10.0.0.1\";\n    .port = \"80\";\n}\n\nsub vcl_recv {\n    # call some detection engine\n\n    if (req.http.X-UA-Device ~ \"^mobile\" || req.http.X-UA-device ~ \"^tablet\") {\n        set req.backend_hint = mobile;\n    }\n}\nsub vcl_hash {\n    if (req.http.X-UA-Device) {\n        hash_data(req.http.X-UA-Device);\n    }\n}\n```\n\n## Redirecting mobile clients\n\nIf you want to redirect mobile clients you can use the following snippet.\n\nVCL:\n\n``` python\nsub vcl_recv {\n    # call some detection engine\n\n    if (req.http.X-UA-Device ~ \"^mobile\" || req.http.X-UA-device ~ \"^tablet\") {\n        return(synth(750, \"Moved Temporarily\"));\n    }\n}\n\nsub vcl_synth {\n    if (obj.status == 750) {\n        set obj.http.Location = \"http://m.example.com\" + req.url;\n        set obj.status = 302;\n        return(deliver);\n    }\n}\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/devicedetection.html](https://varnish-cache.org/docs/7.4/users-guide/devicedetection.html)"
- name: Getting help
  id: installation/help
  summary: Getting hold of the gang behind Varnish is pretty straight forward, we try to help out as much as time permits and have tried to streamline this process as much as possible
  description: "# Getting help\n\nGetting hold of the gang behind Varnish is pretty straight forward, we try to help out as much as time permits and have tried to streamline this process as much as possible.\n\nBut before you grab hold of us, spend a moment composing your thoughts and formulate your question. From our perspective there is nothing as pointless as simply telling us “Varnish does not work for me” with no further information. This does not give us any relevant information to use when trying to figure out whats wrong.\n\nAnd before you even do that, do a couple of searches to see if your question is already answered, if it has been, you will get your answer much faster that way.\n\n## IRC Channel\n\nThe most immediate way to get hold of us is to join our IRC channel:\n\n`#varnish on server irc.linpro.no`\n\nThe main timezone of the channel is Europe work hours.\n\nIf you can explain your problem in a few clear sentences, without too much copy&paste, IRC is a good way to try to get help. If you do need to paste log files, VCL and so on, please use a [pastebin](https://gist.github.com/) service.\n\nIf the channel is all quiet, try again some time later, we do have lives, families and jobs to deal with also.\n\nYou are more than welcome to just hang out, and while we don’t mind the occasional intrusion from the real world into our flow, we try and keep it mostly on topic, and please don’t paste random links unless they are *really* funny, spectacular and intelligent.\n\n## Mailing Lists\n\nSubscribing or unsubscribing to our mailing lists is handled through [mailman](https://www.varnish-cache.org/lists/mailman/listinfo).\n\nIf you are going to use Varnish, subscribing to our `varnish-announce` mailing list is a very good idea. The typical pattern is that people spend some time getting Varnish running, and then more or less forget about it. Therefore the announce list is a good way to be reminded about new releases, bugs or potential (security) vulnerabilities.\n\nThe `varnish-misc` mailing list is for general banter, questions, suggestions, ideas and so on. If you are new to Varnish it may pay off to subscribe to it, simply to have an ear to the telegraph-pole and potentially learn some smart tricks. This is also a good place to ask for help with more complex issues, that may require file-chunks, references to files and/or long explanations.\n\nMake sure to pick a good subject line, and if the subject of the thread changes, please change the subject to match, some of us deal with hundreds of emails per day, after spam-filters, and we need all the help we can get to pick the interesting ones.\n\nThe `varnish-dev` mailing list is used by the developers and is usually quite focused on source-code and such. Everybody on the `-dev` list is also on `-misc`, so cross-posting only serves to annoy those people.\n\n## Trouble Tickets\n\nOur bugtracker lives on Github, but please do not open a trouble ticket, unless you have spotted an actual bug in Varnish. Ask on IRC first if you are in doubt.\n\nThe reason for this policy, is to avoid bugs being drowned in a pile of other `issues`, feature suggestions for future releases, and double postings of calls for help from people who forgot to check back on already opened Tickets.\n\nNew ideas may get parked in our Github wiki, until we have time for them, or until we have thought out a good design.\n\n## Commercial Support\n\nIf you need commercial support, there are companies which offer that and you can find a [list on our homepage.](http://varnish-cache.org/business/).\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/installation/help.html](https://varnish-cache.org/docs/7.4/installation/help.html)"
- name: Grace mode and keep
  id: users-guide/vcl-grace
  summary: Sometimes you want Varnish to serve content that is somewhat stale instead of waiting for a fresh object from the backend
  description: "# Grace mode and keep\n\nSometimes you want Varnish to serve content that is somewhat stale instead of waiting for a fresh object from the backend. For example, if you run a news site, serving a main page that is a few seconds old is not a problem if this gives your site faster load times.\n\nIn Varnish this is achieved by using `grace mode`. A related idea is `keep`, which is also explained here.\n\n## Grace mode\n\nWhen several clients are requesting the same page Varnish will send one request to the backend and place the others on hold while fetching one copy from the backend. In some products this is called request coalescing and Varnish does this automatically.\n\nIf you are serving thousands of hits per second the queue of waiting requests can get huge. There are two potential problems - one is a thundering herd problem - suddenly releasing a thousand threads to serve content might send the load sky high. Secondly - nobody likes to wait.\n\nSetting an object’s `grace` to a positive value tells Varnish that it should serve the object to clients for some time after the TTL has expired, while Varnish fetches a new version of the object. The default value is controlled by the runtime parameter `default_grace`.\n\n## Keep\n\nSetting an object’s `keep` tells Varnish that it should keep an object in the cache for some additional time. The reasons to set `keep` is to use the object to construct a conditional GET backend request (with If-Modified-Since: and/or Ìf-None-Match: headers), allowing the backend to reply with a 304 Not Modified response, which may be more efficient on the backend and saves re-transmitting the unchanged body.\n\nThe values are additive, so if grace is 10 seconds and keep is 1 minute, then objects will survive in cache for 70 seconds after the TTL has expired.\n\n## Setting grace and keep\n\nWe can use VCL to make Varnish keep all objects for 10 minutes beyond their TTL with a grace period of 2 minutes:\n\n``` python\nsub vcl_backend_response {\n     set beresp.grace = 2m;\n     set beresp.keep = 8m;\n}\n```\n\n## The effect of grace and keep\n\nFor most users setting the default grace and/or a suitable grace for each object is enough. The default VCL will do the right thing and behave as described above. However, if you want to customize how Varnish behaves, then you should know some of the details on how this works.\n\nWhen `sub vcl_recv` ends with `return (lookup)` (which is the default behavior), Varnish will look for a matching object in its cache. Then, if it only found an object whose TTL has run out, Varnish will consider the following:\n\n- Is there already an ongoing backend request for the object?\n- Is the object within the `grace period`?\n\nThen, Varnish reacts using the following rules:\n\n- If the `grace period` has run out and there is no ongoing backend request, then `sub vcl_miss` is called immediately, and the object will be used as a 304 candidate.\n- If the `grace period` has run out and there is an ongoing backend request, then the request will wait until the backend request finishes.\n- If there is no backend request for the object, one is scheduled.\n- Assuming the object will be delivered, `sub vcl_hit` is called immediately.\n\nNote that the backend fetch happens asynchronously, and the moment the new object is in it will replace the one we’ve already got.\n\nIf you do not define your own `sub vcl_hit`, then the default one is used. It looks like this:\n\n``` python\nsub vcl_hit {\n     return (deliver);\n}\n```\n\nNote that the condition `obj.ttl + obj.grace > 0s` will (in `sub vcl_hit`) always evaluate to true. In earlier versions (6.0.0 and earlier), this was not the case, and a test in the builtin VCL was necessary to make sure that “keep objects” (objects in the cache where both TTL and grace had run out) would not be delivered to the clients.\n\nIn the current version, when there are only “keep objects” available, `sub vcl_miss` will be called, and a fetch for a new object will be initiated.\n\n## Misbehaving servers\n\nA key feature of Varnish is its ability to shield you from misbehaving web- and application servers.\n\nIf you have enabled [Health checks](vcl-backends#users-guide-advanced-backend-servers-health) you can check if the backend is sick and modify the behavior when it comes to grace. This can done in the following way:\n\n``` python\nsub vcl_backend_response {\n     set beresp.grace = 24h;\n     // no keep - the grace should be enough for 304 candidates\n}\n\nsub vcl_recv {\n     if (std.healthy(req.backend_hint)) {\n          // change the behavior for healthy backends: Cap grace to 10s\n          set req.grace = 10s;\n     }\n}\n```\n\nIn the example above, the special variable `req.grace` is set. The effect is that, when the backend is healthy, objects with grace above 10 seconds will have an `effective` grace of 10 seconds. When the backend is sick, the default VCL kicks in, and the long grace is used.\n\nAdditionally, you might want to stop cache insertion when a backend fetch returns an `5xx` error:\n\n``` python\nsub vcl_backend_response {\n     if (beresp.status >= 500 && bereq.is_bgfetch) {\n          return (abandon);\n     }\n}\n```\n\n## Summary\n\nGrace mode allows Varnish to deliver slightly stale content to clients while getting a fresh version from the backend. The result is faster load times at lower cost.\n\nIt is possible to limit the grace during lookup by setting `req.grace` and then change the behavior when it comes to grace. Often this is done to change the `effective` grace depending on the health of the backend.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-grace.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-grace.html)"
- name: Hashing
  id: users-guide/vcl-hashing
  summary: Internally, when Varnish stores content in the cache indexed by a hash key used to find the object again
  description: "# Hashing\n\nInternally, when Varnish stores content in the cache indexed by a hash key used to find the object again. In the default setup this key is calculated based on `URL`, the `Host:` header, or if there is none, the IP address of the server:\n\n``` python\nsub vcl_hash {\n    hash_data(req.url);\n    if (req.http.host) {\n        hash_data(req.http.host);\n    } else {\n        hash_data(server.ip);\n    }\n    return (lookup);\n}\n```\n\nAs you can see it first hashes `req.url` and then `req.http.host` if it exists. It is worth pointing out that Varnish doesn’t lowercase the hostname or the URL before hashing it so in theory having “Varnish.org/” and “varnish.org/” would result in different cache entries. Browsers however, tend to lowercase hostnames.\n\nYou can change what goes into the hash. This way you can make Varnish serve up different content to different clients based on arbitrary criteria.\n\nLet’s say you want to serve pages in different languages to your users based on where their IP address is located. You would need some Vmod to get a country code and then put it into the hash. It might look like this.\n\nIn `vcl_recv`:\n\n``` python\nset req.http.X-Country-Code = geoip.lookup(client.ip);\n```\n\nAnd then add a `vcl_hash`:\n\n``` python\nsub vcl_hash {\n    hash_data(req.http.X-Country-Code);\n}\n```\n\nBecause there is no `return(lookup)`, the builtin VCL will take care of adding the URL, `Host:` or server IP# to the hash as usual.\n\nIf `vcl_hash` did return, ie:\n\n``` python\nsub vcl_hash {\n    hash_data(req.http.X-Country-Code);\n    return(lookup);\n}\n```\n\nthen *only* the country-code would matter, and Varnish would return seemingly random objects, ignoring the URL, (but they would always have the correct `X-Country-Code`).\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-hashing.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-hashing.html)"
- name: How our website works
  id: dev-guide/homepage_dogfood
  summary: The principle of eating your own dogfood is important for software quality, that is how you experience what your users are dealing with, and I am not the least ashamed to admit that several obvious improvements have happened to Varnish as a result of running the project webserver
  description: "# How our website works\n\nThe principle of eating your own dogfood is important for software quality, that is how you experience what your users are dealing with, and I am not the least ashamed to admit that several obvious improvements have happened to Varnish as a result of running the project webserver.\n\nBut it is also important to externalize what you learn doing so, and therefore I thought I would document here how the projects new “internal IT” works.\n\n## Hardware\n\nWho cares?\n\nYes, we use some kind of hardware, but to be honest I don’t know what it is.\n\nOur primary site runs on a [RootBSD ‘Omega’](https://www.rootbsd.net/) virtual server somewhere near CDG/Paris.\n\nAnd as backup/integration/testing server we can use any server, virtual or physical, as long as it has a internet connection and contemporary performance, because the entire install is scripted and under version control (more below).\n\n## Operating System\n\nSo, dogfood: Obviously FreeBSD.\n\nApart from the obvious reason that I wrote a lot of FreeBSD and can get world-class support by bugging my buddies about it, there are two equally serious reasons for the Varnish Project to run on FreeBSD: Dogfood and jails.\n\nVarnish Cache is not “software for Linux”, it is software for any competent UNIX-like operating system, and FreeBSD is our primary “keep us honest about this” platform.\n\n## Jails\n\nYou have probably heard about Docker and Containers, but FreeBSD have had jails [since I wrote them in 1998](http://phk.freebsd.dk/sagas/jails/) and they’re a wonderful way to keep your server installation sane.\n\nWe currently have three jails:\n\n- Hitch - runs the [Hitch SSL proxy](https://hitch-tls.org/)\n- Varnish - \\<a href=”rimshot.mp3”\\>You guessed it\\</a\\>\n- Tools - backend webserver, currently [ACME Labs’ thttpd](http://acme.com/software/thttpd/)\n\n## Script & Version Control All The Things\n\nWe have a git repos with shell scripts which create these jails from scratch and also a script to configure the host machine properly.\n\nThat means that the procedure to install a clone of the server is, unabridged:\n\n``` python\n# Install FreeBSD (if not already done by hosting)\n# Configure networking (if not already done by hosting)\n# Set the clock\nservice ntpdate forcestart\n# Get git\nenv ASSUME_ALWAYS_YES=yes pkg install git\n# Clone the private git repo\ngit clone ssh://example.com/root/Admin\n# Edit the machines IP numbers in /etc/pf.conf\n# Configure the host\nsh build_host.sh |& tee _.bh\n# Build the jails\nforeach i (Tools Hitch Varnish)\n        (cd $i ; sh build* |& tee _.bj)\nend\n```\n\nFrom bare hardware to ready system in 15-30 minutes.\n\nIt goes without saying that this git repos contains stuff like ssh host keys, so it should *not* go on github.\n\n## Backups\n\nRight now there is nothing we absolutely have to backup, provided we have an up to date copy of the Admin git repos.\n\nIn practice we want to retain history for our development tools (VTEST, GCOV etc.) and I rsync those file of the server on a regular basis.\n\n## The Homepage\n\nThe homepage is built with [Sphinx](http://www.sphinx-doc.org/) and lives in its own [github project](https://github.com/varnishcache/homepage) (Pull requests are very welcome!)\n\nWe have taken snapshots of some of the old webproperties, Trac, the Forum etc as static HTML copies.\n\n## Why on Earth…\n\nIt is a little bit tedious to get a setup like this going, whenever you tweak some config file, you need to remember to pull the change back out and put it in your Admin repos.\n\nBut that extra effort pays of so many times later.\n\nYou never have to wonder “who made that change and why” or even try to remember what changes were needed in the first place.\n\nFor us as a project, it means, that all our sysadmin people can build a clone of our infrastructure, if they have a copy of our “Admin” git repos and access to github.\n\nAnd when [FreeBSD 11](https://www.youtube.com/watch?v=KOO5S4vxi0o) comes out, or a new version of sphinx or something else, mucking about with things until they work can be done at leisure without guess work. (We’re actually at 12 now, but the joke is too good to delete.)\n\nFor instance I just added the forum snapshot, by working out all the kinks on one of my test-machines.\n\nOnce it was as I wanted it, I pushed the changes the live machine and then:\n\n``` python\nvarnishadm vcl.use backup\n# The 'backup' VCL does a \"pass\" of all traffic to my server\ncd Admin\ngit pull\ncd Tools\nsh build_j_tools.sh |& tee _.bj\nvarnishadm vcl.load foobar varnish-live.vcl\nvarnishadm vcl.use foobar\n```\n\nFor a few minutes our website was a bit slower (because of the extra Paris-Denmark hop), but there was never any interruption.\n\nAnd by doing it this way, I *know* it will work next time also.\n\n2016-04-25 /phk\n\nPS: All that buzz about “reproducible builds” ? Yeah, not a new idea.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/dev-guide/homepage_dogfood.html](https://varnish-cache.org/docs/7.4/dev-guide/homepage_dogfood.html)"
- name: How to contribute content to varnish-cache.org
  id: dev-guide/homepage_contrib
  summary: This is where we walk you through the mechanics of adding content to varnish-cache.org (see phk’s note How our website works for an insight into the innards of site)
  description: "# How to contribute content to varnish-cache.org\n\nThis is where we walk you through the mechanics of adding content to varnish-cache.org (see phk’s note [How our website works](homepage_dogfood#homepage-dogfood) for an insight into the innards of site).\n\n## Git Repository\n\nThe web site contents live in github at:\n\n[https://github.com/varnishcache/homepage](https://github.com/varnishcache/homepage)\n\nTo offer your own contribution, fork the project and send us a pull request.\n\n## Sphinx and RST\n\nThe web site sources are written in [RST](http://docutils.sourceforge.net/rst.html) – reStructuredText, the documentation format originally conceived for Python (and also used in the Varnish distribution, as well as for formatting VMOD docs). [Sphinx](http://www.sphinx-doc.org/) is used to render web pages from the RST sources.\n\nSo you’ll need to [learn markup with RST and Sphinx](http://www.sphinx-doc.org/en/stable/markup/index.html); and you will need to [install Sphinx](http://www.sphinx-doc.org/en/stable/install.html) to test the rendering on your local system.\n\n## Makefile\n\nGeneration of web contents from the sources is driven by the `Makefile` in the `R1` directory of the repo:\n\n``` python\n$ cd R1\n$ make help\nPlease use `make <target>' where <target> is one of\nhtml       to make standalone HTML files\ndirhtml    to make HTML files named index.html in directories\nsinglehtml to make a single large HTML file\npickle     to make pickle files\njson       to make JSON files\nhtmlhelp   to make HTML files and a HTML help project\nqthelp     to make HTML files and a qthelp project\napplehelp  to make an Apple Help Book\ndevhelp    to make HTML files and a Devhelp project\nepub       to make an epub\nlatex      to make LaTeX files, you can set PAPER=a4 or PAPER=letter\nlatexpdf   to make LaTeX files and run them through pdflatex\nlatexpdfja to make LaTeX files and run them through platex/dvipdfmx\ntext       to make text files\nman        to make manual pages\ntexinfo    to make Texinfo files\ninfo       to make Texinfo files and run them through makeinfo\ngettext    to make PO message catalogs\nchanges    to make an overview of all changed/added/deprecated items\nxml        to make Docutils-native XML files\npseudoxml  to make pseudoxml-XML files for display purposes\nlinkcheck  to check all external links for integrity\ndoctest    to run all doctests embedded in the documentation (if enabled)\ncoverage   to run coverage check of the documentation (if enabled)\n```\n\nMost of the time, you’ll just need `make html` to test the rendering of your contribution.\n\n## alabaster theme\n\nWe use the [alabaster theme](https://pypi.python.org/pypi/alabaster), which you may need to add to your local Python installation:\n\n``` python\n$ sudo pip install alabaster\n```\n\nWe have found that you may need to link the alabaster package install directory to the directory where Sphinx expects to find themes. For example (on my machine), alabaster was installed into:\n\n``` python\n/usr/local/lib/python2.7/dist-packages/alabaster\n```\n\nAnd Sphinx expects to find themes in:\n\n``` python\n/usr/share/sphinx/themes\n```\n\nSo to get the make targets to run successfully:\n\n``` python\n$ cd /usr/share/sphinx/themes\n$ ln -s /usr/local/lib/python2.7/dist-packages/alabaster\n```\n\n## Test the rendering\n\nNow you can edit contents in the website repo, and test the rendering by calling make targets in the `R1` directory:\n\n``` python\n$ cd $REPO/R1\n$ make html\nsphinx-build -b html -d build/doctrees   source build/html\nRunning Sphinx v1.2.3\nloading pickled environment... done\nbuilding [html]: targets for 1 source files that are out of date\nupdating environment: 0 added, 1 changed, 0 removed\nreading sources... [100%] tips/contribdoc/index\nlooking for now-outdated files... none found\npickling environment... done\nchecking consistency... done\npreparing documents... done\nwriting output... [100%] tips/index\nwriting additional files... genindex search\ncopying static files... done\ncopying extra files... done\ndumping search index... done\ndumping object inventory... done\nbuild succeeded.\n```\n\nAfter a successful build, the newly rendered contents are saved in the `R1/source/build` directory, so you can have a look with your browser.\n\n## Send us a pull request\n\nWhen you have your contribution building successfully, send us a PR, we’ll be happy to hear from you!\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/dev-guide/homepage_contrib.html](https://varnish-cache.org/docs/7.4/dev-guide/homepage_contrib.html)"
- name: Ignoring the Vary header for bots
  id: vcl-design-patterns/req-hash_ignore_vary
  summary: Varnish supports HTTP variants out of the box, but the Vary header is somewhat limited since it operates on complete header values
  description: "# Ignoring the Vary header for bots\n\nVarnish supports HTTP variants out of the box, but the *Vary* header is somewhat limited since it operates on complete header values. If you want for example to conduct an A/B testing campaign or perform blue/green deployment you can make clients “remember” their path with a first-party cookie.\n\nWhen a search engine bot asks for contents however, there’s a high chance that they don’t process cookies and in all likelihood you would prefer to serve a response quickly. In that case you would probably prefer not to even try to attribute a category to the client, but in that case you create a new variant in your cache that is none of A, B, blue, green, or whatever your backend serves.\n\nIf the way content is served makes no difference to the bot, because you changed the color of a button or something else orthogonal to the content itself, then you risk a cache miss with the detrimental effects of adding a needless variant to the cache and serving it with extra latency.\n\nIf latency is paramount, you can use `req.hash_ignore_vary` to opt out of the Vary match during the lookup and get the freshest variant.\n\nIgnoring how the cookie is set, and assuming the backend always provides an accurate *Cache-Control* even when cookies are present, below is an example of an A/B testing setup where bots are served the freshest variant:\n\n``` python\nimport cookie;\n\ninclude \"devicedetect.vcl\";\n\nsub vcl_recv {\n    call devicedetect;\n    if (req.http.X-UA-Device ~ \"bot\") {\n        set req.hash_ignore_vary = true;\n    }\n}\n\nsub vcl_req_cookie {\n    cookie.parse(req.http.Cookie);\n    set req.http.X-AB-Test = cookie.get(\"ab-test\");\n    return;\n}\n\nsub vcl_deliver {\n    unset resp.http.Vary;\n}\n```\n\nIt is also assumed that the backend replies with a `Vary: X-AB-Test` header and varies on no other header.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/vcl-design-patterns/req-hash_ignore_vary.html](https://varnish-cache.org/docs/7.4/vcl-design-patterns/req-hash_ignore_vary.html)"
- name: Installing on Debian/Ubuntu
  id: installation/install_debian
  summary: 'Starting from Varnish Cache 5.0, we’ve simplified our packaging down to two: the main package and a development package'
  description: "# Installing on Debian/Ubuntu\n\n## From package\n\nType:\n\n``` python\nsudo apt-get install varnish\n```\n\n## Official packages of 6\n\nStarting from Varnish Cache 5.0, we’ve simplified our packaging down to two: the main package and a development package.\n\nThe official Varnish Cache repository is now hosted at Packagecloud.io. Note that while Packagecloud.io provides Bash Script installs, we recommend using the manual installation procedures.\n\nInstructions for installing the official repository which contains the newest Varnish Cache 6 release are available at:\n\n- [https://packagecloud.io/varnishcache/varnish60lts/install#manual-deb](https://packagecloud.io/varnishcache/varnish60lts/install#manual-deb)\n\nWith the release of 6.0.2, users have to switch to switch repositories to get the latest version. Read more about this on [Release 6.0.2](https://varnish-cache.org/releases/rel6.0.2).\n\n## Official packages of 4.1\n\nTo use Varnish Cache 4.1 packages from the official varnish-cache.org repos, follow the instructions available at:\n\n- [https://packagecloud.io/varnishcache/varnish41/install#manual-deb](https://packagecloud.io/varnishcache/varnish41/install#manual-deb)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/installation/install_debian.html](https://varnish-cache.org/docs/7.4/installation/install_debian.html)"
- name: Installing on FreeBSD
  id: installation/install_freebsd
  summary: Copyright © 2006 Verdens Gang AS Copyright © 2006–2020 Varnish Software AS Licensed under the BSD-2-Clause License
  description: "# Installing on FreeBSD\n\n## From package\n\nFreeBSD offers two versions of Varnish pre-packaged:\n\n``` python\npkg install varnish6\n```\n\nor, if for some reason you want the older version:\n\n``` python\npkg install varnish4\n```\n\n## From ports\n\nThe FreeBSD packages are built out of the “ports” tree, and you can install varnish directly from ports if you prefer, for instance to get a newer version of Varnish than the current set of prebuilt packages provide:\n\n``` python\ncd /usr/ports/www/varnish6\nmake all install clean\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/installation/install_freebsd.html](https://varnish-cache.org/docs/7.4/installation/install_freebsd.html)"
- name: Installing on OpenBSD
  id: installation/install_openbsd
  summary: Copyright © 2006 Verdens Gang AS Copyright © 2006–2020 Varnish Software AS Licensed under the BSD-2-Clause License
  description: "# Installing on OpenBSD\n\n## From package\n\nVarnish is distributed in the OpenBSD ports collection as ‘www/varnish’:\n\n``` python\npkg_add varnish\n```\n\n## From ports\n\n``` python\ncd /usr/ports/www/varnish\nmake install\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/installation/install_openbsd.html](https://varnish-cache.org/docs/7.4/installation/install_openbsd.html)"
- name: Installing on RedHat or CentOS
  id: installation/install_redhat
  summary: Varnish is included in the EPEL repository, however due to incompatible syntax changes in newer versions of Varnish, only older versions are available
  description: "# Installing on RedHat or CentOS\n\nVarnish is included in the [EPEL](https://fedoraproject.org/wiki/EPEL) repository, however due to incompatible syntax changes in newer versions of Varnish, only older versions are available.\n\nWe therefore recommend that you install the latest version directly from our repository, as described above.\n\nVarnish Cache is packaged in RPMs for easy installation and upgrade on Red Hat systems. The Varnish Cache project maintains official packages for the current Enterprise Linux versions. Varnish Cache 6.x series are supported on el7 and el8.\n\nWe try to keep the latest version available as prebuilt RPMs (el7 and el8) on [packagecloud.io/varnishcache](https://packagecloud.io/varnishcache/).\n\nStarting with el8 a DNF module will inhibit Varnish packages, and the solution is to disable the module before installing:\n\n``` python\ndnf module disable varnish\n```\n\n## Official packages of 6\n\nStarting from Varnish Cache 5.0, we’ve simplified our packaging down to two: the main package and a development package.\n\nThe official Varnish Cache repository is now hosted at Packagecloud.io. Note that while Packagecloud.io provides Bash Script installs, we recommend using the manual installation procedures.\n\nInstructions for installing the official repository which contains the newest Varnish Cache 6 release are available at:\n\n- [https://packagecloud.io/varnishcache/varnish60lts/install#manual-rpm](https://packagecloud.io/varnishcache/varnish60lts/install#manual-rpm)\n\nWith the release of 6.0.2, users have to switch to switch repositories to get the latest version. Read more about this on [Release 6.0.2](https://varnish-cache.org/releases/rel6.0.2).\n\n## External packaging\n\nVarnish Cache is also distributed in third party package repositories.\n\n- [Fedora EPEL](https://fedoraproject.org/wiki/EPEL) does community packaging of Varnish Cache.\n- RedHat has packaged versions of Varnish Cache available since Software Collections 2.1. Announcement on \\<[http://developers.redhat.com/blog/2015/11/17/software-collections-2-1-generally-available/](http://developers.redhat.com/blog/2015/11/17/software-collections-2-1-generally-available/)\\>.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/installation/install_redhat.html](https://varnish-cache.org/docs/7.4/installation/install_redhat.html)"
- name: Installing Varnish
  id: installation/install
  summary: With open source software, you can choose to install binary packages or compile it yourself from source code
  description: "# Installing Varnish\n\nWith open source software, you can choose to install binary packages or compile it yourself from source code. To install a package or compile from source is a matter of personal taste. If you don’t know which method to choose, we recommend that you read this whole section and then choose the method you feel most comfortable with.\n\nUnfortunately, something as basic as installing a piece of software is highly operating system specific:\n\n- [Installing on Debian/Ubuntu](install_debian)\n  - [From package](install_debian#from-package)\n  - [Official packages of 6](install_debian#official-packages-of-6)\n  - [Official packages of 4.1](install_debian#official-packages-of-4-1)\n- [Installing on FreeBSD](install_freebsd)\n  - [From package](install_freebsd#from-package)\n  - [From ports](install_freebsd#from-ports)\n- [Installing on OpenBSD](install_openbsd)\n  - [From package](install_openbsd#from-package)\n  - [From ports](install_openbsd#from-ports)\n- [Installing on RedHat or CentOS](install_redhat)\n  - [Official packages of 6](install_redhat#official-packages-of-6)\n  - [External packaging](install_redhat#external-packaging)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/installation/install.html](https://varnish-cache.org/docs/7.4/installation/install.html)"
- name: Logging in Varnish
  id: users-guide/operation-logging
  summary: One of the really nice features in Varnish is the way logging works
  description: "# Logging in Varnish\n\nOne of the really nice features in Varnish is the way logging works. Instead of logging to a normal log file Varnish logs to a shared memory segment, called the VSL - the Varnish Shared Log. When the end of the segment is reached we start over, overwriting old data.\n\nThis is much, much faster than logging to a file and it doesn’t require disk space. Besides it gives you much, much more information when you need it.\n\nThe flip side is that if you forget to have a program actually write the logs to disk they will be overwritten.\n\n`varnishlog` is one of the programs you can use to look at what Varnish is logging. `varnishlog` gives you the raw logs, everything that is written to the logs. There are other clients that can access the logs as well, we’ll show you these later.\n\nIn the terminal window you started Varnish now type `varnishlog -g raw` and press enter.\n\nYou’ll see lines like these scrolling slowly by.:\n\n``` python\n0 CLI            - Rd ping\n0 CLI            - Wr 200 19 PONG 1273698726 1.0\n```\n\nThese is the Varnish master process checking up on the caching process to see that everything is OK.\n\nNow go to the browser and reload the page displaying your web app.\n\nYou’ll see lines like these.:\n\n``` python\n11 SessOpen       c 127.0.0.1 58912 :8080 0.0.0.0 8080 1273698726.933590 14\n11 ReqStart       c 127.0.0.1 58912\n11 ReqMethod      c GET\n11 ReqURL         c /\n11 ReqProtocol    c HTTP/1.1\n11 ReqHeader      c Host: localhost:8080\n11 ReqHeader      c Connection: keep-alive\n```\n\nThe first column is an arbitrary number, it identifies the transaction. Lines with the same number are coming from the same transaction. The second column is the *tag* of the log message. All log entries are tagged with a tag indicating what sort of activity is being logged.\n\nThe third column tell us whether this is is data coming from or going to the client (‘c’), or the backend (‘b’). The forth column is the data being logged.\n\nNow, you can filter quite a bit with `varnishlog`. The basic options we think you want to know are:\n\n‘-b’  \nOnly show log lines from traffic going between Varnish and the backend servers. This will be useful when we want to optimize cache hit rates.\n\n‘-c’  \nSame as ‘-b’ but for client side traffic.\n\n‘-g request’  \nGroup transactions by request.\n\n‘-q query’  \nOnly list transactions matching this query.\n\nFor more information on this topic please see [varnishlog](../reference/varnishlog#varnishlog-1).\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/operation-logging.html](https://varnish-cache.org/docs/7.4/users-guide/operation-logging.html)"
- name: Manipulating request headers in VCL
  id: users-guide/vcl-example-manipulating-headers
  summary: Now, when the request is handled to the backend server there will be no cookie header
  description: "# Manipulating request headers in VCL\n\nLets say we want to remove the cookie for all objects in the `/images` directory of our web server:\n\n``` python\nsub vcl_recv {\n  if (req.url ~ \"^/images\") {\n    unset req.http.cookie;\n  }\n}\n```\n\nNow, when the request is handled to the backend server there will be no cookie header. The interesting line is the one with the if-statement. It matches the URL, taken from the request object, and matches it against the regular expression. Note the match operator. If it matches the Cookie: header of the request is unset (deleted).\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-example-manipulating-headers.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-example-manipulating-headers.html)"
- name: Now what?
  id: tutorial/now_what
  summary: You’ve read through the tutorial
  description: "# Now what?\n\nYou’ve read through the tutorial. You should have Varnish up and running. You should know about the logs and you should have a rough idea of what VCL is. Next, you might want to have a look at [The Varnish Users Guide](../users-guide/index#users-guide-index), where we go through the features of Varnish in more detail.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/tutorial/now_what.html](https://varnish-cache.org/docs/7.4/tutorial/now_what.html)"
- name: Parameters
  id: users-guide/params
  summary: Varnish Cache comes with a set of parameters that affects behaviour and performance
  description: "# Parameters\n\nVarnish Cache comes with a set of parameters that affects behaviour and performance. Parameters are set either though command line arguments to `varnishd` or at runtime through `varnishadm` using the `param.set` CLI command.\n\nWe don’t recommend that you tweak parameters unless you’re sure of what you’re doing. We’ve worked hard to make the defaults sane and Varnish should be able to handle most workloads with the default settings.\n\nFor a complete listing of all parameters and their specifics see [List of Parameters](../reference/varnishd#list-of-parameters).\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/params.html](https://varnish-cache.org/docs/7.4/users-guide/params.html)"
- name: Peculiarities
  id: tutorial/peculiarities
  summary: There are a couple of things that are different with Varnish Cache, as opposed to other programs
  description: "# Peculiarities\n\nThere are a couple of things that are different with Varnish Cache, as opposed to other programs. One thing you’ve already seen - VCL. In this section we provide a very quick tour of other peculiarities you need to know about to get the most out of Varnish.\n\n## Configuration\n\nThe Varnish Configuration is written in VCL. When Varnish is ran this configuration is transformed into C code and then fed into a C compiler, loaded and executed.\n\nSo, as opposed to switching various settings on or off, you write polices on how the incoming traffic should be handled.\n\n## varnishadm\n\nVarnish Cache has an admin console. You can connect it through the [varnishadm](../reference/varnishadm#varnishadm-1) command. In order to connect the user needs to be able to read `/etc/varnish/secret` in order to authenticate.\n\nOnce you’ve started the console you can do quite a few operations on Varnish, like stopping and starting the cache process, load VCL, adjust the built in load balancer and invalidate cached content.\n\nIt has a built in command “help” which will give you some hints on what it does.\n\n## varnishlog\n\nVarnish does not log to disk. Instead it logs to a chunk of memory. It is actually streaming the logs. At any time you’ll be able to connect to the stream and see what is going on. Varnish logs quite a bit of information. You can have a look at the logstream with the command [varnishlog](../reference/varnishlog#varnishlog-1).\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/tutorial/peculiarities.html](https://varnish-cache.org/docs/7.4/tutorial/peculiarities.html)"
- name: Platform specific notes
  id: installation/platformnotes
  summary: On some platforms it is necessary to adjust the operating system before running Varnish on it
  description: "# Platform specific notes\n\nOn some platforms it is necessary to adjust the operating system before running Varnish on it. The systems and steps known to us are described in this section.\n\n## Transparent hugepages on Redhat Enterprise Linux 6\n\nOn RHEL6 Transparent Hugepage kernel support is enabled by default. This is known to cause sporadic crashes of Varnish.\n\nIt is recommended to disable transparent hugepages on affected systems. This can be done with `echo never > /sys/kernel/mm/redhat_transparent_hugepage/enabled` (runtime) or by adding “transparent_hugepage=never” to the kernel boot line in the “/etc/grub.conf” file (persistent).\n\nOn Debian/Ubuntu systems running 3.2 kernels the default value is “madvise” and does not need to be changed.\n\n## OpenVZ\n\nIt is possible, but not recommended for high performance, to run Varnish on virtualised hardware. Reduced disk and network -performance will reduce the performance a bit so make sure your system has good IO performance.\n\nIf you are running on 64bit OpenVZ (or Parallels VPS), you must reduce the maximum stack size before starting Varnish.\n\nThe default allocates too much memory per thread, which will make Varnish fail as soon as the number of threads (traffic) increases.\n\nReduce the maximum stack size by adding `ulimit -s 256` before starting Varnish in the init script.\n\n## TCP keep-alive configuration\n\nOn some Solaris, FreeBSD and OS X systems, Varnish is not able to set the TCP keep-alive values per socket, and therefore the *tcp_keepalive\\_* Varnish runtime parameters are not available. On these platforms it can be beneficial to tune the system wide values for these in order to more reliably detect remote close for sessions spending long time on waitinglists. This will help free up resources faster.\n\nSystems that does not support TCP keep-alive values per socket include:\n\n- Solaris releases prior to version 11\n- FreeBSD releases prior to version 9.1\n- OS X releases prior to Mountain Lion\n\nOn platforms with the necessary socket options the defaults are set to:\n\n- `tcp_keepalive_time` = 600 seconds\n- `tcp_keepalive_probes` = 5\n- `tcp_keepalive_intvl` = 5 seconds\n\nNote that Varnish will only apply these run-time parameters so long as they are less than the system default value.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/installation/platformnotes.html](https://varnish-cache.org/docs/7.4/installation/platformnotes.html)"
- name: Prerequisites
  id: installation/prerequisites
  summary: Varnish can be installed on other UNIX systems as well, but it is not extensively or systematically tested by us on other systems than the above
  description: "# Prerequisites\n\nIn order for you to install Varnish you must have the following:\n\n- A recent, preferably server grade, computer.\n- A fairly modern and 64 bit version of either - Linux - FreeBSD, or - Solaris (x86 only).\n- Root access.\n\nVarnish can be installed on other UNIX systems as well, but it is not extensively or systematically tested by us on other systems than the above. Varnish is, from time to time, said to work on:\n\n- 32 bit versions of the before-mentioned systems,\n- OS X,\n- NetBSD,\n- OpenBSD, and\n- Windows with Cygwin.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/installation/prerequisites.html](https://varnish-cache.org/docs/7.4/installation/prerequisites.html)"
- name: Purging and banning
  id: users-guide/purging
  summary: One of the most effective ways of increasing your hit ratio is to increase the time-to-live (ttl) of your objects
  description: "# Purging and banning\n\nOne of the most effective ways of increasing your hit ratio is to increase the time-to-live (ttl) of your objects. But, as you’re aware of, in this twitterific day of age, serving content that is outdated is bad for business.\n\nThe solution is to notify Varnish when there is fresh content available. This can be done through three mechanisms. HTTP purging, banning and forced cache misses. First, lets look at HTTP purging.\n\n## HTTP Purging\n\nA *purge* is what happens when you pick out an object from the cache and discard it along with its variants. Usually a purge is invoked through HTTP with the method `PURGE`.\n\nAn HTTP purge is similar to an HTTP GET request, except that the *method* is `PURGE`. Actually you can call the method whatever you’d like, but most people refer to this as purging. Squid, for example, supports the same mechanism. In order to support purging in Varnish you need the following VCL in place:\n\n``` python\nacl purge {\n        \"localhost\";\n        \"192.168.55.0\"/24;\n}\n\nsub vcl_recv {\n        # allow PURGE from localhost and 192.168.55...\n\n        if (req.method == \"PURGE\") {\n                if (!client.ip ~ purge) {\n                        return(synth(405,\"Not allowed.\"));\n                }\n                return (purge);\n        }\n}\n```\n\nAs you can see we have used a new action - return(purge). This ends execution of vcl_recv and jumps to vcl_hash. This is just like we handle a regular request. When vcl_hash calls return(lookup) Varnish will purge the object and then call vcl_purge. Here you have the option of adding any particular actions you want Varnish to take once it has purge the object.\n\nSo for example.com to invalidate their front page they would call out to Varnish like this:\n\n``` python\nPURGE / HTTP/1.0\nHost: example.com\n```\n\nAnd Varnish would then discard the front page. This will remove all variants as defined by Vary.\n\n## Bans\n\nThere is another way to invalidate content: Bans. You can think of bans as a sort of a filter on objects already in the cache. You `ban` certain content from being served from your cache. You can ban content based on any metadata we have. A ban will only work on objects already in the cache, it does not prevent new content from entering the cache or being served.\n\nSupport for bans is built into Varnish and available in the CLI interface. To ban every png object belonging on example.com, issue the following command from the shell:\n\n``` python\nvarnishadm ban req.http.host == example.com '&&' req.url '~' '\\\\.png$'\n```\n\nSee [BOOL ban(STRING)](../reference/vmod_std#std-ban) for details on the syntax of ban expressions. In particular, note that in the example given above, the quotes are required for execution from the shell and escaping the backslash in the regular expression is required by the Varnish cli interface.\n\nBans are checked when we hit an object in the cache, but before we deliver it. *An object is only checked against newer bans*.\n\nBans that only match against `obj.*` are also processed by a background worker threads called the `ban lurker`. The `ban lurker` will walk the heap and try to match objects and will evict the matching objects. How aggressive the `ban lurker` is can be controlled by the parameter ‘ban_lurker_sleep’. The `ban lurker` can be disabled by setting ‘ban_lurker_sleep’ to 0.\n\nBans that are older than the oldest objects in the cache are discarded without evaluation. If you have a lot of objects with long TTL, that are seldom accessed, you might accumulate a lot of bans. This might impact CPU usage and thereby performance.\n\nYou can also add bans to Varnish via HTTP. Doing so requires a bit of VCL:\n\n``` python\nimport std;\n\nsub vcl_recv {\n        if (req.method == \"BAN\") {\n                # Same ACL check as above:\n                if (!client.ip ~ purge) {\n                        return(synth(403, \"Not allowed.\"));\n                }\n                if (std.ban(\"req.http.host == \" + req.http.host +\n                    \" && req.url == \" + req.url)) {\n                        return(synth(200, \"Ban added\"));\n                } else {\n                        # return ban error in 400 response\n                        return(synth(400, std.ban_error()));\n                }\n        }\n}\n```\n\nThis VCL stanza enables Varnish to handle a `HTTP BAN` method, adding a ban on the URL, including the host part.\n\nThe `ban lurker` can help you keep the ban list at a manageable size, so we recommend that you avoid using `req.*` in your bans, as the request object is not available in the `ban lurker` thread.\n\nYou can use the following template to write `ban lurker` friendly bans:\n\n``` python\nimport std;\n\nsub vcl_backend_response {\n        set beresp.http.url = bereq.url;\n}\n\nsub vcl_deliver {\n        unset resp.http.url; # Optional\n}\n\nsub vcl_recv {\n        if (req.method == \"BAN\") {\n                # Same ACL check as above:\n                if (!client.ip ~ purge) {\n                        return(synth(403, \"Not allowed.\"));\n                }\n                # Assumes req.url is a regex. This might be a bit too simple\n                if (std.ban(\"obj.http.url ~ \" + req.url)) {\n                        return(synth(200, \"Ban added\"));\n                } else {\n                        # return ban error in 400 response\n                        return(synth(400, std.ban_error()));\n                }\n        }\n}\n```\n\nTo inspect the current ban list, issue the `ban.list` command in the CLI. This will produce a status of all current bans:\n\n``` python\n0xb75096d0 1318329475.377475    10      obj.http.url ~ test\n0xb7509610 1318329470.785875    20G     obj.http.url ~ test\n```\n\nThe ban list contains the ID of the ban, the timestamp when the ban entered the ban list. A count of the objects that has reached this point in the ban list, optionally postfixed with a ‘G’ for “Gone”, if the ban is no longer valid. Finally, the ban expression is listed. Notice that durations are not necessarily expressed in the originally given unit, for instance `7d` will get turned into `1w`.\n\nThe ban can be marked as “Gone” if it is a duplicate ban, but is still kept in the list for optimization purposes.\n\n## Forcing a cache miss\n\nThe final way to invalidate an object is a method that allows you to refresh an object by forcing a `hash miss` for a single request. If you set ‘req.hash_always_miss’ to true, Varnish will miss the current object in the cache, thus forcing a fetch from the backend. This can in turn add the freshly fetched object to the cache, thus overriding the current one. The old object will stay in the cache until ttl expires or it is evicted by some other means.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/purging.html](https://varnish-cache.org/docs/7.4/users-guide/purging.html)"
- name: Put Varnish on port 80
  id: tutorial/putting_varnish_on_port_80
  summary: Until now we’ve been running with Varnish on a high port which is great for testing purposes
  description: "# Put Varnish on port 80\n\nUntil now we’ve been running with Varnish on a high port which is great for testing purposes. Let’s now put Varnish on the default HTTP port 80.\n\nFirst we stop varnish: `service varnish stop`\n\nNow we need to edit the configuration file that starts Varnish.\n\n## Debian/Ubuntu (legacy)\n\nOn older Debian/Ubuntu this is `/etc/default/varnish`. In the file you’ll find some text that looks like this:\n\n``` python\nDAEMON_OPTS=\"-a :6081 \\\n             -T localhost:6082 \\\n             -f /etc/varnish/default.vcl \\\n             -S /etc/varnish/secret \\\n             -s default,256m\"\n```\n\nChange it to:\n\n``` python\nDAEMON_OPTS=\"-a :80 \\\n             -T localhost:6082 \\\n             -f /etc/varnish/default.vcl \\\n             -S /etc/varnish/secret \\\n             -s default,256m\"\n```\n\n## Debian (v8+) / Ubuntu (v15.04+)\n\nOn more recent Debian and Ubuntu systems this is configured in the systemd service file.\n\nApplying changes to the default service is best done by creating a new file `/etc/systemd/system/varnish.service.d/customexec.conf`:\n\n``` python\n[Service]\nExecStart=\nExecStart=/usr/sbin/varnishd -a :80 -T localhost:6082 -f /etc/varnish/default.vcl -S /etc/varnish/secret -s default,256m\n```\n\nThis will override the ExecStart part of the default configuration shipped with Varnish Cache.\n\nRun `systemctl daemon-reload` to make sure systemd picks up the new configuration before restarting Varnish.\n\n## Red Hat Enterprise Linux / CentOS\n\nOn Red Hat/CentOS you can find a similar configuration file in `/etc/sysconfig/varnish`.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/tutorial/putting_varnish_on_port_80.html](https://varnish-cache.org/docs/7.4/tutorial/putting_varnish_on_port_80.html)"
- name: Reporting and statistics
  id: users-guide/report
  summary: This section covers how to find out what Varnish is doing, from the detailed per HTTP request blow-by-blow logrecords to the global summary statistics counters
  description: "# Reporting and statistics\n\nThis section covers how to find out what Varnish is doing, from the detailed per HTTP request blow-by-blow logrecords to the global summary statistics counters.\n\n- [Logging in Varnish](operation-logging)\n- [Statistics](operation-statistics)\n  - [varnishtop](operation-statistics#varnishtop)\n  - [varnishhist](operation-statistics#varnishhist)\n  - [varnishstat](operation-statistics#varnishstat)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/report.html](https://varnish-cache.org/docs/7.4/users-guide/report.html)"
- name: Reporting bugs
  id: installation/bugs
  summary: Varnish can be a tricky beast to debug, having potentially thousands of threads crowding into a few data structures makes for interesting core dumps
  description: "# Reporting bugs\n\nVarnish can be a tricky beast to debug, having potentially thousands of threads crowding into a few data structures makes for *interesting* core dumps.\n\nActually, let me rephrase that without irony: You tire of the “no, not thread 438 either, lets look at 439 then…” routine really fast.\n\nSo if you run into a bug, it is important that you spend a little bit of time collecting the right information, to help us fix the bug.\n\nThe most valuable information you can give us, is **always** how to trigger and reproduce the problem. If you can tell us that, we rarely need anything else to solve it.The caveat being, that we do not have a way to simulate high levels of real-life web-traffic, so telling us to “have 10.000 clients hit at once” does not really allow us to reproduce.\n\nTo report a bug please follow the suggested procedure described in the “Trouble Tickets” section of the documentation (above).\n\nRoughly we categorize bugs in to three kinds of bugs (described below) with Varnish. The information we need to debug them depends on what kind of bug we are facing.\n\n## Varnish crashes\n\nPlain and simple: **boom**\n\nVarnish is split over two processes, the manager and the child. The child does all the work, and the manager hangs around to resurrect it if it crashes.\n\nTherefore, the first thing to do if you see a Varnish crash, is to examine your syslogs to see if it has happened before. (One site is rumoured to have had Varnish restarting every 10 minutes and *still* provide better service than their CMS system.)\n\nWhen it crashes, which is highly unlikely to begin with, Varnish will spew out a crash dump that looks something like:\n\n``` python\nChild (32619) died signal=6 (core dumped)\nChild (32619) Panic message: Assert error in ccf_panic(), cache_cli.c line 153:\n  Condition(!strcmp(\"\", \"You asked for it\")) not true.\nerrno = 9 (Bad file descriptor)\nthread = (cache-main)\nident = FreeBSD,9.0-CURRENT,amd64,-sfile,-hcritbit,kqueue\nBacktrace:\n  0x42bce1: pan_ic+171\n  0x4196af: ccf_panic+4f\n  0x8006b3ef2: _end+80013339a\n  0x8006b4307: _end+8001337af\n  0x8006b8b76: _end+80013801e\n  0x8006b8d84: _end+80013822c\n  0x8006b51c1: _end+800134669\n  0x4193f6: CLI_Run+86\n  0x429f8b: child_main+14b\n  0x43ef68: start_child+3f8\n[...]\n```\n\nIf you can get that information to us, we are usually able to see exactly where things went haywire, and that speeds up bugfixing a lot.\n\nThere will be a lot more information in the crash dump besides this, and before sending it all to us, you should obscure any sensitive/secret data/cookies/passwords/ip# etc. Please make sure to keep context when you do so, ie: do not change all the IP# to “X.X.X.X”, but change each IP# to something unique, otherwise we are likely to be more confused than informed.\n\nThe most important line is the “Panic Message”, which comes in two general forms:\n\n“Missing errorhandling code in …”  \nThis is a situation where we can conceive Varnish ending up, which we have not (yet) written the padded-box error handling code for.\n\nThe most likely cause here, is that you need a larger workspace for HTTP headers and Cookies.\n\nPlease try that before reporting a bug.\n\n“Assert error in …”  \nThis is something bad that should never happen, and a bug report is almost certainly in order. As always, if in doubt ask us on IRC before opening the ticket.\n\nIn your syslog it may all be joined into one single line, but if you can reproduce the crash, do so while running [varnishd](../reference/varnishd#varnishd-1) manually:\n\n`varnishd -d <your other arguments> |& tee /tmp/_catch_bug`\n\nThat will get you the entire panic message into a file.\n\n(Remember to type `start` to launch the worker process, that is not automatic when `-d` is used.)\n\n## Varnish goes on vacation\n\nThis kind of bug is nasty to debug, because usually people tend to kill the process and send us an email saying “Varnish hung, I restarted it” which gives us only about 1.01 bit of usable debug information to work with.\n\nWhat we need here is all the information you can squeeze out of your operating system **before** you kill the Varnish process.\n\nOne of the most valuable bits of information, is if all Varnish’ threads are waiting for something or if one of them is spinning furiously on some futile condition.\n\nCommands like `top -H` or `ps -Haxlw` or `ps -efH` should be able to figure that out.\n\nIf one or more threads are spinning, use `strace` or `ktrace` or `truss` (or whatever else your OS provides) to get a trace of which system calls the Varnish process issues. Be aware that this may generate a lot of very repetitive data, usually one second worth of data is more than enough.\n\nAlso, run [varnishlog](../reference/varnishlog#varnishlog-1) for a second, and collect the output for us, and if [varnishstat](../reference/varnishstat#varnishstat-1) shows any activity, capture that also.\n\nWhen you have done this, kill the Varnish *child* process, and let the *master* process restart it. Remember to tell us if that does or does not work. If it does not, kill all Varnish processes, and start from scratch. If that does not work either, tell us, that means that we have wedged your kernel.\n\n## Varnish does something wrong\n\nThese are the easy bugs: usually all we need from you is the relevant transactions recorded with [varnishlog](../reference/varnishlog#varnishlog-1) and your explanation of what is wrong about what Varnish does.\n\nBe aware, that often Varnish does exactly what you asked it to, rather than what you intended it to do. If it sounds like a bug that would have tripped up everybody else, take a moment to read through your VCL and see if it really does what you think it does.\n\nYou can also try setting the `vsl_mask=+VCL_trace` parameter (or use `varnishadm param.set vsl_mask +VCL_trace` on a running instance), that will generate log records with like and character number for each statement executed in your VCL program.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/installation/bugs.html](https://varnish-cache.org/docs/7.4/installation/bugs.html)"
- name: Request and response VCL objects
  id: users-guide/vcl-variables
  summary: In VCL, there several important objects that you need to be aware of
  description: "# Request and response VCL objects\n\nIn VCL, there several important objects that you need to be aware of. These objects can be accessed and manipulated using VCL.\n\n*req*  \nThe request object. When Varnish has received the request the `req` object is created and populated. Most of the work you do in `vcl_recv` you do on or with the `req` object.\n\n*bereq*  \nThe backend request object. Varnish constructs this before sending it to the backend. It is based on the `req` object.\n\n*beresp*  \nThe backend response object. It contains the headers of the object coming from the backend. If you want to modify the response coming from the server you modify this object in `vcl_backend_response`.\n\n*resp*  \nThe HTTP response right before it is delivered to the client. It is typically modified in `vcl_deliver`.\n\n*obj*  \nThe object as it is stored in cache. Read only.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-variables.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-variables.html)"
- name: Required command line arguments
  id: users-guide/command-line
  summary: The ‘-a’ argument defines what address Varnish should listen to, and service HTTP requests from
  description: "# Required command line arguments\n\nThere a two command line arguments you have to set when starting Varnish, these are:\n\n- what TCP port to serve HTTP from, and\n- where the backend server can be contacted.\n\nIf you have installed Varnish through using a provided operating system bound package, you will find the startup options here:\n\n- Debian, Ubuntu: `/etc/default/varnish`\n- Red Hat, Centos: `/etc/sysconfig/varnish`\n- FreeBSD: `/etc/rc.conf` (See also: /usr/local/etc/rc.d/varnishd)\n\n## ‘-a’ *listen_address*\n\nThe ‘-a’ argument defines what address Varnish should listen to, and service HTTP requests from.\n\nYou will most likely want to set this to “:80” which is the Well Known Port for HTTP.\n\nYou can specify multiple addresses separated by a comma, and you can use numeric or host/service names if you like, Varnish will try to open and service as many of them as possible, but if none of them can be opened, `varnishd` will not start.\n\nHere are some examples:\n\n``` python\n-a :80\n-a localhost:80\n-a 192.168.1.100:8080\n-a '[fe80::1]:80'\n-a '0.0.0.0:8080,[::]:8081'\n```\n\nIf your webserver runs on the same machine, you will have to move it to another port number first.\n\n## ‘-f’ *VCL-file* or ‘-b’ *backend*\n\nVarnish needs to know where to find the HTTP server it is caching for. You can either specify it with the ‘-b’ argument, or you can put it in your own VCL file, specified with the ‘-f’ argument.\n\nUsing ‘-b’ is a quick way to get started:\n\n``` python\n-b localhost:81\n-b thatotherserver.example.com:80\n-b 192.168.1.2:80\n```\n\nNotice that if you specify a name, it can at most resolve to one IPv4 *and* one IPv6 address.\n\nFor more advanced use, you will want to specify a VCL program with `-f`, but you can start with as little as just:\n\n``` python\nbackend default {\n        .host = \"localhost:81\";\n}\n```\n\nwhich is, by the way, *precisely* what ‘-b’ does.\n\n## Optional arguments\n\nFor a complete list of the command line arguments please see [varnishd(1) options](../reference/varnishd#ref-varnishd-options).\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/command-line.html](https://varnish-cache.org/docs/7.4/users-guide/command-line.html)"
- name: Security first
  id: users-guide/run_security
  summary: If you are the only person involved in running Varnish, or if all the people involved are trusted to the same degree, you can skip this chapter
  description: "# Security first\n\nIf you are the only person involved in running Varnish, or if all the people involved are trusted to the same degree, you can skip this chapter. We have protected Varnish as well as we can from anything which can come in through an HTTP socket.\n\nIf parts of your web infrastructure are outsourced or otherwise partitioned along administrative lines, you need to think about security.\n\nVarnish provides four levels of authority, roughly related to how and where control comes into Varnish:\n\n- The command line arguments,\n- The CLI interface,\n- VCL programs, and\n- HTTP requests.\n\n## Command line arguments\n\nThe top level security decisions is decided and defined when starting Varnish in the form of command line arguments, we use this strategy in order to make them invulnerable to subsequent manipulation.\n\nThe important decisions to make are:\n\n1.  Who should have access to the Command Line Interface?\n2.  Which parameters can they change?\n3.  Will inline-C code be allowed?\n4.  If/how VMODs will be restricted?\n5.  How child processes will be jailed?\n\n### CLI interface access\n\nThe command line interface can be accessed in three ways.\n\n`varnishd` can be told to listen and offer CLI connections on a TCP socket. You can bind the socket to pretty much anything the kernel will accept:\n\n``` python\n-T 127.0.0.1:631\n-T localhost:9999\n-T 192.168.1.1:34\n-T '[fe80::1]:8082'\n```\n\nThe default is `-T localhost:0` which will pick a random port number, which `varnishadm(8)` can learn from the shared memory.\n\nBy using a “localhost” address, you restrict CLI access to the local machine.\n\nYou can also bind the CLI port to an IP address reachable across the net, and let other machines connect directly.\n\nThis gives you no secrecy, i.e. the CLI commands will go across the network as ASCII text with no encryption, but the -S/PSK authentication requires the remote end to know the shared secret.\n\nAlternatively you can bind the CLI port to a ‘localhost’ address, and give remote users access via a secure connection to the local machine, using ssh/VPN or similar.\n\nIf you use `ssh` you can restrict which commands each user can execute to just `varnishadm`, or even use a wrapper scripts around `varnishadm` to allow specific CLI commands.\n\nIt is also possible to configure `varnishd` for “reverse mode”, using the ‘-M’ argument. In that case `varnishd` will attempt to open a TCP connection to the specified address, and initiate a CLI connection to your central Varnish management facility.\n\nThe connection in this case is also without encryption, but the remote end must still authenticate using -S/PSK.\n\nFinally, if you run varnishd with the ‘-d’ option, you get a CLI command on stdin/stdout, but since you started the process, it would be hard to prevent you getting CLI access, wouldn’t it ?\n\n### CLI interface authentication\n\nBy default the CLI interface is protected with a simple, yet powerful “Pre Shared Key” authentication method, which do not provide secrecy (ie: The CLI commands and responses are not encrypted).\n\nThe way -S/PSK works is really simple: During startup a file is created with a random content and the file is only accessible to the user who started `varnishd` (or the superuser).\n\nTo authenticate and use a CLI connection, you need to know the contents of that file, in order to answer the cryptographic challenge `varnishd` issues, see [Authentication CLI connections](../reference/cli_protocol#ref-psk-auth).\n\n`varnishadm` uses all of this to restrict access, it will only function, provided it can read the secret file.\n\nIf you want to allow other users, local or remote, to be able to access CLI connections, you must create your own secret file and make it possible for (only!) these users to read it.\n\nA good way to create the secret file is:\n\n``` python\ndd if=/dev/random of=/etc/varnish_secret count=1\n```\n\nWhen you start `varnishd`, you specify the filename with ‘-S’, and it goes without saying that the `varnishd` master process needs to be able to read the file too.\n\nYou can change the contents of the secret file while `varnishd` runs, it is read every time a CLI connection is authenticated.\n\nOn the local system, `varnishadm` can retrieve the filename from shared memory, but on remote systems, you need to give `varnishadm` a copy of the secret file, with the -S argument.\n\nIf you want to disable -S/PSK authentication, specify ‘-S’ with an empty argument to varnishd:\n\n``` python\nvarnishd [...] -S \"\" [...]\n```\n\n### Parameters\n\nParameters can be set from the command line, and made “read-only” (using ‘-r’) so they cannot subsequently be modified from the CLI interface.\n\nPretty much any parameter can be used to totally mess up your HTTP service, but a few can do more damage than others:\n\n[cc_command](../reference/varnishd#ref-param-cc-command)  \nExecute arbitrary programs\n\n[vcc_allow_inline_c](../reference/varnishd#ref-param-vcc-allow-inline-c)  \nAllow inline C in VCL, which would allow any C code from VCL to be executed by Varnish.\n\nFurthermore you may want to look at and lock down:\n\n[syslog_cli_traffic](../reference/varnishd#ref-param-syslog-cli-traffic)  \nLog all CLI commands to `syslog(8)`, so you know what goes on.\n\n[vcc_unsafe_path](../reference/varnishd#ref-param-vcc-unsafe-path)  \nRestrict VCL/VMODs to [vcl_path](../reference/varnishd#ref-param-vcl-path) and [vmod_path](../reference/varnishd#ref-param-vmod-path)\n\n[vmod_path](../reference/varnishd#ref-param-vmod-path)  \nThe directory (or colon separated list of directories) where Varnish will look for modules. This could potentially be used to load rogue modules into Varnish.\n\n## The CLI interface\n\nThe CLI interface in Varnish is very powerful, if you have access to the CLI interface, you can do almost anything to the Varnish process.\n\nAs described above, some of the damage can be limited by restricting certain parameters, but that will only protect the local filesystem, and operating system, it will not protect your HTTP service.\n\nWe do not currently have a way to restrict specific CLI commands to specific CLI connections. One way to get such an effect is to “wrap” all CLI access in pre-approved scripts which use `varnishadm(1)`\n\nto submit the sanitized CLI commands, and restrict a remote user to only those scripts, for instance using sshd(8)’s configuration.\n\n## VCL programs\n\nThere are two “dangerous” mechanisms available in VCL code: VMODs and inline-C.\n\nBoth of these mechanisms allow execution of arbitrary code and will thus allow a person to get access to the machine, with the privileges of the child process.\n\nIf `varnishd` is started as root/superuser, we sandbox the child process, using whatever facilities are available on the operating system, but if `varnishd` is not started as root/superuser, this is not possible. No, don’t ask me why you have to be superuser to lower the privilege of a child process…\n\nInline-C is disabled by default since Varnish version 4, so unless you enable it, you don’t have to worry about it.\n\nThe parameters mentioned above can restrict the loading of VMODs to only be loaded from a designated directory, restricting VCL wranglers to a pre-approved subset of VMODs.\n\nIf you do that, we are confident that your local system cannot be compromised from VCL code.\n\n## HTTP requests\n\nWe have gone to great lengths to make Varnish resistant to anything coming in through the socket where HTTP requests are received, and you should, generally speaking, not need to protect it any further.\n\nThe caveat is that since VCL is a programming language which lets you decide exactly what to do with HTTP requests, you can also decide to do stupid and potentially dangerous things with them, including opening yourself up to various kinds of attacks and subversive activities.\n\nIf you have “administrative” HTTP requests, for instance PURGE requests, we strongly recommend that you restrict them to trusted IP numbers/nets using VCL’s [Access control lists (ACLs)](vcl-syntax#vcl-syntax-acl).\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/run_security.html](https://varnish-cache.org/docs/7.4/users-guide/run_security.html)"
- name: Separate VCL files
  id: users-guide/vcl-separate
  summary: Having multiple different vhosts in the same Varnish is a very typical use-case, and from Varnish 5.0 it is possible to have a separate VCL files for separate vhosts or any other distinct subset of requests
  description: "# Separate VCL files\n\nHaving multiple different vhosts in the same Varnish is a very typical use-case, and from Varnish 5.0 it is possible to have a separate VCL files for separate vhosts or any other distinct subset of requests.\n\nAssume that we want to handle `varnish.org` with one VCL file and `varnish-cache.org` with another VCL file.\n\nFirst load the two VCL files:\n\n``` python\nvcl.load vo_1 /somewhere/vo.vcl\nvcl.load vc_1 /somewhere/vc.vcl\n```\n\nThese are 100% normal VCL files, as they would look if you ran only that single domain on your Varnish instance.\n\nNext we need to point VCL labels to them:\n\n``` python\nvcl.label l_vo vo_1\nvcl.label l_vc vc_1\n```\n\nNext we write the top-level VCL program, which branches out to the other two, depending on the Host: header in the request:\n\n``` python\nimport std;\n\n# We have to have a backend, even if we do not use it\nbackend default { .host = \"127.0.0.1\"; }\n\nsub vcl_recv {\n    # Normalize host header\n    set req.http.host = std.tolower(req.http.host);\n\n    if (req.http.host ~ \"\\.?varnish\\.org$\") {\n        return (vcl(l_vo));\n    }\n    if (req.http.host ~ \"\\.?varnish-cache\\.org$\") {\n        return (vcl(l_vc));\n    }\n    return (synth(302, \"http://varnish-cache.org\"));\n}\n\nsub vcl_synth {\n    if (resp.status == 301 || resp.status == 302) {\n        set resp.http.location = resp.reason;\n        set resp.reason = \"Moved\";\n        return (deliver);\n    }\n}\n```\n\nFinally, we load the top level VCL and make it the active VCL:\n\n``` python\nvcl.load top_1 /somewhere/top.vcl\nvcl.use top_1\n```\n\nIf you want to update one of the separated VCLs, you load the new one and change the label to point to it:\n\n``` python\nvcl.load vo_2 /somewhere/vo.vcl\nvcl.label l_vo vo_2\n```\n\nIf you want to change the top level VCL, do as you always did:\n\n``` python\nvcl.load top_2 /somewhere/top.vcl\nvcl.use top_2\n```\n\n## Details, details, details:\n\n- All requests *always* start in the active VCL - the one from `vcl.use`\n- Only VCL labels can be used in `return(vcl(name))`. Without this restriction the top level VCL would have to be reloaded every time one of the separate VCLs were changed.\n- You can only switch VCLs from the active VCL. If you try it from one of the separate VCLs, you will get a 503\n- You cannot remove VCL labels (with `vcl.discard`) if any VCL contains `return(vcl(name_of_that_label))`\n- You cannot remove VCLs which have a label attached to them.\n- This code is tested in testcase c00077\n- This is a very new feature, it may change\n- We would very much like feedback how this works for you\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-separate.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-separate.html)"
- name: Shell Tricks
  id: reference/shell_tricks
  summary: Copyright © 2006 Verdens Gang AS Copyright © 2006–2020 Varnish Software AS Licensed under the BSD-2-Clause License
  description: "# Shell Tricks\n\nAll the varnish programs can be invoked with the single argument `--optstring` to request their `getopt()` specification, which simplifies wrapper scripts:\n\n``` text\noptstring=$(varnishfoo --optstring)\n\nwhile getopts \"$optstring\" opt\ndo\n    case $opt in\n    n)\n        # handle $OPTARG\n        ;;\n    # handle other options\n    *)\n        # ignore unneeded options\n        ;;\n    esac\ndone\n\nvarnishfoo \"$@\"\n\n# do something with the options\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/shell_tricks.html](https://varnish-cache.org/docs/7.4/reference/shell_tricks.html)"
- name: Sizing your cache
  id: users-guide/sizing-your-cache
  summary: Deciding on cache size can be a tricky task
  description: "# Sizing your cache\n\nDeciding on cache size can be a tricky task. A few things to consider:\n\n- How big is your *hot* data set. For a portal or news site that would be the size of the front page with all the stuff on it, and the size of all the pages and objects linked from the first page.\n- How expensive is it to generate an object? Sometimes it makes sense to only cache images a little while or not to cache them at all if they are cheap to serve from the backend and you have a limited amount of memory.\n- Watch the `n_lru_nuked` counter with [varnishstat](../reference/varnishstat#varnishstat-1) or some other tool. If you have a lot of LRU activity then your cache is evicting objects due to space constraints and you should consider increasing the size of the cache.\n\nBe aware that every object that is stored also carries overhead that is kept outside the actually storage area. So, even if you specify `-s malloc,16G` Varnish might actually use **double** that. Varnish has a overhead of about 1KB per object. So, if you have lots of small objects in your cache the overhead might be significant.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/sizing-your-cache.html](https://varnish-cache.org/docs/7.4/users-guide/sizing-your-cache.html)"
- name: Starting and running Varnish
  id: users-guide/running
  summary: This section covers starting, running, and stopping Varnish, command line flags and options, and communicating with the running Varnish processes, configuring storage and sockets and, and about securing and protecting Varnish against attacks
  description: "# Starting and running Varnish\n\nThis section covers starting, running, and stopping Varnish, command line flags and options, and communicating with the running Varnish processes, configuring storage and sockets and, and about securing and protecting Varnish against attacks.\n\n- [Security first](run_security)\n  - [Command line arguments](run_security#command-line-arguments)\n  - [The CLI interface](run_security#the-cli-interface)\n  - [VCL programs](run_security#vcl-programs)\n  - [HTTP requests](run_security#http-requests)\n- [Required command line arguments](command-line)\n  - [‘-a’ listen_address](command-line#a-listen-address)\n  - [‘-f’ VCL-file or ‘-b’ backend](command-line#f-vcl-file-or-b-backend)\n  - [Optional arguments](command-line#optional-arguments)\n- [CLI - bossing Varnish around](run_cli)\n  - [What can you do with the CLI](run_cli#what-can-you-do-with-the-cli)\n- [Storage backends](storage-backends)\n  - [Intro](storage-backends#intro)\n  - [default](storage-backends#default)\n  - [malloc](storage-backends#malloc)\n  - [umem](storage-backends#umem)\n  - [file](storage-backends#file)\n  - [deprecated_persistent](storage-backends#deprecated-persistent)\n- [Transient Storage](storage-backends#transient-storage)\n- [Parameters](params)\n- [Sizing your cache](sizing-your-cache)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/running.html](https://varnish-cache.org/docs/7.4/users-guide/running.html)"
- name: Starting Varnish
  id: tutorial/starting_varnish
  summary: This tutorial will assume that you are running Varnish on Ubuntu, Debian, Red Hat Enterprise Linux or CentOS
  description: "# Starting Varnish\n\nThis tutorial will assume that you are running Varnish on Ubuntu, Debian, Red Hat Enterprise Linux or CentOS. Those of you running on other platforms might have to do some mental translation exercises in order to follow this. Since you’re on a “weird” platform you’re probably used to it. :-)\n\nMake sure you have Varnish successfully installed (following one of the procedures described in “Installing Varnish” above.\n\nWhen properly installed you start Varnish with `service varnish start`. This will start Varnish if it isn’t already running.\n\nNow you have Varnish running. Let us make sure that it works properly. Use your browser to go to [http://127.0.0.1:6081/](http://127.0.0.1:6081/) (Replace the IP address with the IP for the machine that runs Varnish) The default configuration will try to forward requests to a web application running on the same machine as Varnish was installed on. Varnish expects the web application to be exposed over http on port 8080.\n\nIf there is no web application being served up on that location Varnish will issue an error. Varnish Cache is very conservative about telling the world what is wrong so whenever something is amiss it will issue the same generic “Error 503 Service Unavailable”.\n\nYou might have a web application running on some other port or some other machine. Let’s edit the configuration and make it point to something that actually works.\n\nFire up your favorite editor and edit `/etc/varnish/default.vcl`. Most of it is commented out but there is some text that is not. It will probably look like this:\n\n``` python\nvcl 4.0;\n\nbackend default {\n    .host = \"127.0.0.1\";\n    .port = \"8080\";\n}\n```\n\nWe’ll change it and make it point to something that works. Hopefully [http://www.varnish-cache.org/](http://www.varnish-cache.org/) is up. Let’s use that. Replace the text with:\n\n``` python\nvcl 4.0;\n\nbackend default {\n    .host = \"www.varnish-cache.org\";\n    .port = \"80\";\n}\n```\n\nNow issue `service varnish reload` to make Varnish reload it’s configuration. If that succeeded visit [http://127.0.0.1:6081/](http://127.0.0.1:6081/) in your browser and you should see some directory listing. It works! The reason you’re not seeing the Varnish official website is because your client isn’t sending the appropriate `Host` header in the request and it ends up showing a listing of the default webfolder on the machine usually serving up [http://www.varnish-cache.org/](http://www.varnish-cache.org/) .\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/tutorial/starting_varnish.html](https://varnish-cache.org/docs/7.4/tutorial/starting_varnish.html)"
- name: Statistics
  id: users-guide/operation-statistics
  summary: Varnish comes with a couple of nifty and very useful statistics generating tools that generates statistics in real time by constantly updating and presenting a specific dataset by aggregating and analyzing logdata from the shared memory logs
  description: "# Statistics\n\nVarnish comes with a couple of nifty and very useful statistics generating tools that generates statistics in real time by constantly updating and presenting a specific dataset by aggregating and analyzing logdata from the shared memory logs.\n\n## varnishtop\n\nThe [varnishtop](../reference/varnishtop#varnishtop-1) utility reads the shared memory logs and presents a continuously updated list of the most commonly occurring log entries.\n\nWith suitable filtering using the -I, -i, -X and -x options, it can be used to display a ranking of requested documents, clients, user agents, or any other information which is recorded in the log.\n\n`varnishtop -i ReqURL` will show you what URLs are being asked for by the client. `varnishtop -i BereqURL` will show you what your backend is being asked the most. `varnishtop -I ReqHeader:Accept-Encoding` will show the most popular Accept-Encoding header the client are sending you.\n\n## varnishhist\n\nThe [varnishhist](../reference/varnishhist#varnishhist-1) utility reads [varnishd](../reference/varnishd#varnishd-1) shared memory logs and presents a continuously updated histogram showing the distribution of the last N requests by their processing. The value of N and the vertical scale are displayed in the top left corner. The horizontal scale is logarithmic. Hits are marked with a pipe character (“\\|”), and misses are marked with a hash character (“#”).\n\n## varnishstat\n\nVarnish has lots of counters. We count misses, hits, information about the storage, threads created, deleted objects. Just about everything. [varnishstat](../reference/varnishstat#varnishstat-1) will dump these counters. This is useful when tuning Varnish.\n\nThere are programs that can poll [varnishstat](../reference/varnishstat#varnishstat-1) regularly and make nice graphs of these counters. One such program is Munin. Munin can be found at [http://munin-monitoring.org/](http://munin-monitoring.org/) . There is a plugin for munin in the Varnish source code.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/operation-statistics.html](https://varnish-cache.org/docs/7.4/users-guide/operation-statistics.html)"
- name: Storage backends
  id: users-guide/storage-backends
  summary: Varnish has pluggable storage backends
  description: "# Storage backends\n\n## Intro\n\nVarnish has pluggable storage backends. It can store data in various backends which can have different performance characteristics. The default configuration is to use the malloc backend with a limited size. For a serious Varnish deployment you probably would want to adjust the storage settings.\n\n## default\n\nsyntax: default\\[,size\\]\n\nThe default storage backend is an alias to umem, where available, or malloc otherwise.\n\n## malloc\n\nsyntax: malloc\\[,size\\]\n\nMalloc is a memory based backend. Each object will be allocated from memory. If your system runs low on memory swap will be used.\n\nBe aware that the size limitation only limits the actual storage and that the approximately 1k of memory per object, used for various internal structures, is included in the actual storage as well.\n\nThe size parameter specifies the maximum amount of memory `varnishd` will allocate. The size is assumed to be in bytes, unless followed by one of the following suffixes:\n\nK, k The size is expressed in kibibytes.\n\nM, m The size is expressed in mebibytes.\n\nG, g The size is expressed in gibibytes.\n\nT, t The size is expressed in tebibytes.\n\nThe default size is unlimited.\n\nmalloc’s performance is bound to memory speed so it is very fast. If the dataset is bigger than available memory performance will depend on the operating systems ability to page effectively.\n\n## umem\n\nsyntax: umem\\[,size\\]\n\nUmem is a better alternative to the malloc backend where [libumem](http://dtrace.org/blogs/ahl/2004/07/13/number-11-of-20-libumem/) is available. All other configuration aspects are considered equal to malloc.\n\n[libumem](http://dtrace.org/blogs/ahl/2004/07/13/number-11-of-20-libumem/) implements a slab allocator similar to the kernel memory allocator used in virtually all modern operating systems and is considered more efficient and scalable than classical implementations. In particular, [libumem](http://dtrace.org/blogs/ahl/2004/07/13/number-11-of-20-libumem/) is included in the family of OpenSolaris descendent operating systems where jemalloc(3) is not commonly available.\n\nIf [libumem](http://dtrace.org/blogs/ahl/2004/07/13/number-11-of-20-libumem/) is not used otherwise, Varnish will only use it for storage allocations and keep the default libc allocator for all other Varnish memory allocation purposes.\n\nIf [libumem](http://dtrace.org/blogs/ahl/2004/07/13/number-11-of-20-libumem/) is already loaded when Varnish initializes, this message is output:\n\n``` python\nnotice: libumem was already found to be loaded\n        and will likely be used for all allocations\n```\n\nto indicate that [libumem](http://dtrace.org/blogs/ahl/2004/07/13/number-11-of-20-libumem/) will not only be used for storage. Likely reasons for this to be the case are:\n\n- some library `varnishd` is linked against was linked against [libumem](http://dtrace.org/blogs/ahl/2004/07/13/number-11-of-20-libumem/) (most likely `libpcre2-8`, check with `ldd`)\n- `LD_PRELOAD_64=/usr/lib/amd64/libumem.so.1`, `LD_PRELOAD_32=/usr/lib/libumem.so.1` or `LD_PRELOAD=/usr/lib/libumem.so.1` is set\n\nVarnish will also output this message to recommend settings for using [libumem](http://dtrace.org/blogs/ahl/2004/07/13/number-11-of-20-libumem/) for all allocations:\n\n``` python\nit is recommended to set UMEM_OPTIONS=perthread_cache=0,backend=mmap\nbefore starting varnish\n```\n\nThis recommendation should be followed to achieve an optimal [libumem](http://dtrace.org/blogs/ahl/2004/07/13/number-11-of-20-libumem/) configuration for Varnish. Setting this environment variable before starting Varnish is required because [libumem](http://dtrace.org/blogs/ahl/2004/07/13/number-11-of-20-libumem/) cannot be reconfigured once loaded.\n\n## file\n\nsyntax: file,path\\[,size\\[,granularity\\[,advice\\]\\]\\]\n\nThe file backend stores objects in virtual memory backed by an unlinked file on disk with `mmap`, relying on the kernel to handle paging as parts of the file are being accessed.\n\nThis implies that sufficient *virtual* memory needs to be available to accomodate the file size in addition to any memory Varnish requires anyway. Traditionally, the virtual memory limit is configured with `ulimit -v`, but modern operating systems have other abstractions for this limit like control groups (Linux) or resource controls (Solaris).\n\nThe ‘path’ parameter specifies either the path to the backing file or the path to a directory in which `varnishd` will create the backing file.\n\nThe size parameter specifies the size of the backing file. The size is assumed to be in bytes, unless followed by one of the following suffixes:\n\nK, k The size is expressed in kibibytes.\n\nM, m The size is expressed in mebibytes.\n\nG, g The size is expressed in gibibytes.\n\nT, t The size is expressed in tebibytes.\n\nIf ‘path’ points to an existing file and no size is specified, the size of the existing file will be used. If ‘path’ does not point to an existing file it is an error to not specify the size.\n\nIf the backing file already exists, it will be truncated or expanded to the specified size.\n\nNote that if `varnishd` has to create or expand the file, it will not pre-allocate the added space, leading to fragmentation, which may adversely impact performance on rotating hard drives. Pre-creating the storage file using `dd(1)` will reduce fragmentation to a minimum.\n\nThe ‘granularity’ parameter specifies the granularity of allocation. All allocations are rounded up to this size. The granularity is assumed to be expressed in bytes, unless followed by one of the suffixes described for size.\n\nThe default granularity is the VM page size. The size should be reduced if you have many small objects.\n\nFile performance is typically limited to the write speed of the device, and depending on use, the seek time.\n\nThe ‘advice’ parameter tells the kernel how `varnishd` expects to use this mapped region so that the kernel can choose the appropriate read-ahead and caching techniques. Possible values are `normal`, `random` and `sequential`, corresponding to MADV_NORMAL, MADV_RANDOM and MADV_SEQUENTIAL madvise() advice argument, respectively. Defaults to `random`.\n\nOn Linux, large objects and rotational disk should benefit from “sequential”.\n\n## deprecated_persistent\n\nsyntax: deprecated_persistent,path,size {experimental}\n\n*Before using, read* [A persistent message](https://varnish-cache.org/docs/7.4/phk/persistent.html#phk-persistent)*!*\n\nPersistent storage. Varnish will store objects in a file in a manner that will secure the survival of *most* of the objects in the event of a planned or unplanned shutdown of Varnish.\n\nThe ‘path’ parameter specifies the path to the backing file. If the file doesn’t exist Varnish will create it.\n\nThe ‘size’ parameter specifies the size of the backing file. The size is expressed in bytes, unless followed by one of the following suffixes:\n\nK, k The size is expressed in kibibytes.\n\nM, m The size is expressed in mebibytes.\n\nG, g The size is expressed in gibibytes.\n\nT, t The size is expressed in tebibytes.\n\nVarnish will split the file into logical *silos* and write to the silos in the manner of a circular buffer. Only one silo will be kept open at any given point in time. Full silos are *sealed*. When Varnish starts after a shutdown it will discard the content of any silo that isn’t sealed.\n\nNote that taking persistent silos offline and at the same time using bans can cause problems. This is due to the fact that bans added while the silo was offline will not be applied to the silo when it reenters the cache. Consequently enabling previously banned objects to reappear.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/storage-backends.html](https://varnish-cache.org/docs/7.4/users-guide/storage-backends.html)"
- name: The Big Varnish Picture
  id: users-guide/intro
  summary: The two main parts of Varnish are the two processes in the varnishd program
  description: "# The Big Varnish Picture\n\nIn this section we will cover answers to the questions: - What is in this package called “Varnish”? - what are all the different bits and pieces named? - Will you need a hex-wrench for assembly?\n\nThe two main parts of Varnish are the two processes in the `varnishd` program. The first process is called “the manager”, and its job is to talk to you, the administrator, and make the things you ask for happen.\n\nThe second process is called “the worker” or just “the child” and this is the process which does all the actual work with your HTTP traffic.\n\nWhen you start `varnishd`, you start the manager process, and once it is done handling all the command line flags, it will start the child process for you. Should the child process die, the manager will start it again for you, automatically and right away.\n\nThe main reason for this division of labor is security: The manager process will typically run with “root” permissions, in order to open TCP socket port 80, but it starts the child process with minimal permissions, as a defensive measure.\n\nThe manager process is interactive, it offers a CLI – Command Line Interface, which can be used manually, from scripts or programs. The CLI offers almost full control of what Varnish actually does to your HTTP traffic, and we have gone to great lengths to ensure that you should not need to restart the Varnish processes, unless you need to change something very fundamental.\n\nThe CLI can be safely accessed remotely, using a simple and flexible PSK – Pre Shared Key, access control scheme, so it is easy to integrate Varnish into your operations and management infrastructure or tie it to your CMS.\n\nAll this is covered in [Starting and running Varnish](running#users-running).\n\nThings like, how the child process should deal with the HTTP requests, what to cache, which headers to remove etc, is all specified using a small programming language called VCL – Varnish Configuration Language. The manager process will compile the VCL program and check it for errors,\n\nbut it is the child process which runs the VCL program, for each and every HTTP request which comes in.\n\nBecause the VCL is compiled to C code, and the C code is compiled to machine instructions, even very complex VCL programs execute in a few microseconds, without impacting performance at all.\n\nAnd don’t fret if you are not really a programmer, VCL is very simple to do simple things with:\n\n``` python\nsub vcl_recv {\n        # Remove the cookie header to enable caching\n        unset req.http.cookie;\n}\n```\n\nThe CLI interface allows you to compile and load new VCL programs at any time, and you can switch between the loaded VCL programs instantly, without restarting the child process and without missing a single HTTP request.\n\nVCL code can be extended using external modules, called VMODs or even by inline C-code if you are brave, so in terms of what Varnish can do for your HTTP traffic, there really is no limit.\n\n[VCL - Varnish Configuration Language](vcl#users-vcl) describes VCL and what it can do in great detail.\n\nVarnish uses a segment of shared memory to report and log its activities and status. For each HTTP request, a number of very detailed records will be appended to the log memory segment. Other processes can subscribe to log-records, filter them, and format them, for instance as Apache/NCSA style log records.\n\nAnother segment in shared memory is used for statistics counters, this allows real-time, down to microsecond resolution monitoring of cache hit-rate, resource usage and specific performance indicating metrics.\n\nVarnish comes with a number of tools which reports from shared memory, `varnishlog`, `varnishstats`, `varnishncsa` etc, and with an API library so you can write your own tools, should you need that.\n\n[Reporting and statistics](report#users-report) explains how all that work.\n\nPresumably the reason for your interest in Varnish, is that you want your website to work better. There are many aspects of performance tuning a website, from relatively simple policy decisions about what to cache, to designing a geographically diverse multilevel CDNs using ESI and automatic failover.\n\n[Varnish and Website Performance](performance#users-performance) will take you through the possibilities and facilities Varnish offers.\n\nFinally, Murphys Law must be referenced here: Things will go wrong, and more likely than not, they will do so at zero-zero-dark O’clock. Most likely during a hurricane, when your phone battery is flat and your wife had prepared a intimate evening to celebrate your anniversary.\n\nYes, we’ve all been there, haven’t we?\n\nWhen things go wrong [Troubleshooting Varnish](troubleshooting#users-trouble) will hopefully be of some help.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/intro.html](https://varnish-cache.org/docs/7.4/users-guide/intro.html)"
- name: The Varnish Developers Guide
  id: dev-guide/index
  summary: This is the deliberately short and to the point list of things Varnish Developers should know
  description: "# The Varnish Developers Guide\n\nThis is the deliberately short and to the point list of things Varnish Developers should know.\n\n## Behaviour\n\n- Be sensible.\n- If in doubt, think.\n- If still in doubt, ask.\n- Admit your mistakes, it’s faster that way.\n- Thou SHALL not paint [bikesheds.](http://bikeshed.org/)\n- We will toss you out of the project rather than add another rule.\n\n## Technical stuff\n\n- Our coding style guideline is FreeBSD’s [style(9)](https://www.freebsd.org/cgi/man.cgi?query=style&sektion=9)\n- See autogen.des script for developer options to the toolchain.\n- We always -Werror, there are no harmless warnings, only source code that does not express intent well enough.\n- We prefer the source code, rather than the comments explain what is going on, that way tools like FlexeLint and Coverity also gets a chance.\n- Our reference platforms are Ubuntu and FreeBSD.\n- Asserts have negative cost, they save developer time next time around.\n- Our license is BSD 2-clause or looser, no GPL or LGPL.\n- It took 11 years for the first major security issue, and that was too soon.\n\n## Bugs, issues, feature requests & VIPs\n\nBugs, issues and feature requests start out as github issues.\n\nMonday at 15:00-16:00 (EU time) we “bug-wash” on IRC (#varnish-hacking on irc.linpro.no) to decide who and how issues are dealt with.\n\nIssues we cannot do anything about are closed.\n\nIf feature-requests make sense, they get moved to a wiki/VIP page until somebody implements them.\n\nVarnishtest cases for bugs is the norm, not the exception.\n\n## Architectural stuff\n\nThese rules are imported from the X11 project:\n\n- It is as important to decide what a system is not as to decide what it is.\n- Do not serve all the world’s needs; rather, make the system extensible so that additional needs can be met in an upwardly compatible fashion.\n- The only thing worse than generalizing from one example is generalizing from no examples at all.\n- If a problem is not completely understood, it is probably best to provide no solution at all.\n- If you can get 90 percent of the desired effect for 10 percent of the work, use the simpler solution.\n- Isolate complexity as much as possible.\n- Provide mechanism, rather than policy.\n\n## Various policies\n\n- [Varnish Cache organization and day-to-day operation](https://varnish-cache.org/organization/index.html)\n- [Bundling VMODs with the Varnish distribution](policy_vmods)\n\n## The varnish-cache.org homepage\n\n- [How our website works](homepage_dogfood)\n- [How to contribute content to varnish-cache.org](homepage_contrib)\n\n## Project metadata\n\n- [Who is … ?](who)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/dev-guide/index.html](https://varnish-cache.org/docs/7.4/dev-guide/index.html)"
- name: The Varnish Reference Manual
  id: reference/index
  summary: A collection of VCL Design Patterns is available in addition to these reference manuals
  description: "# The Varnish Reference Manual\n\n## The VCL language\n\n- [VCL - The Varnish Configuration Language](vcl)\n- [VCL Variables](vcl-var)\n- [VCL Steps](vcl-step)\n- [VCL backend configuration](vcl-backend)\n- [VCL backend health probe](vcl-probe)\n- [Varnish Processing States](states)\n\nA collection of [VCL Design Patterns](../vcl-design-patterns/index#vcl-design-patterns-index) is available in addition to these reference manuals.\n\n## Bundled VMODs\n\n- [VMOD blob - Utilities for the VCL blob type, encoding and decoding](vmod_blob)\n- [VMOD cookie - Varnish Cookie Module](vmod_cookie)\n- [VMOD directors - Varnish Directors Module](vmod_directors)\n- [VMOD proxy - Varnish Module to extract TLV attributes from PROXYv2](vmod_proxy)\n- [VMOD purge - Varnish Purge Module](vmod_purge)\n- [VMOD std - Varnish Standard Module](vmod_std)\n- [VMOD unix - Utilities for Unix domain sockets](vmod_unix)\n\n## The CLI interface\n\n- [VarnishAdm - Control program for Varnish](varnishadm)\n- [CLI - The commands varnish understands](varnish-cli)\n\n## Logging and monitoring\n\n- [VSL - The log records Varnish generates](vsl)\n- [VSLQ - Filter/Query expressions for VSL](vsl-query)\n- [VarnishLog - Logging raw VSL](varnishlog)\n- [VarnishNCSA - Logging in NCSA format](varnishncsa)\n- [VarnishHist - Realtime response histogram display](varnishhist)\n- [VarnishTop - Realtime activity display](varnishtop)\n\n## Counters and statistics\n\n- [VSC - The statistics Varnish collects](varnish-counters)\n- [VarnishStat - Watching and logging statistics](varnishstat)\n\n## The Varnishd program\n\n- [VarnishD - The program which does the actual work](varnishd)\n\n## Varnishtest\n\n- [VTC - Language for writing test cases](vtc)\n- [VarnishTest - execute test cases](varnishtest)\n- [VMOD vtc - Utility module for varnishtest](vmod_vtc)\n\n## For Developers & DevOps\n\n- [Shell tricks](shell_tricks)\n- [VMODS - Extensions to VCL](vmod)\n- [VEXT - Varnish Extensions](vext)\n- [VSM - Shared memory use](vsm)\n- [VDIR - Backends & Directors](directors)\n- [VCLI - CLI protocol API](cli_protocol)\n\n## Code-book\n\n- [VTLA - Varnish Three Letter Acronyms](vtla)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/index.html](https://varnish-cache.org/docs/7.4/reference/index.html)"
- name: The Varnish Tutorial
  id: tutorial/index
  summary: This section covers the Varnish basics in a tutorial form
  description: "# The Varnish Tutorial\n\nThis section covers the Varnish basics in a tutorial form. It will cover what Varnish is and how it works. It also covers how to get Varnish up and running. After this section you probably would want to continue with the users guide ([The Varnish Users Guide](../users-guide/index#users-guide-index)).\n\nIf you’re reading this on the web note the “Next topic” and “Previous topic” links on the right side of each page.\n\n- [Varnish: The beef in the sandwich](introduction)\n- [How Varnish works](introduction#how-varnish-works)\n- [Caching with Varnish](introduction#caching-with-varnish)\n- [Content Composition with Varnish](introduction#content-composition-with-varnish)\n- [Content Policy with Varnish](introduction#content-policy-with-varnish)\n- [Varnish is general purpose](introduction#varnish-is-general-purpose)\n- [Starting Varnish](starting_varnish)\n- [Put Varnish on port 80](putting_varnish_on_port_80)\n- [Restarting Varnish again](putting_varnish_on_port_80#restarting-varnish-again)\n- [Backend servers](backend_servers)\n- [Peculiarities](peculiarities)\n- [Now what?](now_what)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/tutorial/index.html](https://varnish-cache.org/docs/7.4/tutorial/index.html)"
- name: The Varnish Users Guide
  id: users-guide/index
  summary: Starting and running Varnish is about getting Varnish configured, with respect to storage, sockets, security and how you can control and communicate with Varnish once it is running
  description: "# The Varnish Users Guide\n\nThe Varnish documentation consists of three main documents:\n\n- [The Varnish Tutorial](../tutorial/index#tutorial-index) explains the basics and gets you started with Varnish.\n- [The Varnish Users Guide](#users-guide-index) (this document), explains how Varnish works and how you can use it to improve your website.\n- [The Varnish Reference Manual](../reference/index#reference-index) contains hard facts and is useful for looking up specific questions.\n\nAfter [The Big Varnish Picture](intro#users-intro), this Users Guide is organized in sections following the major interfaces to Varnish as a service:\n\n[Starting and running Varnish](running#users-running) is about getting Varnish configured, with respect to storage, sockets, security and how you can control and communicate with Varnish once it is running.\n\n[VCL - Varnish Configuration Language](vcl#users-vcl) is about getting Varnish to handle the HTTP requests the way you want, what to cache, how to cache it, modifying HTTP headers etc. etc.\n\n[Reporting and statistics](report#users-report) explains how you can monitor what Varnish does, from a transactional level to aggregating statistics.\n\n[Varnish and Website Performance](performance#users-performance) is about tuning your website with Varnish.\n\n[Troubleshooting Varnish](troubleshooting#users-trouble) is for locating and fixing common issues with Varnish.\n\n- [The Big Varnish Picture](intro)\n- [Starting and running Varnish](running)\n  - [Security first](run_security)\n  - [Required command line arguments](command-line)\n  - [CLI - bossing Varnish around](run_cli)\n  - [Storage backends](storage-backends)\n  - [Transient Storage](storage-backends#transient-storage)\n  - [Parameters](params)\n  - [Sizing your cache](sizing-your-cache)\n- [VCL - Varnish Configuration Language](vcl)\n  - [VCL Syntax](vcl-syntax)\n  - [Built-in VCL](vcl-built-in-code)\n  - [Request and response VCL objects](vcl-variables)\n  - [Backend servers](vcl-backends)\n  - [The “none” backend](vcl-backends#the-none-backend)\n  - [Multiple backends](vcl-backends#multiple-backends)\n  - [Backends and virtual hosts in Varnish](vcl-backends#backends-and-virtual-hosts-in-varnish)\n  - [Connecting Through a Proxy](vcl-backends#connecting-through-a-proxy)\n  - [Directors](vcl-backends#directors)\n  - [Health checks](vcl-backends#health-checks)\n  - [Layering](vcl-backends#layering)\n  - [Director Resolution](vcl-backends#director-resolution)\n  - [Connection Pooling](vcl-backends#connection-pooling)\n  - [Hashing](vcl-hashing)\n  - [Grace mode and keep](vcl-grace)\n  - [Separate VCL files](vcl-separate)\n  - [Using inline C to extend Varnish](vcl-inline-c)\n  - [VCL Examples](vcl-examples)\n  - [Device detection](devicedetection)\n- [Reporting and statistics](report)\n  - [Logging in Varnish](operation-logging)\n  - [Statistics](operation-statistics)\n- [Varnish and Website Performance](performance)\n  - [Achieving a high hitrate](increasing-your-hitrate)\n  - [The role of HTTP Headers](increasing-your-hitrate#the-role-of-http-headers)\n  - [HTTP Vary](increasing-your-hitrate#http-vary)\n  - [Cache misses](increasing-your-hitrate#cache-misses)\n  - [Uncacheable content](increasing-your-hitrate#uncacheable-content)\n  - [Purging and banning](purging)\n  - [Compression](compression)\n- [Content composition with Edge Side Includes](esi)\n  - [Example: esi:include](esi#example-esi-include)\n  - [Example: esi:remove and \\<!–esi … –\\>](esi#example-esi-remove-and-esi)\n  - [What happens when it fails ?](esi#what-happens-when-it-fails)\n  - [Can an ESI fragment also use ESI-includes ?](esi#can-an-esi-fragment-also-use-esi-includes)\n  - [Doing ESI on JSON and other non-XML’ish content](esi#doing-esi-on-json-and-other-non-xml-ish-content)\n  - [Ignoring BOM in ESI objects](esi#ignoring-bom-in-esi-objects)\n  - [ESI on invalid XML](esi#esi-on-invalid-xml)\n  - [ESI includes with HTTPS protocol](esi#esi-includes-with-https-protocol)\n  - [ESI on partial responses (206)](esi#esi-on-partial-responses-206)\n  - [ESI and return(vcl(…))](esi#esi-and-return-vcl)\n  - [ESI and gzip compression](esi#esi-and-gzip-compression)\n- [Troubleshooting Varnish](troubleshooting)\n  - [When Varnish won’t start](troubleshooting#when-varnish-won-t-start)\n  - [Varnish is crashing - panics](troubleshooting#varnish-is-crashing-panics)\n  - [Varnish is crashing - stack overflows](troubleshooting#varnish-is-crashing-stack-overflows)\n  - [Varnish is crashing - segfaults](troubleshooting#varnish-is-crashing-segfaults)\n  - [Varnish gives me Guru meditation](troubleshooting#varnish-gives-me-guru-meditation)\n  - [Varnish doesn’t cache](troubleshooting#varnish-doesn-t-cache)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/index.html](https://varnish-cache.org/docs/7.4/users-guide/index.html)"
- name: Troubleshooting Varnish
  id: users-guide/troubleshooting
  summary: Sometimes Varnish misbehaves or rather behaves the way you told it to behave but not necessarily the way you want it to behave
  description: "# Troubleshooting Varnish\n\nSometimes Varnish misbehaves or rather behaves the way you told it to behave but not necessarily the way you want it to behave. In order for you to understand whats going on there are a couple of places you can check. [varnishlog](../reference/varnishlog#varnishlog-1), `/var/log/syslog`, `/var/log/messages` are all good places where Varnish might leave clues of whats going on. This section will guide you through basic troubleshooting in Varnish.\n\n## When Varnish won’t start\n\nSometimes Varnish wont start. There is a plethora of possible reasons why Varnish wont start on your machine. We’ve seen everything from wrong permissions on `/dev/null` to other processes blocking the ports.\n\nStarting Varnish in debug mode to see what is going on.\n\nTry to start Varnish with the same arguments as otherwise, but `-d` added. This will give you some more information on what is going on. Let us see how Varnish will react when something else is listening on its port.:\n\n``` python\n# varnishd -n foo -f /usr/local/etc/varnish/default.vcl -s malloc,1G -T 127.0.0.1:2000  -a 0.0.0.0:8080 -d\nstorage_malloc: max size 1024 MB.\nUsing old SHMFILE\nPlatform: Linux,2.6.32-21-generic,i686,-smalloc,-hcritbit\n200 193\n-----------------------------\nVarnish Cache CLI.\n-----------------------------\nType 'help' for command list.\nType 'quit' to close CLI session.\nType 'start' to launch worker process.\n```\n\nNow Varnish is running but only the master process is running, in debug mode the cache does not start. Now you’re on the console. You can instruct the master process to start the cache by issuing “start”.:\n\n``` python\nstart\nbind(): Address already in use\n300 22\nCould not open sockets\n```\n\nAnd here we have our problem. Something else is bound to the HTTP port of Varnish. If this doesn’t help try `strace` or `truss` or come find us on IRC.\n\n## Varnish is crashing - panics\n\nWhen Varnish goes bust the child processes crashes. Most of the crashes are caught by one of the many consistency checks we have included in the Varnish source code. When Varnish hits one of these the caching process will crash itself in a controlled manner, leaving a nice stack trace with the mother process.\n\nYou can inspect any panic messages by typing `panic.show` in the CLI.:\n\n``` python\npanic.show\nLast panic at: Tue, 15 Mar 2011 13:09:05 GMT\nAssert error in ESI_Deliver(), cache_esi_deliver.c line 354:\n  Condition(i == Z_OK || i == Z_STREAM_END) not true.\nthread = (cache-worker)\nident = Linux,2.6.32-28-generic,x86_64,-sfile,-smalloc,-hcritbit,epoll\nBacktrace:\n  0x42cbe8: pan_ic+b8\n  0x41f778: ESI_Deliver+438\n  0x42f838: RES_WriteObj+248\n  0x416a70: cnt_deliver+230\n  0x4178fd: CNT_Session+31d\n  (..)\n```\n\nThe crash might be due to misconfiguration or a bug. If you suspect it is a bug you can use the output in a bug report, see the “Trouble Tickets” section in the Introduction chapter above.\n\n## Varnish is crashing - stack overflows\n\nBugs put aside, the most likely cause of crashes are stack overflows, which is why we have added a heuristic to add a note when a crash looks like it was caused by one. In this case, the panic message contains something like this:\n\n``` python\nSignal 11 (Segmentation fault) received at 0x7f631f1b2f98 si_code 1\nTHIS PROBABLY IS A STACK OVERFLOW - check thread_pool_stack parameter\n```\n\nas a first measure, please follow this advise and check if crashes still occur when you add 128k to whatever the value of the `thread_pool_stack` parameter and restart varnish.\n\nIf varnish stops crashing with a larger `thread_pool_stack` parameter, it’s not a bug (at least most likely).\n\n## Varnish is crashing - segfaults\n\nSometimes a bug escapes the consistency checks and Varnish gets hit with a segmentation error. When this happens with the child process it is logged, the core is dumped and the child process starts up again.\n\nA core dumped is usually due to a bug in Varnish. However, in order to debug a segfault the developers need you to provide a fair bit of data.\n\n- Make sure you have Varnish installed with debugging symbols.\n\n- Check where your operating system writes core files and ensure that you actually get them. For example on linux, learn about `/proc/sys/kernel/core_pattern` from the `core(5)` manpage.\n\n- Make sure core dumps are allowed in the parent shell from which varnishd is being started. In shell, this would be:\n\n  ``` python\n  ulimit -c unlimited\n  ```\n\n  but if varnish is started from an init-script, that would need to be adjusted or in the case of systemd, `LimitCORE=infinity` set in the service’s `[Service]]` section of the unit file.\n\nOnce you have the core, `cd` into varnish’s working directory (as given by the `-n` parameter, whose default is `$PREFIX/var/varnish/$HOSTNAME` with `$PREFIX` being the installation prefix, usually `/usr/local`, open the core with `gdb` and issue the command `bt` to get a stack trace of the thread that caused the segfault.\n\nA basic debug session for varnish installed under `/usr/local` could look like this:\n\n``` python\n$ cd /usr/local/var/varnish/`uname -n`/\n$ gdb /usr/local/sbin/varnishd core\nGNU gdb (Debian 7.12-6) 7.12.0.20161007-git\nCopyright (C) 2016 Free Software Foundation, Inc.\n[...]\nCore was generated by `/usr/local/sbin/varnishd -a 127.0.0.1:8080 -b 127.0.0.1:8080'.\nProgram terminated with signal SIGABRT, Aborted.\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\n51      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\n[Current thread is 1 (Thread 0x7f7749ea3700 (LWP 31258))]\n\n(gdb) bt\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\n#1  0x00007f775132342a in __GI_abort () at abort.c:89\n#2  0x000000000045939f in pan_ic (func=0x7f77439fb811 \"VCL\", file=0x7f77439fb74c \"\", line=0,\n    cond=0x7f7740098130 \"PANIC: deliberately!\", kind=VAS_VCL) at cache/cache_panic.c:839\n#3  0x0000000000518cb1 in VAS_Fail (func=0x7f77439fb811 \"VCL\", file=0x7f77439fb74c \"\", line=0,\n    cond=0x7f7740098130 \"PANIC: deliberately!\", kind=VAS_VCL) at vas.c:51\n#4  0x00007f77439fa6e9 in vmod_panic (ctx=0x7f7749ea2068, str=0x7f7749ea2018) at vmod_vtc.c:109\n#5  0x00007f77449fa5b8 in VGC_function_vcl_recv (ctx=0x7f7749ea2068) at vgc.c:1957\n#6  0x0000000000491261 in vcl_call_method (wrk=0x7f7749ea2dd0, req=0x7f7740096020, bo=0x0,\n    specific=0x0, method=2, func=0x7f77449fa550 <VGC_function_vcl_recv>) at cache/cache_vrt_vcl.c:462\n#7  0x0000000000493025 in VCL_recv_method (vcl=0x7f775083f340, wrk=0x7f7749ea2dd0, req=0x7f7740096020,\n    bo=0x0, specific=0x0) at ../../include/tbl/vcl_returns.h:192\n#8  0x0000000000462979 in cnt_recv (wrk=0x7f7749ea2dd0, req=0x7f7740096020) at cache/cache_req_fsm.c:880\n#9  0x0000000000461553 in CNT_Request (req=0x7f7740096020) at ../../include/tbl/steps.h:36\n#10 0x00000000004a7fc6 in HTTP1_Session (wrk=0x7f7749ea2dd0, req=0x7f7740096020)\n    at http1/cache_http1_fsm.c:417\n#11 0x00000000004a72c3 in http1_req (wrk=0x7f7749ea2dd0, arg=0x7f7740096020)\n    at http1/cache_http1_fsm.c:86\n#12 0x0000000000496bb6 in Pool_Work_Thread (pp=0x7f774980e140, wrk=0x7f7749ea2dd0)\n    at cache/cache_wrk.c:406\n#13 0x00000000004963e3 in WRK_Thread (qp=0x7f774980e140, stacksize=57344, thread_workspace=2048)\n    at cache/cache_wrk.c:144\n#14 0x000000000049610b in pool_thread (priv=0x7f774880ec80) at cache/cache_wrk.c:439\n#15 0x00007f77516954a4 in start_thread (arg=0x7f7749ea3700) at pthread_create.c:456\n#16 0x00007f77513d7d0f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\n```\n\n## Varnish gives me Guru meditation\n\nFirst find the relevant log entries in [varnishlog](../reference/varnishlog#varnishlog-1). That will probably give you a clue. Since [varnishlog](../reference/varnishlog#varnishlog-1) logs a lot of data it might be hard to track the entries down. You can set [varnishlog](../reference/varnishlog#varnishlog-1) to log all your 503 errors by issuing the following command:\n\n``` python\n$ varnishlog -q 'RespStatus == 503' -g request\n```\n\nIf the error happened just a short time ago the transaction might still be in the shared memory log segment. To get [varnishlog](../reference/varnishlog#varnishlog-1) to process the whole shared memory log just add the ‘-d’ parameter:\n\n``` python\n$ varnishlog -d -q 'RespStatus == 503' -g request\n```\n\nPlease see the [vsl-query](../reference/vsl-query#vsl-query-7) and [varnishlog](../reference/varnishlog#varnishlog-1) man pages for elaborations on further filtering capabilities and explanation of the various options.\n\n## Varnish doesn’t cache\n\nSee [Achieving a high hitrate](increasing-your-hitrate#users-guide-increasing-your-hitrate).\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/troubleshooting.html](https://varnish-cache.org/docs/7.4/users-guide/troubleshooting.html)"
- name: Using extra digits in resp.status
  id: vcl-design-patterns/resp-status
  summary: Copyright © 2006 Verdens Gang AS Copyright © 2006–2020 Varnish Software AS Licensed under the BSD-2-Clause License
  description: "# Using extra digits in resp.status\n\nIn Varnish the `.status` variables can hold more than three digits, which is useful to send information to `vcl_synth{}` about which error message to produce:\n\n``` python\nsub vcl_recv {\n   if ([...]) {\n       return(synth(12404));\n   }\n}\n\nsub vcl_synth {\n    if (resp.status == 12404) {\n        [...]       // this specific 404\n    } else if (resp.status % 1000 == 404) {\n        [...]       // all other 404's\n    }\n}\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/vcl-design-patterns/resp-status.html](https://varnish-cache.org/docs/7.4/vcl-design-patterns/resp-status.html)"
- name: Using inline C to extend Varnish
  id: users-guide/vcl-inline-c
  summary: (Here there be dragons
  description: "# Using inline C to extend Varnish\n\n(Here there be dragons. Big and mean ones.)\n\nYou can use *inline C* to extend Varnish. Please note that you can seriously mess up Varnish this way. The C code runs within the Varnish Cache process so if your code generates a segfault the cache will crash.\n\nOne of the first uses of inline C was logging to `syslog`.:\n\n``` python\n# The include statements must be outside the subroutines.\nC{\n        #include <syslog.h>\n}C\n\nsub vcl_something {\n        C{\n                syslog(LOG_INFO, \"Something happened at VCL line XX.\");\n        }C\n}\n```\n\nTo use inline C you need to enable it with the `vcc_allow_inline_c` parameter.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-inline-c.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-inline-c.html)"
- name: Varnish and Website Performance
  id: users-guide/performance
  summary: This section focuses on how to tune the performance of your Varnish server, and how to tune the performance of your website using Varnish
  description: "# Varnish and Website Performance\n\nThis section focuses on how to tune the performance of your Varnish server, and how to tune the performance of your website using Varnish.\n\nThe section is split in three subsections. The first subsection deals with the various tools and functions of Varnish that you should be aware of. The next subsection focuses on the how to purge content out of your cache. Purging of content is essential in a performance context because it allows you to extend the *time-to-live* (TTL) of your cached objects. Having a long TTL allows Varnish to keep the content in cache longer, meaning Varnish will make fewer requests to your relatively slower backend.\n\nThe final subsection deals with compression of web content. Varnish can gzip content when fetching it from the backend and then deliver it compressed. This will reduce the time it takes to download the content thereby increasing the performance of your website.\n\n- [Achieving a high hitrate](increasing-your-hitrate)\n  - [Tool: varnishtop](increasing-your-hitrate#tool-varnishtop)\n  - [Tool: varnishlog](increasing-your-hitrate#tool-varnishlog)\n  - [Tool: lwp-request](increasing-your-hitrate#tool-lwp-request)\n  - [Tool: Live HTTP Headers](increasing-your-hitrate#tool-live-http-headers)\n- [The role of HTTP Headers](increasing-your-hitrate#the-role-of-http-headers)\n  - [Cookies](increasing-your-hitrate#cookies)\n  - [Cache-Control](increasing-your-hitrate#cache-control)\n  - [Age](increasing-your-hitrate#age)\n  - [Pragma](increasing-your-hitrate#pragma)\n  - [Authorization](increasing-your-hitrate#authorization)\n  - [Overriding the time-to-live (TTL)](increasing-your-hitrate#overriding-the-time-to-live-ttl)\n  - [Forcing caching for certain requests and certain responses](increasing-your-hitrate#forcing-caching-for-certain-requests-and-certain-responses)\n  - [Normalizing your namespace](increasing-your-hitrate#normalizing-your-namespace)\n- [HTTP Vary](increasing-your-hitrate#http-vary)\n  - [Vary parse errors](increasing-your-hitrate#vary-parse-errors)\n  - [Pitfall - Vary: User-Agent](increasing-your-hitrate#pitfall-vary-user-agent)\n- [Cache misses](increasing-your-hitrate#cache-misses)\n- [Uncacheable content](increasing-your-hitrate#uncacheable-content)\n  - [Passing client requests](increasing-your-hitrate#passing-client-requests)\n  - [hit-for-miss](increasing-your-hitrate#hit-for-miss)\n  - [hit-for-pass](increasing-your-hitrate#hit-for-pass)\n- [Purging and banning](purging)\n  - [HTTP Purging](purging#http-purging)\n  - [Bans](purging#bans)\n  - [Forcing a cache miss](purging#forcing-a-cache-miss)\n- [Compression](compression)\n  - [Default behaviour](compression#default-behaviour)\n  - [Compressing content if backends don’t](compression#compressing-content-if-backends-don-t)\n  - [Uncompressing content before entering the cache](compression#uncompressing-content-before-entering-the-cache)\n  - [GZIP and ESI](compression#gzip-and-esi)\n  - [Turning off gzip support](compression#turning-off-gzip-support)\n  - [A random outburst](compression#a-random-outburst)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/performance.html](https://varnish-cache.org/docs/7.4/users-guide/performance.html)"
- name: Varnish Installation
  id: installation/index
  summary: This section covers installation prerequisites, a step-by-step installation procedure, how and where to get help, and how to report bugs
  description: "# Varnish Installation\n\nThis section covers installation prerequisites, a step-by-step installation procedure, how and where to get help, and how to report bugs.\n\n- [Prerequisites](prerequisites)\n- [Installing Varnish](install)\n  - [Installing on Debian/Ubuntu](install_debian)\n  - [Installing on FreeBSD](install_freebsd)\n  - [Installing on OpenBSD](install_openbsd)\n  - [Installing on RedHat or CentOS](install_redhat)\n- [Compiling Varnish from source](install#compiling-varnish-from-source)\n  - [Compiling Varnish from source](install_source)\n- [Other pre-built Varnish packages](install#other-pre-built-varnish-packages)\n- [Getting help](help)\n  - [IRC Channel](help#irc-channel)\n  - [Mailing Lists](help#mailing-lists)\n  - [Trouble Tickets](help#trouble-tickets)\n  - [Commercial Support](help#commercial-support)\n- [Reporting bugs](bugs)\n  - [Varnish crashes](bugs#varnish-crashes)\n  - [Varnish goes on vacation](bugs#varnish-goes-on-vacation)\n  - [Varnish does something wrong](bugs#varnish-does-something-wrong)\n- [Platform specific notes](platformnotes)\n  - [Transparent hugepages on Redhat Enterprise Linux 6](platformnotes#transparent-hugepages-on-redhat-enterprise-linux-6)\n  - [OpenVZ](platformnotes#openvz)\n  - [TCP keep-alive configuration](platformnotes#tcp-keep-alive-configuration)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/installation/index.html](https://varnish-cache.org/docs/7.4/installation/index.html)"
- name: Varnish Processing States
  id: reference/states
  summary: Varnish processing of client and backend requests is implemented as state machines
  description: "# Varnish Processing States\n\n## Introduction\n\nVarnish processing of client and backend requests is implemented as state machines. Whenever a state is entered, a C function is called, which in turn calls the appropriate Varnish core code functions to process the request or response at this stage. For most states, core code also calls into a state-specific function compiled from VCL, a VCL subroutine (see [VCL Steps](vcl-step#id7)).\n\nAs a general guideline, core code aims to prepare objects accessible from VCL with good defaults for the most common cases before calling into the respective VCL subroutine. These can then be modified from VCL where necessary.\n\nThe following graphs attempt to provide an overview over the processing states, their transitions and the most relevant functions in core code. They represent a compromise between usefulness for core/VMOD developers and administrators and are intended to serve as the reference basis for derivative work, such as more VCL-centric views.\n\n## Client Side\n\n## Backend Side\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/states.html](https://varnish-cache.org/docs/7.4/reference/states.html)"
- name: varnish-cli
  id: reference/varnish-cli
  summary: Varnish has a command line interface (CLI) which can control and change most of the operational parameters and the configuration of Varnish, without interrupting the running service
  description: "# varnish-cli\n\n## Varnish Command Line Interface\n\nManual section:  \n7\n\n### DESCRIPTION\n\nVarnish has a command line interface (CLI) which can control and change most of the operational parameters and the configuration of Varnish, without interrupting the running service.\n\nThe CLI can be used for the following tasks:\n\nconfiguration  \nYou can upload, change and delete VCL files from the CLI.\n\nparameters  \nYou can inspect and change the various parameters Varnish has available through the CLI. The individual parameters are documented in the varnishd(1) man page.\n\nbans  \nBans are filters that are applied to keep Varnish from serving stale content. When you issue a ban Varnish will not serve any *banned* object from cache, but rather re-fetch it from its backend servers.\n\nprocess management  \nYou can stop and start the cache (child) process though the CLI. You can also retrieve the latest stack trace if the child process has crashed.\n\nIf you invoke varnishd(1) with -T, -M or -d the CLI will be available. In debug mode (-d) the CLI will be in the foreground, with -T you can connect to it with varnishadm or telnet and with -M varnishd will connect back to a listening service *pushing* the CLI to that service. Please see [varnishd](varnishd#varnishd-1) for details.\n\n#### Syntax\n\nThe Varnish CLI is similar to another command line interface, the Bourne Shell. Commands are usually terminated with a newline, and they may take arguments. The command and its arguments are *tokenized* before parsing, and as such arguments containing spaces must be enclosed in double quotes.\n\nIt means that command parsing of\n\n``` python\nhelp banner\n```\n\nis equivalent to\n\n``` python\n\"help\" banner\n```\n\nbecause the double quotes only indicate the boundaries of the `help` token.\n\nWithin double quotes you can escape characters with \\\\ (backslash). The \\n, \\r, and \\t get translated to newlines, carriage returns, an tabs. Double quotes and backslashes themselves can be escaped with \\\\ and \\\\ respectively.\n\nTo enter characters in octals use the \\nnn syntax. Hexadecimals can be entered with the \\xnn syntax.\n\nCommands may not end with a newline when a shell-style *here document* (here-document or heredoc) is used. The format of a here document is:\n\n``` python\n<< word\n     here document\nword\n```\n\n*word* can be any continuous string chosen to make sure it doesn’t appear naturally in the following *here document*. Traditionally EOF or END is used.\n\n#### Quoting pitfalls\n\nIntegrating with the Varnish CLI can be sometimes surprising when quoting is involved. For instance in Bourne Shell the delimiter used with here documents may or may not be separated by spaces from the `<<` token:\n\n``` python\ncat <<EOF\nhello\nworld\nEOF\nhello\nworld\n```\n\nWith the Varnish CLI, the `<<` and `EOF` tokens must be separated by at least one blank:\n\n``` python\nvcl.inline boot <<EOF\n106 258\nMessage from VCC-compiler:\nVCL version declaration missing\nUpdate your VCL to Version 4 syntax, and add\n        vcl 4.0;\non the first line of the VCL files.\n('<vcl.inline>' Line 1 Pos 1)\n<<EOF\n##---\n\nRunning VCC-compiler failed, exited with 2\nVCL compilation failed\n```\n\nWith the missing space, the here document can be added and the actual VCL can be loaded:\n\n``` python\nvcl.inline test << EOF\nvcl 4.0;\n\nbackend be {\n        .host = \"localhost\";\n}\nEOF\n200 14\nVCL compiled.\n```\n\nA big difference with a shell here document is the handling of the `<<` token. Just like command names can be quoted, the here document token keeps its meaning, even quoted:\n\n``` python\nvcl.inline test \"<<\" EOF\nvcl 4.0;\n\nbackend be {\n        .host = \"localhost\";\n}\nEOF\n200 14\nVCL compiled.\n```\n\nWhen using a front-end to the Varnish-CLI like `varnishadm`, one must take into account the double expansion happening. First in the shell launching the `varnishadm` command and then in the Varnish CLI itself. When a command’s parameter require spaces, you need to ensure that the Varnish CLI will see the double quotes:\n\n``` python\nvarnishadm param.set cc_command '\"my alternate cc command\"'\n\nChange will take effect when VCL script is reloaded\n```\n\nOtherwise if you don’t quote the quotes, you may get a seemingly unrelated error message:\n\n``` python\nvarnishadm param.set cc_command \"my alternate cc command\"\nUnknown request.\nType 'help' for more info.\nToo many parameters\n\nCommand failed with error code 105\n```\n\nIf you are quoting with a here document, you must wrap it inside a shell multi-line argument:\n\n``` python\nvarnishadm vcl.inline test '<< EOF\nvcl 4.0;\n\nbackend be {\n        .host = \"localhost\";\n}\nEOF'\nVCL compiled.\n```\n\nAnother difference with a shell here document is that only one here document can be used on a single command line. For example, it is possible to do this in a shell script:\n\n``` python\n#!/bin/sh\n\ncat << EOF1 ; cat << EOF2\nhello\nEOF1\nworld\nEOF2\n```\n\nThe expected output is:\n\n``` python\nhello\nworld\n```\n\nWith the Varnish CLI, only the last parameter may use the here document form, which greatly restricts the number of commands that can effectively use them. Trying to use multiple here documents only takes the last one into account.\n\nFor example:\n\n``` python\ncommand argument << EOF1 << EOF2\nheredoc1\nEOF1\nheredoc2\nEOF2\n```\n\nThis conceptually results in the following command line:\n\n- `\"command\"`\n- `\"argument\"`\n- `\"<<\"`\n- `\"EOF1\"`\n- `\"heredoc1\\nEOF1\\nheredoc2\\n\"`\n\nOther pitfalls include variable expansion of the shell invoking `varnishadm` but this is not directly related to the Varnish CLI. If you get the quoting right you should be fine even with complex commands.\n\n#### JSON\n\nA number of commands with informational responses support a `-j` parameter for JSON output, as specified below. The top-level structure of the JSON response is an array with these first three elements:\n\n- A version number for the JSON format (integer)\n- An array of strings that comprise the CLI command just received\n- The time at which the response was generated, as a Unix epoch time in seconds with millisecond precision (floating point)\n\nThe remaining elements of the array form the data that are specific to the CLI command, and their structure and content depend on the command.\n\nFor example, the response to `status -j` just contains a string in the top-level array indicating the state of the child process (`\"running\"`, `\"stopped\"` and so forth):\n\n``` python\n[ 2, [\"status\", \"-j\"], 1538031732.632, \"running\"\n]\n```\n\nThe JSON responses to other commands may have longer lists of elements, which may have simple data types or form structured objects.\n\nJSON output is only returned if command execution was successful. The output for an error response is always the same as it would have been for the command without the `-j` parameter.\n\n#### Commands\n\n##### auth \\<response\\>\n\nAuthenticate.\n\n##### backend.list \\[-j\\] \\[-p\\] \\[\\<backend_pattern\\>\\]\n\nList backends.\n\n`-p` also shows probe status.\n\n`-j` specifies JSON output.\n\nUnless `-j` is specified for JSON output, the output format is five columns of dynamic width, separated by white space with the fields:\n\n- Backend name\n\n- Admin: How health state is determined:\n\n  - `healthy`: Set `healthy` through `backend.set_health`.\n  - `sick`: Set `sick` through `backend.set_health`.\n  - `probe`: Health state determined by a probe or some other dynamic mechanism.\n  - `deleted`: Backend has been deleted, but not yet cleaned up.\n\n  Admin has precedence over Health\n\n- Probe `X/Y`: *X* out of *Y* checks have succeeded\n\n  *X* and *Y* are backend specific and may represent probe checks, other backends or any other metric.\n\n  If there is no probe or the director does not provide details on probe check results, `0/0` is output.\n\n- Health: Probe health state\n\n  - `healthy`\n  - `sick`\n\n  If there is no probe, `healthy` is output.\n\n- Last change: Timestamp when the health state last changed.\n\nThe health state reported here is generic. A backend’s health may also depend on the context it is being used in (e.g. the object’s hash), so the actual health state as visible from VCL (e.g. using `std.healthy()`) may differ.\n\nFor `-j`, the object members should be self explanatory, matching the fields described above. `probe_message` has the format `[X, Y, \"state\"]` as described above for Probe. JSON Probe details (`-j -p` arguments) are director specific.\n\n##### backend.set_health \\<backend_pattern\\> \\[auto\\|healthy\\|sick\\]\n\nSet health status of backend(s) matching \\<backend_pattern\\>.\n\n- With `auto`, the health status is determined by a probe or some other dynamic mechanism, if any\n- `healthy` sets the backend as usable\n- `sick` sets the backend as unsable\n\n##### ban \\<field\\> \\<operator\\> \\<arg\\> \\[&& \\<field\\> \\<oper\\> \\<arg\\> …\\]\n\nMark obsolete all objects where all the conditions match.\n\nSee [ban(STRING)](vcl#vcl-7-ban) for details\n\n##### ban.list \\[-j\\]\n\nList the active bans.\n\nUnless `-j` is specified for JSON output, the output format is:\n\n- Time the ban was issued.\n\n- Objects referencing this ban.\n\n- `C` if ban is completed = no further testing against it.\n\n- if `lurker` debugging is enabled:\n\n  - `R` for req.\\* tests\n  - `O` for obj.\\* tests\n  - Pointer to ban object\n\n- Ban specification\n\nDurations of ban specifications get normalized, for example “7d” gets changed into “1w”.\n\n##### banner\n\nPrint welcome banner.\n\n##### help \\[-j\\|\\<command\\>\\]\n\nShow command/protocol help.\n\n`-j` specifies JSON output.\n\n##### panic.clear \\[-z\\]\n\nClear the last panic, if any, -z will clear related varnishstat counter(s)\n\n##### panic.show \\[-j\\]\n\nReturn the last panic, if any.\n\n`-j` specifies JSON output – the panic message is returned as an unstructured JSON string.\n\n##### param.reset \\<param\\>\n\nReset parameter to default value.\n\n##### param.set \\[-j\\] \\<param\\> \\<value\\>\n\nSet parameter value.\n\nThe JSON output is the same as `param.show -j <param>` and contains the updated value as it would be represented by a subsequent execution of `param.show`.\n\nThis can be useful to later verify that a parameter value didn’t change and to use the value from the JSON output to reset the parameter to the desired value.\n\n##### param.show \\[-l\\|-j\\] \\[\\<param\\>\\|changed\\]\n\nShow parameters and their values.\n\nThe long form with `-l` shows additional information, including documentation and minimum, maximum and default values, if defined for the parameter. JSON output is specified with `-j`, in which the information for the long form is included; only one of `-l` or `-j` is permitted. If a parameter is specified with `<param>`, show only that parameter. If `changed` is specified, show only those parameters whose values differ from their defaults.\n\n##### pid \\[-j\\]\n\nShow the pid of the master process, and the worker if it’s running.\n\n`-j` specifies JSON output.\n\n##### ping \\[-j\\] \\[\\<timestamp\\>\\]\n\nKeep connection alive.\n\nThe response is formatted as JSON if `-j` is specified.\n\n##### quit\n\nClose connection.\n\n##### start\n\nStart the Varnish cache process.\n\n##### status \\[-j\\]\n\nCheck status of Varnish cache process.\n\n`-j` specifies JSON output.\n\n##### stop\n\nStop the Varnish cache process.\n\n##### storage.list \\[-j\\]\n\nList storage devices.\n\n`-j` specifies JSON output.\n\n##### vcl.deps \\[-j\\]\n\nList all loaded configuration and their dependencies.\n\nUnless `-j` is specified for JSON output, the output format is up to two columns of dynamic width separated by white space with the fields:\n\n- VCL: a VCL program\n- Dependency: another VCL program it depends on\n\nOnly direct dependencies are listed, and VCLs with multiple dependencies are listed multiple times.\n\n##### vcl.discard \\<name_pattern\\>…\n\nUnload the named configurations (when possible).\n\nUnload the named configurations and labels matching at least one name pattern. All matching configurations and labels are discarded in the correct order with respect to potential dependencies. If one configuration or label could not be discarded because one of its dependencies would remain, nothing is discarded. Each individual name pattern must match at least one named configuration or label.\n\n##### vcl.inline \\<configname\\> \\<quoted_VCLstring\\> \\[auto\\|cold\\|warm\\]\n\nCompile and load the VCL data under the name provided.\n\nMulti-line VCL can be input using the here document [Syntax](#ref-syntax).\n\n##### vcl.label \\<label\\> \\<configname\\>\n\nApply label to configuration.\n\nA VCL label is like a UNIX symbolic link, a name without substance, which points to another VCL.\n\nLabels are mandatory whenever one VCL references another.\n\n##### vcl.list \\[-j\\]\n\nList all loaded configuration.\n\nUnless `-j` is specified for JSON output, the output format is five or seven columns of dynamic width, separated by white space with the fields:\n\n- status: active, available or discarded\n\n- state: label, cold, warm, or auto\n\n- temperature: init, cold, warm, busy or cooling\n\n- busy: number of references to this vcl (integer)\n\n- name: the name given to this vcl or label\n\n- \\[ `<-` \\| `->` \\] and label info last two fields)\n\n  - `->` \\<vcl\\> : label “points to” the named \\<vcl\\>\n  - `<-` (\\<n\\> label\\[s\\]): the vcl has \\<n\\> label(s)\n\n##### vcl.load \\<configname\\> \\<filename\\> \\[auto\\|cold\\|warm\\]\n\nCompile and load the VCL file under the name provided.\n\n##### vcl.show \\[-v\\] \\[\\<configname\\>\\]\n\nDisplay the source code for the specified configuration.\n\n##### vcl.state \\<configname\\> auto\\|cold\\|warm\n\nForce the state of the named configuration.\n\n##### vcl.symtab\n\nDump the VCL symbol-tables.\n\n##### vcl.use \\<configname\\|label\\>\n\nSwitch to the named configuration immediately.\n\n#### Backend Pattern\n\nA backend pattern can be a backend name or a combination of a VCL name and backend name in “VCL.backend” format. If the VCL name is omitted, the active VCL is assumed. Partial matching on the backend and VCL names is supported using shell-style wildcards, e.g. asterisk (\\*).\n\nExamples:\n\n``` python\nbackend.list def*\nbackend.list b*.def*\nbackend.set_health default sick\nbackend.set_health def* healthy\nbackend.set_health * auto\n```\n\n#### Ban Expressions\n\nA ban expression consists of one or more conditions. A condition consists of a field, an operator, and an argument. Conditions can be ANDed together with “&&”.\n\nA field can be any of the variables from VCL, for instance req.url, req.http.host or obj.http.set-cookie.\n\nOperators are “==” for direct comparison, “~” for a regular expression match, and “\\>” or “\\<” for size comparisons. Prepending an operator with “!” negates the expression.\n\nThe argument could be a quoted string, a regexp, or an integer. Integers can have “KB”, “MB”, “GB” or “TB” appended for size related fields.\n\n#### VCL Temperature\n\nA VCL program goes through several states related to the different commands: it can be loaded, used, and later discarded. You can load several VCL programs and switch at any time from one to another. There is only one active VCL, but the previous active VCL will be maintained active until all its transactions are over.\n\nOver time, if you often refresh your VCL and keep the previous versions around, resource consumption will increase, you can’t escape that. However, most of the time you want to pay the price only for the active VCL and keep older VCLs in case you’d need to rollback to a previous version.\n\nThe VCL temperature allows you to minimize the footprint of inactive VCLs. Once a VCL becomes cold, Varnish will release all the resources that can be be later reacquired. You can manually set the temperature of a VCL or let varnish automatically handle it.\n\n### EXAMPLES\n\nLoad a multi-line VCL using shell-style *here document*:\n\n``` python\nvcl.inline example << EOF\nvcl 4.0;\n\nbackend www {\n    .host = \"127.0.0.1\";\n    .port = \"8080\";\n}\nEOF\n```\n\nBan all requests where req.url exactly matches the string /news:\n\n``` python\nban req.url == \"/news\"\n```\n\nBan all documents where the serving host is “example.com” or “www.example.com”, and where the Set-Cookie header received from the backend contains “USERID=1663”:\n\n``` python\nban req.http.host ~ \"^(?i)(www\\\\.)?example\\\\.com$\" && obj.http.set-cookie ~ \"USERID=1663\"\n```\n\n### AUTHORS\n\nThis manual page was originally written by Per Buer and later modified by Federico G. Schwindt, Dridi Boukelmoune, Lasse Karstensen and Poul-Henning Kamp.\n\n### SEE ALSO\n\n- [varnishadm](varnishadm#varnishadm-1)\n- [varnishd](varnishd#varnishd-1)\n- [VCL](vcl#vcl-7)\n- For API use of the CLI: The Reference Manual.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/varnish-cli.html](https://varnish-cache.org/docs/7.4/reference/varnish-cli.html)"
- name: varnish-counters
  id: reference/varnish-counters
  summary: Counters which track the activity in the different classes of mutex-locks
  description: "# varnish-counters\n\n## Varnish counter field definitions\n\nManual section:  \n7\n\n### LCK – Lock Counters\n\nCounters which track the activity in the different classes of mutex-locks.\n\nThe counts may be slightly wrong if there are more than one lock instantiated in each class (ie: .creat \\> 1)\n\n`creat` – `counter` - debug\n\nCreated locks\n\n`destroy` – `counter` - debug\n\nDestroyed locks\n\n`locks` – `counter` - debug\n\nLock Operations\n\n`dbg_busy` – `counter` - debug\n\nContended lock operations\n\nIf the `lck` debug bit is set: Lock operations which returned EBUSY on the first locking attempt.\n\nIf the `lck` debug bit is unset, this counter will never be incremented even if lock operations are contended.\n\n`dbg_try_fail` – `counter` - debug\n\nContended trylock operations\n\nIf the `lck` debug bit is set: Trylock operations which returned EBUSY.\n\nIf the `lck` debug bit is unset, this counter will never be incremented even if lock operations are contended.\n\n### MAIN – Main counters\n\n`summs` – `counter` - debug\n\nstat summ operations\n\nNumber of times per-thread statistics were summed into the global counters.\n\n`uptime` – `counter` - info\n\nChild process uptime\n\nHow long the child process has been running.\n\n`sess_conn` – `counter` - info\n\nSessions accepted\n\nCount of sessions successfully accepted\n\n`sess_fail` – `counter` - info\n\nSession accept failures\n\nCount of failures to accept TCP connection.\n\nThis counter is the sum of the sess_fail\\_\\* counters, which give more detailed information.\n\n`sess_fail_econnaborted` – `counter` - info\n\nSession accept failures: connection aborted\n\nDetailed reason for sess_fail: Connection aborted by the client, usually harmless.\n\n`sess_fail_eintr` – `counter` - info\n\nSession accept failures: interrupted system call\n\nDetailed reason for sess_fail: The accept() call was interrupted, usually harmless\n\n`sess_fail_emfile` – `counter` - info\n\nSession accept failures: too many open files\n\nDetailed reason for sess_fail: No file descriptor was available. Consider raising RLIMIT_NOFILE (see ulimit -n).\n\n`sess_fail_ebadf` – `counter` - info\n\nSession accept failures: bad file descriptor\n\nDetailed reason for sess_fail: The listen socket file descriptor was invalid. Should never happen.\n\n`sess_fail_enomem` – `counter` - info\n\nSession accept failures: not enough memory\n\nDetailed reason for sess_fail: Most likely insufficient socket buffer memory. Should never happen\n\n`sess_fail_other` – `counter` - info\n\nSession accept failures: other\n\nDetailed reason for sess_fail: neither of the above, see SessError log (varnishlog -g raw -i SessError).\n\n`client_req_400` – `counter` - info\n\nClient requests received, subject to 400 errors\n\n400 means we couldn’t make sense of the request, it was malformed in some drastic way.\n\n`client_req_417` – `counter` - info\n\nClient requests received, subject to 417 errors\n\n417 means that something went wrong with an Expect: header.\n\n`client_req` – `counter` - info\n\nGood client requests received\n\nThe count of parseable client requests seen.\n\n`esi_req` – `counter` - info\n\nESI subrequests\n\nNumber of ESI subrequests made.\n\n`cache_hit` – `counter` - info\n\nCache hits\n\nCount of cache hits. A cache hit indicates that an object has been delivered to a client without fetching it from a backend server.\n\n`cache_hit_grace` – `counter` - info\n\nCache grace hits\n\nCount of cache hits with grace. A cache hit with grace is a cache hit where the object is expired. Note that such hits are also included in the cache_hit counter.\n\n`cache_hitpass` – `counter` - info\n\nCache hits for pass.\n\nCount of hits for pass. A cache hit for pass indicates that Varnish is going to pass the request to the backend and this decision has been cached in it self. This counts how many times the cached decision is being used.\n\n`cache_hitmiss` – `counter` - info\n\nCache hits for miss.\n\nCount of hits for miss. A cache hit for miss indicates that Varnish is going to proceed as for a cache miss without request coalescing, and this decision has been cached. This counts how many times the cached decision is being used.\n\n`cache_miss` – `counter` - info\n\nCache misses\n\nCount of misses. A cache miss indicates the object was fetched from the backend before delivering it to the client.\n\n`beresp_uncacheable` – `counter` - info\n\nUncacheable backend responses\n\nCount of backend responses considered uncacheable.\n\n`beresp_shortlived` – `counter` - info\n\nShortlived objects\n\nCount of objects created with ttl+grace+keep shorter than the ‘shortlived’ runtime parameter.\n\n`backend_conn` – `counter` - info\n\nBackend conn. success\n\nHow many backend connections have successfully been established.\n\n`backend_unhealthy` – `counter` - info\n\nBackend conn. not attempted\n\n`backend_busy` – `counter` - info\n\nBackend conn. too many\n\n`backend_fail` – `counter` - info\n\nBackend conn. failures\n\n`backend_reuse` – `counter` - info\n\nBackend conn. reuses\n\nCount of backend connection reuses. This counter is increased whenever we reuse a recycled connection.\n\n`backend_recycle` – `counter` - info\n\nBackend conn. recycles\n\nCount of backend connection recycles. This counter is increased whenever we have a keep-alive connection that is put back into the pool of connections. It has not yet been used, but it might be, unless the backend closes it.\n\n`backend_retry` – `counter` - info\n\nBackend conn. retry\n\n`fetch_head` – `counter` - info\n\nFetch no body (HEAD)\n\nberesp with no body because the request is HEAD.\n\n`fetch_length` – `counter` - info\n\nFetch with Length\n\nberesp.body with Content-Length.\n\n`fetch_chunked` – `counter` - info\n\nFetch chunked\n\nberesp.body with Chunked.\n\n`fetch_eof` – `counter` - info\n\nFetch EOF\n\nberesp.body with EOF.\n\n`fetch_bad` – `counter` - info\n\nFetch bad T-E\n\nberesp.body length/fetch could not be determined.\n\n`fetch_none` – `counter` - info\n\nFetch no body\n\nberesp.body empty\n\n`fetch_1xx` – `counter` - info\n\nFetch no body (1xx)\n\nberesp with no body because of 1XX response.\n\n`fetch_204` – `counter` - info\n\nFetch no body (204)\n\nberesp with no body because of 204 response.\n\n`fetch_304` – `counter` - info\n\nFetch no body (304)\n\nberesp with no body because of 304 response.\n\n`fetch_failed` – `counter` - info\n\nFetch failed (all causes)\n\nberesp fetch failed.\n\n`bgfetch_no_thread` – `counter` - info\n\nBackground fetch failed (no thread)\n\nA bgfetch triggered by a grace hit failed, no thread available.\n\n`pools` – `gauge` - info\n\nNumber of thread pools\n\nNumber of thread pools. See also parameter thread_pools. NB: Presently pools cannot be removed once created.\n\n`threads` – `gauge` - info\n\nTotal number of threads\n\nNumber of threads in all pools. See also parameters thread_pools, thread_pool_min and thread_pool_max.\n\n`threads_limited` – `counter` - info\n\nThreads hit max\n\nNumber of times more threads were needed, but limit was reached in a thread pool. See also parameter thread_pool_max.\n\n`threads_created` – `counter` - info\n\nThreads created\n\nTotal number of threads created in all pools.\n\n`threads_destroyed` – `counter` - info\n\nThreads destroyed\n\nTotal number of threads destroyed in all pools.\n\n`threads_failed` – `counter` - info\n\nThread creation failed\n\nNumber of times creating a thread failed. See VSL::Debug for diagnostics. See also parameter thread_fail_delay.\n\n`thread_queue_len` – `gauge` - info\n\nLength of session queue\n\nLength of session queue waiting for threads. NB: Only updates once per second. See also parameter thread_queue_limit.\n\n`busy_sleep` – `counter` - info\n\nNumber of requests sent to sleep on busy objhdr\n\nNumber of requests sent to sleep without a worker thread because they found a busy object.\n\n`busy_wakeup` – `counter` - info\n\nNumber of requests woken after sleep on busy objhdr\n\nNumber of requests taken off the busy object sleep list and rescheduled.\n\n`busy_killed` – `counter` - info\n\nNumber of requests killed after sleep on busy objhdr\n\nNumber of requests killed from the busy object sleep list due to lack of resources.\n\n`sess_queued` – `counter` - info\n\nSessions queued for thread\n\nNumber of times session was queued waiting for a thread. See also parameter thread_queue_limit.\n\n`sess_dropped` – `counter` - info\n\nSessions dropped for thread\n\nNumber of times an HTTP/1 session was dropped because the queue was too long already. See also parameter thread_queue_limit.\n\n`req_dropped` – `counter` - info\n\nRequests dropped\n\nNumber of times an HTTP/2 stream was refused because the queue was too long already. See also parameter thread_queue_limit.\n\n`n_object` – `gauge` - info\n\nobject structs made\n\nApproximate number of HTTP objects (headers + body, if present) in the cache.\n\n`n_vampireobject` – `gauge` - diag\n\nunresurrected objects\n\nNumber of unresurrected objects\n\n`n_objectcore` – `gauge` - info\n\nobjectcore structs made\n\nApproximate number of object metadata elements in the cache. Each object needs an objectcore, extra objectcores are for hit-for-miss, hit-for-pass and busy objects.\n\n`n_objecthead` – `gauge` - info\n\nobjecthead structs made\n\nApproximate number of different hash entries in the cache.\n\n`n_backend` – `gauge` - info\n\nNumber of backends\n\nNumber of backends known to us.\n\n`n_expired` – `counter` - info\n\nNumber of expired objects\n\nNumber of objects that expired from cache because of old age.\n\n`n_lru_nuked` – `counter` - info\n\nNumber of LRU nuked objects\n\nHow many objects have been forcefully evicted from storage to make room for a new object.\n\n`n_lru_moved` – `counter` - diag\n\nNumber of LRU moved objects\n\nNumber of move operations done on the LRU list.\n\n`n_lru_limited` – `counter` - info\n\nReached nuke_limit\n\nNumber of times more storage space were needed, but limit was reached in a nuke_limit. See also parameter nuke_limit.\n\n`losthdr` – `counter` - info\n\nHTTP header overflows\n\n`s_sess` – `counter` - info\n\nTotal sessions seen\n\n`n_pipe` – `gauge` - info\n\nNumber of ongoing pipe sessions\n\n`pipe_limited` – `counter` - info\n\nPipes hit pipe_sess_max\n\nNumber of times more pipes were needed, but the limit was reached. See also parameter pipe_sess_max.\n\n`s_pipe` – `counter` - info\n\nTotal pipe sessions seen\n\n`s_pass` – `counter` - info\n\nTotal pass-ed requests seen\n\n`s_fetch` – `counter` - info\n\nTotal backend fetches initiated\n\nTotal backend fetches initiated, including background fetches.\n\n`s_bgfetch` – `counter` - info\n\nTotal backend background fetches initiated\n\n`s_synth` – `counter` - info\n\nTotal synthetic responses made\n\n`s_req_hdrbytes` – `counter` - info\n\nRequest header bytes\n\nTotal request header bytes received\n\n`s_req_bodybytes` – `counter` - info\n\nRequest body bytes\n\nTotal request body bytes received\n\n`s_resp_hdrbytes` – `counter` - info\n\nResponse header bytes\n\nTotal response header bytes transmitted\n\n`s_resp_bodybytes` – `counter` - info\n\nResponse body bytes\n\nTotal response body bytes transmitted\n\n`s_pipe_hdrbytes` – `counter` - info\n\nPipe request header bytes\n\nTotal request bytes received for piped sessions\n\n`s_pipe_in` – `counter` - info\n\nPiped bytes from client\n\nTotal number of bytes forwarded from clients in pipe sessions\n\n`s_pipe_out` – `counter` - info\n\nPiped bytes to client\n\nTotal number of bytes forwarded to clients in pipe sessions\n\n`sess_closed` – `counter` - info\n\nSession Closed\n\n`sess_closed_err` – `counter` - info\n\nSession Closed with error\n\nTotal number of sessions closed with errors. See sc\\_\\* diag counters for detailed breakdown\n\n`sess_readahead` – `counter` - info\n\nSession Read Ahead\n\n`sess_herd` – `counter` - diag\n\nSession herd\n\nNumber of times the timeout_linger triggered\n\n`sc_rem_close` – `counter` - diag\n\nSession OK REM_CLOSE\n\nNumber of session closes with REM_CLOSE (Client Closed)\n\n`sc_req_close` – `counter` - diag\n\nSession OK REQ_CLOSE\n\nNumber of session closes with REQ_CLOSE (Client requested close)\n\n`sc_req_http10` – `counter` - diag\n\nSession Err REQ_HTTP10\n\nNumber of session closes with Error REQ_HTTP10 (Proto \\< HTTP/1.1)\n\n`sc_rx_bad` – `counter` - diag\n\nSession Err RX_BAD\n\nNumber of session closes with Error RX_BAD (Received bad req/resp)\n\n`sc_rx_body` – `counter` - diag\n\nSession Err RX_BODY\n\nNumber of session closes with Error RX_BODY (Failure receiving req.body)\n\n`sc_rx_junk` – `counter` - diag\n\nSession Err RX_JUNK\n\nNumber of session closes with Error RX_JUNK (Received junk data)\n\n`sc_rx_overflow` – `counter` - diag\n\nSession Err RX_OVERFLOW\n\nNumber of session closes with Error RX_OVERFLOW (Received buffer overflow)\n\n`sc_rx_timeout` – `counter` - diag\n\nSession Err RX_TIMEOUT\n\nNumber of session closes with Error RX_TIMEOUT (Receive timeout)\n\n`sc_rx_close_idle` – `counter` - diag\n\nSession Err RX_CLOSE_IDLE\n\nNumber of session closes with Error RX_CLOSE_IDLE: timeout_idle has been exceeded while waiting for a client request.\n\n`sc_tx_pipe` – `counter` - diag\n\nSession OK TX_PIPE\n\nNumber of session closes with TX_PIPE (Piped transaction)\n\n`sc_tx_error` – `counter` - diag\n\nSession Err TX_ERROR\n\nNumber of session closes with Error TX_ERROR (Error transaction)\n\n`sc_tx_eof` – `counter` - diag\n\nSession OK TX_EOF\n\nNumber of session closes with TX_EOF (EOF transmission)\n\n`sc_resp_close` – `counter` - diag\n\nSession OK RESP_CLOSE\n\nNumber of session closes with RESP_CLOSE (Backend/VCL requested close)\n\n`sc_overload` – `counter` - diag\n\nSession Err OVERLOAD\n\nNumber of session closes with Error OVERLOAD (Out of some resource)\n\n`sc_pipe_overflow` – `counter` - diag\n\nSession Err PIPE_OVERFLOW\n\nNumber of session closes with Error PIPE_OVERFLOW (Session pipe overflow)\n\n`sc_range_short` – `counter` - diag\n\nSession Err RANGE_SHORT\n\nNumber of session closes with Error RANGE_SHORT (Insufficient data for range)\n\n`sc_req_http20` – `counter` - diag\n\nSession Err REQ_HTTP20\n\nNumber of session closes with Error REQ_HTTP20 (HTTP2 not accepted)\n\n`sc_vcl_failure` – `counter` - diag\n\nSession Err VCL_FAILURE\n\nNumber of session closes with Error VCL_FAILURE (VCL failure)\n\n`client_resp_500` – `counter` - diag\n\nDelivery failed due to insufficient workspace.\n\nNumber of times we failed a response due to running out of workspace memory during delivery.\n\n`ws_backend_overflow` – `counter` - diag\n\nworkspace_backend overflows\n\nNumber of times we ran out of space in workspace_backend.\n\n`ws_client_overflow` – `counter` - diag\n\nworkspace_client overflows\n\nNumber of times we ran out of space in workspace_client.\n\n`ws_thread_overflow` – `counter` - diag\n\nworkspace_thread overflows\n\nNumber of times we ran out of space in workspace_thread.\n\n`ws_session_overflow` – `counter` - diag\n\nworkspace_session overflows\n\nNumber of times we ran out of space in workspace_session.\n\n`shm_records` – `counter` - diag\n\nSHM records\n\nNumber of log records written to the shared memory log.\n\n`shm_writes` – `counter` - diag\n\nSHM writes\n\nNumber of individual writes to the shared memory log. A single write may batch multiple records for bufferred tasks.\n\n`shm_flushes` – `counter` - diag\n\nSHM flushes due to overflow\n\nNumber of writes performed before the end of a bufferred task because adding a record to a batch would exceed vsl_buffer.\n\n`shm_cont` – `counter` - diag\n\nSHM lock contention\n\nNumber of times a write had to wait for the lock.\n\n`shm_cycles` – `counter` - diag\n\nSHM cycles through VSL space\n\nNumber of times a write of log records would reach past the end of the shared memory log, cycling back to the beginning.\n\n`shm_bytes` – `counter` - diag\n\nSHM bytes\n\nNumber of bytes written to the shared memory log.\n\n`backend_req` – `counter` - info\n\nBackend requests made\n\n`n_vcl` – `gauge` - info\n\nNumber of loaded VCLs in total\n\n`n_vcl_avail` – `gauge` - diag\n\nNumber of VCLs available\n\n`n_vcl_discard` – `gauge` - diag\n\nNumber of discarded VCLs\n\n`vcl_fail` – `counter` - info\n\nVCL failures\n\nCount of failures which prevented VCL from completing.\n\n`bans` – `gauge` - info\n\nCount of bans\n\nNumber of all bans in system, including bans superseded by newer bans and bans already checked by the ban-lurker.\n\n`bans_completed` – `gauge` - diag\n\nNumber of bans marked ‘completed’\n\nNumber of bans which are no longer active, either because they got checked by the ban-lurker or superseded by newer identical bans.\n\n`bans_obj` – `gauge` - diag\n\nNumber of bans using obj.\\*\n\nNumber of bans which use obj.\\* variables. These bans can possibly be washed by the ban-lurker.\n\n`bans_req` – `gauge` - diag\n\nNumber of bans using req.\\*\n\nNumber of bans which use req.\\* variables. These bans can not be washed by the ban-lurker.\n\n`bans_added` – `counter` - diag\n\nBans added\n\nCounter of bans added to ban list.\n\n`bans_deleted` – `counter` - diag\n\nBans deleted\n\nCounter of bans deleted from ban list.\n\n`bans_tested` – `counter` - diag\n\nBans tested against objects (lookup)\n\nCount of how many bans and objects have been tested against each other during hash lookup.\n\n`bans_obj_killed` – `counter` - diag\n\nObjects killed by bans (lookup)\n\nNumber of objects killed by bans during object lookup.\n\n`bans_lurker_tested` – `counter` - diag\n\nBans tested against objects (lurker)\n\nCount of how many bans and objects have been tested against each other by the ban-lurker.\n\n`bans_tests_tested` – `counter` - diag\n\nBan tests tested against objects (lookup)\n\nCount of how many tests and objects have been tested against each other during lookup. ‘ban req.url == foo && req.http.host == bar’ counts as one in ‘bans_tested’ and as two in ‘bans_tests_tested’\n\n`bans_lurker_tests_tested` – `counter` - diag\n\nBan tests tested against objects (lurker)\n\nCount of how many tests and objects have been tested against each other by the ban-lurker. ‘ban req.url == foo && req.http.host == bar’ counts as one in ‘bans_tested’ and as two in ‘bans_tests_tested’\n\n`bans_lurker_obj_killed` – `counter` - diag\n\nObjects killed by bans (lurker)\n\nNumber of objects killed by the ban-lurker.\n\n`bans_lurker_obj_killed_cutoff` – `counter` - diag\n\nObjects killed by bans for cutoff (lurker)\n\nNumber of objects killed by the ban-lurker to keep the number of bans below ban_cutoff.\n\n`bans_dups` – `counter` - diag\n\nBans superseded by other bans\n\nCount of bans replaced by later identical bans.\n\n`bans_lurker_contention` – `counter` - diag\n\nLurker gave way for lookup\n\nNumber of times the ban-lurker had to wait for lookups.\n\n`bans_persisted_bytes` – `gauge` - diag\n\nBytes used by the persisted ban lists\n\nNumber of bytes used by the persisted ban lists.\n\n`bans_persisted_fragmentation` – `gauge` - diag\n\nExtra bytes in persisted ban lists due to fragmentation\n\nNumber of extra bytes accumulated through dropped and completed bans in the persistent ban lists.\n\n`n_purges` – `counter` - info\n\nNumber of purge operations executed\n\n`n_obj_purged` – `counter` - info\n\nNumber of purged objects\n\n`exp_mailed` – `counter` - diag\n\nNumber of objects mailed to expiry thread\n\nNumber of objects mailed to expiry thread for handling.\n\n`exp_received` – `counter` - diag\n\nNumber of objects received by expiry thread\n\nNumber of objects received by expiry thread for handling.\n\n`hcb_nolock` – `counter` - debug\n\nHCB Lookups without lock\n\n`hcb_lock` – `counter` - debug\n\nHCB Lookups with lock\n\n`hcb_insert` – `counter` - debug\n\nHCB Inserts\n\n`esi_errors` – `counter` - diag\n\nESI parse errors (unlock)\n\n`esi_warnings` – `counter` - diag\n\nESI parse warnings (unlock)\n\n`vmods` – `gauge` - info\n\nLoaded VMODs\n\n`n_gzip` – `counter` - info\n\nGzip operations\n\n`n_gunzip` – `counter` - info\n\nGunzip operations\n\n`n_test_gunzip` – `counter` - info\n\nTest gunzip operations\n\nThose operations occur when Varnish receives a compressed object from a backend. They are done to verify the gzip stream while it’s inserted in storage.\n\n`http1_iovs_flush` – `counter` - info\n\nPremature iovec flushes\n\nNumber of additional writes performed on HTTP1 connections because the number of IO vectors was too small to submit all possible IO in one go. This number is configured through the `http1_iovs` parameter for client connections and implicitly defined by the amount of free workspace for backend connections.\n\n### MEMPOOL – Memory Pool Counters\n\n`live` – `gauge` - debug\n\nIn use\n\n`pool` – `gauge` - debug\n\nIn Pool\n\n`sz_wanted` – `gauge` - debug\n\nSize requested\n\n`sz_actual` – `gauge` - debug\n\nSize allocated\n\n`allocs` – `counter` - debug\n\nAllocations\n\n`frees` – `counter` - debug\n\nFrees\n\n`recycle` – `counter` - debug\n\nRecycled from pool\n\n`timeout` – `counter` - debug\n\nTimed out from pool\n\n`toosmall` – `counter` - debug\n\nToo small to recycle\n\n`surplus` – `counter` - debug\n\nToo many for pool\n\n`randry` – `counter` - debug\n\nPool ran dry\n\n### MGT – Management Process Counters\n\n`uptime` – `counter` - info\n\nManagement process uptime\n\nUptime in seconds of the management process\n\n`child_start` – `counter` - diag\n\nChild process started\n\nNumber of times the child process has been started\n\n`child_exit` – `counter` - diag\n\nChild process normal exit\n\nNumber of times the child process has been cleanly stopped\n\n`child_stop` – `counter` - diag\n\nChild process unexpected exit\n\nNumber of times the child process has exited with an unexpected return code\n\n`child_died` – `counter` - diag\n\nChild process died (signal)\n\nNumber of times the child process has died due to signals\n\n`child_dump` – `counter` - diag\n\nChild process core dumped\n\nNumber of times the child process has produced core dumps\n\n`child_panic` – `counter` - diag\n\nChild process panic\n\nNumber of times the management process has caught a child panic\n\n### SMA – Malloc Stevedore Counters\n\n`c_req` – `counter` - info\n\nAllocator requests\n\nNumber of times the storage has been asked to provide a storage segment.\n\n`c_fail` – `counter` - info\n\nAllocator failures\n\nNumber of times the storage has failed to provide a storage segment.\n\n`c_bytes` – `counter` - info\n\nBytes allocated\n\nNumber of total bytes allocated by this storage.\n\n`c_freed` – `counter` - info\n\nBytes freed\n\nNumber of total bytes returned to this storage.\n\n`g_alloc` – `gauge` - info\n\nAllocations outstanding\n\nNumber of storage allocations outstanding.\n\n`g_bytes` – `gauge` - info\n\nBytes outstanding\n\nNumber of bytes allocated from the storage.\n\n`g_space` – `gauge` - info\n\nBytes available\n\nNumber of bytes left in the storage.\n\n### SMF – File Stevedore Counters\n\n`c_req` – `counter` - info\n\nAllocator requests\n\nNumber of times the storage has been asked to provide a storage segment.\n\n`c_fail` – `counter` - info\n\nAllocator failures\n\nNumber of times the storage has failed to provide a storage segment.\n\n`c_bytes` – `counter` - info\n\nBytes allocated\n\nNumber of total bytes allocated by this storage.\n\n`c_freed` – `counter` - info\n\nBytes freed\n\nNumber of total bytes returned to this storage.\n\n`g_alloc` – `gauge` - info\n\nAllocations outstanding\n\nNumber of storage allocations outstanding.\n\n`g_bytes` – `gauge` - info\n\nBytes outstanding\n\nNumber of bytes allocated from the storage.\n\n`g_space` – `gauge` - info\n\nBytes available\n\nNumber of bytes left in the storage.\n\n`g_smf` – `gauge` - info\n\nN struct smf\n\n`g_smf_frag` – `gauge` - info\n\nN small free smf\n\n`g_smf_large` – `gauge` - info\n\nN large free smf\n\n### SMU – Umem Stevedore Counters\n\n`c_req` – `counter` - info\n\nAllocator requests\n\nNumber of times the storage has been asked to provide a storage segment.\n\n`c_fail` – `counter` - info\n\nAllocator failures\n\nNumber of times the storage has failed to provide a storage segment.\n\n`c_bytes` – `counter` - info\n\nBytes allocated\n\nNumber of total bytes allocated by this storage.\n\n`c_freed` – `counter` - info\n\nBytes freed\n\nNumber of total bytes returned to this storage.\n\n`g_alloc` – `gauge` - info\n\nAllocations outstanding\n\nNumber of storage allocations outstanding.\n\n`g_bytes` – `gauge` - info\n\nBytes outstanding\n\nNumber of bytes allocated from the storage.\n\n`g_space` – `gauge` - info\n\nBytes available\n\nNumber of bytes left in the storage.\n\n### VBE – Backend Counters\n\n`happy` – `bitmap` - info\n\nHappy health probes\n\nRepresents the last probe results as a bitmap. Happy probes are bits set to 1, and the unhappy ones are set to 0. The highest bits represent the oldest probes.\n\n`bereq_hdrbytes` – `counter` - info\n\nRequest header bytes\n\nTotal backend request header bytes sent\n\n`bereq_bodybytes` – `counter` - info\n\nRequest body bytes\n\nTotal backend request body bytes sent\n\n`beresp_hdrbytes` – `counter` - info\n\nResponse header bytes\n\nTotal backend response header bytes received\n\n`beresp_bodybytes` – `counter` - info\n\nResponse body bytes\n\nTotal backend response body bytes received\n\n`pipe_hdrbytes` – `counter` - info\n\nPipe request header bytes\n\nTotal request bytes sent for piped sessions\n\n`pipe_out` – `counter` - info\n\nPiped bytes to backend\n\nTotal number of bytes forwarded to backend in pipe sessions\n\n`pipe_in` – `counter` - info\n\nPiped bytes from backend\n\nTotal number of bytes forwarded from backend in pipe sessions\n\n`conn` – `gauge` - info\n\nConcurrent connections used\n\nThe number of currently used connections to the backend. This number is always less or equal to the number of connections to the backend (as, for example shown as ESTABLISHED for TCP connections in netstat) due to connection pooling.\n\n`req` – `counter` - info\n\nBackend requests sent\n\n`unhealthy` – `counter` - info\n\nFetches not attempted due to backend being unhealthy\n\n`busy` – `counter` - info\n\nFetches not attempted due to backend being busy\n\nNumber of times the max_connections limit was reached\n\n`fail` – `counter` - info\n\nConnections failed\n\nCounter of failed opens. Detailed reasons are given in the fail\\_\\* counters (DIAG level) and in the log under the FetchError tag.\n\nThis counter is the sum of all detailed fail\\_\\* counters.\n\nAll fail\\_\\* counters may be slightly inaccurate for efficiency.\n\n`fail_eacces` – `counter` - diag\n\nConnections failed with EACCES or EPERM\n\n`fail_eaddrnotavail` – `counter` - diag\n\nConnections failed with EADDRNOTAVAIL\n\n`fail_econnrefused` – `counter` - diag\n\nConnections failed with ECONNREFUSED\n\n`fail_enetunreach` – `counter` - diag\n\nConnections failed with ENETUNREACH\n\n`fail_etimedout` – `counter` - diag\n\nConnections failed ETIMEDOUT\n\n`fail_other` – `counter` - diag\n\nConnections failed for other reason\n\n`helddown` – `counter` - diag\n\nConnection opens not attempted\n\nConnections not attempted during the backend_local_error_holddown or backend_remote_error_holddown interval after a fundamental connection issue.\n\n### AUTHORS\n\nThis man page was written by Lasse Karstensen, using content from vsc2rst written by Tollef Fog Heen.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/varnish-counters.html](https://varnish-cache.org/docs/7.4/reference/varnish-counters.html)"
- name: 'Varnish: The beef in the sandwich'
  id: tutorial/introduction
  summary: The top layer of the sandwich, ‘TLS’ is responsible for handling the TLS (“https”) encryption, which means it must have access to the cryptographic certificate which authenticates your website
  description: "# Varnish: The beef in the sandwich\n\nYou may have heard the term “web-delivery-sandwich” used in relation to Varnish, and it is a pretty apt metafor:\n\n``` python\n┌─────────┐\n│ browser │\n└─────────┘                                            ┌─────────┐\n           \\                                          ┌─────────┐│\n┌─────┐     ╔═════════╗    ┌─────┐    ┌─────────┐    ┌─────────┐│┘\n│ app │ --- ║ Network ║ -- │ TLS │ -- │ Varnish │ -- │ Backend │┘\n└─────┘     ╚═════════╝    └─────┘    └─────────┘    └─────────┘\n            /\n┌────────────┐\n│ API-client │\n└────────────┘\n```\n\nThe top layer of the sandwich, ‘TLS’ is responsible for handling the TLS (“https”) encryption, which means it must have access to the cryptographic certificate which authenticates your website.\n\nThe bottom layer of the sandwich are your webservers, CDNs, API-servers, business backend systems and all the other sources for your web-content.\n\nVarnish goes in the middle, where it provides caching, policy, analytics, visibility and mitigation for your webtraffic.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/tutorial/introduction.html](https://varnish-cache.org/docs/7.4/tutorial/introduction.html)"
- name: varnishadm
  id: reference/varnishadm
  summary: The varnishadm utility establishes a CLI connection to varnishd either using -n name or using the -T and -S arguments
  description: "# varnishadm\n\n## Control a running Varnish instance\n\nManual section:  \n1\n\n### SYNOPSIS\n\nvarnishadm \\[-h\\] \\[-n ident\\] \\[-p\\] \\[-S secretfile\\] \\[-T \\[address\\]:port\\] \\[-t timeout\\] \\[command \\[…\\]\\]\n\n### DESCRIPTION\n\nThe `varnishadm` utility establishes a CLI connection to varnishd either using -n *name* or using the -T and -S arguments. If -n *name* is given the location of the secret file and the address:port is looked up in shared memory. If neither is given `varnishadm` will look for an instance without a given name.\n\nIf a command is given, the command and arguments are sent over the CLI connection and the result returned on stdout.\n\nIf no command argument is given `varnishadm` will pass commands and replies between the CLI socket and stdin/stdout.\n\n### OPTIONS\n\n-h  \nPrint program usage and exit.\n\n-n `ident`  \nConnect to the instance of `varnishd` with this name.\n\n-p  \nForce `pass` mode and make the output follow the VCLI protocol. This disables command-history/command-completion and makes it easier for programs to parse the response(s).\n\n-S `secretfile`  \nSpecify the authentication secret file. This should be the same -S argument as was given to `varnishd`. Only processes which can read the contents of this file, will be able to authenticate the CLI connection.\n\n-T `<address:port>`  \nConnect to the management interface at the specified address and port.\n\n-t `timeout`  \nWait no longer than this many seconds for an operation to finish.\n\nThe syntax and operation of the actual CLI interface is described in the [varnish-cli](varnish-cli#varnish-cli-7) manual page. Parameters are described in [varnishd](varnishd#varnishd-1) manual page.\n\nAdditionally, a summary of commands can be obtained by issuing the *help* command, and a summary of parameters can be obtained by issuing the *param.show* command.\n\n### EXIT STATUS\n\nIf a command is given, the exit status of the `varnishadm` utility is zero if the command succeeded, and non-zero otherwise.\n\n### EXAMPLES\n\nSome ways you can use varnishadm:\n\n``` python\nvarnishadm -T localhost:999 -S /var/db/secret vcl.use foo\necho vcl.use foo | varnishadm -T localhost:999 -S /var/db/secret\necho vcl.use foo | ssh vhost varnishadm -T localhost:999 -S /var/db/secret\n```\n\n### SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [varnish-cli](varnish-cli#varnish-cli-7)\n\n### AUTHORS\n\nThe `varnishadm` utility and this manual page were written by Cecilie Fritzvold. This man page has later been modified by Per Buer, Federico G. Schwindt and Lasse Karstensen.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/varnishadm.html](https://varnish-cache.org/docs/7.4/reference/varnishadm.html)"
- name: varnishd
  id: reference/varnishd
  summary: The varnishd daemon accepts HTTP requests from clients, passes them on to a backend server and caches the returned documents to better satisfy future requests for the same document
  description: "# varnishd\n\n## HTTP accelerator daemon\n\nManual section:  \n1\n\n### SYNOPSIS\n\nvarnishd  \n\\[-a \\[name=\\]\\[listen_address\\[,PROTO\\]\\] \\[-b \\[host\\[:port\\]\\|path\\]\\] \\[-C\\] \\[-d\\] \\[-F\\] \\[-f config\\] \\[-h type\\[,options\\]\\] \\[-I clifile\\] \\[-i identity\\] \\[-j jail\\[,jailoptions\\]\\] \\[-l vsl\\] \\[-M address:port\\] \\[-n workdir\\] \\[-P file\\] \\[-p param=value\\] \\[-r param\\[,param…\\]\\] \\[-S secret-file\\] \\[-s \\[name=\\]kind\\[,options\\]\\] \\[-T address\\[:port\\]\\] \\[-t TTL\\] \\[-V\\] \\[-W waiter\\]\n\nvarnishd \\[-x parameter\\|vsl\\|cli\\|builtin\\|optstring\\]\n\nvarnishd \\[-?\\]\n\n### DESCRIPTION\n\nThe `varnishd` daemon accepts HTTP requests from clients, passes them on to a backend server and caches the returned documents to better satisfy future requests for the same document.\n\n### OPTIONS\n\n#### Basic options\n\n-a `<[name=][listen_address[,PROTO]]>`  \nAccept for client requests on the specified listen_address (see below).\n\nName is referenced in logs. If name is not specified, “a0”, “a1”, etc. is used.\n\nPROTO can be “HTTP” (the default) or “PROXY”. Both version 1 and 2 of the proxy protocol can be used.\n\nMultiple -a arguments are allowed.\n\nIf no -a argument is given, the default `-a :80` will listen to all IPv4 and IPv6 interfaces.\n\n-a `<[name=][ip_address][:port][,PROTO]>`  \nThe ip_address can be a host name (“localhost”), an IPv4 dotted-quad (“127.0.0.1”) or an IPv6 address enclosed in square brackets (“\\[::1\\]”)\n\nIf port is not specified, port 80 (http) is used.\n\nAt least one of ip_address or port is required.\n\n-a `<[name=][path][,PROTO][,user=name][,group=name][,mode=octal]>`  \n(VCL4.1 and higher)\n\nAccept connections on a Unix domain socket. Path must be absolute (“/path/to/listen.sock”) or “@” followed by the name of an abstract socket (“@myvarnishd”).\n\nThe user, group and mode sub-arguments may be used to specify the permissions of the socket file – use names for user and group, and a 3-digit octal value for mode. These sub-arguments do not apply to abstract sockets.\n\n-b `<[host[:port]|path]>`  \nUse the specified host as backend server. If port is not specified, the default is 8080.\n\nIf the value of `-b` begins with `/`, it is interpreted as the absolute path of a Unix domain socket to which Varnish connects. In that case, the value of `-b` must satisfy the conditions required for the `.path` field of a backend declaration, see [VCL](vcl#vcl-7). Backends with Unix socket addresses may only be used with VCL versions \\>= 4.1.\n\n-b can be used only once, and not together with f.\n\n-f `config`  \nUse the specified VCL configuration file instead of the builtin default. See [VCL](vcl#vcl-7) for details on VCL syntax.\n\nIf a single -f option is used, then the VCL instance loaded from the file is named “boot” and immediately becomes active. If more than one -f option is used, the VCL instances are named “boot0”, “boot1” and so forth, in the order corresponding to the -f arguments, and the last one is named “boot”, which becomes active.\n\nEither -b or one or more -f options must be specified, but not both, and they cannot both be left out, unless -d is used to start `varnishd` in debugging mode. If the empty string is specified as the sole -f option, then `varnishd` starts without starting the worker process, and the management process will accept CLI commands. You can also combine an empty -f option with an initialization script (-I option) and the child process will be started if there is an active VCL at the end of the initialization.\n\nWhen used with a relative file name, config is searched in the `vcl_path`. It is possible to set this path prior to using `-f` options with a `-p` option. During startup, `varnishd` doesn’t complain about unsafe VCL paths: unlike the `varnish-cli(7)` that could later be accessed remotely, starting `varnishd` requires local privileges.\n\n-n `workdir`  \nRuntime directory for the shared memory, compiled VCLs etc.\n\nIn performance critical applications, this directory should be on a RAM backed filesystem.\n\nRelative paths will be appended to `/var/run/` (NB: Binary packages of Varnish may have adjusted this to the platform.)\n\nThe default value is `/var/run/varnishd` (NB: as above.)\n\n#### Documentation options\n\nFor these options, `varnishd` prints information to standard output and exits. When a -x option is used, it must be the only option (it outputs documentation in reStructuredText, aka RST).\n\n-?\n\nPrint the usage message.\n\n-x `parameter`  \nPrint documentation of the runtime parameters (-p options), see [List of Parameters](#list-of-parameters).\n\n-x `vsl`  \nPrint documentation of the tags used in the Varnish shared memory log, see [VSL](vsl#vsl-7).\n\n-x `cli`  \nPrint documentation of the command line interface, see [varnish-cli](varnish-cli#varnish-cli-7).\n\n-x `builtin`  \nPrint the contents of the default VCL program `builtin.vcl`.\n\n-x `optstring`  \nPrint the optstring parameter to `getopt(3)` to help writing wrapper scripts.\n\n#### Operations options\n\n-F  \nDo not fork, run in the foreground. Only one of -F or -d can be specified, and -F cannot be used together with -C.\n\n-T `<address[:port]>`  \nOffer a management interface on the specified address and port. See [varnish-cli](varnish-cli#varnish-cli-7) for documentation of the management commands. To disable the management interface use `none`.\n\n-M `<address:port>`  \nConnect to this port and offer the command line interface. Think of it as a reverse shell. When running with -M and there is no backend defined the child process (the cache) will not start initially.\n\n-P `file`  \nWrite the PID of the process to the specified file.\n\n-i `identity`  \nSpecify the identity of the Varnish server. This can be accessed using `server.identity` from VCL.\n\nThe server identity is used for the `received-by` field of `Via` headers generated by Varnish. For this reason, it must be a valid token as defined by the HTTP grammar.\n\nIf not specified the output of `gethostname(3)` is used, in which case the syntax is assumed to be correct.\n\n-I `clifile`  \nExecute the management commands in the file given as `clifile` before the the worker process starts, see [CLI Command File](#cli-command-file).\n\n#### Tuning options\n\n-t `TTL`  \nSpecifies the default time to live (TTL) for cached objects. This is a shortcut for specifying the *default_ttl* run-time parameter.\n\n-p `<param=value>`  \nSet the parameter specified by param to the specified value, see [List of Parameters](#list-of-parameters) for details. This option can be used multiple times to specify multiple parameters.\n\n-s `<[name=]type[,options]>`  \nUse the specified storage backend. See [Storage Backend](#storage-backend) section.\n\nThis option can be used multiple times to specify multiple storage files. Name is referenced in logs, VCL, statistics, etc. If name is not specified, “s0”, “s1” and so forth is used.\n\n-l `<vsl>`  \nSpecifies size of the space for the VSL records, shorthand for `-p vsl_space=<vsl>`. Scaling suffixes like ‘K’ and ‘M’ can be used up to (G)igabytes. See [vsl_space](#vsl-space) for more information.\n\n#### Security options\n\n-r `<param[,param…]>`  \nMake the listed parameters read only. This gives the system administrator a way to limit what the Varnish CLI can do. Consider making parameters such as *cc_command*, *vcc_allow_inline_c* and *vmod_path* read only as these can potentially be used to escalate privileges from the CLI.\n\n-S `secret-file`  \nPath to a file containing a secret used for authorizing access to the management port. To disable authentication use `none`.\n\nIf this argument is not provided, a secret drawn from the system PRNG will be written to a file called `_.secret` in the working directory (see [opt_n](#opt-n)) with default ownership and permissions of the user having started varnish.\n\nThus, users wishing to delegate control over varnish will probably want to create a custom secret file with appropriate permissions (ie. readable by a unix group to delegate control to).\n\n-j `<jail[,jailoptions]>`  \nSpecify the jailing mechanism to use. See [Jail](#jail) section.\n\n#### Advanced, development and debugging options\n\n-d  \nEnables debugging mode: The parent process runs in the foreground with a CLI connection on stdin/stdout, and the child process must be started explicitly with a CLI command. Terminating the parent process will also terminate the child.\n\nOnly one of -d or -F can be specified, and -d cannot be used together with -C.\n\n-C  \nPrint VCL code compiled to C language and exit. Specify the VCL file to compile with the -f option. Either -f or -b must be used with -C, and -C cannot be used with -F or -d.\n\n-V  \nDisplay the version number and exit. This must be the only option.\n\n-h `<type[,options]>`  \nSpecifies the hash algorithm. See [Hash Algorithm](#hash-algorithm) section for a list of supported algorithms.\n\n-W `waiter`  \nSpecifies the waiter type to use.\n\n#### Hash Algorithm\n\nThe following hash algorithms are available:\n\n-h `critbit`  \nself-scaling tree structure. The default hash algorithm in Varnish Cache 2.1 and onwards. In comparison to a more traditional B tree the critbit tree is almost completely lockless. Do not change this unless you are certain what you’re doing.\n\n-h `simple_list`  \nA simple doubly-linked list. Not recommended for production use.\n\n-h `<classic[,buckets]>`  \nA standard hash table. The hash key is the CRC32 of the object’s URL modulo the size of the hash table. Each table entry points to a list of elements which share the same hash key. The buckets parameter specifies the number of entries in the hash table. The default is 16383.\n\n#### Storage Backend\n\nThe argument format to define storage backends is:\n\n-s `<[name]=kind[,options]>`  \nIf *name* is omitted, Varnish will name storages `s`*N*, starting with `s0` and incrementing *N* for every new storage.\n\nFor *kind* and *options* see details below.\n\nStorages can be used in vcl as `storage.`*name*, so, for example if `myStorage` was defined by `-s myStorage=malloc,5G`, it could be used in VCL like so:\n\n``` python\nset beresp.storage = storage.myStorage;\n```\n\nA special *name* is `Transient` which is the default storage for uncacheable objects as resulting from a pass, hit-for-miss or hit-for-pass.\n\nIf no `-s` options are given, the default is:\n\n``` python\n-s default,100m\n```\n\nIf no `Transient` storage is defined, the default is an unbound `default` storage as if defined as:\n\n``` python\n-s Transient=default\n```\n\nThe following storage types and options are available:\n\n-s `<default[,size]>`  \nThe default storage type resolves to `umem` where available and `malloc` otherwise.\n\n-s `<malloc[,size]>`  \nmalloc is a memory based backend.\n\n-s `<umem[,size]>`  \numem is a storage backend which is more efficient than malloc on platforms where it is available.\n\nSee the section on umem in chapter `Storage backends` of `The Varnish Users Guide` for details.\n\n-s `<file,path[,size[,granularity[,advice]]]>`  \nThe file backend stores data in a file on disk. The file will be accessed using mmap. Note that this storage provide no cache persistence.\n\nThe path is mandatory. If path points to a directory, a temporary file will be created in that directory and immediately unlinked. If path points to a non-existing file, the file will be created.\n\nIf size is omitted, and path points to an existing file with a size greater than zero, the size of that file will be used. If not, an error is reported.\n\nGranularity sets the allocation block size. Defaults to the system page size or the filesystem block size, whichever is larger.\n\nAdvice tells the kernel how `varnishd` expects to use this mapped region so that the kernel can choose the appropriate read-ahead and caching techniques. Possible values are `normal`, `random` and `sequential`, corresponding to MADV_NORMAL, MADV_RANDOM and MADV_SEQUENTIAL madvise() advice argument, respectively. Defaults to `random`.\n\n-s `<persistent,path,size>`  \nPersistent storage. Varnish will store objects in a file in a manner that will secure the survival of *most* of the objects in the event of a planned or unplanned shutdown of Varnish. The persistent storage backend has multiple issues with it and will likely be removed from a future version of Varnish.\n\n#### Jail\n\nVarnish jails are a generalization over various platform specific methods to reduce the privileges of varnish processes. They may have specific options. Available jails are:\n\n-j `` <solaris[,worker=`privspec`]> ``  \nReduce `privileges(5)` for `varnishd` and sub-processes to the minimally required set. Only available on platforms which have the `setppriv(2)` call.\n\nThe optional `worker` argument can be used to pass a privilege-specification (see `ppriv(1)`) by which to extend the effective set of the varnish worker process. While extended privileges may be required by custom vmods, *not* using the `worker` option is always more secure.\n\nExample to grant basic privileges to the worker process:\n\n``` python\n-j solaris,worker=basic\n```\n\n-j `` <unix[,user=`user`][,ccgroup=`group`][,workuser=`user`]> ``  \nDefault on all other platforms when `varnishd` is started with an effective uid of 0 (“as root”).\n\nWith the `unix` jail mechanism activated, varnish will switch to an alternative user for subprocesses and change the effective uid of the master process whenever possible.\n\nThe optional `user` argument specifies which alternative user to use. It defaults to `varnish`.\n\nThe optional `ccgroup` argument specifies a group to add to varnish subprocesses requiring access to a c-compiler. There is no default.\n\nThe optional `workuser` argument specifies an alternative user to use for the worker process. It defaults to `vcache`.\n\nThe users given for the `user` and `workuser` arguments need to have the same primary (“login”) group.\n\nTo set up a system for the default users with a group name `varnish`, shell commands similar to these may be used:\n\n``` python\ngroupadd varnish\nuseradd -g varnish -d /nonexistent -s /bin/false \\\n  -c \"Varnish-Cache Daemon User\" varnish\nuseradd -g varnish -d /nonexistent -s /bin/false \\\n  -c \"Varnish-Cache Worker User\" vcache\n```\n\n-j `none`  \nlast resort jail choice: With jail mechanism `none`, varnish will run all processes with the privileges it was started with.\n\n#### Management Interface\n\nIf the -T option was specified, `varnishd` will offer a command-line management interface on the specified address and port. The recommended way of connecting to the command-line management interface is through [varnishadm](varnishadm#varnishadm-1).\n\nThe commands available are documented in [varnish-cli](varnish-cli#varnish-cli-7).\n\n#### CLI Command File\n\nThe -I option makes it possible to run arbitrary management commands when `varnishd` is launched, before the worker process is started. In particular, this is the way to load configurations, apply labels to them, and make a VCL instance active that uses those labels on startup:\n\n``` python\nvcl.load panic /etc/varnish_panic.vcl\nvcl.load siteA0 /etc/varnish_siteA.vcl\nvcl.load siteB0 /etc/varnish_siteB.vcl\nvcl.load siteC0 /etc/varnish_siteC.vcl\nvcl.label siteA siteA0\nvcl.label siteB siteB0\nvcl.label siteC siteC0\nvcl.load main /etc/varnish_main.vcl\nvcl.use main\n```\n\nEvery line in the file, including the last line, must be terminated by a newline or carriage return.\n\nIf a command in the file is prefixed with ‘-’, failure will not abort the startup.\n\nNote that it is necessary to include an explicit `vcl.use` command to select which VCL should be the active VCL when relying on CLI Command File to load the configurations at startup.\n\n### RUN TIME PARAMETERS\n\n#### Run Time Parameter Flags\n\nRuntime parameters are marked with shorthand flags to avoid repeating the same text over and over in the table below. The meaning of the flags are:\n\n- `experimental`\n\n  We have no solid information about good/bad/optimal values for this parameter. Feedback with experience and observations are most welcome.\n\n- `delayed`\n\n  This parameter can be changed on the fly, but will not take effect immediately.\n\n- `restart`\n\n  The worker process must be stopped and restarted, before this parameter takes effect.\n\n- `reload`\n\n  The VCL programs must be reloaded for this parameter to take effect.\n\n- `wizard`\n\n  Do not touch unless you *really* know what you’re doing.\n\n- `only_root`\n\n  Only works if `varnishd` is running as root.\n\n#### Default Value Exceptions on 32 bit Systems\n\nBe aware that on 32 bit systems, certain default or maximum values are reduced relative to the values listed below, in order to conserve VM space:\n\n- workspace_client: 24k\n- workspace_backend: 20k\n- http_resp_size: 8k\n- http_req_size: 12k\n- gzip_buffer: 4k\n- vsl_buffer: 4k\n- vsl_space: 1G (maximum)\n- thread_pool_stack: 64k\n\n#### List of Parameters\n\nThis text is produced from the same text you will find in the CLI if you use the param.show command:\n\n##### accept_filter\n\nNB: This parameter depends on a feature which is not available on all platforms.\n\n- Units: bool\n- Default: on (if your platform supports accept filters)\n- Flags: must_restart\n\nEnable kernel accept-filters. This may require a kernel module to be loaded to have an effect when enabled.\n\nEnabling accept_filter may prevent some requests to reach Varnish in the first place. Malformed requests may go unnoticed and not increase the client_req_400 counter. GET or HEAD requests with a body may be blocked altogether.\n\n##### acceptor_sleep_decay\n\n- Default: 0.9\n- Minimum: 0\n- Maximum: 1\n- Flags: experimental\n\nIf we run out of resources, such as file descriptors or worker threads, the acceptor will sleep between accepts. This parameter (multiplicatively) reduce the sleep duration for each successful accept. (ie: 0.9 = reduce by 10%)\n\n##### acceptor_sleep_incr\n\n- Units: seconds\n- Default: 0.000\n- Minimum: 0.000\n- Maximum: 1.000\n- Flags: experimental\n\nIf we run out of resources, such as file descriptors or worker threads, the acceptor will sleep between accepts. This parameter control how much longer we sleep, each time we fail to accept a new connection.\n\n##### acceptor_sleep_max\n\n- Units: seconds\n- Default: 0.050\n- Minimum: 0.000\n- Maximum: 10.000\n- Flags: experimental\n\nIf we run out of resources, such as file descriptors or worker threads, the acceptor will sleep between accepts. This parameter limits how long it can sleep between attempts to accept new connections.\n\n##### auto_restart\n\n- Units: bool\n- Default: on\n\nAutomatically restart the child/worker process if it dies.\n\n##### backend_idle_timeout\n\n- Units: seconds\n- Default: 60.000\n- Minimum: 1.000\n\nTimeout before we close unused backend connections.\n\n##### backend_local_error_holddown\n\n- Units: seconds\n- Default: 10.000\n- Minimum: 0.000\n- Flags: experimental\n\nWhen connecting to backends, certain error codes (EADDRNOTAVAIL, EACCESS, EPERM) signal a local resource shortage or configuration issue for which retrying connection attempts may worsen the situation due to the complexity of the operations involved in the kernel. This parameter prevents repeated connection attempts for the configured duration.\n\n##### backend_remote_error_holddown\n\n- Units: seconds\n- Default: 0.250\n- Minimum: 0.000\n- Flags: experimental\n\nWhen connecting to backends, certain error codes (ECONNREFUSED, ENETUNREACH) signal fundamental connection issues such as the backend not accepting connections or routing problems for which repeated connection attempts are considered useless This parameter prevents repeated connection attempts for the configured duration.\n\n##### ban_cutoff\n\n- Units: bans\n- Default: 0\n- Minimum: 0\n- Flags: experimental\n\nExpurge long tail content from the cache to keep the number of bans below this value. 0 disables.\n\nWhen this parameter is set to a non-zero value, the ban lurker continues to work the ban list as usual top to bottom, but when it reaches the ban_cutoff-th ban, it treats all objects as if they matched a ban and expurges them from cache. As actively used objects get tested against the ban list at request time and thus are likely to be associated with bans near the top of the ban list, with ban_cutoff, least recently accessed objects (the “long tail”) are removed.\n\nThis parameter is a safety net to avoid bad response times due to bans being tested at lookup time. Setting a cutoff trades response time for cache efficiency. The recommended value is proportional to rate(bans_lurker_tests_tested) / n_objects while the ban lurker is working, which is the number of bans the system can sustain. The additional latency due to request ban testing is in the order of ban_cutoff / rate(bans_lurker_tests_tested). For example, for rate(bans_lurker_tests_tested) = 2M/s and a tolerable latency of 100ms, a good value for ban_cutoff may be 200K.\n\n##### ban_dups\n\n- Units: bool\n- Default: on\n\nEliminate older identical bans when a new ban is added. This saves CPU cycles by not comparing objects to identical bans. This is a waste of time if you have many bans which are never identical.\n\n##### ban_lurker_age\n\n- Units: seconds\n- Default: 60.000\n- Minimum: 0.000\n\nThe ban lurker will ignore bans until they are this old. When a ban is added, the active traffic will be tested against it as part of object lookup. Because many applications issue bans in bursts, this parameter holds the ban-lurker off until the rush is over. This should be set to the approximate time which a ban-burst takes.\n\n##### ban_lurker_batch\n\n- Default: 1000\n- Minimum: 1\n\nThe ban lurker sleeps ${ban_lurker_sleep} after examining this many objects. Use this to pace the ban-lurker if it eats too many resources.\n\n##### ban_lurker_holdoff\n\n- Units: seconds\n- Default: 0.010\n- Minimum: 0.000\n- Flags: experimental\n\nHow long the ban lurker sleeps when giving way to lookup due to lock contention.\n\n##### ban_lurker_sleep\n\n- Units: seconds\n- Default: 0.010\n- Minimum: 0.000\n\nHow long the ban lurker sleeps after examining ${ban_lurker_batch} objects. Use this to pace the ban-lurker if it eats too many resources. A value of zero will disable the ban lurker entirely.\n\n##### between_bytes_timeout\n\n- Units: seconds\n- Default: 60.000\n- Minimum: 0.000\n\nWe only wait for this many seconds between bytes received from the backend before giving up the fetch. VCL values, per backend or per backend request take precedence. This parameter does not apply to pipe’ed requests.\n\n##### cc_command\n\nNB: The actual default value for this parameter depends on the Varnish build environment and options.\n\n- Default: exec $CC $CFLAGS %w -shared -o %o %s\n- Flags: must_reload\n\nThe command used for compiling the C source code to a dlopen(3) loadable object. The following expansions can be used:\n\n- %s: the source file name\n- %o: the output file name\n- %w: the cc_warnings parameter\n- %d: the raw default cc_command\n- %D: the expanded default cc_command\n- %n: the working directory (-n option)\n- %%: a percent sign\n\nUnknown percent expansion sequences are ignored, and to avoid future incompatibilities percent characters should be escaped with a double percent sequence.\n\nThe %d and %D expansions allow passing the parameter’s default value to a wrapper script to perform additional processing.\n\n##### cc_warnings\n\nNB: The actual default value for this parameter depends on the Varnish build environment and options.\n\n- Default: -Wall -Werror\n- Flags: must_reload\n\nWarnings used when compiling the C source code with the cc_command parameter. By default, VCL is compiled with the same set of warnings as Varnish itself.\n\n##### cli_limit\n\n- Units: bytes\n- Default: 48k\n- Minimum: 128b\n- Maximum: 99999999b\n\nMaximum size of CLI response. If the response exceeds this limit, the response code will be 201 instead of 200 and the last line will indicate the truncation.\n\n##### cli_timeout\n\n- Units: seconds\n- Default: 60.000\n- Minimum: 0.000\n\nTimeout for the child’s replies to CLI requests.\n\n##### clock_skew\n\n- Units: seconds\n- Default: 10\n- Minimum: 0\n\nHow much clockskew we are willing to accept between the backend and our own clock.\n\n##### clock_step\n\n- Units: seconds\n- Default: 1.000\n- Minimum: 0.000\n\nHow much observed clock step we are willing to accept before we panic.\n\n##### connect_timeout\n\n- Units: seconds\n- Default: 3.500\n- Minimum: 0.000\n\nDefault connection timeout for backend connections. We only try to connect to the backend for this many seconds before giving up. VCL can override this default value for each backend and backend request.\n\n##### critbit_cooloff\n\n- Units: seconds\n- Default: 180.000\n- Minimum: 60.000\n- Maximum: 254.000\n- Flags: wizard\n\nHow long the critbit hasher keeps deleted objheads on the cooloff list.\n\n##### debug\n\n- Default: none\n\nEnable/Disable various kinds of debugging.\n\n*none*  \nDisable all debugging\n\nUse +/- prefix to set/reset individual bits:\n\n*req_state*  \nVSL Request state engine\n\n*workspace*  \nVSL Workspace operations\n\n*waitinglist*  \nVSL Waitinglist events\n\n*syncvsl*  \nMake VSL synchronous\n\n*hashedge*  \nEdge cases in Hash\n\n*vclrel*  \nRapid VCL release\n\n*lurker*  \nVSL Ban lurker\n\n*esi_chop*  \nChop ESI fetch to bits\n\n*flush_head*  \nFlush after http1 head\n\n*vtc_mode*  \nVarnishtest Mode\n\n*witness*  \nEmit WITNESS lock records\n\n*vsm_keep*  \nKeep the VSM file on restart\n\n*slow_acceptor*  \nSlow down Acceptor\n\n*h2_nocheck*  \nDisable various H2 checks\n\n*vmod_so_keep*  \nKeep copied VMOD libraries\n\n*processors*  \nFetch/Deliver processors\n\n*protocol*  \nProtocol debugging\n\n*vcl_keep*  \nKeep VCL C and so files\n\n*lck*  \nAdditional lock statistics\n\n##### default_grace\n\n- Units: seconds\n- Default: 10s\n- Minimum: 0.000\n- Flags: obj_sticky\n\nDefault grace period. We will deliver an object this long after it has expired, provided another thread is attempting to get a new copy.\n\n##### default_keep\n\n- Units: seconds\n- Default: 0s\n- Minimum: 0.000\n- Flags: obj_sticky\n\nDefault keep period. We will keep a useless object around this long, making it available for conditional backend fetches. That means that the object will be removed from the cache at the end of ttl+grace+keep.\n\n##### default_ttl\n\n- Units: seconds\n- Default: 2m\n- Minimum: 0.000\n- Flags: obj_sticky\n\nThe TTL assigned to objects if neither the backend nor the VCL code assigns one.\n\n##### experimental\n\n- Default: none\n\nEnable/Disable experimental features.\n\n*none*  \nDisable all experimental features\n\nUse +/- prefix to set/reset individual bits:\n\n*drop_pools*  \nDrop thread pools\n\n##### feature\n\n- Default: +validate_headers\n\nEnable/Disable various minor features.\n\n*default*  \nSet default value\n\n*none*  \nDisable all features.\n\nUse +/- prefix to enable/disable individual feature:\n\n*http2*  \nEnable HTTP/2 protocol support.\n\n*short_panic*  \nShort panic message.\n\n*no_coredump*  \nNo coredumps. Must be set before child process starts.\n\n*https_scheme*  \nExtract host from full URI in the HTTP/1 request line, if the scheme is https.\n\n*http_date_postel*  \nTolerate non compliant timestamp headers like `Date`, `Last-Modified`, `Expires` etc.\n\n*esi_ignore_https*  \nConvert `<esi:include src”https://…` to `http://…`\n\n*esi_disable_xml_check*  \nAllow ESI processing on non-XML ESI bodies\n\n*esi_ignore_other_elements*  \nIgnore XML syntax errors in ESI bodies.\n\n*esi_remove_bom*  \nIgnore UTF-8 BOM in ESI bodies.\n\n*esi_include_onerror*  \nParse the onerror attribute of \\<esi:include\\> tags.\n\n*wait_silo*  \nWait for persistent silos to completely load before serving requests.\n\n*validate_headers*  \nValidate all header set operations to conform to RFC7230.\n\n*busy_stats_rate*  \nMake busy workers comply with thread_stats_rate.\n\n*trace*  \nEnable VCL tracing by default (enable (be)req.trace). Required for tracing vcl_init / vcl_fini\n\n##### fetch_chunksize\n\n- Units: bytes\n- Default: 16k\n- Minimum: 4k\n- Flags: experimental\n\nThe default chunksize used by fetcher. This should be bigger than the majority of objects with short TTLs. Internal limits in the storage_file module makes increases above 128kb a dubious idea.\n\n##### fetch_maxchunksize\n\n- Units: bytes\n- Default: 0.25G\n- Minimum: 64k\n- Flags: experimental\n\nThe maximum chunksize we attempt to allocate from storage. Making this too large may cause delays and storage fragmentation.\n\n##### first_byte_timeout\n\n- Units: seconds\n- Default: 60.000\n- Minimum: 0.000\n\nDefault timeout for receiving first byte from backend. We only wait for this many seconds for the first byte before giving up. VCL can override this default value for each backend and backend request. This parameter does not apply to pipe’ed requests.\n\n##### gzip_buffer\n\n- Units: bytes\n- Default: 32k\n- Minimum: 2k\n- Flags: experimental\n\nSize of malloc buffer used for gzip processing. These buffers are used for in-transit data, for instance gunzip’ed data being sent to a client.Making this space to small results in more overhead, writes to sockets etc, making it too big is probably just a waste of memory.\n\n##### gzip_level\n\n- Default: 6\n- Minimum: 0\n- Maximum: 9\n\nGzip compression level: 0=debug, 1=fast, 9=best\n\n##### gzip_memlevel\n\n- Default: 8\n- Minimum: 1\n- Maximum: 9\n\nGzip memory level 1=slow/least, 9=fast/most compression. Memory impact is 1=1k, 2=2k, … 9=256k.\n\n##### h2_header_table_size\n\n- Units: bytes\n- Default: 4k\n- Minimum: 0b\n\nHTTP2 header table size. This is the size that will be used for the HPACK dynamic decoding table.\n\n##### h2_initial_window_size\n\n- Units: bytes\n- Default: 65535b\n- Minimum: 65535b\n- Maximum: 2147483647b\n\nHTTP2 initial flow control window size.\n\n##### h2_max_concurrent_streams\n\n- Units: streams\n- Default: 100\n- Minimum: 0\n\nHTTP2 Maximum number of concurrent streams. This is the number of requests that can be active at the same time for a single HTTP2 connection.\n\n##### h2_max_frame_size\n\n- Units: bytes\n- Default: 16k\n- Minimum: 16k\n- Maximum: 16777215b\n\nHTTP2 maximum per frame payload size we are willing to accept.\n\n##### h2_max_header_list_size\n\n- Units: bytes\n- Default: 2147483647b\n- Minimum: 0b\n\nHTTP2 maximum size of an uncompressed header list.\n\n##### h2_rx_window_increment\n\n- Units: bytes\n- Default: 1M\n- Minimum: 1M\n- Maximum: 1G\n- Flags: wizard\n\nHTTP2 Receive Window Increments. How big credits we send in WINDOW_UPDATE frames Only affects incoming request bodies (ie: POST, PUT etc.)\n\n##### h2_rx_window_low_water\n\n- Units: bytes\n- Default: 10M\n- Minimum: 65535b\n- Maximum: 1G\n- Flags: wizard\n\nHTTP2 Receive Window low water mark. We try to keep the window at least this big Only affects incoming request bodies (ie: POST, PUT etc.)\n\n##### h2_rxbuf_storage\n\n- Default: Transient\n- Flags: must_restart\n\nThe name of the storage backend that HTTP/2 receive buffers should be allocated from.\n\n##### http1_iovs\n\n- Units: struct iovec (=16 bytes)\n- Default: 64\n- Minimum: 5\n- Maximum: 1024\n- Flags: wizard\n\nNumber of io vectors to allocate for HTTP1 protocol transmission. A HTTP1 header needs 7 + 2 per HTTP header field. Allocated from workspace_thread. This parameter affects only io vectors used for client delivery. For backend fetches, the maximum number of io vectors (up to IOV_MAX) is allocated from available workspace_thread memory.\n\n##### http_gzip_support\n\n- Units: bool\n- Default: on\n\nEnable gzip support. When enabled Varnish request compressed objects from the backend and store them compressed. If a client does not support gzip encoding Varnish will uncompress compressed objects on demand. Varnish will also rewrite the Accept-Encoding header of clients indicating support for gzip to:  \nAccept-Encoding: gzip\n\nClients that do not support gzip will have their Accept-Encoding header removed. For more information on how gzip is implemented please see the chapter on gzip in the Varnish reference.\n\nWhen gzip support is disabled the variables beresp.do_gzip and beresp.do_gunzip have no effect in VCL.\n\n##### http_max_hdr\n\n- Units: header lines\n- Default: 64\n- Minimum: 32\n- Maximum: 65535\n\nMaximum number of HTTP header lines we allow in {req\\|resp\\|bereq\\|beresp}.http (obj.http is autosized to the exact number of headers). Cheap, ~20 bytes, in terms of workspace memory. Note that the first line occupies five header lines.\n\n##### http_range_support\n\n- Units: bool\n- Default: on\n\nEnable support for HTTP Range headers.\n\n##### http_req_hdr_len\n\n- Units: bytes\n- Default: 8k\n- Minimum: 40b\n\nMaximum length of any HTTP client request header we will allow. The limit is inclusive its continuation lines.\n\n##### http_req_size\n\n- Units: bytes\n- Default: 32k\n- Minimum: 0.25k\n\nMaximum number of bytes of HTTP client request we will deal with. This is a limit on all bytes up to the double blank line which ends the HTTP request. The memory for the request is allocated from the client workspace (param: workspace_client) and this parameter limits how much of that the request is allowed to take up.\n\n##### http_resp_hdr_len\n\n- Units: bytes\n- Default: 8k\n- Minimum: 40b\n\nMaximum length of any HTTP backend response header we will allow. The limit is inclusive its continuation lines.\n\n##### http_resp_size\n\n- Units: bytes\n- Default: 32k\n- Minimum: 0.25k\n\nMaximum number of bytes of HTTP backend response we will deal with. This is a limit on all bytes up to the double blank line which ends the HTTP response. The memory for the response is allocated from the backend workspace (param: workspace_backend) and this parameter limits how much of that the response is allowed to take up.\n\n##### idle_send_timeout\n\n- Units: seconds\n- Default: 60.000\n- Minimum: 0.000\n- Flags: delayed\n\nSend timeout for individual pieces of data on client connections. May get extended if ‘send_timeout’ applies.\n\nWhen this timeout is hit, the session is closed.\n\nSee the man page for `setsockopt(2)` or `socket(7)` under `SO_SNDTIMEO` for more information.\n\n##### listen_depth\n\n- Units: connections\n- Default: 1024\n- Minimum: 0\n- Flags: must_restart\n\nListen queue depth.\n\n##### lru_interval\n\n- Units: seconds\n- Default: 2.000\n- Minimum: 0.000\n- Flags: experimental\n\nGrace period before object moves on LRU list. Objects are only moved to the front of the LRU list if they have not been moved there already inside this timeout period. This reduces the amount of lock operations necessary for LRU list access.\n\n##### max_esi_depth\n\n- Units: levels\n- Default: 5\n- Minimum: 0\n\nMaximum depth of esi:include processing.\n\n##### max_restarts\n\n- Units: restarts\n- Default: 4\n- Minimum: 0\n\nUpper limit on how many times a request can restart.\n\n##### max_retries\n\n- Units: retries\n- Default: 4\n- Minimum: 0\n\nUpper limit on how many times a backend fetch can retry.\n\n##### max_vcl\n\n- Default: 100\n- Minimum: 0\n\nThreshold of loaded VCL programs. (VCL labels are not counted.) Parameter max_vcl_handling determines behaviour.\n\n##### max_vcl_handling\n\n- Default: 1\n- Minimum: 0\n- Maximum: 2\n\nBehaviour when attempting to exceed max_vcl loaded VCL.\n\n- 0 - Ignore max_vcl parameter.\n- 1 - Issue warning.\n- 2 - Refuse loading VCLs.\n\n##### nuke_limit\n\n- Units: allocations\n- Default: 50\n- Minimum: 0\n- Flags: experimental\n\nMaximum number of objects we attempt to nuke in order to make space for a object body.\n\n##### pcre2_depth_limit\n\n- Default: 20\n- Minimum: 1\n\nThe recursion depth-limit for the internal match logic in a pcre2_match().\n\n(See: pcre2_set_depth_limit() in pcre2 docs.)\n\nThis puts an upper limit on the amount of stack used by PCRE2 for certain classes of regular expressions.\n\nWe have set the default value low in order to prevent crashes, at the cost of possible regexp matching failures.\n\nMatching failures will show up in the log as VCL_Error messages.\n\n##### pcre2_jit_compilation\n\n- Units: bool\n- Default: on\n\nUse the pcre2 JIT compiler if available.\n\n##### pcre2_match_limit\n\n- Default: 10000\n- Minimum: 1\n\nThe limit for the number of calls to the internal match logic in pcre2_match().\n\n(See: pcre2_set_match_limit() in pcre2 docs.)\n\nThis parameter limits how much CPU time regular expression matching can soak up.\n\n##### ping_interval\n\n- Units: seconds\n- Default: 3\n- Minimum: 0\n- Flags: must_restart\n\nInterval between pings from parent to child. Zero will disable pinging entirely, which makes it possible to attach a debugger to the child.\n\n##### pipe_sess_max\n\n- Units: connections\n- Default: 0\n- Minimum: 0\n\nMaximum number of sessions dedicated to pipe transactions.\n\n##### pipe_timeout\n\n- Units: seconds\n- Default: 60.000\n- Minimum: 0.000\n\nIdle timeout for PIPE sessions. If nothing have been received in either direction for this many seconds, the session is closed.\n\n##### pool_req\n\n- Default: 10,100,10\n\nParameters for per worker pool request memory pool.\n\nThe three numbers are:\n\n*min_pool*  \nminimum size of free pool.\n\n*max_pool*  \nmaximum size of free pool.\n\n*max_age*  \nmax age of free element.\n\n##### pool_sess\n\n- Default: 10,100,10\n\nParameters for per worker pool session memory pool.\n\nThe three numbers are:\n\n*min_pool*  \nminimum size of free pool.\n\n*max_pool*  \nmaximum size of free pool.\n\n*max_age*  \nmax age of free element.\n\n##### pool_vbo\n\n- Default: 10,100,10\n\nParameters for backend object fetch memory pool.\n\nThe three numbers are:\n\n*min_pool*  \nminimum size of free pool.\n\n*max_pool*  \nmaximum size of free pool.\n\n*max_age*  \nmax age of free element.\n\n##### prefer_ipv6\n\n- Units: bool\n- Default: off\n\nPrefer IPv6 address when connecting to backends which have both IPv4 and IPv6 addresses.\n\n##### rush_exponent\n\n- Units: requests per request\n- Default: 3\n- Minimum: 2\n- Flags: experimental\n\nHow many parked request we start for each completed request on the object. NB: Even with the implict delay of delivery, this parameter controls an exponential increase in number of worker threads.\n\n##### send_timeout\n\n- Units: seconds\n- Default: 600.000\n- Minimum: 0.000\n- Flags: delayed\n\nTotal timeout for ordinary HTTP1 responses. Does not apply to some internally generated errors and pipe mode.\n\nWhen ‘idle_send_timeout’ is hit while sending an HTTP1 response, the timeout is extended unless the total time already taken for sending the response in its entirety exceeds this many seconds.\n\nWhen this timeout is hit, the session is closed\n\n##### shortlived\n\n- Units: seconds\n- Default: 10.000\n- Minimum: 0.000\n\nObjects created with (ttl+grace+keep) shorter than this are always put in transient storage.\n\n##### sigsegv_handler\n\n- Units: bool\n- Default: on\n- Flags: must_restart\n\nInstall a signal handler which tries to dump debug information on segmentation faults, bus errors and abort signals.\n\n##### startup_timeout\n\n- Units: seconds\n- Default: 0.000\n- Minimum: 0.000\n\nAlternative timeout for the initial worker process startup. If cli_timeout is longer than startup_timeout, it is used instead.\n\n##### syslog_cli_traffic\n\n- Units: bool\n- Default: on\n\nLog all CLI traffic to syslog(LOG_INFO).\n\n##### tcp_fastopen\n\nNB: This parameter depends on a feature which is not available on all platforms.\n\n- Units: bool\n- Default: off\n- Flags: must_restart\n\nEnable TCP Fast Open extension.\n\n##### tcp_keepalive_intvl\n\nNB: This parameter depends on a feature which is not available on all platforms.\n\n- Units: seconds\n- Default: platform dependent\n- Minimum: 1.000\n- Maximum: 100.000\n- Flags: experimental\n\nThe number of seconds between TCP keep-alive probes. Ignored for Unix domain sockets.\n\n##### tcp_keepalive_probes\n\nNB: This parameter depends on a feature which is not available on all platforms.\n\n- Units: probes\n- Default: platform dependent\n- Minimum: 1\n- Maximum: 100\n- Flags: experimental\n\nThe maximum number of TCP keep-alive probes to send before giving up and killing the connection if no response is obtained from the other end. Ignored for Unix domain sockets.\n\n##### tcp_keepalive_time\n\nNB: This parameter depends on a feature which is not available on all platforms.\n\n- Units: seconds\n- Default: platform dependent\n- Minimum: 1.000\n- Maximum: 7200.000\n- Flags: experimental\n\nThe number of seconds a connection needs to be idle before TCP begins sending out keep-alive probes. Ignored for Unix domain sockets.\n\n##### thread_pool_add_delay\n\n- Units: seconds\n- Default: 0.000\n- Minimum: 0.000\n- Flags: experimental\n\nWait at least this long after creating a thread.\n\nSome (buggy) systems may need a short (sub-second) delay between creating threads. Set this to a few milliseconds if you see the ‘threads_failed’ counter grow too much.\n\nSetting this too high results in insufficient worker threads.\n\n##### thread_pool_destroy_delay\n\n- Units: seconds\n- Default: 1.000\n- Minimum: 0.010\n- Flags: delayed, experimental\n\nWait this long after destroying a thread.\n\nThis controls the decay of thread pools when idle(-ish).\n\n##### thread_pool_fail_delay\n\n- Units: seconds\n- Default: 0.200\n- Minimum: 0.010\n- Flags: experimental\n\nWait at least this long after a failed thread creation before trying to create another thread.\n\nFailure to create a worker thread is often a sign that the end is near, because the process is running out of some resource. This delay tries to not rush the end on needlessly.\n\nIf thread creation failures are a problem, check that thread_pool_max is not too high.\n\nIt may also help to increase thread_pool_timeout and thread_pool_min, to reduce the rate at which treads are destroyed and later recreated.\n\n##### thread_pool_max\n\n- Units: threads\n- Default: 5000\n- Minimum: thread_pool_min\n- Flags: delayed\n\nThe maximum number of worker threads in each pool.\n\nDo not set this higher than you have to, since excess worker threads soak up RAM and CPU and generally just get in the way of getting work done.\n\n##### thread_pool_min\n\n- Units: threads\n- Default: 100\n- Minimum: 5\n- Maximum: thread_pool_max\n- Flags: delayed\n\nThe minimum number of worker threads in each pool.\n\nIncreasing this may help ramp up faster from low load situations or when threads have expired.\n\nTechnical minimum is 5 threads, but this parameter is strongly recommended to be at least 10\n\n##### thread_pool_reserve\n\n- Units: threads\n- Default: 0 (auto-tune: 5% of thread_pool_min)\n- Maximum: 95% of thread_pool_min\n- Flags: delayed\n\nThe number of worker threads reserved for vital tasks in each pool.\n\nTasks may require other tasks to complete (for example, client requests may require backend requests, http2 sessions require streams, which require requests). This reserve is to ensure that lower priority tasks do not prevent higher priority tasks from running even under high load.\n\nThe effective value is at least 5 (the number of internal priority classes), irrespective of this parameter.\n\n##### thread_pool_stack\n\n- Units: bytes\n- Default: 80k\n- Minimum: sysconf(\\_SC_THREAD_STACK_MIN)\n- Flags: delayed\n\nWorker thread stack size. This will likely be rounded up to a multiple of 4k (or whatever the page_size might be) by the kernel.\n\nThe required stack size is primarily driven by the depth of the call-tree. The most common relevant determining factors in varnish core code are GZIP (un)compression, ESI processing and regular expression matches. VMODs may also require significant amounts of additional stack. The nesting depth of VCL subs is another factor, although typically not predominant.\n\nThe stack size is per thread, so the maximum total memory required for worker thread stacks is in the order of size = thread_pools x thread_pool_max x thread_pool_stack.\n\nThus, in particular for setups with many threads, keeping the stack size at a minimum helps reduce the amount of memory required by Varnish.\n\nOn the other hand, thread_pool_stack must be large enough under all circumstances, otherwise varnish will crash due to a stack overflow. Usually, a stack overflow manifests itself as a segmentation fault (aka segfault / SIGSEGV) with the faulting address being near the stack pointer (sp).\n\nUnless stack usage can be reduced, thread_pool_stack must be increased when a stack overflow occurs. Setting it in 150%-200% increments is recommended until stack overflows cease to occur.\n\n##### thread_pool_timeout\n\n- Units: seconds\n- Default: 300.000\n- Minimum: 10.000\n- Flags: delayed, experimental\n\nThread idle threshold.\n\nThreads in excess of thread_pool_min, which have been idle for at least this long, will be destroyed.\n\n##### thread_pool_watchdog\n\n- Units: seconds\n- Default: 60.000\n- Minimum: 0.100\n- Flags: experimental\n\nThread queue stuck watchdog.\n\nIf no queued work have been released for this long, the worker process panics itself.\n\n##### thread_pools\n\n- Units: pools\n- Default: 2\n- Minimum: 1\n- Maximum: 32\n- Flags: delayed, experimental\n\nNumber of worker thread pools.\n\nIncreasing the number of worker pools decreases lock contention. Each worker pool also has a thread accepting new connections, so for very high rates of incoming new connections on systems with many cores, increasing the worker pools may be required.\n\nToo many pools waste CPU and RAM resources, and more than one pool for each CPU is most likely detrimental to performance.\n\nCan be increased on the fly, but decreases require a restart to take effect, unless the drop_pools experimental debug flag is set.\n\n##### thread_queue_limit\n\n- Units: requests\n- Default: 20\n- Minimum: 0\n- Flags: experimental\n\nPermitted request queue length per thread-pool.\n\nThis sets the number of requests we will queue, waiting for an available thread. Above this limit sessions will be dropped instead of queued.\n\n##### thread_stats_rate\n\n- Units: requests\n- Default: 10\n- Minimum: 0\n- Flags: experimental\n\nWorker threads accumulate statistics, and dump these into the global stats counters if the lock is free when they finish a job (request/fetch etc.) This parameters defines the maximum number of jobs a worker thread may handle, before it is forced to dump its accumulated stats into the global counters.\n\n##### timeout_idle\n\n- Units: seconds\n- Default: 5.000\n- Minimum: 0.000\n\nIdle timeout for client connections.\n\nA connection is considered idle until we have received the full request headers.\n\nThis parameter is particularly relevant for HTTP1 keepalive connections which are closed unless the next request is received before this timeout is reached.\n\n##### timeout_linger\n\n- Units: seconds\n- Default: 0.050\n- Minimum: 0.000\n- Flags: experimental\n\nHow long the worker thread lingers on an idle session before handing it over to the waiter. When sessions are reused, as much as half of all reuses happen within the first 100 msec of the previous request completing. Setting this too high results in worker threads not doing anything for their keep, setting it too low just means that more sessions take a detour around the waiter.\n\n##### transit_buffer\n\n- Units: bytes\n- Default: 0b\n- Minimum: 0b\n\nThe number of bytes which Varnish buffers for uncacheable backend streaming fetches - in other words, how many bytes Varnish reads from the backend ahead of what has been sent to the client. A zero value means no limit, the object is fetched as fast as possible.\n\nWhen dealing with slow clients, setting this parameter to non-zero can prevent large uncacheable objects from being stored in full when the intent is to simply stream them to the client. As a result, a slow client transaction holds onto a backend connection until the end of the delivery.\n\nThis parameter is the default to the VCL variable `beresp.transit_buffer`, which can be used to control the transit buffer per backend request.\n\n##### vary_notice\n\n- Units: variants\n- Default: 10\n- Minimum: 1\n\nHow many variants need to be evaluated to log a Notice that there might be too many variants.\n\n##### vcc_allow_inline_c\n\nDeprecated alias for the vcc_feature parameter.\n\n##### vcc_err_unref\n\nDeprecated alias for the vcc_feature parameter.\n\n##### vcc_feature\n\n- Default: +err_unref,+unsafe_path\n\nEnable/Disable various VCC behaviors.\n\n*default*  \nSet default value\n\n*none*  \nDisable all behaviors.\n\nUse +/- prefix to enable/disable individual behavior:\n\n*err_unref*  \nUnreferenced VCL objects result in error.\n\n*allow_inline_c*  \nAllow inline C code in VCL.\n\n*unsafe_path*  \nAllow ‘/’ in vmod & include paths. Allow ‘import … from …’.\n\n##### vcc_unsafe_path\n\nDeprecated alias for the vcc_feature parameter.\n\n##### vcl_cooldown\n\n- Units: seconds\n- Default: 600.000\n- Minimum: 1.000\n\nHow long a VCL is kept warm after being replaced as the active VCL (granularity approximately 30 seconds).\n\n##### vcl_path\n\nNB: The actual default value for this parameter depends on the Varnish build environment and options.\n\n- Default: ${sysconfdir}/varnish:${datadir}/varnish/vcl\n\nDirectory (or colon separated list of directories) from which relative VCL filenames (vcl.load and include) are to be found. By default Varnish searches VCL files in both the system configuration and shared data directories to allow packages to drop their VCL files in a standard location where relative includes would work.\n\n##### vmod_path\n\nNB: The actual default value for this parameter depends on the Varnish build environment and options.\n\n- Default: ${libdir}/varnish/vmods\n\nDirectory (or colon separated list of directories) where VMODs are to be found.\n\n##### vsl_buffer\n\n- Units: bytes\n- Default: 16k\n- Minimum: vsl_reclen + 12 bytes\n\nBytes of (req-/backend-)workspace dedicated to buffering VSL records. When this parameter is adjusted, most likely workspace_client and workspace_backend will have to be adjusted by the same amount.\n\nSetting this too high costs memory, setting it too low will cause more VSL flushes and likely increase lock-contention on the VSL mutex.\n\n##### vsl_mask\n\n- Default: -Debug,-ObjProtocol,-ObjStatus,-ObjReason,-ObjHeader,-ExpKill,-WorkThread,-Hash,-VfpAcct,-H2RxHdr,-H2RxBody,-H2TxHdr,-H2TxBody,-VdpAcct\n\nMask individual VSL messages from being logged.\n\n*default*  \nSet default value\n\nUse +/- prefix in front of VSL tag name to unmask/mask individual VSL messages.\n\n##### vsl_reclen\n\n- Units: bytes\n- Default: 255b\n- Minimum: 16b\n- Maximum: vsl_buffer - 12 bytes\n\nMaximum number of bytes in SHM log record.\n\n##### vsl_space\n\n- Units: bytes\n- Default: 80M\n- Minimum: 1M\n- Maximum: 4G\n- Flags: must_restart\n\nThe amount of space to allocate for the VSL fifo buffer in the VSM memory segment. If you make this too small, varnish{ncsa\\|log} etc will not be able to keep up. Making it too large just costs memory resources.\n\n##### vsm_free_cooldown\n\n- Units: seconds\n- Default: 60.000\n- Minimum: 10.000\n- Maximum: 600.000\n\nHow long VSM memory is kept warm after a deallocation (granularity approximately 2 seconds).\n\n##### workspace_backend\n\n- Units: bytes\n- Default: 96k\n- Minimum: 1k\n- Flags: delayed\n\nBytes of HTTP protocol workspace for backend HTTP req/resp. If larger than 4k, use a multiple of 4k for VM efficiency.\n\n##### workspace_client\n\n- Units: bytes\n- Default: 96k\n- Minimum: 9k\n- Flags: delayed\n\nBytes of HTTP protocol workspace for clients HTTP req/resp. Use a multiple of 4k for VM efficiency. For HTTP/2 compliance this must be at least 20k, in order to receive fullsize (=16k) frames from the client. That usually happens only in POST/PUT bodies. For other traffic-patterns smaller values work just fine.\n\n##### workspace_session\n\n- Units: bytes\n- Default: 0.75k\n- Minimum: 384b\n- Flags: delayed\n\nAllocation size for session structure and workspace. The workspace is primarily used for TCP connection addresses. If larger than 4k, use a multiple of 4k for VM efficiency.\n\n##### workspace_thread\n\n- Units: bytes\n- Default: 2k\n- Minimum: 0.25k\n- Maximum: 8k\n- Flags: delayed\n\nBytes of auxiliary workspace per thread. This workspace is used for certain temporary data structures during the operation of a worker thread. One use is for the IO-vectors used during delivery. Setting this parameter too low may increase the number of writev() syscalls, setting it too high just wastes space. ~0.1k + UIO_MAXIOV \\* sizeof(struct iovec) (typically = ~16k for 64bit) is considered the maximum sensible value under any known circumstances (excluding exotic vmod use).\n\n### EXIT CODES\n\nVarnish and bundled tools will, in most cases, exit with one of the following codes\n\n- `0` OK\n- `1` Some error which could be system-dependent and/or transient\n- `2` Serious configuration / parameter error - retrying with the same configuration / parameters is most likely useless\n\nThe `varnishd` master process may also OR its exit code\n\n- with `0x20` when the `varnishd` child process died,\n- with `0x40` when the `varnishd` child process was terminated by a signal and\n- with `0x80` when a core was dumped.\n\n### SEE ALSO\n\n- [varnishlog](varnishlog#varnishlog-1)\n- [varnishhist](varnishhist#varnishhist-1)\n- [varnishncsa](varnishncsa#varnishncsa-1)\n- [varnishstat](varnishstat#varnishstat-1)\n- [varnishtop](varnishtop#varnishtop-1)\n- [varnish-cli](varnish-cli#varnish-cli-7)\n- [VCL](vcl#vcl-7)\n\n### HISTORY\n\nThe `varnishd` daemon was developed by Poul-Henning Kamp in cooperation with Verdens Gang AS and Varnish Software.\n\nThis manual page was written by Dag-Erling Smørgrav with updates by Stig Sandbeck Mathisen \\<[ssm@debian.org](mailto:ssm%40debian.org)\\>, Nils Goroll and others.\n\n### COPYRIGHT\n\nThis document is licensed under the same licence as Varnish itself. See LICENCE for details.\n\n- Copyright (c) 2007-2015 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/varnishd.html](https://varnish-cache.org/docs/7.4/reference/varnishd.html)"
- name: varnishhist
  id: reference/varnishhist
  summary: The varnishhist utility reads varnishd(1) shared memory logs and presents a continuously updated histogram showing the distribution of the last N requests by their processing
  description: "# varnishhist\n\n## Varnish request histogram\n\nManual section:  \n1\n\n### SYNOPSIS\n\nvarnishhist \\[-B \\<factor\\>\\] \\[-C\\] \\[-d\\] \\[-g \\<request\\|vxid\\>\\] \\[-h\\] \\[-L \\<limit\\>\\] \\[-n \\<dir\\>\\] \\[-p \\<period\\>\\] \\[-P responsetime\\] \\[-P reqbodytime\\] \\[-P size\\] \\[-P Bereqtime\\] \\[-P Beresptime\\] \\[-P BerespBodytime\\] \\[-P Besize\\] \\[-P \\<\\[cb:\\]tag:\\[prefix\\]:field_num\\[:min:max\\]\\>\\] \\[-Q \\<file\\>\\] \\[-q \\<query\\>\\] \\[-r \\<filename\\>\\] \\[-t \\<seconds\\|off\\>\\] \\[-T \\<seconds\\>\\] \\[-V\\]\n\n### DESCRIPTION\n\nThe varnishhist utility reads varnishd(1) shared memory logs and presents a continuously updated histogram showing the distribution of the last N requests by their processing. The value of N and the vertical scale are displayed in the top left corner. The horizontal scale is logarithmic. Hits are marked with a pipe character (“\\|”), and misses are marked with a hash character (“#”).\n\nThe following options are available:\n\n-B `<factor>`  \nFactor to bend time by. Particularly useful when \\[-r\\]eading from a vsl file. =1 process in near real time, \\<1 slow-motion, \\>1 time-lapse (useless unless reading from a file). At runtime, \\< halves and \\> doubles.\n\n-C  \nDo all regular expression and string matching caseless.\n\n-d  \nProcess log records at the head of the log and exit.\n\n-g `<request|vxid>`  \nThe grouping of the log records. The default is to group by vxid.\n\n-h  \nPrint program usage and exit\n\n-L `<limit>`  \nSets the upper limit of incomplete transactions kept before the oldest transaction is force completed. A warning record is synthesized when this happens. This setting keeps an upper bound on the memory usage of running queries. Defaults to 1000 transactions.\n\n-n `<dir>`  \nSpecify the varnishd working directory (also known as instance name) to get logs from. If -n is not specified, the host name is used.\n\n-p `<period>`  \nSpecified the number of seconds between screen refreshes. Default is 1 second, and can be changed at runtime by pressing the \\[0-9\\] keys (powers of 2 in seconds or + and - (double/halve the speed).\n\n-P `responsetime`  \nPredefined client profile: graph the total time from start of request processing (first byte received) until ready to deliver the client response (field 3 of SLT_Timestamp Process: VSL tag).\n\n-P `reqbodytime`  \nPredefined client profile: graph the time for reading the request body (field 3 of SLT_Timestamp ReqBody: VSL tag).\n\n-P `size`  \nPredefined client profile: graph the size of responses (field 5 of SLT_ReqAcct VSL tag).\n\n-P `Bereqtime`  \nPredefined backend profile: graph the time from beginning of backend processing until a backend request is sent completely (field 3 of SLT_Timestamp Bereq: VSL tag).\n\n-P `Beresptime`  \nPredefined backend profile: graph the time from beginning of backend processing until the response headers are being received completely (field 3 of SLT_Timestamp Beresp: VSL tag).\n\n-P `BerespBodytime`  \nPredefined backend profile: graph the time from beginning of backend processing until the response body has been received (field 3 of SLT_Timestamp BerespBody: VSL tag).\n\n-P `Besize`  \nPredefined backend profile: graph the backend response body size (field 5 of SLT_BereqAcct VSL tag).\n\n-P `<[cb:]tag:[prefix]:field_num[:min:max]>`  \nGraph the given custom definition defined as: an optional (c)lient, (b)ackend or (E)SI filter (defaults to client), the tag we’ll look for, a prefix to look for (can be empty, but must be terminated by a colon) and the field number of the value we are interested in. min and max are the boundaries of the graph in powers of ten and default to -6 and 3.\n\n-Q `<file>`  \nSpecifies the file containing the VSL query to use. When multiple -Q or -q options are specified, all queries are considered as if the ‘or’ operator was used to combine them.\n\n-q `<query>`  \nSpecifies the VSL query to use. When multiple -q or -Q options are specified, all queries are considered as if the ‘or’ operator was used to combine them.\n\n-r `<filename>`  \nRead log in binary file format from this file. The file can be created with `varnishlog -w filename`. If the filename is -, logs are read from the standard input. and cannot work as a daemon.\n\n-t `<seconds|off>`  \nTimeout before returning error on initial VSM connection. If set the VSM connection is retried every 0.5 seconds for this many seconds. If zero the connection is attempted only once and will fail immediately if unsuccessful. If set to “off”, the connection will not fail, allowing the utility to start and wait indefinetely for the Varnish instance to appear. Defaults to 5 seconds.\n\n-T `<seconds>`  \nSets the transaction timeout in seconds. This defines the maximum number of seconds elapsed between a Begin tag and the End tag. If the timeout expires, a warning record is synthesized and the transaction is force completed. Defaults to 120 seconds.\n\n-V  \nPrint version information and exit.\n\n--optstring  \nPrint the optstring parameter to `getopt(3)` to help writing wrapper scripts.\n\n### SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [varnishlog](varnishlog#varnishlog-1)\n- [varnishncsa](varnishncsa#varnishncsa-1)\n- [varnishstat](varnishstat#varnishstat-1)\n- [varnishtop](varnishtop#varnishtop-1)\n- [VSL](vsl#vsl-7)\n\n### HISTORY\n\nThe varnishhist utility was developed by Poul-Henning Kamp in cooperation with Verdens Gang AS and Varnish Software AS. This manual page was written by Dag-Erling Smørgrav.\n\n### COPYRIGHT\n\nThis document is licensed under the same licence as Varnish itself. See LICENCE for details.\n\n- Copyright (c) 2006 Verdens Gang AS\n- Copyright (c) 2006-2015 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/varnishhist.html](https://varnish-cache.org/docs/7.4/reference/varnishhist.html)"
- name: varnishlog
  id: reference/varnishlog
  summary: -a When writing output to a file with the -w option, append to it rather than overwrite it
  description: "# varnishlog\n\n## Display Varnish logs\n\nManual section:  \n1\n\n### SYNOPSIS\n\nvarnishlog \\[-a\\] \\[-A\\] \\[-b\\] \\[-c\\] \\[-C\\] \\[-d\\] \\[-D\\] \\[-E\\] \\[-g \\<session\\|request\\|vxid\\|raw\\>\\] \\[-h\\] \\[-i \\<taglist\\>\\] \\[-I \\<\\[taglist:\\]regex\\>\\] \\[-k \\<num\\>\\] \\[-L \\<limit\\>\\] \\[-n \\<dir\\>\\] \\[-P \\<file\\>\\] \\[-Q \\<file\\>\\] \\[-q \\<query\\>\\] \\[-r \\<filename\\>\\] \\[-R \\<limit\\[/duration\\]\\>\\] \\[-t \\<seconds\\|off\\>\\] \\[-T \\<seconds\\>\\] \\[-u\\] \\[-v\\] \\[-V\\] \\[-w \\<filename\\>\\] \\[-x \\<taglist\\>\\] \\[-X \\<\\[taglist:\\]regex\\>\\]\n\n### OPTIONS\n\nThe following options are available:\n\n-a  \nWhen writing output to a file with the -w option, append to it rather than overwrite it. This option has no effect without the -w option.\n\n-A  \nWhen writing output to a file with the -w option, output data in ascii format. This option has no effect without the -w option.\n\n-b  \nOnly display transactions and log records coming from backend communication.\n\n-c  \nOnly display transactions and log records coming from client communication.\n\n-C  \nDo all regular expression and string matching caseless.\n\n-d  \nProcess log records at the head of the log and exit.\n\n-D  \nDaemonize.\n\n-E  \nDisplay ESI transactions and other types of sub-requests. This implies the -c option and includes other client transactions.\n\n-g `<session|request|vxid|raw>`  \nThe grouping of the log records. The default is to group by vxid.\n\n-h  \nPrint program usage and exit\n\n-i `<taglist>`  \nInclude log records of these tags in output. Taglist is a comma-separated list of tag globs. Multiple -i options may be given.\n\nIf a tag include option is the first of any tag selection options, all tags are first marked excluded.\n\n-I `<[taglist:]regex>`  \nInclude by regex matching. Output only records matching taglist and regular expression. Applies to any tag if taglist is absent. Multiple -I options may be given.\n\nIf a tag include option is the first of any tag selection options, all tags are first marked excluded.\n\n-k `<num>`  \nProcess this number of matching log transactions before exiting.\n\n-L `<limit>`  \nSets the upper limit of incomplete transactions kept before the oldest transaction is force completed. A warning record is synthesized when this happens. This setting keeps an upper bound on the memory usage of running queries. Defaults to 1000 transactions.\n\n-n `<dir>`  \nSpecify the varnishd working directory (also known as instance name) to get logs from. If -n is not specified, the host name is used.\n\n-P `<file>`  \nWrite the process’ PID to the specified file.\n\n-Q `<file>`  \nSpecifies the file containing the VSL query to use. When multiple -Q or -q options are specified, all queries are considered as if the ‘or’ operator was used to combine them.\n\n-q `<query>`  \nSpecifies the VSL query to use. When multiple -q or -Q options are specified, all queries are considered as if the ‘or’ operator was used to combine them.\n\n-r `<filename>`  \nRead log in binary file format from this file. The file can be created with `varnishlog -w filename`. If the filename is -, logs are read from the standard input. and cannot work as a daemon.\n\n-R `<limit[/duration]>`  \nRestrict the output to the specified limit. Transactions exceeding the limit will be suppressed. The limit is specified as the maximum number of transactions (with respect to the chosen grouping method) and an optional time period. If no duration is specified, a default of `s` is used. The duration field can be formatted as in VCL (e.g. `-R 10/2m`) or as a simple time period without the prefix (e.g. `-R 5/m`). When in `-g raw` grouping mode, this setting can not be used alongside `-i`, `-I`, `-x` or `-X`, and we advise using `-q` instead.\n\n-t `<seconds|off>`  \nTimeout before returning error on initial VSM connection. If set the VSM connection is retried every 0.5 seconds for this many seconds. If zero the connection is attempted only once and will fail immediately if unsuccessful. If set to “off”, the connection will not fail, allowing the utility to start and wait indefinetely for the Varnish instance to appear. Defaults to 5 seconds.\n\n-T `<seconds>`  \nSets the transaction timeout in seconds. This defines the maximum number of seconds elapsed between a Begin tag and the End tag. If the timeout expires, a warning record is synthesized and the transaction is force completed. Defaults to 120 seconds.\n\n-u  \nWhen writing output to a file with the -w option, output data is not buffered. This option has no effect without the -w option.\n\n-v  \nUse verbose output on record set printing, giving the VXID on every log line. Without this option, the VXID will only be given on the header of that transaction.\n\n-V  \nPrint version information and exit.\n\n-w `<filename>`  \nRedirect output to file. The file will be overwritten unless the -a option was specified. If the application receives a SIGHUP in daemon mode the file will be reopened allowing the old one to be rotated away. The file can then be read by varnishlog and other tools with the -r option, unless the -A option was specified. This option is required when running in daemon mode. If the filename is -, varnishlog writes to the standard output and cannot work as a daemon.\n\n-x `<taglist>`  \nExclude log records of these tags in output. Taglist is a comma-separated list of tag globs. Multiple -x options may be given.\n\n-X `<[taglist:]regex>`  \nExclude by regex matching. Do not output records matching taglist and regular expression. Applies to any tag if taglist is absent. Multiple -X options may be given.\n\n--optstring  \nPrint the optstring parameter to `getopt(3)` to help writing wrapper scripts.\n\n### SIGNALS\n\n- SIGHUP\n\n  Rotate the log file (see -w option) in daemon mode, abort the loop and die gracefully when running in the foreground.\n\n- SIGUSR1\n\n  Flush any outstanding transactions\n\n### SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [varnishhist](varnishhist#varnishhist-1)\n- [varnishncsa](varnishncsa#varnishncsa-1)\n- [varnishstat](varnishstat#varnishstat-1)\n- [varnishtop](varnishtop#varnishtop-1)\n- [VSL](vsl#vsl-7)\n- [vsl-query](vsl-query#vsl-query-7)\n\n### HISTORY\n\nThe varnishlog utility was developed by Poul-Henning Kamp \\<[phk@phk.freebsd.dk](mailto:phk%40phk.freebsd.dk)\\> in cooperation with Verdens Gang AS and Varnish Software AS. This manual page was initially written by Dag-Erling Smørgrav, and later updated by Per Buer and Martin Blix Grydeland.\n\n### COPYRIGHT\n\nThis document is licensed under the same licence as Varnish itself. See LICENCE for details.\n\n- Copyright (c) 2006 Verdens Gang AS\n- Copyright (c) 2006-2015 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/varnishlog.html](https://varnish-cache.org/docs/7.4/reference/varnishlog.html)"
- name: varnishncsa
  id: reference/varnishncsa
  summary: The varnishncsa utility reads varnishd(1) shared memory logs and presents them in the Apache / NCSA “combined” log format
  description: "# varnishncsa\n\n## Display Varnish logs in Apache / NCSA combined log format\n\nManual section:  \n1\n\n### SYNOPSIS\n\nvarnishncsa \\[-a\\] \\[-b\\] \\[-c\\] \\[-C\\] \\[-d\\] \\[-D\\] \\[-E\\] \\[-F \\<format\\>\\] \\[-f \\<formatfile\\>\\] \\[-g \\<request\\|vxid\\>\\] \\[-h\\] \\[-j\\] \\[-k \\<num\\>\\] \\[-L \\<limit\\>\\] \\[-n \\<dir\\>\\] \\[-P \\<file\\>\\] \\[-Q \\<file\\>\\] \\[-q \\<query\\>\\] \\[-r \\<filename\\>\\] \\[-R \\<limit\\[/duration\\]\\>\\] \\[-t \\<seconds\\|off\\>\\] \\[-V\\] \\[-w \\<filename\\>\\]\n\n### DESCRIPTION\n\nThe varnishncsa utility reads varnishd(1) shared memory logs and presents them in the Apache / NCSA “combined” log format.\n\nEach log line produced is based on a single Request type transaction gathered from the shared memory log. The Request transaction is then scanned for the relevant parts in order to output one log line. To filter the log lines produced, use the query language to select the applicable transactions. Non-request transactions are ignored.\n\nThe following options are available:\n\n-a  \nWhen writing output to a file, append to it rather than overwrite it. This option has no effect without the -w option.\n\n-b  \nLog backend requests. If -c is not specified, then only backend requests will trigger log lines.\n\n-c  \nLog client requests. This is the default. If -b is specified, then -c is needed to also log client requests\n\n-C  \nDo all regular expression and string matching caseless.\n\n-d  \nProcess log records at the head of the log and exit.\n\n-D  \nDaemonize.\n\n-E  \nShow ESI requests, implies client mode.\n\n-F `<format>`  \nSet the output log format string.\n\n-f `<formatfile>`  \nRead output format from a file. Will read a single line from the specified file, and use that line as the format.\n\n-g `<request|vxid>`  \nThe grouping of the log records. The default is to group by vxid.\n\n-h  \nPrint program usage and exit\n\n-j  \nMake format-specifier replacements JSON-compatible. When escaping characters, use JSON-style \\uXXXX escape sequences instead of C-style \\xXX sequences. Empty strings will be replaced with “” instead of “-”, and empty integers will be replaced with null. Use -F or -f in combination with -j to write JSON logs.\n\n-k `<num>`  \nProcess this number of matching log transactions before exiting.\n\n-L `<limit>`  \nSets the upper limit of incomplete transactions kept before the oldest transaction is force completed. A warning record is synthesized when this happens. This setting keeps an upper bound on the memory usage of running queries. Defaults to 1000 transactions.\n\n-n `<dir>`  \nSpecify the varnishd working directory (also known as instance name) to get logs from. If -n is not specified, the host name is used.\n\n-P `<file>`  \nWrite the process’ PID to the specified file.\n\n-Q `<file>`  \nSpecifies the file containing the VSL query to use. When multiple -Q or -q options are specified, all queries are considered as if the ‘or’ operator was used to combine them.\n\n-q `<query>`  \nSpecifies the VSL query to use. When multiple -q or -Q options are specified, all queries are considered as if the ‘or’ operator was used to combine them.\n\n-r `<filename>`  \nRead log in binary file format from this file. The file can be created with `varnishlog -w filename`. If the filename is -, logs are read from the standard input. and cannot work as a daemon.\n\n-R `<limit[/duration]>`  \nRestrict the output to the specified limit. Transactions exceeding the limit will be suppressed. The limit is specified as the maximum number of transactions (with respect to the chosen grouping method) and an optional time period. If no duration is specified, a default of `s` is used. The duration field can be formatted as in VCL (e.g. `-R 10/2m`) or as a simple time period without the prefix (e.g. `-R 5/m`). When in `-g raw` grouping mode, this setting can not be used alongside `-i`, `-I`, `-x` or `-X`, and we advise using `-q` instead.\n\n-t `<seconds|off>`  \nTimeout before returning error on initial VSM connection. If set the VSM connection is retried every 0.5 seconds for this many seconds. If zero the connection is attempted only once and will fail immediately if unsuccessful. If set to “off”, the connection will not fail, allowing the utility to start and wait indefinetely for the Varnish instance to appear. Defaults to 5 seconds.\n\n-V  \nPrint version information and exit.\n\n-w `<filename>`  \nRedirect output to file. The file will be overwritten unless the -a option was specified. If the application receives a SIGHUP in daemon mode the file will be reopened allowing the old one to be rotated away. This option is required when running in daemon mode. If the filename is -, varnishncsa writes to the standard output and cannot work as a daemon.\n\n--optstring  \nPrint the optstring parameter to `getopt(3)` to help writing wrapper scripts.\n\n### MODES\n\nThe default mode of varnishncsa is “client mode”. In this mode, the log will be similar to what a web server would produce in the absence of varnish. Client mode can be explicitly selected by using -c.\n\nIf the -b switch is specified, varnishncsa will operate in “backend mode”. In this mode, requests generated by varnish to the backends will be logged. Unless -c is also specified, client requests received by varnish will be ignored.\n\nWhen running varnishncsa in both backend and client mode, it is strongly advised to include the format specifier %{Varnish:side}x to distinguish between backend and client requests.\n\nClient requests that results in a pipe (ie. return(pipe) in vcl), will not generate logging in backend mode. This is because varnish is not generating requests, but blindly passes on bytes in both directions. However, a varnishncsa instance running in normal mode can see this case by using the formatter %{Varnish:handling}x, which will be ‘pipe’.\n\nIn backend mode, some of the fields in the format string get different meanings. Most notably, the byte counting formatters (%b, %I, %O) considers varnish to be the client.\n\nIt is possible to keep two varnishncsa instances running, one in backend mode, and one in client mode, logging to different files.\n\n### FORMAT\n\nSpecify the log format to use. If no format is specified the default log format is used:\n\n``` python\n%h %l %u %t \"%r\" %s %b \"%{Referer}i\" \"%{User-agent}i\"\n```\n\nEscape sequences \\n and \\t are supported.\n\nSupported formatters are:\n\n%b  \nIn client mode, size of response in bytes, excluding HTTP headers. In backend mode, the number of bytes received from the backend, excluding HTTP headers. In CLF format, i.e. a ‘-’ rather than a 0 when no bytes are sent.\n\n%D  \nIn client mode, time taken to serve the request, in microseconds. In backend mode, time from the request was sent to the entire body had been received. This is equivalent to %{us}T.\n\n%H  \nThe request protocol. Defaults to HTTP/1.0 if not known.\n\n%h  \nRemote host. Defaults to ‘-’ if not known. In backend mode this is the IP of the backend server.\n\n%I  \nIn client mode, total bytes received from client. In backend mode, total bytes sent to the backend.\n\n%{X}i  \nThe contents of request header X. If the header appears multiple times in a single transaction, the last occurrence is used.\n\n%l  \nRemote logname. Always ‘-‘.\n\n%m  \nRequest method. Defaults to ‘-’ if not known.\n\n%{X}o  \nThe contents of response header X. If the header appears multiple times in a single transaction, the last occurrence is used.\n\n%O  \nIn client mode, total bytes sent to client. In backend mode, total bytes received from the backend.\n\n%q  \nThe query string. Defaults to an empty string if not present.\n\n%r  \nThe first line of the request. Synthesized from other fields, so it may not be the request verbatim. See the NOTES section.\n\n%s  \nStatus sent to the client. In backend mode, status received from the backend.\n\n%t  \nIn client mode, time when the request was received, in HTTP date/time format. In backend mode, time when the request was sent.\n\n%{X}t  \nIn client mode, time when the request was received, in the format specified by X. In backend mode, time when the request was sent. The time specification format is the same as for strftime(3) with these extensions:\n\n- `%{sec}`: number of seconds since the Epoch\n- `%{msec}`: number of milliseconds since the Epoch\n- `%{usec}`: number of milliseconds since the Epoch\n- `%{msec_frac}`: millisecond fraction\n- `%{usec_frac}`: microsecond fraction\n\nThe extensions can not be combined with each other or strftime(3) in the same specification. Use multiple `%{X}t` specifications instead.\n\n%T  \nIn client mode, time taken to serve the request, in seconds. In backend mode, time from the request was sent to the entire body had been received. This is equivalent to %{s}T.\n\n%{X}T  \nIn client mode, time taken to serve the request, in the format specified by X. In backend mode, time from the request was sent to the entire body had been received. The time specification format can be one of the following: s (same as %T), ms or us (same as %D).\n\n%U  \nThe request URL without the query string. Defaults to ‘-’ if not known.\n\n%u  \nRemote user from auth.\n\n%{X}x  \nExtended variables. Supported variables are:\n\nVarnish:time_firstbyte  \nTime from when the request processing starts until the first byte is sent to the client, in seconds. For backend mode: Time from the request was sent to the backend to the entire header had been received.\n\nVarnish:hitmiss  \nIn client mode, one of the ‘hit’ or ‘miss’ strings, depending on whether the request was a cache hit or miss. Pipe, pass and synth are considered misses. In backend mode, this field is blank.\n\nVarnish:handling  \nIn client mode, one of the ‘hit’, ‘miss’, ‘pass’, ‘pipe’ or ‘synth’ strings indicating how the request was handled. In backend mode, this field is blank.\n\nVarnish:side  \nBackend or client side. One of two values, ‘b’ or ‘c’, depending on where the request was made. In pure backend or client mode, this field will be constant.\n\nVarnish:vxid  \nThe VXID of the varnish transaction.\n\nVCL_Log:key  \nThe value set by std.log(“key:value”) in VCL.\n\nVSL:tag:record-prefix\\[field\\]  \nThe value of the VSL entry for the given tag-record prefix-field combination. Tag is mandatory, the other components are optional.\n\nThe record prefix will limit the matches to those records that have this prefix as the first part of the record content followed by a colon.\n\nThe field will, if present, treat the log record as a white space separated list of fields, and only the nth part of the record will be matched against. Fields start counting at 1 and run up to 255.\n\nDefaults to ‘-’ when the tag is not seen, the record prefix does not match or the field is out of bounds. If a tag appears multiple times in a single transaction, the first occurrence is used.\n\n### SIGNALS\n\n- SIGHUP\n\n  Rotate the log file (see -w option) in daemon mode, abort the loop and die gracefully when running in the foreground.\n\n- SIGUSR1\n\n  Flush any outstanding transactions.\n\n### NOTES\n\nThe %r formatter is equivalent to “%m [http:/](http:/)/%{Host}i%U%q %H”. This differs from apache’s %r behavior, equivalent to “%m %U%q %H”. Furthermore, when using the %r formatter, if the Host header appears multiple times in a single transaction, the first occurrence is used.\n\n### EXAMPLE\n\nLog the second field of the Begin record, corresponding to the VXID of the parent transaction:\n\n``` python\nvarnishncsa -F \"%{VSL:Begin[2]}x\"\n```\n\nLog the entire Timestamp record associated with the processing length:\n\n``` python\nvarnishncsa -F \"%{VSL:Timestamp:Process}x\"\n```\n\nLog in JSON, using the -j flag to ensure that the output is valid JSON for all inputs:\n\n``` python\nvarnishncsa -j -F '{\"size\": %b, \"time\": \"%t\", \"ua\": \"%{User-Agent}i\"}'\n```\n\n### SEE ALSO\n\n[varnishd](varnishd#varnishd-1) [varnishlog](varnishlog#varnishlog-1) [varnishstat](varnishstat#varnishstat-1) [VSL](vsl#vsl-7)\n\n### HISTORY\n\nThe varnishncsa utility was developed by Poul-Henning Kamp in cooperation with Verdens Gang AS and Varnish Software AS. This manual page was initially written by Dag-Erling Smørgrav \\<[des@des.no](mailto:des%40des.no)\\>, and later updated by Martin Blix Grydeland and Pål Hermunn Johansen.\n\n### COPYRIGHT\n\nThis document is licensed under the same licence as Varnish itself. See LICENCE for details.\n\n- Copyright (c) 2006 Verdens Gang AS\n- Copyright (c) 2006-2016 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/varnishncsa.html](https://varnish-cache.org/docs/7.4/reference/varnishncsa.html)"
- name: varnishstat
  id: reference/varnishstat
  summary: The varnishstat utility displays statistics from a running varnishd(1) instance
  description: "# varnishstat\n\n## Varnish Cache statistics\n\nManual section:  \n1\n\n### SYNOPSIS\n\nvarnishstat \\[-1\\] \\[-f \\<glob\\>\\] \\[-h\\] \\[-I \\<glob\\>\\] \\[-j\\] \\[-l\\] \\[-n \\<dir\\>\\] \\[-r\\] \\[-t \\<seconds\\|off\\>\\] \\[-V\\] \\[-X \\<glob\\>\\] \\[-x\\]\n\n### DESCRIPTION\n\nThe varnishstat utility displays statistics from a running varnishd(1) instance.\n\nThe following options are available:\n\n-1  \nInstead of presenting a continuously updated display, print the statistics to stdout.\n\n-f `<glob>`  \nLegacy field filtering glob. Use backslash to escape characters. If the argument starts with ‘^’ it is used as an exclusive glob. Multiple -f arguments may be given. Legacy filtering globs are run along with inclusion globs (-I arguments) and exclusion globs (-X arguments) in order on a first-match basis.\n\n-h  \nPrint program usage and exit\n\n-I `<glob>`  \nField inclusion glob. Use backslash to escape characters. Multiple -I arguments may be given. Exclusion globs are run in order along with exclusion globs (-X arguments) and legacy filtering globs (-f arguments) on a first-match basis.\n\n-j  \nPrint statistics to stdout as JSON.\n\n-l  \nLists the available fields to use with the -f option.\n\n-n `<dir>`  \nSpecify the varnishd working directory (also known as instance name) to get logs from. If -n is not specified, the host name is used.\n\n-r  \nToggle raw or adjusted gauges, adjusted is the default.\n\n-t `<seconds|off>`  \nTimeout before returning error on initial VSM connection. If set the VSM connection is retried every 0.5 seconds for this many seconds. If zero the connection is attempted only once and will fail immediately if unsuccessful. If set to “off”, the connection will not fail, allowing the utility to start and wait indefinetely for the Varnish instance to appear. Defaults to 5 seconds.\n\n-V  \nPrint version information and exit.\n\n-X `<glob>`  \nField exclusion glob. Use backslash to escape characters. Multiple -X arguments may be given. Exclusion globs are run in order along with inclusion globs (-I arguments) and legacy filtering globs (-f arguments) on a first-match basis.\n\n-x  \nPrint statistics to stdout as XML.\n\n--optstring  \nPrint the optstring parameter to `getopt(3)` to help writing wrapper scripts.\n\n### CURSES MODE\n\nWhen neither -1, -j nor -x options are given, the application starts up in curses mode. This shows a continuously updated view of the counter values, along with their description.\n\nThe top area shows process uptime information.\n\nThe center area shows a list of counter values.\n\nThe bottom area shows the description of the currently selected counter.\n\nOn startup, only counters at INFO level are shown.\n\n#### Columns\n\nThe following columns are displayed, from left to right:\n\nName  \nThe name of the counter\n\nCurrent  \nThe current value of the counter.\n\nChange  \nThe average per second change over the last update interval.\n\nAverage  \nThe average value of this counter over the runtime of the Varnish daemon, or a period if the counter can’t be averaged.\n\nAvg_10  \nThe moving average over the last 10 update intervals.\n\nAvg_100  \nThe moving average over the last 100 update intervals.\n\nAvg_1000  \nThe moving average over the last 1000 update intervals.\n\n#### Key bindings\n\n\\<h\\>  \nToggle the help screen.\n\n\\<UP\\> or \\<k\\>  \nNavigate the counter list one line up.\n\n\\<DOWN\\> or \\<j\\>  \nNavigate the counter list one line down.\n\n\\<PAGEUP\\> or \\<b\\> or \\<CTRL-B\\>  \nNavigate the counter list one page up.\n\n\\<PAGEDOWN\\> or \\<SPACE\\> or \\<CTRL-F\\>  \nNavigate the counter list one page down.\n\n\\<HOME\\> or \\<g\\>  \nNavigate the counter list to the top.\n\n\\<END\\> or \\<G\\>  \nNavigate the counter list to the bottom.\n\n\\<d\\>  \nToggle between showing and hiding unseen counters. Unseen counters are those that has been zero for the entire runtime of varnishstat. Defaults to hide unseen counters.\n\n\\<r\\>  \nToggle between showing raw and adjusted gauges. When a gauge is decremented faster than it is incremented, it may appear as a large integer with its most significant bit set. By default such values are adjusted to zero.\n\n\\<e\\>  \nToggle scaling of values.\n\n\\<v\\>  \nIncrease verbosity. Defaults to only showing informational counters.\n\n\\<V\\>  \nDecrease verbosity. Defaults to only showing informational counters.\n\n\\<q\\>  \nQuit.\n\n\\<CTRL+T\\>  \nSample now.\n\n\\<+\\>  \nIncrease refresh interval.\n\n\\<-\\>  \nDecrease refresh interval.\n\n### OUTPUTS\n\nThe XML output format is:\n\n``` python\n<varnishstat timestamp=\"YYYY-MM-DDTHH:mm:SS\">\n  <stat>\n    <name>FIELD NAME</name>\n    <value>FIELD VALUE</value>\n    <flag>FIELD SEMANTICS</flag>\n    <format>FIELD DISPLAY FORMAT</format>\n    <description>FIELD DESCRIPTION</description>\n  </stat>\n  [..]\n</varnishstat>\n```\n\nThe JSON output format is:\n\n``` python\n{\n  \"timestamp\": \"YYYY-MM-DDTHH:mm:SS\",\n  \"FIELD NAME\": {\n    \"description\": \"FIELD DESCRIPTION\",\n    \"flag\": \"FIELD SEMANTICS\", \"format\": \"FIELD DISPLAY FORMAT\",\n    \"value\": FIELD VALUE\n  },\n  \"FIELD NAME\": {\n    \"description\": \"FIELD DESCRIPTION\",\n    \"flag\": \"FIELD SEMANTICS\", \"format\": \"FIELD DISPLAY FORMAT\",\n    \"value\": FIELD VALUE\n  },\n  [..]\n}\n```\n\nTimestamp is the time when the report was generated by varnishstat.\n\n### SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [varnishhist](varnishhist#varnishhist-1)\n- [varnishlog](varnishlog#varnishlog-1)\n- [varnishncsa](varnishncsa#varnishncsa-1)\n- [varnishtop](varnishtop#varnishtop-1)\n- curses(3)\n- [varnish-counters](varnish-counters#varnish-counters-7)\n\n### AUTHORS\n\nThis manual page was written by Dag-Erling Smørgrav, Per Buer, Lasse Karstensen and Martin Blix Grydeland.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/varnishstat.html](https://varnish-cache.org/docs/7.4/reference/varnishstat.html)"
- name: varnishtest
  id: reference/varnishtest
  summary: The varnishtest program is a script driven program used to test the Varnish Cache
  description: "# varnishtest\n\n## Test program for Varnish\n\nManual section:  \n1\n\n### SYNOPSIS\n\nvarnishtest \\[-hikLlqv\\] \\[-b size\\] \\[-D name=val\\] \\[-j jobs\\] \\[-n iter\\] \\[-t duration\\] file \\[file …\\]\n\n### DESCRIPTION\n\nThe varnishtest program is a script driven program used to test the Varnish Cache.\n\nThe varnishtest program, when started and given one or more script files, can create a number of threads representing backends, some threads representing clients, and a varnishd process. This is then used to simulate a transaction to provoke a specific behavior.\n\nThe following options are available:\n\n-b `size`  \nSet internal buffer size (default: 1M)\n\n-D name=val Define macro for use in scripts\n\n-h  \nShow help\n\n-i  \nSet PATH and vmod_path to find varnish binaries in build tree\n\n-j `jobs`  \nRun this many tests in parallel\n\n-k  \nContinue on test failure\n\n-L  \nAlways leave temporary vtc.\\*\n\n-l  \nLeave temporary vtc.\\* if test fails\n\n-n `iterations`  \nRun tests this many times\n\n-p name=val Pass parameters to all varnishd command lines\n\n-q  \nQuiet mode: report only failures\n\n-t `duration`  \nTime tests out after this long (default: 60s)\n\n-v  \nVerbose mode: always report test log\n\nfile File to use as a script\n\nIf `TMPDIR` is set in the environment, varnishtest creates temporary `vtc.*` directories for each test in `$TMPDIR`, otherwise in `/tmp`.\n\n### SCRIPTS\n\nThe vtc syntax is documented at length in [VTC](vtc#vtc-7). Should you want more examples than the one below, you can have a look at the Varnish source code repository, under `bin/varnishtest/tests/`, where all the regression tests for Varnish are kept.\n\nAn example:\n\n``` python\nvarnishtest \"#1029\"\n\nserver s1 {\n        rxreq\n        expect req.url == \"/bar\"\n        txresp -gzipbody {[bar]}\n\n        rxreq\n        expect req.url == \"/foo\"\n        txresp -body {<h1>FOO<esi:include src=\"/bar\"/>BARF</h1>}\n\n} -start\n\nvarnish v1 -vcl+backend {\n        sub vcl_backend_response {\n                set beresp.do_esi = true;\n                if (bereq.url == \"/foo\") {\n                        set beresp.ttl = 0s;\n                } else {\n                        set beresp.ttl = 10m;\n                }\n        }\n} -start\n\nclient c1 {\n        txreq -url \"/bar\" -hdr \"Accept-Encoding: gzip\"\n        rxresp\n        gunzip\n        expect resp.bodylen == 5\n\n        txreq -url \"/foo\" -hdr \"Accept-Encoding: gzip\"\n        rxresp\n        expect resp.bodylen == 21\n} -run\n```\n\nWhen run, the above script will simulate a server (s1) that expects two different requests. It will start a Varnish server (v1) and add the backend definition to the VCL specified (-vcl+backend). Finally it starts the c1-client, which is a single client sending two requests.\n\n### TESTING A BUILD TREE\n\nWhether you are building a VMOD or trying to use one that you freshly built, you can tell `varnishtest` to pass a *vmod_path* to `varnishd` instances started using the `varnish -start` command in your test case:\n\n``` python\nvarnishtest -p vmod_path=... /path/to/*.vtc\n```\n\nThis way you can use the same test cases on both installed and built VMODs:\n\n``` python\nserver s1 {...} -start\n\nvarnish v1 -vcl+backend {\n    import wossname;\n\n    ...\n} -start\n\n...\n```\n\nYou are not limited to the *vmod_path* and can pass any parameter, allowing you to run a build matrix without changing the test suite. You can achieve the same with macros, but then they need to be defined on each run.\n\nYou can see the actual `varnishd` command lines in test outputs, they look roughly like this:\n\n``` python\nexec varnishd [varnishtest -p params] [testing params] [vtc -arg params]\n```\n\nParameters you define with `varnishtest -p` may be overridden by parameters needed by `varnishtest` to run properly, and they may in turn be overridden by parameters set in test scripts.\n\nThere’s also a special mode in which `varnishtest` builds itself a PATH and a *vmod_path* in order to find Varnish binaries (programs and VMODs) in the build tree surrounding the `varnishtest` binary. This is meant for testing of Varnish under development and will disregard your *vmod_path* if you set one.\n\nIf you need to test your VMOD against a Varnish build tree, you must install it first, in a temp directory for instance. With information provided by the installation’s *pkg-config(1)* you can build a proper PATH in order to access Varnish programs, and a *vmod_path* to access both your VMOD and the built-in VMODs:\n\n``` python\nexport PKG_CONFIG_PATH=/path/to/install/lib/pkgconfig\n\nBINDIR=\"$(pkg-config --variable=bindir varnishapi)\"\nSBINDIR=\"$(pkg-config --variable=sbindir varnishapi)\"\nPATH=\"SBINDIR:BINDIR:$PATH\"\n\nVMODDIR=\"$(pkg-config --variable=vmoddir varnishapi)\"\nVMOD_PATH=\"/path/to/your/vmod/build/dir:$VMODDIR\"\n\nvarnishtest -p vmod_path=\"$VMOD_PATH\" ...\n```\n\n### SEE ALSO\n\n- varnishtest source code repository with tests\n- [varnishhist](varnishhist#varnishhist-1)\n- [varnishlog](varnishlog#varnishlog-1)\n- [varnishncsa](varnishncsa#varnishncsa-1)\n- [varnishstat](varnishstat#varnishstat-1)\n- [varnishtop](varnishtop#varnishtop-1)\n- [VCL](vcl#vcl-7)\n- [VTC](vtc#vtc-7)\n- [VMOD vtc - Utility module for varnishtest](vmod_vtc#vmod-vtc-3)\n\n### HISTORY\n\nThe varnishtest program was developed by Poul-Henning Kamp \\<[phk@phk.freebsd.dk](mailto:phk%40phk.freebsd.dk)\\> in cooperation with Varnish Software AS. This manual page was originally written by Stig Sandbeck Mathisen \\<[ssm@linpro.no](mailto:ssm%40linpro.no)\\> and updated by Kristian Lyngstøl \\<[kristian@varnish-cache.org](mailto:kristian%40varnish-cache.org)\\>.\n\n### COPYRIGHT\n\nThis document is licensed under the same licence as Varnish itself. See LICENCE for details.\n\n- Copyright (c) 2007-2016 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/varnishtest.html](https://varnish-cache.org/docs/7.4/reference/varnishtest.html)"
- name: varnishtop
  id: reference/varnishtop
  summary: The varnishtop utility reads varnishd shared memory logs and presents a continuously updated list of the most commonly occurring log entries
  description: "# varnishtop\n\n## Varnish log entry ranking\n\nManual section:  \n1\n\n### SYNOPSIS\n\nvarnishtop \\[-1\\] \\[-b\\] \\[-c\\] \\[-C\\] \\[-d\\] \\[-E\\] \\[-f\\] \\[-g \\<session\\|request\\|vxid\\|raw\\>\\] \\[-h\\] \\[-i \\<taglist\\>\\] \\[-I \\<\\[taglist:\\]regex\\>\\] \\[-L \\<limit\\>\\] \\[-n \\<dir\\>\\] \\[-p \\<period\\>\\] \\[-Q \\<file\\>\\] \\[-q \\<query\\>\\] \\[-r \\<filename\\>\\] \\[-t \\<seconds\\|off\\>\\] \\[-T \\<seconds\\>\\] \\[-x \\<taglist\\>\\] \\[-X \\<\\[taglist:\\]regex\\>\\] \\[-V\\]\n\n### DESCRIPTION\n\nThe varnishtop utility reads [varnishd](varnishd#varnishd-1) shared memory logs and presents a continuously updated list of the most commonly occurring log entries. With suitable filtering using the `-I`, `-i`, `-X` and `-x` options, it can be used to display a ranking of requested documents, clients, user agents, or any other information which is recorded in the log.\n\nThe following options are available:\n\n-1  \nInstead of a continuously updated display, print the statistics once and exit. Implies `-d`.\n\n-b  \nOnly display transactions and log records coming from backend communication.\n\n-c  \nOnly display transactions and log records coming from client communication.\n\n-C  \nDo all regular expression and string matching caseless.\n\n-d  \nProcess log records at the head of the log.\n\n-E  \nDisplay ESI transactions and other types of sub-requests. This implies the -c option and includes other client transactions.\n\n-f  \nSort and group only on the first field of each log entry. For log entries in the form `prefix: value` it is the prefix without the colon that is sorted and grouped. This is useful when displaying e.g. ReqStart entries, where the first field is the client IP address.\n\n-g `<session|request|vxid|raw>`  \nThe grouping of the log records. The default is to group by vxid.\n\n-h  \nPrint program usage and exit\n\n-i `<taglist>`  \nInclude log records of these tags in output. Taglist is a comma-separated list of tag globs. Multiple -i options may be given.\n\nIf a tag include option is the first of any tag selection options, all tags are first marked excluded.\n\n-I `<[taglist:]regex>`  \nInclude by regex matching. Output only records matching taglist and regular expression. Applies to any tag if taglist is absent. Multiple -I options may be given.\n\nIf a tag include option is the first of any tag selection options, all tags are first marked excluded.\n\n-L `<limit>`  \nSets the upper limit of incomplete transactions kept before the oldest transaction is force completed. A warning record is synthesized when this happens. This setting keeps an upper bound on the memory usage of running queries. Defaults to 1000 transactions.\n\n-n `<dir>`  \nSpecify the varnishd working directory (also known as instance name) to get logs from. If -n is not specified, the host name is used.\n\n-p `<period>`  \nSpecified the number of seconds to measure over, the default is 60 seconds. The first number in the list is the average number of requests seen over this time period. This option has no effect if -1 option is also used.\n\n-Q `<file>`  \nSpecifies the file containing the VSL query to use. When multiple -Q or -q options are specified, all queries are considered as if the ‘or’ operator was used to combine them.\n\n-q `<query>`  \nSpecifies the VSL query to use. When multiple -q or -Q options are specified, all queries are considered as if the ‘or’ operator was used to combine them.\n\n-r `<filename>`  \nRead log in binary file format from this file. The file can be created with `varnishlog -w filename`. If the filename is -, logs are read from the standard input. and cannot work as a daemon.\n\n-t `<seconds|off>`  \nTimeout before returning error on initial VSM connection. If set the VSM connection is retried every 0.5 seconds for this many seconds. If zero the connection is attempted only once and will fail immediately if unsuccessful. If set to “off”, the connection will not fail, allowing the utility to start and wait indefinetely for the Varnish instance to appear. Defaults to 5 seconds.\n\n-T `<seconds>`  \nSets the transaction timeout in seconds. This defines the maximum number of seconds elapsed between a Begin tag and the End tag. If the timeout expires, a warning record is synthesized and the transaction is force completed. Defaults to 120 seconds.\n\n-x `<taglist>`  \nExclude log records of these tags in output. Taglist is a comma-separated list of tag globs. Multiple -x options may be given.\n\n-X `<[taglist:]regex>`  \nExclude by regex matching. Do not output records matching taglist and regular expression. Applies to any tag if taglist is absent. Multiple -X options may be given.\n\n-V  \nPrint version information and exit.\n\n--optstring  \nPrint the optstring parameter to `getopt(3)` to help writing wrapper scripts.\n\n### EXAMPLES\n\nThe following example displays a continuously updated list of the most frequently requested URLs:\n\n``` python\nvarnishtop -i ReqURL\n```\n\nThe following example displays a continuously updated list of the most commonly used user agents:\n\n``` python\nvarnishtop -C -I ReqHeader:User-Agent\n```\n\n### SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [varnishhist](varnishhist#varnishhist-1)\n- [varnishlog](varnishlog#varnishlog-1)\n- [varnishncsa](varnishncsa#varnishncsa-1)\n- [varnishstat](varnishstat#varnishstat-1)\n\n### HISTORY\n\nThe varnishtop utility was originally developed by Poul-Henning Kamp in cooperation with Verdens Gang AS and Varnish Software AS, and later substantially rewritten by Dag-Erling Smørgrav. This manual page was written by Dag-Erling Smørgrav, and later updated by Martin Blix Grydeland.\n\n### COPYRIGHT\n\nThis document is licensed under the same licence as Varnish itself. See LICENCE for details.\n\n- Copyright (c) 2006 Verdens Gang AS\n- Copyright (c) 2006-2015 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/varnishtop.html](https://varnish-cache.org/docs/7.4/reference/varnishtop.html)"
- name: VCL
  id: reference/vcl
  summary: The VCL language is a small domain-specific language designed to be used to describe request handling and document caching policies for Varnish Cache
  description: "# VCL\n\n## Varnish Configuration Language\n\nManual section:  \n7\n\n### DESCRIPTION\n\nThe VCL language is a small domain-specific language designed to be used to describe request handling and document caching policies for Varnish Cache.\n\nWhen a new configuration is loaded, the varnishd management process translates the VCL code to C and compiles it to a shared object which is then loaded into the server process.\n\nThis document focuses on the syntax of the VCL language. For a full description of syntax and semantics, with ample examples, please see the online documentation at [https://www.varnish-cache.org/docs/](https://www.varnish-cache.org/docs/) .\n\nStarting with Varnish 4.0, each VCL file must start by declaring its version with `vcl` *\\<major\\>.\\<minor\\>*`;` marker at the top of the file. See more about this under Versioning below.\n\n#### Operators\n\nThe following operators are available in VCL:\n\n`=`  \nAssignment operator.\n\n`+, -, *, /, %`  \nBasic math on numerical values.\n\n`+=, -=, *=, /=`  \nAssign and increment/decrement/multiply/divide operator.\n\nFor strings, `+=` appends.\n\n`(, )`  \nEvaluate separately.\n\n`==, !=, <, >, <=, >=`  \nComparisons\n\n`~, !~`  \nMatch / non-match. Can either be used with regular expressions or ACLs.\n\n`!`  \nNegation.\n\n`&& / ||`  \nLogical and/or.\n\n#### Conditionals\n\nVCL has `if` and `else` statements. Nested logic can be implemented with the `elseif` statement (`elsif`/`elif`/`else if` are equivalent).\n\nNote that there are no loops or iterators of any kind in VCL.\n\n#### Variables\n\nVCL does most of the work by examining, `set`’ing and `unset`’ing variables:\n\n``` python\nif (req.url == \"/mistyped_url.html\") {\n    set req.url = \"/correct_url.html\";\n    unset req.http.cookie;\n}\n```\n\nThere are obvious limitations to what can be done, for instance it makes no sense to `unset req.url;` - a request must have some kind of URL to be valid, and likewise trying to manipulate a backend response when there is none (yet) makes no sense. The VCL compiler will detect such errors.\n\nVariables have types. Most of them a STRINGS, and anything in VCL can be turned into a STRING, but some variables have types like `DURATION`, `IP` etc.\n\nWhen setting a such variables, the right hand side of the equal sign must have the correct variables type, you cannot assign a STRING to a variable of type NUMBER, even if the string is `\"42\"`.\n\nExplicit conversion functions are available in [VMOD std - Varnish Standard Module](vmod_std#vmod-std-3).\n\nFor the complete album of VCL variables see: [VCL-Variables](vcl-var#vcl-var-7).\n\n##### Strings\n\nBasic strings are enclosed in double quotes `\"`*…*`\"`, and may not contain newlines. Long strings are enclosed in `{\"`*…*`\"}` or `\"\"\"`*…*`\"\"\"`. They may contain any character including single double quotes `\"`, newline and other control characters except for the *NUL* (0x00) character.\n\n##### Booleans\n\nBooleans can be either `true` or `false`. In addition, in a boolean context some data types will evaluate to `true` or `false` depending on their value.\n\nString types will evaluate to `false` if they are unset. This allows checks of the type `if (req.http.opthdr) {}` to test if a header exists, even if it is empty, whereas `if (req.http.opthdr == \"\") {}` does not distinguish if the header does not exist or if it is empty.\n\nBackend types will evaluate to `false` if they don’t have a backend assigned; integer types will evaluate to `false` if their value is zero; duration types will evaluate to `false` if their value is equal or less than zero.\n\n##### Time\n\nVCL has time. A duration can be added to a time to make another time. In string context they return a formatted string in RFC1123 format, e.g. `Sun, 06 Nov 1994 08:49:37 GMT`.\n\nThe keyword `now` returns a notion of the current time, which is kept consistent during VCL subroutine invocations, so during the execution of a VCL state subroutine (`vcl_* {}`), including all user-defined subroutines being called, `now` always returns the same value.\n\n##### Durations\n\nDurations are defined by a number followed by a unit. The number can include a fractional part, e.g. `1.5s`. The supported units are:\n\n`ms`  \nmilliseconds\n\n`s`  \nseconds\n\n`m`  \nminutes\n\n`h`  \nhours\n\n`d`  \ndays\n\n`w`  \nweeks\n\n`y`  \nyears\n\nIn string context they return a string with their value rounded to 3 decimal places and excluding the unit, e.g. `1.500`.\n\n##### Integers\n\nCertain fields are integers, used as expected. In string context they return a string, e.g. `1234`.\n\n##### Real numbers\n\nVCL understands real numbers. In string context they return a string with their value rounded to 3 decimal places, e.g. `3.142`.\n\n#### Regular Expressions\n\nVarnish uses Perl-compatible regular expressions (PCRE). For a complete description please see the pcre(3) man page.\n\nTo send flags to the PCRE engine, such as to do case insensitive matching, add the flag within parens following a question mark, like this:\n\n``` python\n# If host is NOT example dot com..\nif (req.http.host !~ \"(?i)example\\.com$\") {\n    ...\n}\n```\n\n#### Include statement\n\nTo include a VCL file in another file use the `include` keyword:\n\n``` python\ninclude \"foo.vcl\";\n```\n\nOptionally, the `include` keyword can take a `+glob` flag to include all files matching a glob pattern:\n\n``` python\ninclude +glob \"example.org/*.vcl\";\n```\n\n#### Import statement\n\nThe `import` statement is used to load Varnish Modules (VMODs.)\n\nExample:\n\n``` python\nimport std;\nsub vcl_recv {\n    std.log(\"foo\");\n}\n```\n\n#### Comments\n\nSingle lines of VCL can be commented out using `//` or `#`. Multi-line blocks can be commented out with `/*`*block*`*/`.\n\nExample:\n\n``` python\nsub vcl_recv {\n    // Single line of out-commented VCL.\n    # Another way of commenting out a single line.\n    /*\n        Multi-line block of commented-out VCL.\n    */\n}\n```\n\n#### Backends and health probes\n\nPlease see [VCL-backends](vcl-backend#vcl-backend-7) and [VCL-probe](vcl-probe#vcl-probe-7)\n\n#### Access Control List (ACL)\n\nAn Access Control List (ACL) declaration creates and initialises a named access control list which can later be used to match client addresses:\n\n``` python\nacl localnetwork {\n    \"localhost\";    # myself\n    \"192.0.2.0\"/24; # and everyone on the local network\n    ! \"192.0.2.23\"; # except for the dial-in router\n}\n```\n\nIf an ACL entry specifies a host name which Varnish is unable to resolve, it will match any address it is compared to. Consequently, if it is preceded by a negation mark, it will reject any address it is compared to, which may not be what you intended. If the entry is enclosed in parentheses, however, it will simply be ignored if the host name cannot be resolved.\n\nTo match an IP address against an ACL, simply use the match operator:\n\n``` python\nif (client.ip ~ localnetwork) {\n    return (pipe);\n}\n```\n\nACLs have feature flags which can be set or cleared for each ACL individually:\n\n- `+log` - Emit a `Acl` record in VSL to tell if a match was found or not.\n\n- `+table` - Implement the ACL with a table instead of compiled code. This runs a little bit slower, but compiles large ACLs much faster.\n\n- `-pedantic` - Allow masks to cover non-zero host-bits. This allows the following to work:\n\n  ``` python\n  acl foo -pedantic +log {\n      \"firewall.example.com\" / 24;\n  }\n  ```\n\n  However, if the name resolves to both IPv4 and IPv6 you will still get an error.\n\n#### VCL objects\n\nA VCL object can be instantiated with the `new` keyword:\n\n``` python\nsub vcl_init {\n    new b = directors.round_robin()\n    b.add_backend(node1);\n}\n```\n\nThis is only available in `vcl_init`.\n\n#### Subroutines\n\nA subroutine is used to group code for legibility or reusability:\n\n``` python\nsub pipe_if_local {\n    if (client.ip ~ localnetwork) {\n        return (pipe);\n    }\n}\n```\n\nSubroutines in VCL do not take arguments, nor do they return values. The built in subroutines all have names beginning with `vcl_`, which is reserved.\n\nTo call a subroutine, use the `call` keyword followed by the subroutine’s name:\n\n``` python\nsub vcl_recv {\n    call pipe_if_local;\n}\n```\n\n##### Return statements\n\nThe ongoing `vcl_*` subroutine execution ends when a `return(`*\\<action\\>*`)` statement is made.\n\nThe *\\<action\\>* specifies how execution should proceed. The context defines which actions are available.\n\nIt is possible to exit a subroutine that is not part of the built-in ones using a simple `return` statement without specifying an action. It exits the subroutine without transitioning to a different state:\n\n``` python\nsub filter_cookies {\n    if (!req.http.cookie) {\n        return;\n    }\n    # complex cookie filtering\n}\n```\n\n##### Multiple subroutines\n\nIf multiple subroutines with the name of one of the built-in ones are defined, they are concatenated in the order in which they appear in the source.\n\nThe built-in VCL distributed with Varnish will be implicitly concatenated when the VCL is compiled.\n\n#### Functions\n\nThe following built-in functions are available:\n\n##### ban(STRING)\n\nDeprecated. See [BOOL ban(STRING)](vmod_std#std-ban).\n\nThe `ban()` function is identical to [BOOL ban(STRING)](vmod_std#std-ban), but does not provide error reporting.\n\n##### hash_data(input)\n\nAdds an input to the hash input. In the built-in VCL `hash_data()` is called on the host and URL of the request. Available in `vcl_hash`.\n\n##### synthetic(STRING)\n\nPrepare a synthetic response body containing the *STRING*. Available in `vcl_synth` and `vcl_backend_error`.\n\nIdentical to `set resp.body` / `set beresp.body`.\n\n##### regsub(str, regex, sub)\n\nReturns a copy of *str* with the first occurrence of the regular expression *regex* replaced with *sub*. Within *sub*, `\\0` (which can also be spelled `\\&`) is replaced with the entire matched string, and `\\`*n* is replaced with the contents of subgroup *n* in the matched string.\n\n##### regsuball(str, regex, sub)\n\nAs `regsub()`, but this replaces all occurrences.\n\nFor converting or casting VCL values between data types use the functions available in the std VMOD.\n\n### Versioning\n\nMultiple versions of the VCL syntax can coexist within certain constraints.\n\nThe VCL syntax version at the start of VCL file specified with `-f` sets the hard limit that cannot be exceeded anywhere, and it selects the appropriate version of the builtin VCL.\n\nThat means that you can never include `vcl 9.1;` from `vcl 8.7;`, but the opposite *may* be possible, to the extent the compiler supports it.\n\nFiles pulled in via `include` do not need to have a `vcl` *X.Y*`;` but it may be a good idea to do it anyway, to not have surprises in the future. The syntax version set in an included file only applies to that file and any files it includes - unless these set their own VCL syntax version.\n\nThe version of Varnish this file belongs to supports syntax 4.0 and 4.1.\n\n### EXAMPLES\n\nFor examples, please see the online documentation.\n\n### SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [VCL-backends](vcl-backend#vcl-backend-7)\n- [VCL-probe](vcl-probe#vcl-probe-7)\n- [VCL-steps](vcl-step#vcl-step-7)\n- [VCL-Variables](vcl-var#vcl-var-7)\n- [VMOD directors - Varnish Directors Module](vmod_directors#vmod-directors-3)\n- [VMOD std - Varnish Standard Module](vmod_std#vmod-std-3)\n\n### HISTORY\n\nVCL was developed by Poul-Henning Kamp in cooperation with Verdens Gang AS, Redpill Linpro and Varnish Software. This manual page is written by Per Buer, Poul-Henning Kamp, Martin Blix Grydeland, Kristian Lyngstøl, Lasse Karstensen and others.\n\n### COPYRIGHT\n\nThis document is licensed under the same license as Varnish itself. See LICENSE for details.\n\n- Copyright (c) 2006 Verdens Gang AS\n- Copyright (c) 2006-2015 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vcl.html](https://varnish-cache.org/docs/7.4/reference/vcl.html)"
- name: VCL - Varnish Configuration Language
  id: users-guide/vcl
  summary: This section covers how to tell Varnish how to handle your HTTP traffic, using the Varnish Configuration Language (VCL)
  description: "# VCL - Varnish Configuration Language\n\nThis section covers how to tell Varnish how to handle your HTTP traffic, using the Varnish Configuration Language (VCL).\n\nVarnish has a great configuration system. Most other systems use configuration directives, where you basically turn on and off lots of switches. We have instead chosen to use a domain specific language called VCL for this.\n\nEvery inbound request flows through Varnish and you can influence how the request is being handled by altering the VCL code. You can direct certain requests to particular backends, you can alter the requests and the responses or have Varnish take various actions depending on arbitrary properties of the request or the response. This makes Varnish an extremely powerful HTTP processor, not just for caching.\n\nVarnish translates VCL into binary code which is then executed when requests arrive. The performance impact of VCL is negligible.\n\nThe VCL files are organized into subroutines. The different subroutines are executed at different times. One is executed when we get the request, another when files are fetched from the backend server.\n\nIf you don’t call an action in your subroutine and it reaches the end Varnish will execute some built-in VCL code. You will see this VCL code commented out in the file `builtin.vcl` that ships with Varnish Cache.\n\n- [VCL Syntax](vcl-syntax)\n- [Built-in VCL](vcl-built-in-code)\n- [Request and response VCL objects](vcl-variables)\n- [Backend servers](vcl-backends)\n- [The “none” backend](vcl-backends#the-none-backend)\n- [Multiple backends](vcl-backends#multiple-backends)\n- [Backends and virtual hosts in Varnish](vcl-backends#backends-and-virtual-hosts-in-varnish)\n- [Connecting Through a Proxy](vcl-backends#connecting-through-a-proxy)\n- [Directors](vcl-backends#directors)\n- [Health checks](vcl-backends#health-checks)\n- [Layering](vcl-backends#layering)\n- [Director Resolution](vcl-backends#director-resolution)\n- [Connection Pooling](vcl-backends#connection-pooling)\n- [Hashing](vcl-hashing)\n- [Grace mode and keep](vcl-grace)\n- [Separate VCL files](vcl-separate)\n- [Using inline C to extend Varnish](vcl-inline-c)\n- [VCL Examples](vcl-examples)\n- [Device detection](devicedetection)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl.html](https://varnish-cache.org/docs/7.4/users-guide/vcl.html)"
- name: VCL Design Patterns
  id: vcl-design-patterns/index
  summary: This section showcases design patterns for The VCL language
  description: "# VCL Design Patterns\n\nThis section showcases design patterns for [The VCL language](../reference/index#reference-vcl). To keep code examples short, some aspects not directly related to a given design pattern may be simplified.\n\n- [Using extra digits in resp.status](resp-status)\n- [Ignoring the Vary header for bots](req-hash_ignore_vary)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/vcl-design-patterns/index.html](https://varnish-cache.org/docs/7.4/vcl-design-patterns/index.html)"
- name: VCL Examples
  id: users-guide/vcl-examples
  summary: These are a short collection of examples that showcase some of the capabilities of the VCL language
  description: "# VCL Examples\n\nThese are a short collection of examples that showcase some of the capabilities of the VCL language.\n\n- [Manipulating request headers in VCL](vcl-example-manipulating-headers)\n- [Altering the backend response](vcl-example-manipulating-responses)\n- [ACLs](vcl-example-acls)\n- [Adding WebSockets support](vcl-example-websockets)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-examples.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-examples.html)"
- name: VCL Syntax
  id: users-guide/vcl-syntax
  summary: VCL has inherited a lot from C and it reads much like simple C or Perl
  description: "# VCL Syntax\n\nVCL has inherited a lot from C and it reads much like simple C or Perl.\n\nBlocks are delimited by curly brackets, statements end with semicolons, and comments may be written as in C, C++ or Perl according to your own preferences.\n\nNote that VCL doesn’t contain any loops or jump statements.\n\nThis section provides an outline of the more important parts of the syntax. For a full documentation of VCL syntax please see [VCL](../reference/vcl#vcl-7) in the reference.\n\n## Strings\n\nBasic strings are enclosed in “ … “, and may not contain newlines.\n\nBackslash is not special, so for instance in `regsub()` you do not need to do the “count-the-backslashes” polka:\n\n``` python\nregsub(\"barf\", \"(b)(a)(r)(f)\", \"\\4\\3\\2p\") -> \"frap\"\n```\n\nLong strings are enclosed in {” … “} or “”” … “””. They may contain any character including “, newline and other control characters except for the NUL (0x00) character. If you really want NUL characters in a string there is a VMOD that makes it possible to create such strings.\n\n## Access control lists (ACLs)\n\nAn ACL declaration creates and initializes a named access control list which can later be used to match client addresses:\n\n``` python\nacl local {\n  \"localhost\";         // myself\n  \"192.0.2.0\"/24;      // and everyone on the local network\n  ! \"192.0.2.23\";      // except for the dialin router\n}\n```\n\nIf an ACL entry specifies a host name which Varnish is unable to resolve, it will match any address it is compared to. Consequently, if it is preceded by a negation mark, it will reject any address it is compared to, which may not be what you intended. If the entry is enclosed in parentheses, however, it will simply be ignored.\n\nTo match an IP address against an ACL, simply use the match operator:\n\n``` python\nif (client.ip ~ local) {\n  return (pipe);\n}\n```\n\nIn Varnish versions before 7.0, ACLs would always emit a `VCL_acl` record in the VSL log, from 7.0 and forward, this must be explicitly enabled by specifying the `+log` flag:\n\n``` python\nacl local +log {\n  \"localhost\";         // myself\n  \"192.0.2.0\"/24;      // and everyone on the local network\n  ! \"192.0.2.23\";      // except for the dialin router\n}\n```\n\n## Operators\n\nThe following operators are available in VCL. See the examples further down for, uhm, examples.\n\n=  \nAssignment operator.\n\n==  \nComparison.\n\n&nbsp;\n\n~  \nMatch. Can either be used with regular expressions or ACLs.\n\n&nbsp;\n\n!  \nNegation.\n\n&nbsp;\n\n&&  \nLogical *and*\n\n&nbsp;\n\n\\|\\|  \nLogical *or*\n\n## Built in subroutines\n\nVarnish has quite a few built-in subroutines that are called for each transaction as it flows through Varnish. These built-in subroutines are all named `vcl_*` and are explained in [VCL Steps](../reference/vcl-step#id7).\n\nProcessing in built-in subroutines ends with `return (<action>)` (see [VCL Actions](../reference/vcl-step#vcl-actions)).\n\nThe [Built-in VCL](vcl-built-in-code#vcl-built-in-code) also contains custom assistant subroutines called by the built-in subroutines, also prefixed with `vcl_`.\n\n## Custom subroutines\n\nYou can write your own subroutines, whose names cannot start with `vcl_`.\n\nA subroutine is typically used to group code for legibility or reusability:\n\n``` python\nsub pipe_if_local {\n  if (client.ip ~ local) {\n    return (pipe);\n  }\n}\n```\n\nTo call a subroutine, use the `call` keyword followed by the subroutine’s name:\n\n``` python\ncall pipe_if_local;\n```\n\nCustom subroutines in VCL do not take arguments, nor do they return values.\n\n`return (<action>)` (see [VCL Actions](../reference/vcl-step#vcl-actions)) as shown in the example above returns all the way from the top level built in subroutine (see [VCL Steps](../reference/vcl-step#id7)) which, possibly through multiple steps, lead to the call of the custom subroutine.\n\n`return` without an action resumes execution after the `call` statement of the calling subroutine.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/users-guide/vcl-syntax.html](https://varnish-cache.org/docs/7.4/users-guide/vcl-syntax.html)"
- name: VCL-backends
  id: reference/vcl-backend
  summary: A backend declaration creates and initialises a named backend object
  description: "# VCL-backends\n\n## Configuring Backends\n\nManual section:  \n7\n\n### Backend definition\n\nA backend declaration creates and initialises a named backend object. A declaration start with the keyword `backend` followed by the name of the backend. The actual declaration is in curly brackets, in a key/value fashion.:\n\n``` python\nbackend name {\n    .attribute1 = value;\n    .attribute2 = value;\n    [...]\n}\n```\n\nIf there is a backend named `default` it will be used unless another backend is explicitly set. If no backend is named `default` the first backend in the VCL program becomes the default.\n\nIf you only use dynamic backends created by VMODs, an empty, always failing (503) backend can be specified:\n\n``` python\nbackend default none;\n```\n\nA backend must be specified with either a `.host` or a `.path` attribute, but not both. All other attributes have default values.\n\n### Attribute `.host`\n\nTo specify a networked backend `.host` takes either a numeric IPv4/IPv6 address or a domain name which resolves to *at most* one IPv4 and one IPv6 address:\n\n``` python\n.host = \"127.0.0.1\";\n\n.host = \"[::1]:8080\";\n\n.host = \"example.com:8081\";\n\n.host = \"example.com:http\";\n```\n\n### Attribute `.port`\n\nThe TCP port number or service name can be specified as part of `.host` as above or separately using the `.port` attribute:\n\n``` python\n.port = \"8081\";\n\n.port = \"http\";\n```\n\n### Attribute `.path`\n\nThe absolute path to a Unix(4) domain socket of a local backend:\n\n``` python\n.path = \"/var/run/http.sock\";\n```\n\nor, where available, `@` followed by the name of an abstract socket of a local backend:\n\n``` python\n.path = \"@mybackend\";\n```\n\nA warning will be issued if the uds-socket does not exist when the VCL is loaded. This makes it possible to start the UDS-listening peer, or set the socket file’s permissions afterwards.\n\nIf the uds-socket socket does not exist or permissions deny access, connection attempts will fail.\n\n### Attribute `.host_header`\n\nA host header to add to probes and regular backend requests if they have no such header:\n\n``` python\n.host_header = \"Host: example.com\";\n```\n\n### Timeout Attributes\n\nThese attributes control how patient `varnishd` is during backend fetches:\n\n``` python\n.connect_timeout = 1.4s;\n.first_byte_timeout = 20s;\n.between_bytes_timeout = 10s;\n```\n\nThe default values comes parameters with the same names, see [varnishd](varnishd#varnishd-1).\n\n### Attribute `.max_connections`\n\nLimit how many simultaneous connections varnish can open to the backend:\n\n``` python\n.max_connections = 1000;\n```\n\n### Attribute `.proxy_header`\n\nSend a PROXY protocol header to the backend with the `client.ip` and `server.ip` values:\n\n``` python\n.proxy_header = 2;\n```\n\nLegal values are one and two, depending which version of the PROXY protocol you want.\n\n*Notice* this setting will lead to backend connections being used for a single request only (subject to future improvements). Thus, extra care should be taken to avoid running into failing backend connections with EADDRNOTAVAIL due to no local ports being available. Possible options are:\n\n- Use additional backend connections to extra IP addresses or TCP ports\n- Increase the number of available ports (Linux sysctl `net.ipv4.ip_local_port_range`)\n- Reuse backend connection ports early (Linux sysctl `net.ipv4.tcp_tw_reuse`)\n\n### Attribute `.preamble`\n\nSend a BLOB on all newly opened connections to the backend:\n\n``` python\n.preamble = :SGVsbG8gV29ybGRcbgo=:;\n```\n\n### Attribute `.via`\n\nName of another *proxy* backend through which to make the connection to the *destination* backend using the [PROXY2](https://raw.githubusercontent.com/haproxy/haproxy/master/doc/proxy-protocol.txt) protocol, for example:\n\n``` python\nbackend proxy {\n  .path = \"/path/to/proxy2_endpoint\";\n}\nbackend destination {\n  .host = \"1.2.3.4\";\n  .via = proxy;\n}\n```\n\nThe *proxy* backend can also use a `.host`/`.port` definition rather than `.path`.\n\nUse of the `.path` attribute for the *destination* backend is not supported.\n\nThe `.via` attribute is unrelated to `.proxy_header`. If both are used, a second header is sent as per `.proxy_header` specification.\n\nAs of this release, the *proxy* backend used with `.via` can not be a director, it can not itself use `.via` (error: *Can not stack .via backends*) and the protocol is fixed to [PROXY2](https://raw.githubusercontent.com/haproxy/haproxy/master/doc/proxy-protocol.txt).\n\nImplementation detail:\n\nIf `.via = <proxy>` is used, a [PROXY2](https://raw.githubusercontent.com/haproxy/haproxy/master/doc/proxy-protocol.txt) preamble is created with the *destination* backend’s address information as `dst_addr`/`dst_port` and, optionally, other TLV attributes. The connection is then made to the *proxy* backend’s endpoint (`path` or `host`/`port`). This is technically equivalent to specifying a `backend destination_via_proxy` with a `.preamble` attribute containing the appropriate [PROXY2](https://raw.githubusercontent.com/haproxy/haproxy/master/doc/proxy-protocol.txt) preamble for the *destination* backend.\n\n### Attribute `.authority`\n\nThe HTTP authority to use when connecting to this backend. If unset, `.host_header` or `.host` are used.\n\n`.authority = \"\"` disables sending an authority.\n\nAs of this release, the attribute is only used by `.via` connections as a `PP2_TYPE_AUTHORITY` Type-Length-Value (TLV) in the [PROXY2](https://raw.githubusercontent.com/haproxy/haproxy/master/doc/proxy-protocol.txt) preamble.\n\n### Attribute `.probe`\n\nPlease see [VCL-probe](vcl-probe#vcl-probe-7).\n\n### SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [VCL](vcl#vcl-7)\n- [VCL-probe](vcl-probe#vcl-probe-7)\n- [VMOD directors - Varnish Directors Module](vmod_directors#vmod-directors-3)\n- [VMOD std - Varnish Standard Module](vmod_std#vmod-std-3)\n\n### HISTORY\n\nVCL was developed by Poul-Henning Kamp in cooperation with Verdens Gang AS, Redpill Linpro and Varnish Software. This manual page is written by Per Buer, Poul-Henning Kamp, Martin Blix Grydeland, Kristian Lyngstøl, Lasse Karstensen and others.\n\n### COPYRIGHT\n\nThis document is licensed under the same license as Varnish itself. See LICENSE for details.\n\n- Copyright (c) 2006 Verdens Gang AS\n- Copyright (c) 2006-2021 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vcl-backend.html](https://varnish-cache.org/docs/7.4/reference/vcl-backend.html)"
- name: VCL-probe
  id: reference/vcl-probe
  summary: There are no mandatory attributes, they all have defaults
  description: "# VCL-probe\n\n## Configuring Backend Health Probes\n\nManual section:  \n7\n\n### Backend health probes\n\nVarnish can be configured to periodically send a request to test if a backend is answering and thus “healthy”.\n\nProbes can be configured per backend:\n\n``` python\nbackend foo {\n    [...]\n    .probe = {\n        [...]\n    }\n}\n```\n\nThey can be named and shared between backends:\n\n``` python\nprobe light {\n    [...]\n}\n\nbackend foo {\n    .probe = light;\n}\n\nbackend bar {\n    .probe = light;\n}\n```\n\nOr a `default` probe can be defined, which applies to all backends without a specific `.probe` configured:\n\n``` python\nprobe default {\n    [...]\n}\n```\n\nThe basic syntax is the same as for backends:\n\n``` python\nprobe name {\n    .attribute1 = value;\n    .attribute2 = \"value\";\n    [...]\n}\n```\n\nThere are no mandatory attributes, they all have defaults.\n\n### Attribute `.url`\n\nThe URL to query. Defaults to `/`:\n\n``` python\n.url = \"/health-probe\";\n```\n\n### Attribute `.request`\n\nCan be used to specify a full HTTP/1.1 request to be sent:\n\n``` python\n.request = \"GET / HTTP/1.1\"\n    \"Host: example.com\"\n    \"X-Magic: We're fine with this.\"\n    \"Connection: close\";\n```\n\nEach of the strings will have `CRNL` appended and a final HTTP header block terminating `CRNL` will be appended as well.\n\nBecause connection shutdown is part of the health check, `Connection: close` is mandatory.\n\n### Attribute `.expected_response`\n\nThe expected HTTP status, defaults to `200`:\n\n``` python\n.expected_response = 418;\n```\n\n### Attribute `.expect_close`\n\nWhether or not to expect the backend to close the underlying connection.\n\nAccepts `true` or `false`, defaults to `true`:\n\n``` python\n.expect_close = false;\n```\n\nWarning: when the backend does not close the connection, setting `expect_close` to `false` makes probe tasks wait until they time out before inspecting the response.\n\n### Attribute `.timeout`\n\nHow fast the probe must succeed, default is two seconds:\n\n``` python\n.timeout = 10s;\n```\n\n### Attribute `.interval`\n\nTime between probes, default is five seconds:\n\n``` python\n.interval = 1m;\n```\n\n### The backend health shift register\n\nBackend health probes uses a 64 stage shift register to remember the most recent health probes and to evaluate the total health of the backend.\n\nIn the CLI, a good backend health status looks like this:\n\n``` text\nvarnish> backend.list -p boot.backend\nBackend name    Admin    Probe    Health     Last change\nboot.backend    probe    5/5      healthy    Wed, 13 Jan 2021 10:31:50 GMT\n Current states  good:  5 threshold:  4 window:  5\n  Average response time of good probes: 0.000793\n  Oldest ================================================== Newest\n  4444444444444444444444444444444444444444444444444444444444444444 Good IPv4\n  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Good Xmit\n  RRRRRRRRRRRRRRRRRRRRRRR----RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR Good Recv\n  HHHHHHHHHHHHHHHHHHHHHHH--------HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH Happy\n```\n\nStarting from the bottom, the last line shows that this backend has been declared “Happy” for most the 64 health probes, but there were some trouble some while ago.\n\nHowever, in this case the `.window` is configured to five, and the `.threshold` is set to four, so at this point in time, the backend is considered fully healthy.\n\nAn additional `.initial` fills that many “happy” entries in the shift register when the VCL is loaded, so that backends can quickly become healthy, even if their health is normally considered over many samples:\n\n``` python\n.interval = 1s;\n.window = 60;\n.threshold = 45;\n.initial = 43;\n```\n\nThis backend will be considered healthy if three out of four health probes in the last minute were good, but it becomes healthy as soon as two good probes have happened after the VCL was loaded.\n\nThe default values are:\n\n- `.window` = 8\n- `.threshold` = 3\n- `.initial` = one less than `.threshold`\n\nNote that the default `.initial` means that the backend will be marked unhealthy until the first probe response come back successful. This means that for backends created on demand (by vmods) cannot use the default value for `.initial`, as the freshly created backend would very likely still be unhealthy when the backend request happens.\n\n#### SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [VCL](vcl#vcl-7)\n- [VCL-backends](vcl-backend#vcl-backend-7)\n- [VMOD directors - Varnish Directors Module](vmod_directors#vmod-directors-3)\n- [VMOD std - Varnish Standard Module](vmod_std#vmod-std-3)\n\n#### HISTORY\n\nVCL was developed by Poul-Henning Kamp in cooperation with Verdens Gang AS, Redpill Linpro and Varnish Software. This manual page is written by Per Buer, Poul-Henning Kamp, Martin Blix Grydeland, Kristian Lyngstøl, Lasse Karstensen and others.\n\n#### COPYRIGHT\n\nThis document is licensed under the same license as Varnish itself. See LICENSE for details.\n\n- Copyright (c) 2006 Verdens Gang AS\n- Copyright (c) 2006-2021 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vcl-probe.html](https://varnish-cache.org/docs/7.4/reference/vcl-probe.html)"
- name: VCL-steps
  id: reference/vcl-step
  summary: Various built-in subroutines are called during processing of client and backend requests as well as upon vcl.load and vcl.discard
  description: "# VCL-steps\n\n## Built-in subroutines\n\nManual section:  \n7\n\n### DESCRIPTION\n\nVarious built-in subroutines are called during processing of client and backend requests as well as upon `vcl.load` and `vcl.discard`.\n\nSee [Varnish Processing States](states#reference-states) for a detailed graphical overview of the states and how they relate to core code functions and VCL subroutines.\n\nBuilt-in subroutines always terminate with a `return (<action>)`, where `<action>` determines how processing continues in the request processing state machine.\n\nThe behaviour of actions is identical or at least similar across subroutines, so differences are only documented where relevant.\n\nCommon actions are documented in [VCL Actions](#vcl-actions) in the next section. Actions specific to only one or some subroutines are documented in [VCL Steps](#id7).\n\nA default behavior is provided for all [Varnish Processing States](states#reference-states) in the [Built-in VCL](../users-guide/vcl-built-in-code#vcl-built-in-code) code.\n\n### VCL Actions\n\nActions are used with the `return(<action>)` keyword, which returns control from subroutines back to varnish. The action determines how processing in varnish continues as shown in [Varnish Processing States](states#reference-states).\n\nCommon actions are documented here, while additional actions specific to only one or some subroutines are documented in the next section [VCL Steps](#id7) as well as which action can be used from which built in subroutine.\n\n#### Common actions for the client and backend side\n\n##### `fail`\n\nTransition to [vcl_synth](#vcl-synth) on the client side as for `return(synth(503, \"VCL Failed\"))`, but with any request state changes undone as if `std.rollback()` was called and forcing a connection close.\n\nIntended for fatal errors, for which only minimal error handling is possible.\n\n#### Common actions for the client side\n\n##### `synth(status code, reason)`\n\nTransition to [vcl_synth](#vcl-synth) with `resp.status` and `resp.reason` being preset to the arguments of `synth()`.\n\n##### `pass`\n\nSwitch to pass mode, making the current request not use the cache and not putting its response into it. Control will eventually pass to [vcl_pass](#vcl-pass).\n\n##### `pipe`\n\nSwitch to pipe mode. Control will eventually pass to [vcl_pipe](#vcl-pipe).\n\n##### `restart`\n\nRestart the transaction. Increases the `req.restarts` counter.\n\nIf the number of restarts is higher than the *max_restarts* parameter, control is passed to [vcl_synth](#vcl-synth) as for `return(synth(503, \"Too many restarts\"))`\n\nFor a restart, all modifications to `req` attributes are preserved except for `req.restarts` and `req.xid`, which need to change by design.\n\n#### Common actions for the backend side\n\n##### `abandon`\n\nAbandon the backend request. Unless the backend request was a background fetch, control is passed to [vcl_synth](#vcl-synth) on the client side with `resp.status` preset to 503.\n\n### VCL Steps\n\n#### Client side\n\n##### vcl_recv\n\nCalled at the beginning of a request, after the complete request has been received and parsed, after a `restart` or as the result of an ESI include.\n\nIts purpose is to decide whether or not to serve the request, possibly modify it and decide on how to process it further. A backend hint may be set as a default for the backend processing side.\n\nThe `vcl_recv` subroutine may terminate with calling `return()` on one of the following keywords:\n\n##### vcl_pipe\n\nCalled upon entering pipe mode. In this mode, the request is passed on to the backend, and any further data from both the client and backend is passed on unaltered until either end closes the connection. Basically, Varnish will degrade into a simple TCP proxy, shuffling bytes back and forth. For a connection in pipe mode, no other VCL subroutine will ever get called after `vcl_pipe`.\n\nThe `vcl_pipe` subroutine may terminate with calling `return()` with one of the following keywords:\n\n##### vcl_pass\n\nCalled upon entering pass mode. In this mode, the request is passed on to the backend, and the backend’s response is passed on to the client, but is not entered into the cache. Subsequent requests submitted over the same client connection are handled normally.\n\nThe `vcl_pass` subroutine may terminate with calling `return()` with one of the following keywords:\n\n##### vcl_hash\n\nCalled after `vcl_recv` to create a hash value for the request. This is used as a key to look up the object in Varnish.\n\nThe `vcl_hash` subroutine may terminate with calling `return()` with one of the following keywords:\n\n##### vcl_purge\n\nCalled after the purge has been executed and all its variants have been evicted.\n\nThe `vcl_purge` subroutine may terminate with calling `return()` with one of the following keywords:\n\n##### vcl_miss\n\nCalled after a cache lookup if the requested document was not found in the cache or if [vcl_hit](#vcl-hit) returned `fetch`.\n\nIts purpose is to decide whether or not to attempt to retrieve the document from the backend. A backend hint may be set as a default for the backend processing side.\n\nThe `vcl_miss` subroutine may terminate with calling `return()` with one of the following keywords:\n\n##### vcl_hit\n\nCalled when a cache lookup is successful. The object being hit may be stale: It can have a zero or negative `ttl` with only `grace` or `keep` time left.\n\nThe `vcl_hit` subroutine may terminate with calling `return()` with one of the following keywords:\n\n##### vcl_deliver\n\nCalled before any object except a `vcl_synth` result is delivered to the client.\n\nThe `vcl_deliver` subroutine may terminate with calling `return()` with one of the following keywords:\n\n##### vcl_synth\n\nCalled to deliver a synthetic object. A synthetic object is generated in VCL, not fetched from the backend. Its body may be constructed using the `synthetic()` function.\n\nA `vcl_synth` defined object never enters the cache, contrary to a [vcl_backend_error](#vcl-backend-error) defined object, which may end up in cache.\n\nThe subroutine may terminate with calling `return()` with one of the following keywords:\n\n#### Backend Side\n\n##### vcl_backend_fetch\n\nCalled before sending the backend request. In this subroutine you typically alter the request before it gets to the backend.\n\nThe `vcl_backend_fetch` subroutine may terminate with calling `return()` with one of the following keywords:\n\nBefore calling `vcl_backend_fetch`, Varnish core prepares the `bereq` backend request as follows:\n\n- Unless the request is a `pass`,\n\n  - set `bereq.method` to `GET` and `bereq.proto` to `HTTP/1.1` and\n  - set `bereq.http.Accept_Encoding` to `gzip` if [http_gzip_support](varnishd#ref-param-http-gzip-support) is enabled.\n\n- If there is an existing cache object to be revalidated, set `bereq.http.If-Modified-Since` from its `Last-Modified` header and/or set `bereq.http.If-None-Match` from its `Etag` header\n\n- Set `bereq.http.X-Varnish` to the current transaction id (`vxid`)\n\nThese changes can be undone or modified in `vcl_backend_fetch` before the backend request is issued.\n\nIn particular, to cache non-GET requests, `req.method` needs to be saved to a header or variable in [vcl_recv](#vcl-recv) and restored to `bereq.method`. Notice that caching non-GET requests typically also requires changing the cache key in [vcl_hash](#vcl-hash) e.g. by also hashing the request method and/or request body.\n\nHEAD request can be satisfied from cached GET responses.\n\n##### vcl_backend_response\n\nCalled after the response headers have been successfully retrieved from the backend.\n\nThe `vcl_backend_response` subroutine may terminate with calling `return()` with one of the following keywords:\n\n##### vcl_backend_error\n\nThis subroutine is called if we fail the backend fetch or if *max_retries* has been exceeded.\n\nReturning with [abandon](#abandon) does not leave a cache object.\n\nIf returning with `deliver` and a `beresp.ttl > 0s`, a synthetic cache object is generated in VCL, whose body may be constructed using the `synthetic()` function.\n\nWhen there is a waiting list on the object, the default `ttl` will be positive (currently one second), set before entering `vcl_backend_error`. This is to avoid request serialization and hammering on a potentially failing backend.\n\nSince these synthetic objects are cached in these special circumstances, be cautious with putting private information there. If you really must, then you need to explicitly set `beresp.ttl` to zero in `vcl_backend_error`.\n\nThe `vcl_backend_error` subroutine may terminate with calling `return()` with one of the following keywords:\n\n#### During vcl.load / vcl.discard\n\n##### vcl_init\n\nCalled when VCL is loaded, before any requests pass through it. Typically used to initialize VMODs.\n\nThe `vcl_init` subroutine may terminate with calling `return()` with one of the following keywords:\n\n##### vcl_fini\n\nCalled when VCL is discarded only after all requests have exited the VCL. Typically used to clean up VMODs.\n\nThe `vcl_fini` subroutine may terminate with calling `return()` with one of the following keywords:\n\n### SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [VCL](vcl#vcl-7)\n\n### COPYRIGHT\n\nThis document is licensed under the same license as Varnish itself. See LICENSE for details.\n\n- Copyright (c) 2006 Verdens Gang AS\n- Copyright (c) 2006-2021 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vcl-step.html](https://varnish-cache.org/docs/7.4/reference/vcl-step.html)"
- name: VCL-Variables
  id: reference/vcl-var
  summary: This is a list of all variables in the VCL language
  description: "# VCL-Variables\n\n## The complete album\n\nManual section:  \n7\n\n### DESCRIPTION\n\nThis is a list of all variables in the VCL language.\n\nVariable names take the form `scope.variable[.index]`, for instance:\n\n``` python\nreq.url\nberesp.http.date\nclient.ip\n```\n\nWhich operations are possible on each variable is described below, often with the shorthand “backend” which covers the `vcl_backend_* {}` subroutines and “client” which covers the rest, except `vcl_init {}` and `vcl_fini {}`.\n\n#### local, server, remote and client\n\nThese variables describe the network connection between the client and varnishd.\n\nWithout PROXY protocol:\n\n``` python\n     client    server\n     remote    local\n       v          v\nCLIENT ------------ VARNISHD\n```\n\nWith PROXY protocol:\n\n``` python\n     client    server   remote     local\n       v          v       v          v\nCLIENT ------------ PROXY ------------ VARNISHD\n```\n\nclient.identity\n\nType: STRING\n\nReadable from: client, backend\n\nWritable from: client\n\nIdentification of the client, used to load balance in the client director. Defaults to `client.ip`\n\nThis variable can be overwritten with more precise information, for instance extracted from a `Cookie:` header.\n\nclient.ip\n\nType: IP\n\nReadable from: client, backend\n\nThe client’s IP address, either the same as `remote.ip` or what the PROXY protocol told us.\n\nserver.hostname\n\nType: STRING\n\nReadable from: all\n\nThe host name of the server, as returned by the `gethostname(3)` system function.\n\nserver.identity\n\nType: STRING\n\nReadable from: all\n\nThe identity of the server, as set by the `-i` parameter.\n\nIf an `-i` parameter is not passed to varnishd, the return value from `gethostname(3)` system function will be used.\n\nserver.ip\n\nType: IP\n\nReadable from: client, backend\n\nThe IP address of the socket on which the client connection was received, either the same as `server.ip` or what the PROXY protocol told us.\n\nremote.ip\n\nType: IP\n\nReadable from: client, backend\n\nThe IP address of the other end of the TCP connection. This can either be the clients IP, or the outgoing IP of a proxy server.\n\nIf the connection is a UNIX domain socket, the value will be `0.0.0.0:0`\n\nlocal.endpoint `VCL >= 4.1`\n\nType: STRING\n\nReadable from: client, backend\n\nThe address of the ‘-a’ socket the session was accepted on.\n\nIf the argument was `-a foo=:81` this would be “:81”\n\nlocal.ip\n\nType: IP\n\nReadable from: client, backend\n\nThe IP address (and port number) of the local end of the TCP connection, for instance `192.168.1.1:81`\n\nIf the connection is a UNIX domain socket, the value will be `0.0.0.0:0`\n\nlocal.socket `VCL >= 4.1`\n\nType: STRING\n\nReadable from: client, backend\n\nThe name of the ‘-a’ socket the session was accepted on.\n\nIf the argument was `-a foo=:81` this would be “foo”.\n\nNote that all ‘-a’ gets a default name on the form `a%d` if no name is provided.\n\n#### req and req_top\n\nThese variables describe the present request, and when ESI:include requests are being processed, req_top points to the request received from the client.\n\nreq\n\nType: HTTP\n\nReadable from: client\n\nThe entire request HTTP data structure. Mostly useful for passing to VMODs.\n\nreq.backend_hint\n\nType: BACKEND\n\nReadable from: client\n\nWritable from: client\n\nSet bereq.backend to this if we attempt to fetch. When set to a director, reading this variable returns an actual backend if the director has resolved immediately, or the director otherwise. When used in string context, returns the name of the director or backend, respectively.\n\nreq.can_gzip\n\nType: BOOL\n\nReadable from: client\n\nTrue if the client provided `gzip` or `x-gzip` in the `Accept-Encoding` header.\n\nreq.esi `VCL <= 4.0`\n\nType: BOOL\n\nReadable from: client\n\nWritable from: client\n\nSet to `false` to disable ESI processing regardless of any value in beresp.do_esi. Defaults to `true`. This variable is replaced by `resp.do_esi` in VCL 4.1.\n\nreq.esi_level\n\nType: INT\n\nReadable from: client\n\nA count of how many levels of ESI requests we’re currently at.\n\nreq.grace\n\nType: DURATION\n\nReadable from: client\n\nWritable from: client\n\nUpper limit on the object grace.\n\nDuring lookup the minimum of req.grace and the object’s stored grace value will be used as the object’s grace.\n\nreq.hash\n\nType: BLOB\n\nReadable from: vcl_hit, vcl_miss, vcl_pass, vcl_purge, vcl_deliver\n\nThe hash key of this request. Mostly useful for passing to VMODs, but can also be useful for debugging hit/miss status.\n\nreq.hash_always_miss\n\nType: BOOL\n\nReadable from: client\n\nWritable from: client\n\nDefault: `false`.\n\nForce a cache miss for this request, even if perfectly good matching objects are in the cache.\n\nThis is useful to force-update the cache without invalidating existing entries in case the fetch fails.\n\nreq.hash_ignore_busy\n\nType: BOOL\n\nReadable from: client\n\nWritable from: client\n\nDefault: `false`.\n\nIgnore any busy object during cache lookup.\n\nYou only want to do this when you have two server looking up content sideways from each other to avoid deadlocks.\n\nreq.hash_ignore_vary\n\nType: BOOL\n\nReadable from: client\n\nWritable from: client\n\nDefault: `false`.\n\nIgnore objects vary headers during cache lookup.\n\nThis returns the very first match regardless of the object compatibility with the client request. This is useful when variants are irrelevant to certain clients, and differences in the way the resouce is presented don’t change how the client will interpret it.\n\nUse with caution.\n\nreq.http.\\*\n\nType: HEADER\n\nReadable from: client\n\nWritable from: client\n\nUnsetable from: client\n\nThe headers of request, things like `req.http.date`.\n\nThe RFCs allow multiple headers with the same name, and both `set` and `unset` will remove *all* headers with the name given.\n\nThe header name `*` is a VCL symbol and as such cannot, for example, start with a numeral. To work with valid header that can’t be represented as VCL symbols it is possible to quote the name, like `req.http.\"grammatically.valid\"`. None of the HTTP headers present in IANA registries need to be quoted, so the quoted syntax is discouraged but available for interoperability.\n\nSome headers that cannot be tampered with for proper HTTP fetch or delivery are read-only.\n\nreq.http.content-length\n\nType: HEADER\n\nReadable from: client\n\nThe content-length header field is protected, see [protected_headers](#protected-headers).\n\nreq.http.transfer-encoding\n\nType: HEADER\n\nReadable from: client\n\nThe transfer-encoding header field is protected, see [protected_headers](#protected-headers).\n\nreq.is_hitmiss\n\nType: BOOL\n\nReadable from: client\n\nIf this request resulted in a hitmiss\n\nreq.is_hitpass\n\nType: BOOL\n\nReadable from: client\n\nIf this request resulted in a hitpass\n\nreq.method\n\nType: STRING\n\nReadable from: client\n\nWritable from: client\n\nThe request method (e.g. “GET”, “HEAD”, …)\n\nreq.proto `VCL <= 4.0`\n\nType: STRING\n\nReadable from: client\n\nWritable from: client\n\nThe HTTP protocol version used by the client, usually “HTTP/1.1” or “HTTP/2.0”.\n\nreq.proto `VCL >= 4.1`\n\nType: STRING\n\nReadable from: client\n\nThe HTTP protocol version used by the client, usually “HTTP/1.1” or “HTTP/2.0”.\n\nreq.restarts\n\nType: INT\n\nReadable from: client\n\nA count of how many times this request has been restarted.\n\nreq.storage\n\nType: STEVEDORE\n\nReadable from: client\n\nWritable from: client\n\nThe storage backend to use to save this request body.\n\nreq.time\n\nType: TIME\n\nReadable from: client\n\nThe time when the request was fully received, remains constant across restarts.\n\nreq.trace\n\nType: BOOL\n\nReadable from: client\n\nWritable from: client\n\nControls if `VCL_trace` VSL records are emitted for the current request, see [VSL](vsl#vsl-7).\n\nDefaults to the setting of the `feature trace` parameter, see [varnishd](varnishd#varnishd-1). Does not get reset by a rollback.\n\nreq.transport\n\nType: STRING\n\nReadable from: client\n\nThe transport protocol which brought this request.\n\nreq.ttl\n\nType: DURATION\n\nReadable from: client\n\nWritable from: client\n\nUpper limit on the object age for cache lookups to return hit.\n\nreq.url\n\nType: STRING\n\nReadable from: client\n\nWritable from: client\n\nThe requested URL, for instance “/robots.txt”.\n\nreq.xid\n\nType: INT\n\nReadable from: client\n\nUnique ID of this request.\n\nreq_top.http.\\*\n\nType: HEADER\n\nReadable from: client\n\nHTTP headers of the top-level request in a tree of ESI requests. Identical to req.http. in non-ESI requests.\n\nSee [req.http](#req-http) for general notes.\n\nreq_top.method\n\nType: STRING\n\nReadable from: client\n\nThe request method of the top-level request in a tree of ESI requests. (e.g. “GET”, “HEAD”). Identical to req.method in non-ESI requests.\n\nreq_top.proto\n\nType: STRING\n\nReadable from: client\n\nHTTP protocol version of the top-level request in a tree of ESI requests. Identical to req.proto in non-ESI requests.\n\nreq_top.time\n\nType: TIME\n\nReadable from: client\n\nThe time when the top-level request was fully received, remains constant across restarts.\n\nreq_top.url\n\nType: STRING\n\nReadable from: client\n\nThe requested URL of the top-level request in a tree of ESI requests. Identical to req.url in non-ESI requests.\n\n#### bereq\n\nThis is the request we send to the backend, it is built from the clients `req.*` fields by filtering out “per-hop” fields which should not be passed along (`Connection:`, `Range:` and similar).\n\nSlightly more fields are allowed through for ``` pass` fetches than for `miss` fetches, for instance ``Range ```.\n\nbereq\n\nType: HTTP\n\nReadable from: backend\n\nThe entire backend request HTTP data structure. Mostly useful as argument to VMODs.\n\nbereq.backend\n\nType: BACKEND\n\nReadable from: vcl_pipe, backend\n\nWritable from: vcl_pipe, backend\n\nThis is the backend or director we attempt to fetch from. When set to a director, reading this variable returns an actual backend if the director has resolved immediately, or the director otherwise. When used in string context, returns the name of the director or backend, respectively.\n\nbereq.between_bytes_timeout\n\nType: DURATION\n\nReadable from: backend\n\nWritable from: backend\n\nDefault: `.between_bytes_timeout` attribute from the [Backend definition](vcl-backend#backend-definition), which defaults to the `between_bytes_timeout` parameter, see [varnishd](varnishd#varnishd-1).\n\nThe time in seconds to wait between each received byte from the backend. Not available in pipe mode.\n\nbereq.body\n\nType: BODY\n\nUnsetable from: vcl_backend_fetch\n\nThe request body.\n\nUnset will also remove [bereq.http.content-length](#bereq-http-content-length).\n\nbereq.connect_timeout\n\nType: DURATION\n\nReadable from: vcl_pipe, backend\n\nWritable from: vcl_pipe, backend\n\nDefault: `.connect_timeout` attribute from the [Backend definition](vcl-backend#backend-definition), which defaults to the `connect_timeout` parameter, see [varnishd](varnishd#varnishd-1).\n\nThe time in seconds to wait for a backend connection to be established.\n\nbereq.first_byte_timeout\n\nType: DURATION\n\nReadable from: backend\n\nWritable from: backend\n\nDefault: `.first_byte_timeout` attribute from the [Backend definition](vcl-backend#backend-definition), which defaults to the `first_byte_timeout` parameter, see [varnishd](varnishd#varnishd-1).\n\nThe time in seconds to wait getting the first byte back from the backend. Not available in pipe mode.\n\nbereq.hash\n\nType: BLOB\n\nReadable from: vcl_pipe, backend\n\nThe hash key of this request, a copy of `req.hash`.\n\nbereq.http.\\*\n\nType: HEADER\n\nReadable from: vcl_pipe, backend\n\nWritable from: vcl_pipe, backend\n\nUnsetable from: vcl_pipe, backend\n\nThe headers to be sent to the backend.\n\nSee [req.http](#req-http) for general notes.\n\nbereq.http.content-length\n\nType: HEADER\n\nReadable from: backend\n\nThe content-length header field is protected, see [protected_headers](#protected-headers).\n\nbereq.http.transfer-encoding\n\nType: HEADER\n\nReadable from: backend\n\nThe transfer-encoding header field is protected, see [protected_headers](#protected-headers).\n\nbereq.is_bgfetch\n\nType: BOOL\n\nReadable from: backend\n\nTrue for fetches where the client got a hit on an object in grace, and this fetch was kicked of in the background to get a fresh copy.\n\nbereq.is_hitmiss\n\nType: BOOL\n\nReadable from: backend\n\nIf this backend request was caused by a hitmiss.\n\nbereq.is_hitpass\n\nType: BOOL\n\nReadable from: backend\n\nIf this backend request was caused by a hitpass.\n\nbereq.method\n\nType: STRING\n\nReadable from: vcl_pipe, backend\n\nWritable from: vcl_pipe, backend\n\nThe request type (e.g. “GET”, “HEAD”).\n\nRegular (non-pipe, non-pass) fetches are always “GET”\n\nbereq.proto `VCL <= 4.0`\n\nType: STRING\n\nReadable from: vcl_pipe, backend\n\nWritable from: vcl_pipe, backend\n\nThe HTTP protocol version, “HTTP/1.1” unless a pass or pipe request has “HTTP/1.0” in `req.proto`\n\nbereq.proto `VCL >= 4.1`\n\nType: STRING\n\nReadable from: vcl_pipe, backend\n\nThe HTTP protocol version, “HTTP/1.1” unless a pass or pipe request has “HTTP/1.0” in `req.proto`\n\nbereq.retries\n\nType: INT\n\nReadable from: backend\n\nA count of how many times this request has been retried.\n\nbereq.time\n\nType: TIME\n\nReadable from: vcl_pipe, backend\n\nThe time when we started preparing the first backend request, remains constant across retries.\n\nbereq.trace\n\nType: BOOL\n\nReadable from: backend\n\nWritable from: backend\n\nControls if `VCL_trace` VSL records are emitted for the current request, see [VSL](vsl#vsl-7).\n\nInherits the value of `req.trace` when the backend request is created. Does not get reset by a rollback.\n\nbereq.uncacheable\n\nType: BOOL\n\nReadable from: backend\n\nIndicates whether this request is uncacheable due to a `pass` in the client side or a hit on an hit-for-pass object.\n\nbereq.url\n\nType: STRING\n\nReadable from: vcl_pipe, backend\n\nWritable from: vcl_pipe, backend\n\nThe requested URL, copied from `req.url`\n\nbereq.xid\n\nType: INT\n\nReadable from: vcl_pipe, backend\n\nUnique ID of this request.\n\n#### beresp\n\nThe response received from the backend, one cache misses, the store object is built from `beresp`.\n\nberesp\n\nType: HTTP\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nThe entire backend response HTTP data structure, useful as argument to VMOD functions.\n\nberesp.age\n\nType: DURATION\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nDefault: Age header, or zero.\n\nThe age of the object.\n\nberesp.backend\n\nType: BACKEND\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nThis is the backend we fetched from. If bereq.backend was set to a director, this will be the backend selected by the director. When used in string context, returns its name.\n\nberesp.backend.ip `VCL <= 4.0`\n\nType: IP\n\nReadable from: vcl_backend_response\n\nIP of the backend this response was fetched from.\n\nberesp.backend.name\n\nType: STRING\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nName of the backend this response was fetched from. Same as beresp.backend.\n\nberesp.body\n\nType: BODY\n\nWritable from: vcl_backend_error\n\nFor producing a synthetic body.\n\nberesp.do_esi\n\nType: BOOL\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nDefault: `false`.\n\nSet it to true to parse the object for ESI directives. This is necessary for later ESI processing on the client side. If beresp.do_esi is false when an object enters the cache, client side ESI processing will not be possible (obj.can_esi will be false).\n\nIt is a VCL error to use beresp.do_esi after setting beresp.filters.\n\nberesp.do_gunzip\n\nType: BOOL\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nDefault: `false`.\n\nSet to `true` to gunzip the object while storing it in the cache.\n\nIf `http_gzip_support` is disabled, setting this variable has no effect.\n\nIt is a VCL error to use beresp.do_gunzip after setting beresp.filters.\n\nberesp.do_gzip\n\nType: BOOL\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nDefault: `false`.\n\nSet to `true` to gzip the object while storing it.\n\nIf `http_gzip_support` is disabled, setting this variable has no effect.\n\nIt is a VCL error to use beresp.do_gzip after setting beresp.filters.\n\nberesp.do_stream\n\nType: BOOL\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nDefault: `true`.\n\nDeliver the object to the client while fetching the whole object into varnish.\n\nFor uncacheable objects, storage for parts of the body which have been sent to the client may get freed early, depending on the storage engine used.\n\nThis variable has no effect if beresp.do_esi is true or when the response body is empty.\n\nberesp.filters\n\nType: STRING\n\nReadable from: vcl_backend_response\n\nWritable from: vcl_backend_response\n\nList of Varnish Fetch Processor (VFP) filters the beresp.body will be pulled through. The order left to right signifies processing from backend to cache, iow the leftmost filter is run first on the body as received from the backend after decoding of any transfer encodings.\n\nVFP Filters change the body before going into the cache and/or being handed to the client side, where it may get processed again by resp.filters.\n\nThe following VFP filters exist in varnish-cache:\n\n- `gzip`: compress a body using gzip\n\n- `testgunzip`: Test if a body is valid gzip and refuse it otherwise\n\n- `gunzip`: Uncompress gzip content\n\n- `esi`: ESI-process plain text content\n\n- `esi_gzip`: Save gzipped snippets for efficient ESI-processing\n\n  This filter enables stitching together ESI from individually gzipped fragments, saving processing power for re-compression on the client side at the expense of some compression efficiency.\n\nAdditional VFP filters are available from VMODs.\n\nBy default, beresp.filters is constructed as follows:\n\n- `gunzip` gets added for gzipped content if `beresp.do_gunzip` or `beresp.do_esi` are true.\n- `esi_gzip` gets added if `beresp.do_esi` is true together with `beresp.do_gzip` or content is already compressed.\n- `esi` gets added if `beresp.do_esi` is true\n- `gzip` gets added for uncompressed content if `beresp.do_gzip` is true\n- `testgunzip` gets added for compressed content if `beresp.do_gunzip` is false.\n\nAfter beresp.filters is set, using any of the beforementioned `beresp.do_*` switches is a VCL error.\n\nberesp.grace\n\nType: DURATION\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nDefault: Cache-Control `stale-while-revalidate` directive, or `default_grace` parameter.\n\nSet to a period to enable grace.\n\nberesp.http.\\*\n\nType: HEADER\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nUnsetable from: vcl_backend_response, vcl_backend_error\n\nThe HTTP headers returned from the server.\n\nSee [req.http](#req-http) for general notes.\n\nberesp.http.content-length\n\nType: HEADER\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nThe content-length header field is protected, see [protected_headers](#protected-headers).\n\nberesp.http.transfer-encoding\n\nType: HEADER\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nThe transfer-encoding header field is protected, see [protected_headers](#protected-headers).\n\nberesp.keep\n\nType: DURATION\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nDefault: `default_keep` parameter.\n\nSet to a period to enable conditional backend requests.\n\nThe keep time is cache lifetime in addition to the ttl.\n\nObjects with ttl expired but with keep time left may be used to issue conditional (If-Modified-Since / If-None-Match) requests to the backend to refresh them.\n\nberesp.proto `VCL <= 4.0`\n\nType: STRING\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nThe HTTP protocol version the backend replied with.\n\nberesp.proto `VCL >= 4.1`\n\nType: STRING\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nThe HTTP protocol version the backend replied with.\n\nberesp.reason\n\nType: STRING\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nThe HTTP status message returned by the server.\n\nberesp.status\n\nType: INT\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nThe HTTP status code returned by the server.\n\nMore information in the [HTTP response status](#http-response-status) section.\n\nberesp.storage\n\nType: STEVEDORE\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nThe storage backend to use to save this object.\n\nberesp.storage_hint `VCL <= 4.0`\n\nType: STRING\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nDeprecated since varnish 5.1 and discontinued since VCL 4.1 (varnish 6.0). Use beresp.storage instead.\n\nHint to Varnish that you want to save this object to a particular storage backend.\n\nberesp.time\n\nType: TIME\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWhen the backend headers were fully received just before `vcl_backend_response {}` was entered, or when `vcl_backend_error {}` was entered.\n\nberesp.transit_buffer\n\nType: BYTES\n\nReadable from: vcl_backend_response\n\nWritable from: vcl_backend_response\n\nDefault: `transit_buffer` parameter, see [varnishd](varnishd#varnishd-1).\n\nThe maximum number of bytes the client can be ahead of the backend during a streaming pass if `beresp` is uncacheable. See also `transit_buffer` parameter documentation in [varnishd](varnishd#varnishd-1).\n\nberesp.ttl\n\nType: DURATION\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nDefault: Cache-Control `s-maxage` or `max-age` directives, or a value computed from the Expires header’s deadline, or the `default_ttl` parameter.\n\nThe object’s remaining time to live, in seconds.\n\nberesp.uncacheable\n\nType: BOOL\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWritable from: vcl_backend_response, vcl_backend_error\n\nInherited from bereq.uncacheable, see there.\n\nSetting this variable makes the object uncacheable.\n\nThis may may produce a hit-for-miss object in the cache.\n\nClearing the variable has no effect and will log the warning “Ignoring attempt to reset beresp.uncacheable”.\n\nberesp.was_304\n\nType: BOOL\n\nReadable from: vcl_backend_response, vcl_backend_error\n\nWhen `true` this indicates that we got a 304 response to our conditional fetch from the backend and turned that into `beresp.status = 200`\n\n#### obj\n\nThis is the object we found in cache. It cannot be modified.\n\nobj.age\n\nType: DURATION\n\nReadable from: vcl_hit, vcl_deliver\n\nThe age of the object.\n\nobj.can_esi\n\nType: BOOL\n\nReadable from: vcl_hit, vcl_deliver\n\nIf the object can be ESI processed, that is if setting `resp.do_esi` or adding `esi` to `resp.filters` in `vcl_deliver {}` would cause the response body to be ESI processed.\n\nobj.grace\n\nType: DURATION\n\nReadable from: vcl_hit, vcl_deliver\n\nThe object’s grace period in seconds.\n\nobj.hits\n\nType: INT\n\nReadable from: vcl_hit, vcl_deliver\n\nThe count of cache-hits on this object.\n\nIn `vcl_deliver` a value of 0 indicates a cache miss.\n\nobj.http.\\*\n\nType: HEADER\n\nReadable from: vcl_hit\n\nThe HTTP headers stored in the object.\n\nSee [req.http](#req-http) for general notes.\n\nobj.keep\n\nType: DURATION\n\nReadable from: vcl_hit, vcl_deliver\n\nThe object’s keep period in seconds.\n\nobj.proto\n\nType: STRING\n\nReadable from: vcl_hit\n\nThe HTTP protocol version stored in the object.\n\nobj.reason\n\nType: STRING\n\nReadable from: vcl_hit\n\nThe HTTP reason phrase stored in the object.\n\nobj.status\n\nType: INT\n\nReadable from: vcl_hit\n\nThe HTTP status code stored in the object.\n\nMore information in the [HTTP response status](#http-response-status) section.\n\nobj.storage\n\nType: STEVEDORE\n\nReadable from: vcl_hit, vcl_deliver\n\nThe storage backend where this object is stored.\n\nobj.time\n\nType: TIME\n\nReadable from: vcl_hit, vcl_deliver\n\nThe time the object was created from the perspective of the server which generated it. This will roughly be equivalent to `now` - `obj.age`.\n\nobj.ttl\n\nType: DURATION\n\nReadable from: vcl_hit, vcl_deliver\n\nThe object’s remaining time to live, in seconds.\n\nobj.uncacheable\n\nType: BOOL\n\nReadable from: vcl_deliver\n\nWhether the object is uncacheable (pass, hit-for-pass or hit-for-miss).\n\n#### resp\n\nThis is the response we send to the client, it is built from either `beresp` (pass/miss), `obj` (hits) or created from whole cloth (synth).\n\nWith the exception of `resp.body` all `resp.*` variables available in both `vcl_deliver{}` and `vcl_synth{}` as a matter of symmetry.\n\nresp\n\nType: HTTP\n\nReadable from: vcl_deliver, vcl_synth\n\nThe entire response HTTP data structure, useful as argument to VMODs.\n\nresp.body\n\nType: BODY\n\nWritable from: vcl_synth\n\nTo produce a synthetic response body, for instance for errors.\n\nresp.do_esi `VCL >= 4.1`\n\nType: BOOL\n\nReadable from: vcl_deliver, vcl_synth\n\nWritable from: vcl_deliver, vcl_synth\n\nDefault: obj.can_esi\n\nThis can be used to selectively disable ESI processing, even though ESI parsing happened during fetch (see beresp.do_esi). This is useful when Varnish caches peer with each other.\n\nIt is a VCL error to use resp.do_esi after setting resp.filters.\n\nresp.filters\n\nType: STRING\n\nReadable from: vcl_deliver, vcl_synth\n\nWritable from: vcl_deliver, vcl_synth\n\nList of VDP filters the resp.body will be pushed through.\n\nBefore resp.filters is set, the value read will be the default filter list as determined by varnish based on resp.do_esi and request headers.\n\nAfter resp.filters is set, changing any of the conditions which otherwise determine the filter selection will have no effiect. Using resp.do_esi is an error once resp.filters is set.\n\nresp.http.\\*\n\nType: HEADER\n\nReadable from: vcl_deliver, vcl_synth\n\nWritable from: vcl_deliver, vcl_synth\n\nUnsetable from: vcl_deliver, vcl_synth\n\nThe HTTP headers that will be returned.\n\nSee [req.http](#req-http) for general notes.\n\nresp.http.content-length\n\nType: HEADER\n\nReadable from: vcl_deliver, vcl_synth\n\nThe content-length header field is protected, see [protected_headers](#protected-headers).\n\nresp.http.transfer-encoding\n\nType: HEADER\n\nReadable from: vcl_deliver, vcl_synth\n\nThe transfer-encoding header field is protected, see [protected_headers](#protected-headers).\n\nresp.is_streaming\n\nType: BOOL\n\nReadable from: vcl_deliver, vcl_synth\n\nReturns true when the response will be streamed while being fetched from the backend.\n\nresp.proto `VCL <= 4.0`\n\nType: STRING\n\nReadable from: vcl_deliver, vcl_synth\n\nWritable from: vcl_deliver, vcl_synth\n\nThe HTTP protocol version to use for the response.\n\nresp.proto `VCL >= 4.1`\n\nType: STRING\n\nReadable from: vcl_deliver, vcl_synth\n\nThe HTTP protocol version to use for the response.\n\nresp.reason\n\nType: STRING\n\nReadable from: vcl_deliver, vcl_synth\n\nWritable from: vcl_deliver, vcl_synth\n\nThe HTTP status message that will be returned.\n\nresp.status\n\nType: INT\n\nReadable from: vcl_deliver, vcl_synth\n\nWritable from: vcl_deliver, vcl_synth\n\nThe HTTP status code that will be returned.\n\nMore information in the [HTTP response status](#http-response-status) section.\n\nresp.status 200 will get changed into 304 by core code after a return(deliver) from vcl_deliver for conditional requests to cached content if validation succeeds.\n\nFor the validation, first `req.http.If-None-Match` is compared against `resp.http.Etag`. If they compare equal according to the rules for weak validation (see RFC7232), a 304 is sent.\n\nSecondly, `req.http.If-Modified-Since` is compared against `resp.http.Last-Modified` or, if it is unset or weak, against the point in time when the object was last modified based on the `Date` and `Age` headers received with the backend response which created the object. If the object has not been modified based on that comparison, a 304 is sent.\n\nresp.time\n\nType: TIME\n\nReadable from: vcl_deliver, vcl_synth\n\nThe time when we started preparing the response, just before entering `vcl_synth {}` or `vcl_deliver {}`.\n\n#### Special variables\n\nnow\n\nType: TIME\n\nReadable from: all\n\nThe current time, in seconds since the UNIX epoch.\n\nWhen converted to STRING in expressions it returns a formatted timestamp like `Tue, 20 Feb 2018 09:30:31 GMT`\n\n`now` remains stable for the duration of any built-in VCL subroutine to make time-based calculations predictable and avoid edge cases.\n\nIn other words, even if considerable amounts of time are spent in VCL, `now` will always represent the point in time when the respective built-in VCL subroutine was entered. `now` is thus not suitable for any kind of time measurements. See [VOID timestamp(STRING s)](vmod_std#std-timestamp), [TIME now()](vmod_std#std-now) and [DURATION timed_call(SUB)](vmod_std#std-timed-call) in [VMOD std - Varnish Standard Module](vmod_std#vmod-std-3).\n\n#### sess\n\nA session corresponds to the “conversation” that Varnish has with a single client connection, over which one or more request/response transactions may take place. It may comprise the traffic over an HTTP/1 keep-alive connection, or the multiplexed traffic over an HTTP/2 connection.\n\nsess.idle_send_timeout\n\nType: DURATION\n\nReadable from: client\n\nWritable from: client\n\nSend timeout for individual pieces of data on client connections, defaults to the `idle_send_timeout` parameter, see [varnishd](varnishd#varnishd-1)\n\nsess.send_timeout\n\nType: DURATION\n\nReadable from: client\n\nWritable from: client\n\nTotal timeout for ordinary HTTP1 responses, defaults to the `send_timeout` parameter, see [varnishd](varnishd#varnishd-1)\n\nsess.timeout_idle\n\nType: DURATION\n\nReadable from: client\n\nWritable from: client\n\nIdle timeout for this session, defaults to the `timeout_idle` parameter, see [varnishd](varnishd#varnishd-1)\n\nsess.timeout_linger\n\nType: DURATION\n\nReadable from: client\n\nWritable from: client\n\nLinger timeout for this session, defaults to the `timeout_linger` parameter, see [varnishd](varnishd#varnishd-1)\n\nsess.xid `VCL >= 4.1`\n\nType: INT\n\nReadable from: client, backend\n\nUnique ID of this session.\n\n#### storage\n\nstorage.\\<name\\>.free_space\n\nType: BYTES\n\nReadable from: client, backend\n\nFree space available in the named stevedore. Only available for the malloc stevedore.\n\nstorage.\\<name\\>.happy\n\nType: BOOL\n\nReadable from: client, backend\n\nHealth status for the named stevedore. Not available in any of the current stevedores.\n\nstorage.\\<name\\>.used_space\n\nType: BYTES\n\nReadable from: client, backend\n\nUsed space in the named stevedore. Only available for the malloc stevedore.\n\n#### Protected header fields\n\nThe `content-length` and `transfer-encoding` headers are read-only. They must be preserved to ensure HTTP/1 framing remains consistent and maintain a proper request and response synchronization with both clients and backends.\n\nVMODs can still update these headers, when there is a reason to change the framing, such as a transformation of a request or response body.\n\n#### HTTP response status\n\nA HTTP status code has 3 digits XYZ where X must be between 1 and 5 included. Since it is not uncommon to see HTTP clients or servers relying on non-standard or even invalid status codes, Varnish can work with any status between 100 and 999.\n\nWithin VCL code it is even possible to use status codes in the form VWXYZ as long as the overall value is lower than 65536, but only the XYZ part will be sent to the client, by which time the X must also have become non-zero.\n\nThe VWXYZ form of status codes can be communicate extra information in `resp.status` and `beresp.status` to `return(synth(...))` and `return(error(...))`, to indicate which synthetic content to produce:\n\n``` python\nsub vcl_recv {\n    if ([...]) {\n        return synth(12404);\n    }\n}\n\nsub vcl_synth {\n    if (resp.status == 12404) {\n        [...]       // this specific 404\n    } else if (resp.status % 1000 == 404) {\n        [...]       // all other 404's\n    }\n}\n```\n\nThe `obj.status` variable will inherit the VWXYZ form, but in a ban expression only the XYZ part will be available. The VWXYZ form is strictly limited to VCL execution.\n\nAssigning an HTTP standardized code to `resp.status` or `beresp.status` will also set `resp.reason` or `beresp.reason` to the corresponding status message.\n\n##### 304 handling\n\nFor a 304 response, Varnish core code amends `beresp` before calling `vcl_backend_response`:\n\n- If the gzip status changed, `Content-Encoding` is unset and any `Etag` is weakened\n- Any headers not present in the 304 response are copied from the existing cache object. `Content-Length` is copied if present in the existing cache object and discarded otherwise.\n- The status gets set to 200.\n\n`beresp.was_304` marks that this conditional response processing has happened.\n\nNote: Backend conditional requests are independent of client conditional requests, so clients may receive 304 responses no matter if a backend request was conditional.\n\n##### beresp.ttl / beresp.grace / beresp.keep\n\nBefore calling `vcl_backend_response`, core code sets `beresp.ttl` based on the response status and the response headers `Age`, `Cache-Control` or `Expires` and `Date` as follows:\n\n- If present and valid, the value of the `Age` header is effectively deduced from all ttl calculations.\n\n- For status codes 200, 203, 204, 300, 301, 304, 404, 410 and 414:\n\n  - If `Cache-Control` contains an `s-maxage` or `max-age` field (in that order of preference), the ttl is set to the respective non-negative value or 0 if negative.\n\n  - Otherwise, if no `Expires` header exists, the default ttl is used.\n\n  - Otherwise, if `Expires` contains a time stamp before `Date`, the ttl is set to 0.\n\n  - Otherwise, if no `Date` header is present or the `Date` header timestamp differs from the local clock by no more than the `clock_skew` parameter, the ttl is set to\n\n    - 0 if `Expires` denotes a past timestamp or\n    - the difference between the local clock and the `Expires` header otherwise.\n\n  - Otherwise, the ttl is set to the difference between `Expires` and `Date`\n\n- For status codes 302 and 307, the calculation is identical except that the default ttl is not used and -1 is returned if neither `Cache-Control` nor `Expires` exists.\n\n- For all other status codes, ttl -1 is returned.\n\n`beresp.grace` defaults to the `default_grace` parameter.\n\nFor a non-negative ttl, if `Cache-Control` contains a `stale-while-revalidate` field value, `beresp.grace` is set to that value if non-negative or 0 otherwise.\n\n`beresp.keep` defaults to the `default_keep` parameter.\n\n### SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [VCL](vcl#vcl-7)\n\n### HISTORY\n\nVCL was developed by Poul-Henning Kamp in cooperation with Verdens Gang AS, Redpill Linpro and Varnish Software. This manual page is written by Per Buer, Poul-Henning Kamp, Martin Blix Grydeland, Kristian Lyngstøl, Lasse Karstensen and others.\n\n### COPYRIGHT\n\nThis document is licensed under the same license as Varnish itself. See LICENSE for details.\n\n- Copyright (c) 2006 Verdens Gang AS\n- Copyright (c) 2006-2021 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vcl-var.html](https://varnish-cache.org/docs/7.4/reference/vcl-var.html)"
- name: VCLI protocol - Scripting the CLI interface
  id: reference/cli_protocol
  summary: The Varnish CLI has a few bells&whistles when used as an API
  description: "# VCLI protocol - Scripting the CLI interface\n\nThe Varnish CLI has a few bells&whistles when used as an API.\n\nFirst: `vcli.h` contains magic numbers.\n\nSecond: If you use `varnishadm` to connect to `varnishd` for API purposes, use the `-p` argument to get “pass” mode.\n\nIn “pass” mode, or with direct CLI connections (more below), the first line of responses is always exactly 13 bytes long, including the NL, and it contains two numbers: The status code and the count of bytes in the “body” of the response:\n\n``` python\n200␣19␣␣␣␣␣␣␤\nPONG␣1613397488␣1.0\n```\n\nThis makes parsing the response unambiguous, even in cases like this where the response does not end with a NL.\n\nThe varnishapi library contains functions to implement the basics of the CLI protocol, for more, see the `vcli.h` include file.\n\n## Local and remote CLI connections\n\nThe `varnishd` process receives the CLI commands via TCP connections which require PSK authentication (see below), but which provide no secrecy.\n\n“No secrecy” means that if you configure these TCP connections to run across a network, anybody who can sniff packets can see your CLI commands. If you need secrecy, use `ssh` to run `varnishadm` or to tunnel the TCP connection.\n\nBy default `varnishd` binds to `localhost` and ask the kernel to assign a random port number. The resulting listen address is stored in the shared memory, where the `varnishadm` program finds it.\n\nYou can configure `varnishd` to listen to a specific address with the `-T` argument, this will also be written to shared memory, so `varnishadm` keeps working:\n\n``` python\n# Bind to internal network\nvarnishd -T 192.168.10.21:3245\n```\n\nYou can also configure `varnishd` to actively open a TCP connection to another “controller” program, with the `-M` argument.\n\nFinally, when run in “debug mode” with the `-d` argument, `varnishd` will stay in the foreground and turn stdin/stdout into a CLI connection.\n\n## Authentication CLI connections\n\nCLI connections to `varnishd` are authenticated with a “pre-shared-key” authentication scheme, where the other end must prove they know *the contents of* the secret file `varnishd` uses.\n\nThey do not have to read the precise same file on that specific computer, they could read an entirely different file on a different computer or fetch the secret from a server.\n\nThe name of the file can be configured with the `-S` option, and `varnishd` records the name in shared memory, so `varnishadm` can find it.\n\nAs a bare minimum `varnishd` needs to be able to read the file, but other than that, it can be restricted any way you want.\n\nSince it is not the file, but only the content of it that matter, you can make the file unreadable by everybody, and instead place a copy of the file in the home directories of the authorized users.\n\nThe file is read only at the moment when the `auth` CLI command is issued and the contents is not cached in `varnishd`, so you can change it as often as you want.\n\nAn authenticated session looks like this:\n\n``` text\ncritter phk> telnet localhost 1234\nTrying ::1...\nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is '^]'.\n107 59\nixslvvxrgkjptxmcgnnsdxsvdmvfympg\n\nAuthentication required.\n\nauth 455ce847f0073c7ab3b1465f74507b75d3dc064c1e7de3b71e00de9092fdc89a\n200 279\n-----------------------------\nVarnish Cache CLI 1.0\n-----------------------------\nFreeBSD,13.0-CURRENT,amd64,-jnone,-sdefault,-sdefault,-hcritbit\nvarnish-trunk revision 89a558e56390d425c52732a6c94087eec9083115\n\nType 'help' for command list.\nType 'quit' to close CLI session.\nType 'start' to launch worker process.\n```\n\nThe CLI status of 107 indicates that authentication is necessary. The first 32 characters of the response text is the challenge “ixsl…mpg”. The challenge is randomly generated for each CLI connection, and changes each time a 107 is emitted.\n\nThe most recently emitted challenge must be used for calculating the authenticator “455c…c89a”.\n\nThe authenticator is calculated by applying the SHA256 function to the following byte sequence:\n\n- Challenge string\n- Newline (0x0a) character.\n- Contents of the secret file\n- Challenge string\n- Newline (0x0a) character.\n\nand dumping the resulting digest in lower-case hex.\n\nIn the above example, the secret file contains `foo\\n` and thus:\n\n``` text\ncritter phk> hexdump secret\n00000000  66 6f 6f 0a                                       |foo.|\n00000004\ncritter phk> cat > tmpfile\nixslvvxrgkjptxmcgnnsdxsvdmvfympg\nfoo\nixslvvxrgkjptxmcgnnsdxsvdmvfympg\n^D\ncritter phk> hexdump -C tmpfile\n00000000  69 78 73 6c 76 76 78 72  67 6b 6a 70 74 78 6d 63  |ixslvvxrgkjptxmc|\n00000010  67 6e 6e 73 64 78 73 76  64 6d 76 66 79 6d 70 67  |gnnsdxsvdmvfympg|\n00000020  0a 66 6f 6f 0a 69 78 73  6c 76 76 78 72 67 6b 6a  |.foo.ixslvvxrgkj|\n00000030  70 74 78 6d 63 67 6e 6e  73 64 78 73 76 64 6d 76  |ptxmcgnnsdxsvdmv|\n00000040  66 79 6d 70 67 0a                                 |fympg.|\n00000046\ncritter phk> sha256 tmpfile\nSHA256 (tmpfile) = 455ce847f0073c7ab3b1465f74507b75d3dc064c1e7de3b71e00de9092fdc89a\ncritter phk> openssl dgst -sha256 < tmpfile\n455ce847f0073c7ab3b1465f74507b75d3dc064c1e7de3b71e00de9092fdc89a\n```\n\nThe sourcefile `lib/libvarnish/cli_auth.c` contains a useful function which calculates the response, given an open filedescriptor to the secret file, and the challenge string.\n\n## See also:\n\n- [varnishadm](varnishadm#varnishadm-1)\n- [varnishd](varnishd#varnishd-1)\n- [VCL](vcl#vcl-7)\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/cli_protocol.html](https://varnish-cache.org/docs/7.4/reference/cli_protocol.html)"
- name: VEXT - Varnish Extensions
  id: reference/vext
  summary: A Varnish Extension is a shared library, loaded into the worker process during startup, before privilges are dropped for good
  description: "# VEXT - Varnish Extensions\n\nA Varnish Extension is a shared library, loaded into the worker process during startup, before privilges are dropped for good. This allows a VEXT to do pretty much anything it wants to do in the worker process.\n\nA VEXT can (also) contain a VMOD, and it will work just like any other VMOD, which also means that VMODs can be loaded as VEXTs.\n\nThe VEXTs are loaded in the child process, in the order they are specified on the commandline, after the `heritage` has been established, but before stevedores are initialized and jail privileges are dropped.\n\nThere is currently no `init` entrypoint defined, but a VEXT can use a static initializer to get activated on loading.\n\nIf those static initializers want to bail out, `stderr` and `exit(3)` can be used to convey diagnostics.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vext.html](https://varnish-cache.org/docs/7.4/reference/vext.html)"
- name: VMOD - Varnish Modules
  id: reference/vmod
  summary: For all you can do in VCL, there are things you can not do
  description: "# VMOD - Varnish Modules\n\nFor all you can do in VCL, there are things you can not do. Look an IP number up in a database file for instance. VCL provides for inline C code, and there you can do everything, but it is not a convenient or even readable way to solve such problems.\n\nThis is where VMODs come into the picture: A VMOD is a shared library with some C functions which can be called from VCL code.\n\nFor instance:\n\n``` python\nimport std;\n\nsub vcl_deliver {\n        set resp.http.foo = std.toupper(req.url);\n}\n```\n\nThe “std” vmod is one you get with Varnish, it will always be there and we will put “boutique” functions in it, such as the “toupper” function shown above. The full contents of the “std” module is documented in vmod_std(3).\n\nThis part of the manual is about how you go about writing your own VMOD, how the language interface between C and VCC works, where you can find contributed VMODs etc. This explanation will use the “std” VMOD as example, having a Varnish source tree handy may be a good idea.\n\n## VMOD Directory\n\nThe VMOD directory is an up-to-date compilation of maintained extensions written for Varnish Cache:\n\n[https://www.varnish-cache.org/vmods](https://www.varnish-cache.org/vmods)\n\n## The vmod.vcc file\n\nThe interface between your VMOD and the VCL compiler (“VCC”) and the VCL runtime (“VRT”) is defined in the vmod.vcc file which a python script called “vmodtool.py” turns into thaumaturgically challenged C data structures that do all the hard work.\n\nThe std VMODs vmod.vcc file looks somewhat like this:\n\n``` python\n$ABI strict\n$Module std 3 \"Varnish Standard Module\"\n$Event event_function\n$Function STRING toupper(STRANDS s)\n$Function STRING tolower(STRANDS s)\n$Function VOID set_ip_tos(INT)\n```\n\nThe `$ABI` line is optional. Possible values are `strict` (default) and `vrt`. It allows to specify that a vmod is integrating with the blessed `vrt` interface provided by `varnishd` or go deeper in the stack.\n\nAs a rule of thumb you, if the VMOD uses more than the VRT (Varnish RunTime), in which case it needs to be built for the exact Varnish version, use `strict`. If it complies to the VRT and only needs to be rebuilt when breaking changes are introduced to the VRT API, use `vrt`.\n\nThe `$Module` line gives the name of the module, the manual section where the documentation will reside, and the description.\n\nThe `$Event` line specifies an optional “Event” function, which will be called whenever a VCL program which imports this VMOD is loaded or transitions to any of the warm, active, cold or discarded states. More on this below.\n\nThe `$Function` lines define three functions in the VMOD, along with the types of the arguments, and that is probably where the hardest bit of writing a VMOD is to be found, so we will talk about that at length in a moment.\n\nNotice that the third function returns VOID, that makes it a “procedure” in VCL lingo, meaning that it cannot be used in expressions, right side of assignments and such. Instead it can be used as a primary action, something functions which return a value can not:\n\n``` python\nsub vcl_recv {\n        std.set_ip_tos(32);\n}\n```\n\nRunning vmodtool.py on the vmod.vcc file, produces a “vcc_if.c” and “vcc_if.h” files, which you must use to build your shared library file.\n\nForget about vcc_if.c everywhere but your Makefile, you will never need to care about its contents, and you should certainly never modify it, that voids your warranty instantly.\n\nBut vcc_if.h is important for you, it contains the prototypes for the functions you want to export to VCL.\n\nFor the std VMOD, the compiled vcc_if.h file looks like this:\n\n``` python\nVCL_STRING vmod_toupper(VRT_CTX, VCL_STRANDS);\nVCL_STRING vmod_tolower(VRT_CTX, VCL_STRANDS);\nVCL_VOID vmod_set_ip_tos(VRT_CTX, VCL_INT);\n\nvmod_event_f event_function;\n```\n\nThose are your C prototypes. Notice the `vmod_` prefix on the function names.\n\n### Named arguments and default values\n\nThe basic vmod.vcc function declaration syntax introduced above makes all arguments mandatory for calls from vcl - which implies that they need to be given in order.\n\nNaming the arguments as in:\n\n``` python\n$Function BOOL match_acl(ACL acl, IP ip)\n```\n\nallows calls from VCL with named arguments in any order, for example:\n\n``` python\nif (debug.match_acl(ip=client.ip, acl=local)) { # ...\n```\n\nNamed arguments also take default values, so for this example from the debug vmod:\n\n``` python\n$Function STRING argtest(STRING one, REAL two=2, STRING three=\"3\",\n                         STRING comma=\",\", INT four=4)\n```\n\nonly argument `one` is required, so that all of the following are valid invocations from vcl:\n\n``` python\ndebug.argtest(\"1\", 2.1, \"3a\")\ndebug.argtest(\"1\", two=2.2, three=\"3b\")\ndebug.argtest(\"1\", three=\"3c\", two=2.3)\ndebug.argtest(\"1\", 2.4, three=\"3d\")\ndebug.argtest(\"1\", 2.5)\ndebug.argtest(\"1\", four=6);\n```\n\nThe C interface does not change with named arguments and default values, arguments remain positional and default values appear no different to user specified values.\n\n`Note` that default values have to be given in the native C-type syntax, see below. As a special case, `NULL` has to be given as `0`.\n\n### Optional arguments\n\nThe vmod.vcc declaration also allows for optional arguments in square brackets like so:\n\n``` python\n$Function VOID opt(PRIV_TASK priv, INT four = 4, [STRING opt])\n```\n\nWith any optional argument present, the C function prototype looks completely different:\n\n- Only the `VRT_CTX` and object pointer arguments (only for methods) remain positional\n- All other arguments get passed in a struct as the last argument of the C function.\n\nThe argument struct is simple, vmod authors should check the `vmodtool`-generated `vcc_if.c` file for the function and struct declarations:\n\n- for each optional argument, a `valid_``argument` member is used to signal the presence of the respective optional argument.\n\n  `valid_` argstruct members should only be used as truth values, irrespective of their actual data type.\n\n- named arguments are passed in argument struct members by the same name and with the same data type.\n\n- unnamed (positional) arguments are passed as `arg``n` with `n` starting at 1 and incrementing with the argument’s position.\n\n### Objects and methods\n\nVarnish also supports a simple object model for vmods. Objects and methods are declared in the vcc file as:\n\n``` python\n$Object class(...)\n$Method .method(...)\n```\n\nFor declared object classes of a vmod, object instances can then be created in `vcl_init { }` using the `new` statement:\n\n``` python\nsub vcl_init {\n        new foo = vmod.class(...);\n}\n```\n\nand have their methods called anywhere (including in `vcl_init {}` after the instantiation):\n\n``` python\nsub somewhere {\n        foo.method(...);\n}\n```\n\nNothing prevents a method to be named like the constructor and the meaning of such a method is up to the vmod author:\n\n``` python\n$Object foo(...)\n$Method .bar(...)\n$Method .foo(...)\n```\n\nObject instances are represented as pointers to vmod-implemented C structs. Varnish only provides space to store the address of object instances and ensures that the right object address gets passed to C functions implementing methods.\n\n- Objects’ scope and lifetime are the vcl\n- Objects can only be created in `vcl_init {}` and have their destructors called by varnish after `vcl_fini {}` has completed.\n\nvmod authors are advised to understand the prototypes in the `vmodtool`-generated `vcc_if.c` file:\n\n- For `$Object` declarations, a constructor and destructor function must be implemented\n\n- The constructor is named by the suffix `__init`, always is of `VOID` return type and has the following arguments before the vcc-declared parameters:\n\n  - `VRT_CTX` as usual\n  - a pointer-pointer to return the address of the created oject\n  - a string containing the vcl name of the object instance\n\n- The destructor is named by the suffix `__fini`, always is of `VOID` return type and has a single argument, the pointer-pointer to the address of the object. The destructor is expected clear the address of the object stored in that pointer-pointer.\n\n- Methods gain the pointer to the object as an argument after  \n  the `VRT_CTX`.\n\nAs varnish is in no way involved in managing object instances other than passing their addresses, vmods need to implement all aspects of managing instances, in particular their memory management. As the lifetime of object instances is the vcl, they will usually be allocated from the heap.\n\n### Functions and Methods scope restriction\n\nThe `$Restrict` stanza offers a way to limit the scope of the preceding vmod function or method, so that they can only be called from restricted vcl call sites. It must only appear after a `$Method` or `$Function` and has the following syntax:\n\n``` python\n$Restrict scope1 [scope2 ...]\n```\n\nPossible scope values are: `backend, client, housekeeping, vcl_recv, vcl_pipe, vcl_pass, vcl_hash, vcl_purge, vcl_miss, vcl_hit, vcl_deliver, vcl_synth, vcl_backend_fetch, vcl_backend_response, vcl_backend_error, vcl_init, vcl_fini`\n\n### Deprecated Aliases\n\nThe `$Alias` stanza offers a mechanism to rename a function or an object’s method without removing the previous name. This allows name changes to maintain compatibility until the alias is dropped.\n\nThe syntax for a function is:\n\n``` python\n$Alias deprecated_function original_function\n\n[description]\n```\n\nThe syntax for a method is:\n\n``` python\n$Alias .deprecated_method object.original_method\n\n[description]\n```\n\nThe `$Alias` stanza can appear anywhere, this allows grouping them in a dedicated “deprecated” section of their manual. The optional description can be used to explain why a function was renamed.\n\n## VCL and C data types\n\nVCL data types are targeted at the job, so for instance, we have data types like “DURATION” and “HEADER”, but they all have some kind of C language representation. Here is a description of them.\n\nAll but the PRIV types have typedefs: VCL_INT, VCL_REAL, etc.\n\nNotice that most of the non-native (C pointer) types are `const`, which, if returned by a vmod function/method, are assumed to be immutable. In other words, a vmod `must not` modify any data which was previously returned.\n\nWhen returning non-native values, the producing function is responsible for arranging memory management. Either by freeing the structure later by whatever means available or by using storage allocated from the client or backend workspaces.\n\nACL  \nC-type: `const struct vrt_acl *`\n\nA type for named ACLs declared in VCL.\n\nBACKEND  \nC-type: `const struct director *`\n\nA type for backend and director implementations. See [Writing a Director](directors#ref-writing-a-director).\n\nBLOB  \nC-type: `const struct vmod_priv *`\n\nAn opaque type to pass random bits of memory between VMOD functions.\n\nBODY  \nC-type: `const void *`\n\nA type only used on the LHS of an assignment that can take either a blob or an expression that can be converted to a string.\n\nBOOL  \nC-type: `unsigned`\n\nZero means false, anything else means true.\n\nBYTES  \nC-type: `double`\n\nUnit: bytes.\n\nA storage space, as in 1024 bytes.\n\nDURATION  \nC-type: `double`\n\nUnit: seconds.\n\nA time interval, as in 25 seconds.\n\nENUM  \nvcc syntax: ENUM { val1, val2, … }\n\nvcc example: `ENUM { one, two, three } number=\"one\"`\n\nC-type: `const char *`\n\nAllows values from a set of constant strings. `Note` that the C-type is a string, not a C enum.\n\nEnums will be passed as fixed pointers, so instead of string comparisons, also pointer comparisons with `VENUM(name)` are possible.\n\nHEADER  \nC-type: `const struct gethdr_s *`\n\nThese are VCL compiler generated constants referencing a particular header in a particular HTTP entity, for instance `req.http.cookie` or `beresp.http.last-modified`. By passing a reference to the header, the VMOD code can both read and write the header in question.\n\nIf the header was passed as STRING, the VMOD code only sees the value, but not where it came from.\n\nHTTP  \nC-type: `struct http *`\n\nA reference to a header object as `req.http` or `bereq.http`.\n\nINT  \nC-type: `long`\n\nA (long) integer as we know and love them.\n\nIP  \nC-type: `const struct suckaddr *`\n\nThis is an opaque type, see the `include/vsa.h` file for which primitives we support on this type.\n\nPRIV_CALL  \nSee [Private Pointers](#ref-vmod-private-pointers) below.\n\nPRIV_TASK  \nSee [Private Pointers](#ref-vmod-private-pointers) below.\n\nPRIV_TOP  \nSee [Private Pointers](#ref-vmod-private-pointers) below.\n\nPRIV_VCL  \nSee [Private Pointers](#ref-vmod-private-pointers) below.\n\nPROBE  \nC-type: `const struct vrt_backend_probe *`\n\nA named standalone backend probe definition.\n\nREAL  \nC-type: `double`\n\nA floating point value.\n\nREGEX  \nC-type: `const struct vre *`\n\nThis is an opaque type for regular expressions with a VCL scope. The REGEX type is only meant for regular expression literals managed by the VCL compiler. For dynamic regular expressions or complex usage see the API from the `include/vre.h` file.\n\nSTRING  \nC-type: `const char *`\n\nA NUL-terminated text-string.\n\nCan be NULL to indicate a nonexistent string, for instance in:\n\n``` python\nmymod.foo(req.http.foobar);\n```\n\nIf there were no “foobar” HTTP header, the vmod_foo() function would be passed a NULL pointer as argument.\n\nSTEVEDORE  \nC-type: `const struct stevedore *`\n\nA storage backend.\n\nSTRANDS  \nC-Type: `const struct strands *`\n\nStrands are a list of strings that gets passed in a struct with the following members:\n\n- `int n`: the number of strings\n- `const char **p`: the array of strings with `n` elements\n\nA VMOD should never hold onto strands beyond a function or method execution. See `include/vrt.h` for the details.\n\nTIME  \nC-type: `double`\n\nUnit: seconds since UNIX epoch.\n\nAn absolute time, as in 1284401161.\n\nVCL_SUB  \nC-type: `const struct vcl_sub *`\n\nOpaque handle on a VCL subroutine.\n\nReferences to subroutines can be passed into VMODs as arguments and called later through `VRT_call()`. The scope strictly is the VCL: vmods must ensure that `VCL_SUB` references never be called from a different VCL.\n\n`VRT_call()` fails the VCL for recursive calls and when the `VCL_SUB` can not be called from the current context (e.g. calling a subroutine accessing `req` from the backend side).\n\nFor more than one invocation of `VRT_call()`, VMODs *must* check if `VRT_handled()` returns non-zero inbetween calls: The called SUB may have returned with an action (any `return(x)` other than plain `return`) or may have failed the VCL, and in both cases the calling VMOD *must* return also, possibly after having conducted some cleanup. Note that undoing the handling through `VRT_handling()` is a bug.\n\n`VRT_check_call()` can be used to check if a `VRT_call()` would succeed in order to avoid the potential VCL failure. It returns `NULL` if `VRT_call()` would make the call or an error string why not.\n\nVOID  \nC-type: `void`\n\nCan only be used for return-value, which makes the function a VCL procedure.\n\n## Private Pointers\n\nIt is often useful for library functions to maintain local state, this can be anything from a precompiled regexp to open file descriptors and vast data structures.\n\nThe VCL compiler supports the following private pointers:\n\n- `PRIV_CALL` “per call” private pointers are useful to cache/store state relative to the specific call or its arguments, for instance a compiled regular expression specific to a regsub() statement or simply caching the most recent output of some expensive operation. These private pointers live for the duration of the loaded VCL.\n\n- `PRIV_TASK` “per task” private pointers are useful for state that applies to calls for either a specific request or a backend request. For instance this can be the result of a parsed cookie specific to a client. Note that `PRIV_TASK` contexts are separate for the client side and the backend side, so use in `vcl_backend_*` will yield a different private pointer from the one used on the client side. These private pointers live only for the duration of their task.\n\n- `PRIV_TOP` “per top-request” private pointers live for the duration of one request and all its ESI-includes. They are only defined for the client side. When used from backend VCL subs, a NULL pointer will potentially be passed and a VCL failure triggered. These private pointers live only for the duration of their top level request\n\n- `PRIV_VCL` “per vcl” private pointers are useful for such global state that applies to all calls in this VCL, for instance flags that determine if regular expressions are case-sensitive in this vmod or similar. The `PRIV_VCL` object is the same object that is passed to the VMOD’s event function. This private pointer lives for the duration of the loaded VCL.\n\n  The `PRIV_CALL` vmod_privs are finalized before `PRIV_VCL`.\n\nThe way it works in the vmod code, is that a `struct vmod_priv *` is passed to the functions where one of the `PRIV_*` argument types is specified.\n\nThis structure contains three members:\n\n``` python\nstruct vmod_priv {\n        void                            *priv;\n        long                            len;\n        const struct vmod_priv_methods  *methods;\n};\n```\n\nThe `.priv` and `.len` elements can be used for whatever the vmod code wants to use them for.\n\n`.methods` can be an optional pointer to a struct of callbacks:\n\n``` python\ntypedef void vmod_priv_fini_f(VRT_CTX, void *);\n\nstruct vmod_priv_methods {\n        unsigned                        magic;\n        const char                      *type;\n        vmod_priv_fini_f                *fini;\n};\n```\n\n`.magic` has to be initialized to `VMOD_PRIV_METHODS_MAGIC`. `.type` should be a descriptive name to help debugging.\n\n`.fini` will be called for a non-NULL `.priv` of the `struct vmod_priv` when the scope ends with that `.priv` pointer as its second argument besides a `VRT_CTX`.\n\nThe common case where a private data structure is allocated with malloc(3) would look like this:\n\n``` python\nstatic void\nmyfree(VRT_CTX, void *p)\n{\n        CHECK_OBJ_NOTNULL(ctx, VRT_CTX_MAGIC);\n        free (p);\n}\n\nstatic const struct vmod_priv_methods mymethods[1] = {{\n        .magic = VMOD_PRIV_METHODS_MAGIC,\n        .type = \"mystate\",\n        .fini = myfree\n}};\n\n// ....\n\nif (priv->priv == NULL) {\n        priv->priv = calloc(1, sizeof(struct myfoo));\n        AN(priv->priv);\n        priv->methods = mymethods;\n        mystate = priv->priv;\n        mystate->foo = 21;\n        ...\n} else {\n        mystate = priv->priv;\n}\nif (foo > 25) {\n        ...\n}\n```\n\n### Private Pointers Memory Management\n\nThe generic malloc(3) / free(3) approach documented above works for all private pointers. It is the simplest and less error prone (as long as allocated memory is properly freed though the fini callback), but comes at the cost of calling into the heap memory allocator.\n\nPer-vmod constant data structures can be assigned to any private pointer type, but, obviously, free(3) must not be used on them.\n\nDynamic data stored in `PRIV_TASK` and `PRIV_TOP` pointers can also come from the workspace:\n\n- For `PRIV_TASK`, any allocation from `ctx->ws` works, like so:\n\n  ``` python\n  if (priv->priv == NULL) {\n          priv->priv = WS_Alloc(ctx->ws, sizeof(struct myfoo));\n          if (priv->priv == NULL) {\n                  VRT_fail(ctx, \"WS_Alloc failed\");\n                  return (...);\n          }\n          priv->methods = mymethods;\n          mystate = priv->priv;\n          mystate->foo = 21;\n          ...\n  ```\n\n- For `PRIV_TOP`, first of all keep in mind that it must only be used from the client context, so vmod code should error out for `ctx->req == NULL`.\n\n  For dynamic data, the *top request’s* workspace must be used, which complicates things a bit:\n\n  ``` python\n  if (priv->priv == NULL) {\n          struct ws *ws;\n\n          CHECK_OBJ_NOTNULL(ctx->req, REQ_MAGIC);\n          CHECK_OBJ_NOTNULL(ctx->req->top, REQTOP_MAGIC);\n          CHECK_OBJ_NOTNULL(ctx->req->top->topreq, REQ_MAGIC);\n          ws = ctx->req->top->topreq->ws;\n\n          priv->priv = WS_Alloc(ws, sizeof(struct myfoo));\n          // ... same as above for PRIV_TASK\n  ```\n\nNotice that allocations on the workspace do not need to be freed, their lifetime is the respective task.\n\n### Private Pointers and Objects\n\n`PRIV_TASK` and `PRIV_TOP` arguments to methods are not per object instance, but per vmod as for ordinary vmod functions. Thus, vmods requiring per-task / per top-request state for object instances need to implement other means to associate storage with object instances.\n\nThis is what `VRT_priv_task()` / `VRT_priv_task_get()` and `VRT_priv_top()` / `VRT_priv_top_get()` are for:\n\nThe non-get functions either return an existing `PRIV_TASK` / `PRIV_TOP` for a given `void *` argument or create one. They return `NULL` in case of an allocation failure.\n\nThe `_get()` functions do not create a `PRIV_*`, but return either an existing one or `NULL`.\n\nBy convention, private pointers for object instance are created on the address of the object, as in this example for a `PRIV_TASK`:\n\n``` python\nVCL_VOID\nmyvmod_obj_method(VRT_CTX, struct myvmod_obj *o)\n{\n    struct vmod_priv *p;\n\n    p = VRT_priv_task(ctx, o);\n\n    // ... see above\n```\n\nThe `PRIV_TOP` case looks identical except for calling `VRT_priv_top(ctx, o)` in place of `VRT_priv_task(ctx, o)`, but be reminded that the `VRT_priv_top*()` functions must only be called from client context (if `ctx->req != NULL`).\n\n## Event functions\n\nVMODs can have an “event” function which is called when a VCL which imports the VMOD is loaded or discarded. This corresponds to the `VCL_EVENT_LOAD` and `VCL_EVENT_DISCARD` events, respectively. In addition, this function will be called when the VCL temperature is changed to cold or warm, corresponding to the `VCL_EVENT_COLD` and `VCL_EVENT_WARM` events.\n\nThe first argument to the event function is a VRT context.\n\nThe second argument is the vmod_priv specific to this particular VCL, and if necessary, a VCL specific VMOD “fini” function can be attached to its “free” hook.\n\nThe third argument is the event.\n\nIf the VMOD has private global state, which includes any sockets or files opened, any memory allocated to global or private variables in the C-code etc, it is the VMODs own responsibility to track how many VCLs were loaded or discarded and free this global state when the count reaches zero.\n\nVMOD writers are *strongly* encouraged to release all per-VCL resources for a given VCL when it emits a `VCL_EVENT_COLD` event. You will get a chance to reacquire the resources before the VCL becomes active again and be notified first with a `VCL_EVENT_WARM` event. Unless a user decides that a given VCL should always be warm, an inactive VMOD will eventually become cold and should manage resources accordingly.\n\nAn event function must return zero upon success. It is only possible to fail an initialization with the `VCL_EVENT_LOAD` or `VCL_EVENT_WARM` events. Should such a failure happen, a `VCL_EVENT_DISCARD` or `VCL_EVENT_COLD` event will be sent to the VMODs that succeeded to put them back in a cold state. The VMOD that failed will not receive this event, and therefore must not be left half-initialized should a failure occur.\n\nIf your VMOD is running an asynchronous background job you can hold a reference to the VCL to prevent it from going cold too soon and get the same guarantees as backends with ongoing requests for instance. For that, you must acquire the reference by calling `VRT_VCL_Prevent_Discard` when you receive a `VCL_EVENT_WARM` and later calling `VRT_VCL_Allow_Discard` once the background job is over. Receiving a `VCL_EVENT_COLD` is your cue to terminate any background job bound to a VCL.\n\nYou can find an example of VCL references in vmod-debug:\n\n``` python\npriv_vcl->vclref = VRT_VCL_Prevent_Discard(ctx, \"vmod-debug\");\n...\nVRT_VCL_Allow_Discard(&ctx, &priv_vcl->vclref);\n```\n\nIn this simplified version, you can see that you need at least a VCL-bound data structure like a `PRIV_VCL` or a VMOD object to keep track of the reference and later release it. You also have to provide a description, it will be printed to the user if they try to warm up a cooling VCL:\n\n``` python\n$ varnishadm vcl.list\navailable  auto/cooling       0 vcl1\nactive     auto/warm          0 vcl2\n\n$ varnishadm vcl.state vcl1 warm\nCommand failed with error code 300\nFailed <vcl.state vcl1 auto>\nMessage:\n        VCL vcl1 is waiting for:\n        - vmod-debug\n```\n\nIn the case where properly releasing resources may take some time, you can opt for an asynchronous worker, either by spawning a thread and tracking it, or by using Varnish’s worker pools.\n\n## When to lock, and when not to lock\n\nVarnish is heavily multithreaded, so by default VMODs must implement their own locking to protect shared resources.\n\nWhen a VCL is loaded or unloaded, the event and priv-\\>free are run sequentially all in a single thread, and there is guaranteed to be no other activity related to this particular VCL, nor are there init/fini activity in any other VCL or VMOD at this time.\n\nThat means that the VMOD init, and any object init/fini functions are already serialized in sensible order, and won’t need any locking, unless they access VMOD specific global state, shared with other VCLs.\n\nTraffic in other VCLs which also import this VMOD, will be happening while housekeeping is going on.\n\n## Statistics Counters\n\nStarting in Varnish 6.0, VMODs can define their own counters that appear in *varnishstat*.\n\nIf you’re using autotools, see the `VARNISH_COUNTERS` macro in varnish.m4 for documentation on getting your build set up.\n\nCounters are defined in a .vsc file. The `VARNISH_COUNTERS` macro calls *vsctool.py* to turn a *foo.vsc* file into *VSC_foo.c* and *VSC_foo.h* files, just like *vmodtool.py* turns *foo.vcc* into *vcc_foo_if.c* and *vcc_foo_if.h* files. Similarly to the VCC files, the generated VSC files give you a structure and functions that you can use in your VMOD’s code to create and destroy the counters your defined. The *vsctool.py* tool also generates a *VSC_foo.rst* file that you can include in your documentation to describe the counters your VMOD has.\n\nThe .vsc file looks like this:\n\n``` none\n.. varnish_vsc_begin:: xkey\n        :oneliner:      xkey Counters\n        :order:         70\n\n        Metrics from vmod_xkey\n\n.. varnish_vsc:: g_keys\n        :type:          gauge\n        :oneliner:      Number of surrogate keys\n\n        Number of surrogate keys in use. Increases after a request that includes a new key in the xkey header. Decreases when a key is purged or when all cache objects associated with a key expire.\n\n.. varnish_vsc_end:: xkey\n```\n\nCounters can have the following parameters:\n\ntype  \nThe type of metric this is. Can be one of `counter`, `gauge`, or `bitmap`.\n\nctype  \nThe type that this counter will have in the C code. This can only be `uint64_t` and does not need to be specified.\n\nlevel  \nThe verbosity level of this counter. *varnishstat* will only show counters with a higher verbosity level than the one currently configured. Can be one of `info`, `diag`, or `debug`.\n\noneliner  \nA short, one line description of the counter.\n\ngroup  \nI don’t know what this does.\n\nformat  \nCan be one of `integer`, `bytes`, `bitmap`, or `duration`.\n\nAfter these parameters, a counter can have a longer description, though this description has to be all on one line in the .vsc file.\n\nYou should call `VSC_*_New()` when your VMOD is loaded and `VSC_*_Destroy()` when it is unloaded. See the generated `VSC_*.h` file for the full details about the structure that contains your counters.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vmod.html](https://varnish-cache.org/docs/7.4/reference/vmod.html)"
- name: VMOD blob - Utilities for the VCL blob type, encoding and decoding
  id: reference/vmod_blob
  summary: This VMOD provides utility functions and an object for the VCL data type BLOB, which may contain arbitrary data of any length
  description: "# VMOD blob - Utilities for the VCL blob type, encoding and decoding\n\n## SYNOPSIS\n\n``` literal-block\nimport blob [as name] [from \"path\"]\n\nBLOB decode(ENUM decoding, INT length, STRING encoded)\n\nSTRING encode(ENUM encoding, ENUM case, BLOB blob)\n\nSTRING transcode(ENUM decoding, ENUM encoding, ENUM case, INT length, STRING encoded)\n\nBOOL same(BLOB, BLOB)\n\nBOOL equal(BLOB, BLOB)\n\nINT length(BLOB)\n\nBLOB sub(BLOB, BYTES length, BYTES offset=0)\n\nnew xblob = blob.blob(ENUM decoding, STRING encoded)\n\n    BLOB xblob.get()\n\n    STRING xblob.encode(ENUM encoding, ENUM case)\n```\n\n## DESCRIPTION\n\nThis VMOD provides utility functions and an object for the VCL data type `BLOB`, which may contain arbitrary data of any length.\n\nExamples:\n\n``` python\nsub vcl_init {\n    # Create blob objects from encodings such as base64 or hex.\n    new myblob   = blob.blob(BASE64, \"Zm9vYmFy\");\n    new yourblob = blob.blob(encoded=\"666F6F\", decoding=HEX);\n}\n\nsub vcl_deliver {\n    # The .get() method retrieves the BLOB from an object.\n    set resp.http.MyBlob-As-Hex\n        = blob.encode(blob=myblob.get(), encoding=HEX);\n\n    # The .encode() method efficiently retrieves an encoding.\n    set resp.http.YourBlob-As-Base64 = yourblob.encode(BASE64);\n\n    # decode() and encode() functions convert blobs to text and\n    # vice versa at runtime.\n    set resp.http.Base64-Encoded\n        = blob.encode(BASE64,\n                      blob=blob.decode(HEX,\n                                       encoded=req.http.Hex-Encoded));\n}\n\nsub vcl_recv {\n    # transcode() converts from one encoding to another.\n    # case=UPPER specifies upper-case hex digits A-F.\n    set req.http.Hex-Encoded\n        = blob.transcode(decoding=BASE64, encoding=HEX,\n                         case=UPPER, encoded=\"YmF6\");\n\n    # transcode() from URL to IDENTITY effects a URL decode.\n    set req.url = blob.transcode(encoded=req.url, decoding=URL);\n\n    # transcode() from IDENTITY to URL effects a URL encode.\n    set req.http.url_urlcoded\n        = blob.transcode(encoded=req.url, encoding=URL);\n}\n```\n\n### ENCODING SCHEMES\n\nBinary-to-text encoding schemes are specified by ENUMs in the VMOD’s constructor, methods and functions. Decodings convert a (possibly concatenated) string into a blob, while encodings convert a blob into a string.\n\nENUM values for an encoding scheme can be one of:\n\n- `IDENTITY`\n- `BASE64`\n- `BASE64URL`\n- `BASE64URLNOPAD`\n- `BASE64CF`\n- `HEX`\n- `URL`\n\nEmpty strings are decoded into a “null blob” (of length 0), and conversely a null blob is encoded as the empty string.\n\nFor encodings with `HEX` or `URL`, you may also specify a *case* ENUM with one of the values `LOWER`, `UPPER` or `DEFAULT` to produce a string with lower- or uppercase hex digits (in `[a-f]` or `[A-F]`). The default value for *case* is `DEFAULT`, which for `HEX` and `URL` means the same as `LOWER`.\n\nThe *case* ENUM is not relevant for decodings; `HEX` or `URL` strings to be decoded as BLOBs may have hex digits in either case, or in mixed case.\n\nThe *case* ENUM MUST be set to `DEFAULT` for the other encodings (`BASE64*` and `IDENTITY`). You cannot, for example, produce an uppercase string by using the `IDENTITY` scheme with `case=UPPER`. To change the case of a string, use the `std.toupper()` or `std.tolower()` functions from [VMOD std - Varnish Standard Module](vmod_std#vmod-std-3).\n\n#### IDENTITY\n\nThe simplest encoding converts between the BLOB and STRING data types, leaving the contents byte-identical.\n\nNote that a BLOB may contain a null byte at any position before its end; if such a BLOB is decoded with `IDENTITY`, the resulting STRING will have a null byte at that position. Since VCL strings, like C strings, are represented with a terminating null byte, the string will be truncated, appearing to contain less data than the original blob. For example:\n\n``` python\n# Decode from the hex encoding for \"foo\\0bar\".\n# The header will be seen as \"foo\".\nset resp.http.Trunced-Foo1\n    = blob.encode(IDENTITY, blob=blob.decode(HEX,\n                                             encoded=\"666f6f00626172\"));\n```\n\n`IDENTITY` is the default encoding and decoding. So the above can also be written as:\n\n``` python\n# Decode from the hex encoding for \"foo\\0bar\".\n# The header will be seen as \"foo\".\nset resp.http.Trunced-Foo2\n  = blob.encode(blob=blob.decode(HEX, encoded=\"666f6f00626172\"));\n```\n\nThe *case* ENUM MUST be set to `DEFAULT` for `IDENTITY` encodings.\n\n#### BASE64\\*\n\nThe base64 encoding schemes use 4 characters to encode 3 bytes. There are no newlines or maximal line lengths – whitespace is not permitted.\n\nThe `BASE64` encoding uses the alphanumeric characters, `+` and `/`; and encoded strings are padded with the `=` character so that their length is always a multiple of four.\n\nThe `BASE64URL` encoding also uses the alphanumeric characters, but `-` and `_` instead of `+` and `/`, so that an encoded string can be used safely in a URL. This scheme also uses the padding character `=`.\n\nThe `BASE64URLNOPAD` encoding uses the same alphabet as `BASE6URL`, but leaves out the padding. Thus the length of an encoding with this scheme is not necessarily a multiple of four.\n\nThe ``` BASE64CF` is similar to ``BASE64URL ```, with the following changes to `BASE64`: `+` replaced with `-`, `/` replaced with `~` and `_` as the padding character. It is used by a certain CDN provider who also inspired the name.\n\nThe *case* ENUM MUST be set to `DEFAULT` for for all of the `BASE64*` encodings.\n\n#### HEX\n\nThe `HEX` encoding scheme converts hex strings into blobs and vice versa. For encodings, you may use the *case* ENUM to specify upper- or lowercase hex digits `A` through `f` (default `DEFAULT`, which means the same as `LOWER`). A prefix such as `0x` is not used for an encoding and is illegal for a decoding.\n\nIf a hex string to be decoded has an odd number of digits, it is decoded as if a `0` is prepended to it; that is, the first digit is interpreted as representing the least significant nibble of the first byte. For example:\n\n``` python\n# The concatenated string is \"abcdef0\", and is decoded as \"0abcdef0\".\nset resp.http.First = \"abc\";\nset resp.http.Second = \"def0\";\nset resp.http.Hex-Decoded\n    = blob.encode(HEX, blob=blob.decode(HEX,\n                       encoded=resp.http.First + resp.http.Second));\n```\n\n#### URL\n\nThe `URL` decoding replaces any `%<2-hex-digits>` substrings with the binary value of the hexadecimal number after the `%` sign.\n\nThe `URL` encoding implements “percent encoding” as per RFC3986. The *case* ENUM determines the case of the hex digits, but does not affect alphabetic characters that are not percent-encoded.\n\n### BLOB decode(ENUM decoding, INT length, STRING encoded)\n\n``` python\nBLOB decode(\n   ENUM {IDENTITY, BASE64, BASE64URL, BASE64URLNOPAD, BASE64CF, HEX, URL} decoding=IDENTITY,\n   INT length=0,\n   STRING encoded\n)\n```\n\nReturns the BLOB derived from the string *encoded* according to the scheme specified by *decoding*.\n\nIf *length* \\> 0, only decode the first *length* characters of the encoded string. If *length* \\<= 0 or greater than the length of the string, then decode the entire string. The default value of *length* is 0.\n\n*decoding* defaults to IDENTITY.\n\nExample:\n\n``` python\nblob.decode(BASE64, encoded=\"Zm9vYmFyYmF6\");\n\n# same with named parameters\nblob.decode(encoded=\"Zm9vYmFyYmF6\", decoding=BASE64);\n\n# convert string to blob\nblob.decode(encoded=\"foo\");\n```\n\n### STRING encode(ENUM encoding, ENUM case, BLOB blob)\n\n``` python\nSTRING encode(\n   ENUM {IDENTITY, BASE64, BASE64URL, BASE64URLNOPAD, BASE64CF, HEX, URL} encoding=IDENTITY,\n   ENUM {LOWER, UPPER, DEFAULT} case=DEFAULT,\n   BLOB blob\n)\n```\n\nReturns a string representation of the BLOB *blob* as specified by *encoding*. *case* determines the case of hex digits for the `HEX` and `URL` encodings, and is ignored for the other encodings.\n\n*encoding* defaults to `IDENTITY`, and *case* defaults to `DEFAULT`. `DEFAULT` is interpreted as `LOWER` for the `HEX` and `URL` encodings, and is the required value for the other encodings.\n\nExample:\n\n``` python\nset resp.http.encode1\n    = blob.encode(HEX,\n                  blob=blob.decode(BASE64, encoded=\"Zm9vYmFyYmF6\"));\n\n# same with named parameters\nset resp.http.encode2\n    = blob.encode(blob=blob.decode(encoded=\"Zm9vYmFyYmF6\",\n                                           decoding=BASE64),\n                      encoding=HEX);\n\n# convert blob to string\nset resp.http.encode3\n    = blob.encode(blob=blob.decode(encoded=\"foo\"));\n```\n\n### STRING transcode(ENUM decoding, ENUM encoding, ENUM case, INT length, STRING encoded)\n\n``` python\nSTRING transcode(\n   ENUM {IDENTITY, BASE64, BASE64URL, BASE64URLNOPAD, BASE64CF, HEX, URL} decoding=IDENTITY,\n   ENUM {IDENTITY, BASE64, BASE64URL, BASE64URLNOPAD, BASE64CF, HEX, URL} encoding=IDENTITY,\n   ENUM {LOWER, UPPER, DEFAULT} case=DEFAULT,\n   INT length=0,\n   STRING encoded\n)\n```\n\nTranslates from one encoding to another, by first decoding the string *encoded* according to the scheme *decoding*, and then returning the encoding of the resulting blob according to the scheme *encoding*. *case* determines the case of hex digits for the `HEX` and `URL` encodings, and is ignored for other encodings.\n\nAs with [blob.decode()](#blob-decode): If *length* \\> 0, only decode the first *length* characters of the encoded string, otherwise decode the entire string. The default value of *length* is 0.\n\n*decoding* and *encoding* default to IDENTITY, and *case* defaults to `DEFAULT`. `DEFAULT` is interpreted as `LOWER` for the `HEX` and `URL` encodings, and is the required value for the other encodings.\n\nExample:\n\n``` python\nset resp.http.Hex2Base64-1\n     = blob.transcode(HEX, BASE64, encoded=\"666f6f\");\n\n # same with named parameters\n set resp.http.Hex2Base64-2\n    = blob.transcode(encoded=\"666f6f\",\n                          encoding=BASE64, decoding=HEX);\n\n # URL decode -- recall that IDENTITY is the default encoding.\n set resp.http.urldecoded\n    = blob.transcode(encoded=\"foo%20bar\", decoding=URL);\n\n # URL encode\n set resp.http.urlencoded\n     = blob.transcode(encoded=\"foo bar\", encoding=URL);\n```\n\n### BOOL same(BLOB, BLOB)\n\nReturns `true` if and only if the two BLOB arguments are the same object, i.e. they specify exactly the same region of memory, or both are empty.\n\nIf the BLOBs are both empty (length is 0 and/or the internal pointer is `NULL`), then [blob.same()](#blob-same) returns `true`. If any non-empty BLOB is compared to an empty BLOB, then [blob.same()](#blob-same) returns `false`.\n\n### BOOL equal(BLOB, BLOB)\n\nReturns true if and only if the two BLOB arguments have equal contents (possibly in different memory regions).\n\nAs with [blob.same()](#blob-same): If the BLOBs are both empty, then [blob.equal()](#blob-equal) returns `true`. If any non-empty BLOB is compared to an empty BLOB, then [blob.equal()](#blob-equal) returns `false`.\n\n### INT length(BLOB)\n\nReturns the length of the BLOB.\n\n### BLOB sub(BLOB, BYTES length, BYTES offset=0)\n\nReturns a new BLOB formed from *length* bytes of the BLOB argument starting at *offset* bytes from the start of its memory region. The default value of *offset* is `0B`.\n\n[blob.sub()](#blob-sub) fails and returns NULL if the BLOB argument is empty, or if `offset + length` requires more bytes than are available in the BLOB.\n\n### new xblob = blob.blob(ENUM decoding, STRING encoded)\n\n``` python\nnew xblob = blob.blob(\n   ENUM {IDENTITY, BASE64, BASE64URL, BASE64URLNOPAD, BASE64CF, HEX, URL} decoding=IDENTITY,\n   STRING encoded\n)\n```\n\nCreates an object that contains the BLOB derived from the string *encoded* according to the scheme *decoding*.\n\nExample:\n\n``` python\nnew theblob1 = blob.blob(BASE64, encoded=\"YmxvYg==\");\n\n# same with named arguments\nnew theblob2 = blob.blob(encoded=\"YmxvYg==\", decoding=BASE64);\n\n# string as a blob\nnew stringblob = blob.blob(encoded=\"bazz\");\n```\n\n### BLOB xblob.get()\n\nReturns the BLOB created by the constructor.\n\nExample:\n\n``` python\nset resp.http.The-Blob1 =\n    blob.encode(blob=theblob1.get());\n\nset resp.http.The-Blob2 =\n    blob.encode(blob=theblob2.get());\n\nset resp.http.The-Stringblob =\n    blob.encode(blob=stringblob.get());\n```\n\n### STRING xblob.encode(ENUM encoding, ENUM case)\n\n``` python\nSTRING xblob.encode(\n      ENUM {IDENTITY, BASE64, BASE64URL, BASE64URLNOPAD, BASE64CF, HEX, URL} encoding=IDENTITY,\n      ENUM {LOWER, UPPER, DEFAULT} case=DEFAULT\n)\n```\n\nReturns an encoding of BLOB created by the constructor, according to the scheme *encoding*. *case* determines the case of hex digits for the `HEX` and `URL` encodings, and MUST be set to `DEFAULT` for the other encodings.\n\nExample:\n\n``` python\n# blob as text\nset resp.http.The-Blob = theblob1.encode();\n\n# blob as base64\nset resp.http.The-Blob-b64 = theblob1.encode(BASE64);\n```\n\nFor any [blob.blob()](#blob-blob) object, `encoding` and `case`, encodings via the [xblob.encode()](#xblob-encode) method and the [blob.encode()](#blob-encode) function are equal:\n\n``` python\n# Always true:\nblob.encode(ENC, CASE, blob.get()) == blob.encode(ENC, CASE)\n```\n\nBut the [xblob.encode()](#xblob-encode) object method is more efficient – the encoding is computed once and cached (with allocation in heap memory), and the cached encoding is retrieved on every subsequent call. The [blob.encode()](#blob-encode) function computes the encoding on every call, allocating space for the string in Varnish workspaces.\n\nSo if the data in a BLOB are fixed at VCL initialization time, so that its encodings will always be the same, it is better to create a [blob.blob()](#blob-blob) object. The VMOD’s functions should be used for data that are not known until runtime.\n\n## ERRORS\n\nThe encoders, decoders and [blob.sub()](#blob-sub) may fail if there is insufficient space to create the new blob or string. Decoders may also fail if the encoded string is an illegal format for the decoding scheme. Encoders will fail for the `IDENTITY` and `BASE64*` encoding schemes if the *case* ENUM is not set to `DEFAULT`.\n\nIf any of the VMOD’s methods, functions or constructor fail, then VCL failure is invoked, just as if `return(fail)` had been called in the VCL source. This means that:\n\n- If the [blob.blob()](#blob-blob) object constructor fails, or if any methods or functions fail during `vcl_init{}`, then the VCL program will fail to load, and the VCC compiler will emit an error message.\n- If a method or function fails in any other VCL subroutine besides `vcl_synth{}`, then control is directed to `vcl_synth{}`. The response status is set to 503 with the reason string `\"VCL failed\"`, and an error message will be written to the [VSL](vsl#vsl-7) using the tag `VCL_Error`.\n- If the failure occurs during `vcl_synth{}`, then `vcl_synth{}` is aborted. The response line `\"503 VCL failed\"` is returned, and the `VCL_Error` message is written to the log.\n\n## LIMITATIONS\n\nThe VMOD allocates memory in various ways for new blobs and strings. The [blob.blob()](#blob-blob) object and its methods allocate memory from the heap, and hence they are only limited by available virtual memory.\n\nThe [blob.encode()](#blob-encode), [blob.decode()](#blob-decode) and [blob.transcode()](#blob-transcode) functions allocate Varnish workspace, as does [blob.sub()](#blob-sub) for the newly created BLOB. If these functions are failing, as indicated by “out of space” messages in the Varnish log (with the `VCL_Error` tag), then you will need to increase the varnishd parameters `workspace_client` and/or `workspace_backend`.\n\nThe [blob.transcode()](#blob-transcode) function also allocates space on the stack for a temporary BLOB. If this function causes stack overflow, you may need to increase the varnishd parameter `thread_pool_stack`.\n\n## SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [VCL](vcl#vcl-7)\n- [VSL](vsl#vsl-7)\n- [VMOD std - Varnish Standard Module](vmod_std#vmod-std-3)\n\n## COPYRIGHT\n\n``` python\nThis document is licensed under the same conditions as Varnish itself.\nSee LICENSE for details.\n\nSPDX-License-Identifier: BSD-2-Clause\n\nAuthors: Nils Goroll <nils.goroll@uplex.de>\n         Geoffrey Simmons <geoffrey.simmons@uplex.de>\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vmod_blob.html](https://varnish-cache.org/docs/7.4/reference/vmod_blob.html)"
- name: VMOD cookie - Varnish Cookie Module
  id: reference/vmod_cookie
  summary: Handle HTTP cookies more easily in Varnish VCL
  description: "# VMOD cookie - Varnish Cookie Module\n\n## SYNOPSIS\n\n``` literal-block\nimport cookie [as name] [from \"path\"]\n\nVOID clean()\n\nVOID delete(STRING cookiename)\n\nVOID filter(STRING filterstring)\n\nVOID filter_re(REGEX expression)\n\nVOID keep(STRING filterstring)\n\nVOID keep_re(REGEX expression)\n\nSTRING format_date(TIME now, DURATION timedelta)\n\nSTRING get(STRING cookiename)\n\nSTRING get_re(REGEX expression)\n\nSTRING get_string()\n\nBOOL isset(STRING cookiename)\n\nVOID parse(STRING cookieheader)\n\nVOID set(STRING cookiename, STRING value)\n```\n\n## DESCRIPTION\n\nHandle HTTP cookies more easily in Varnish VCL.\n\nParses a cookie header into an internal data store, where per-cookie get/set/delete functions are available. A keep() function removes all but a set comma-separated list of cookies. A filter() function removes a comma- separated list of cookies.\n\nRegular expressions can be used for either selecting cookies, deleting matching cookies and deleting non-matching cookie names.\n\nA convenience function for formatting the Set-Cookie Expires date field is also included.\n\nThe state loaded with cookie.parse() has a lifetime of the current request or backend request context. To pass variables to the backend request, store the contents as fake bereq headers.\n\nFiltering example:\n\n``` python\nimport cookie;\n\nsub vcl_recv {\n    if (req.http.cookie) {\n        cookie.parse(req.http.cookie);\n        # Either delete the ones you want to get rid of:\n        cookie.delete(\"cookie2\");\n        # or delete all but a few:\n        cookie.keep(\"SESSIONID,PHPSESSID\");\n\n        # Store it back into req so it will be passed to the backend.\n        set req.http.cookie = cookie.get_string();\n\n        # If empty, unset so the builtin VCL can consider it for caching.\n        if (req.http.cookie == \"\") {\n            unset req.http.cookie;\n        }\n    }\n}\n```\n\n### VOID clean()\n\nClean up previously parsed cookies. It is not necessary to run clean() in normal operations.\n\nExample:\n\n``` python\nsub vcl_recv {\n    cookie.clean();\n}\n```\n\n### VOID delete(STRING cookiename)\n\nDelete `cookiename` from internal vmod storage if it exists.\n\nExample:\n\n``` python\nsub vcl_recv {\n    cookie.parse(\"cookie1=value1; cookie2=value2\");\n    cookie.delete(\"cookie2\");\n    # get_string() will now yield \"cookie1=value1\"\n}\n```\n\n### VOID filter(STRING filterstring)\n\nDelete all cookies from internal vmod storage that are in the comma-separated argument cookienames.\n\nExample:\n\n``` python\nsub vcl_recv {\n    cookie.parse(\"cookie1=value1; cookie2=value2; cookie3=value3\");\n    cookie.filter(\"cookie1,cookie2\");\n    # get_string() will now yield \"cookie3=value3\"\n}\n```\n\n### VOID filter_re(REGEX expression)\n\nDelete all cookies from internal vmod storage that matches the regular expression `expression`.\n\nExample:\n\n``` python\nsub vcl_recv {\n    cookie.parse(\"cookie1=value1; cookie2=value2; cookie3=value3\");\n    cookie.filter_re(\"^cookie[12]$\");\n    # get_string() will now yield \"cookie3=value3\"\n}\n```\n\n### VOID keep(STRING filterstring)\n\nDelete all cookies from internal vmod storage that is not in the comma-separated argument cookienames.\n\nExample:\n\n``` python\nsub vcl_recv {\n    cookie.parse(\"cookie1=value1; cookie2=value2; cookie3=value3\");\n    cookie.keep(\"cookie1,cookie2\");\n    # get_string() will now yield \"cookie1=value1; cookie2=value2\"\n}\n```\n\n### VOID keep_re(REGEX expression)\n\nDelete all cookies from internal vmod storage that does not match expression `expression`.\n\nExample:\n\n``` python\nsub vcl_recv {\n    cookie.parse(\"cookie1=value1; cookie2=value2; cookie3=value3\");\n    cookie.keep_re(\"^cookie[12]$\");\n    # get_string() will now yield \"cookie1=value1; cookie2=value2\"\n}\n```\n\n### STRING format_date(TIME now, DURATION timedelta)\n\nGet a RFC1123 formatted date string suitable for inclusion in a Set-Cookie response header.\n\nCare should be taken if the response has multiple Set-Cookie headers. In that case the header vmod should be used.\n\nExample:\n\n``` python\nsub vcl_deliver {\n    # Set a userid cookie on the client that lives for 5 minutes.\n    set resp.http.Set-Cookie = \"userid=\" + req.http.userid +\n        \"; Expires=\" + cookie.format_date(now, 5m) + \"; httpOnly\";\n}\n```\n\n### STRING get(STRING cookiename)\n\nGet the value of `cookiename`, as stored in internal vmod storage.\n\nExample:\n\n``` python\nimport std;\nsub vcl_recv {\n    cookie.parse(\"cookie1=value1; cookie2=value2\");\n    std.log(\"cookie1 value is: \" + cookie.get(\"cookie1\"));\n}\n```\n\nIf `cookiename` does not exist, the `NULL` string is returned. Note that a `NULL` string is converted to an empty string when assigned to a header. This means that the following is correct:\n\n``` python\nif (req.http.Cookie) {\n        cookie.parse(req.http.Cookie);\n        set req.http.X-tmp = cookie.get(\"a_cookie\");\n} else {\n        set req.http.X-tmp = \"\";\n}\nif (req.http.X-tmp != \"\") {\n        // do something with the X-tmp header...\n} else {\n        // fallback case\n}\n```\n\nHowever, using `cookie.isset()` is often a better way to check if a particular cookie is present, like this:\n\n``` python\nunset req.http.X-tmp; # unnecessary if no fallback is needed\nif (req.http.Cookie) {\n        cookie.parse(req.http.Cookie);\n        if (cookie.isset(\"a_cookie\")) {\n                set req.http.X-tmp = cookie.get(\"a_cookie\");\n                # do something with the X-tmp header...\n        }\n}\n# if necessary, do something when a_cookie is not there\nif (!req.http.X-tmp) {\n        # fallback case\n}\n```\n\n### STRING get_re(REGEX expression)\n\nGet the value of the first cookie in internal vmod storage that matches the regular expression `expression`. If nothing matches, the `NULL` string is returned.\n\nExample:\n\n``` python\nimport std;\nsub vcl_recv {\n    cookie.parse(\"cookie1=value1; cookie2=value2\");\n    std.log(\"cookie1 value is: \" + cookie.get_re(\"^cookie1$\"));\n}\n```\n\n### STRING get_string()\n\nGet a Cookie string value with all cookies in internal vmod storage. Does not modify internal storage.\n\nExample:\n\n``` python\nsub vcl_recv {\n    cookie.parse(req.http.cookie);\n    cookie.keep(\"SESSIONID,PHPSESSID\");\n    set req.http.cookie = cookie.get_string();\n}\n```\n\n### BOOL isset(STRING cookiename)\n\nCheck if `cookiename` is set in the internal vmod storage.\n\nExample:\n\n``` python\nimport std;\nsub vcl_recv {\n    cookie.parse(\"cookie1=value1; cookie2=value2\");\n    if (cookie.isset(\"cookie2\")) {\n        std.log(\"cookie2 is set.\");\n    }\n}\n```\n\n### VOID parse(STRING cookieheader)\n\nParse the cookie string in `cookieheader`. If state already exists, `clean()` will be run first.\n\nExample:\n\n``` python\nsub vcl_recv {\n    cookie.parse(req.http.Cookie);\n}\n```\n\n### VOID set(STRING cookiename, STRING value)\n\nSet the internal vmod storage for `cookiename` to `value`.\n\nExample:\n\n``` python\nsub vcl_recv {\n    cookie.set(\"cookie1\", \"value1\");\n    std.log(\"cookie1 value is: \" + cookie.get(\"cookie1\"));\n}\n```\n\n## DEPRECATED\n\n### ALIAS format_rfc1123()\n\nDeprecated alias for `format_date()`.\n\n## COPYRIGHT\n\n``` python\nThis document is licensed under the same conditions as Varnish itself.\nSee LICENSE for details.\n\nSPDX-License-Identifier: BSD-2-Clause\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vmod_cookie.html](https://varnish-cache.org/docs/7.4/reference/vmod_cookie.html)"
- name: VMOD directors - Varnish Directors Module
  id: reference/vmod_directors
  summary: vmod_directors enables backend load balancing in Varnish
  description: "# VMOD directors - Varnish Directors Module\n\n## SYNOPSIS\n\n``` literal-block\nimport directors [as name] [from \"path\"]\n\nnew xround_robin = directors.round_robin()\n\n    VOID xround_robin.add_backend(BACKEND)\n\n    VOID xround_robin.remove_backend(BACKEND)\n\n    BACKEND xround_robin.backend()\n\nnew xfallback = directors.fallback(BOOL sticky=0)\n\n    VOID xfallback.add_backend(BACKEND)\n\n    VOID xfallback.remove_backend(BACKEND)\n\n    BACKEND xfallback.backend()\n\nnew xrandom = directors.random()\n\n    VOID xrandom.add_backend(BACKEND, REAL)\n\n    VOID xrandom.remove_backend(BACKEND)\n\n    BACKEND xrandom.backend()\n\nnew xhash = directors.hash()\n\n    VOID xhash.add_backend(BACKEND, REAL weight=1.0)\n\n    VOID xhash.remove_backend(BACKEND)\n\n    BACKEND xhash.backend(STRING)\n\nnew xshard = directors.shard()\n\n    VOID xshard.set_warmup(REAL probability=0.0)\n\n    VOID xshard.set_rampup(DURATION duration=0)\n\n    VOID xshard.associate(BLOB param=0)\n\n    BOOL xshard.add_backend(BACKEND backend, [STRING ident], [DURATION rampup], [REAL weight])\n\n    BOOL xshard.remove_backend([BACKEND backend], [STRING ident])\n\n    BOOL xshard.clear()\n\n    BOOL xshard.reconfigure(INT replicas=67)\n\n    INT xshard.key(STRING)\n\n    BACKEND xshard.backend([ENUM by], [INT key], [BLOB key_blob], [INT alt], [REAL warmup], [BOOL rampup], [ENUM healthy], [BLOB param], [ENUM resolve])\n\n    VOID xshard.debug(INT)\n\nnew xshard_param = directors.shard_param()\n\n    VOID xshard_param.clear()\n\n    VOID xshard_param.set([ENUM by], [INT key], [BLOB key_blob], [INT alt], [REAL warmup], [BOOL rampup], [ENUM healthy])\n\n    STRING xshard_param.get_by()\n\n    INT xshard_param.get_key()\n\n    INT xshard_param.get_alt()\n\n    REAL xshard_param.get_warmup()\n\n    BOOL xshard_param.get_rampup()\n\n    STRING xshard_param.get_healthy()\n\n    BLOB xshard_param.use()\n\nBACKEND lookup(STRING)\n```\n\n## DESCRIPTION\n\n*vmod_directors* enables backend load balancing in Varnish.\n\nThe module implements load balancing techniques, and also serves as an example on how one could extend the load balancing capabilities of Varnish.\n\nTo enable load balancing you must import this vmod (directors).\n\nThen you define your backends. Once you have the backends declared you can add them to a director. This happens in executed VCL code. If you want to emulate the previous behavior of Varnish 3.0 you can just initialize the directors in `vcl_init{}`, like this:\n\n``` python\nsub vcl_init {\n    new vdir = directors.round_robin();\n    vdir.add_backend(backend1);\n    vdir.add_backend(backend2);\n}\n```\n\nAs you can see there is nothing keeping you from manipulating the directors elsewhere in VCL. So, you could have VCL code that would add more backends to a director when a certain URL is called.\n\nNote that directors can use other directors as backends.\n\n### new xround_robin = directors.round_robin()\n\nCreate a round robin director.\n\nThis director will pick backends in a round robin fashion.\n\nExample:\n\n``` python\nnew vdir = directors.round_robin();\n```\n\n### VOID xround_robin.add_backend(BACKEND)\n\nAdd a backend to the round-robin director.\n\nExample:\n\n``` python\nvdir.add_backend(backend1);\n```\n\n### VOID xround_robin.remove_backend(BACKEND)\n\nRemove a backend from the round-robin director.\n\nExample:\n\n``` python\nvdir.remove_backend(backend1);\n```\n\n### BACKEND xround_robin.backend()\n\nPick a backend from the director.\n\nExample:\n\n``` python\nset req.backend_hint = vdir.backend();\n```\n\n### new xfallback = directors.fallback(BOOL sticky=0)\n\nCreate a fallback director.\n\nA fallback director will try each of the added backends in turn, and return the first one that is healthy.\n\nIf *sticky* is set to `true`, the director will keep using the healthy backend, even if a higher-priority backend becomes available. Once the whole backend list is exhausted, it’ll start over at the beginning.\n\nExample:\n\n``` python\nnew vdir = directors.fallback();\n```\n\n### VOID xfallback.add_backend(BACKEND)\n\nAdd a backend to the director.\n\nNote that the order in which this is done matters for the fallback director.\n\nExample:\n\n``` python\nvdir.add_backend(backend1);\n```\n\n### VOID xfallback.remove_backend(BACKEND)\n\nRemove a backend from the director.\n\nExample:\n\n``` python\nvdir.remove_backend(backend1);\n```\n\n### BACKEND xfallback.backend()\n\nPick a backend from the director.\n\nExample:\n\n``` python\nset req.backend_hint = vdir.backend();\n```\n\n### new xrandom = directors.random()\n\nCreate a random backend director.\n\nThe random director distributes load over the backends using a weighted random probability distribution.\n\nThe “testable” random generator in varnishd is used, which enables deterministic tests to be run (See: `d00004.vtc`).\n\nExample:\n\n``` python\nnew vdir = directors.random();\n```\n\n### VOID xrandom.add_backend(BACKEND, REAL)\n\nAdd a backend to the director with a given weight.\n\nEach backend will receive approximately 100 \\* (weight / (sum(all_added_weights))) per cent of the traffic sent to this director.\n\nExample:\n\n``` python\n# 2/3 to backend1, 1/3 to backend2.\nvdir.add_backend(backend1, 10.0);\nvdir.add_backend(backend2, 5.0);\n```\n\n### VOID xrandom.remove_backend(BACKEND)\n\nRemove a backend from the director.\n\nExample:\n\n``` python\nvdir.remove_backend(backend1);\n```\n\n### BACKEND xrandom.backend()\n\nPick a backend from the director.\n\nExample:\n\n``` python\nset req.backend_hint = vdir.backend();\n```\n\n### new xhash = directors.hash()\n\nCreate a hashing backend director.\n\nThe director chooses the backend server by computing a hash/digest of the string given to [xhash.backend()](#xhash-backend).\n\nCommonly used with `client.ip` or a session cookie to get sticky sessions.\n\nExample:\n\n``` python\nnew vdir = directors.hash();\n```\n\n### VOID xhash.add_backend(BACKEND, REAL weight=1.0)\n\nAdd a backend to the director with a certain weight.\n\nWeight is used as in the random director. Recommended and default value is 1.0 unless you have special needs.\n\nExample:\n\n``` python\nvdir.add_backend(normal_backend);\nvdir.add_backend(larger_backend, 1.5);\n```\n\n### VOID xhash.remove_backend(BACKEND)\n\nRemove a backend from the director.\n\nExample::  \nvdir.remove_backend(larger_backend);\n\n### BACKEND xhash.backend(STRING)\n\nPick a backend from the hash director.\n\nUse the string or list of strings provided to pick the backend.\n\nExample::  \n\\# pick a backend based on the cookie header from the client set req.backend_hint = vdir.backend(req.http.cookie);\n\n### new xshard = directors.shard()\n\nCreate a shard director.\n\n#### Introduction\n\nThe shard director selects backends by a key, which can be provided directly or derived from strings. For the same key, the shard director will always return the same backend, unless the backend configuration or health state changes. Conversely, for differing keys, the shard director will likely choose different backends. In the default configuration, unhealthy backends are not selected.\n\nThe shard director resembles the hash director, but its main advantage is that, when the backend configuration or health states change, the association of keys to backends remains as stable as possible.\n\nIn addition, the rampup and warmup features can help to further improve user-perceived response times.\n\n#### Sharding\n\nThis basic technique allows for numerous applications like optimizing backend server cache efficiency, Varnish clustering or persisting sessions to servers without keeping any state, and, in particular, without the need to synchronize state between nodes of a cluster of Varnish servers:\n\n- Many applications use caches for data objects, so, in a cluster of application servers, requesting similar objects from the same server may help to optimize efficiency of such caches.\n\n  For example, sharding by URL or some *id* component of the url has been shown to drastically improve the efficiency of many content management systems.\n\n- As special case of the previous example, in clusters of Varnish servers without additional request distribution logic, each cache will need store all hot objects, so the effective cache size is approximately the smallest cache size of any server in the cluster.\n\n  Sharding allows to segregate objects within the cluster such that each object is only cached on one of the servers (or on one primary and one backup, on a primary for long and others for short etc…). Effectively, this will lead to a cache size in the order of the sum of all individual caches, with the potential to drastically increase efficiency (scales by the number of servers).\n\n- Another application is to implement persistence of backend requests, such that all requests sharing a certain criterion (such as an IP address or session ID) get forwarded to the same backend server.\n\nWhen used with clusters of varnish servers, the shard director will, if otherwise configured equally, make the same decision on all servers. In other words, requests sharing a common criterion used as the shard key will be balanced onto the same backend server(s) no matter which Varnish server handles the request.\n\nThe drawbacks are:\n\n- the distribution of requests depends on the number of requests per key and the uniformity of the distribution of key values. In short, while this technique may lead to much better efficiency overall, it may also lead to less good load balancing for specific cases.\n- When a backend server becomes unavailable, every persistence technique has to reselect a new backend server, but this technique will also switch back to the preferred server once it becomes healthy again, so when used for persistence, it is generally less stable compared to stateful techniques (which would continue to use a selected server for as long as possible (or dictated by a TTL)).\n\n#### Method\n\nWhen [xshard.reconfigure()](#xshard-reconfigure) is called explicitly (or implicitly at the end of any task containing reconfigurations like [xshard.add_backend()](#xshard-add-backend)), a consistent hashing circular data structure gets built from the last 32 bits of SHA256 hash values of *\\<ident\\>\\<n\\>* (default *ident* being the backend name) for each backend and for a running number *n* from 1 to the *replicas* argument to [xshard.reconfigure()](#xshard-reconfigure). Hashing creates the seemingly random order for placement of backends on the consistent hashing ring. When [xshard.add_backend()](#xshard-add-backend) was called with a *weight* argument, *replicas* is scaled by that weight to add proportionally more copies of the that backend on the ring.\n\nWhen [xshard.backend()](#xshard-backend) is called, a load balancing key gets generated unless provided. The smallest hash value in the circle is looked up that is larger than the key (searching clockwise and wrapping around as necessary). The backend for this hash value is the preferred backend for the given key.\n\nIf a healthy backend is requested, the search is continued linearly on the ring as long as backends found are unhealthy or all backends have been checked. The order of these “alternative backends” on the ring is likely to differ for different keys. Alternative backends can also be selected explicitly.\n\nOn consistent hashing see:\n\n- [http://www8.org/w8-papers/2a-webserver/caching/paper2.html](http://www8.org/w8-papers/2a-webserver/caching/paper2.html)\n- [http://www.audioscrobbler.net/development/ketama/](http://www.audioscrobbler.net/development/ketama/)\n- svn://svn.audioscrobbler.net/misc/ketama\n- [http://en.wikipedia.org/wiki/Consistent_hashing](http://en.wikipedia.org/wiki/Consistent_hashing)\n\n#### Error Reporting\n\nFailing methods should report errors to VSL with the Error tag, so when configuring the shard director, you are advised to check:\n\n``` python\nvarnishlog -I Error:^vmod_directors.shard\n```\n\nAdditional information may be provided as Notices, which can be checked using\n\nvarnishlog -I Notice:^vmod_directors.shard\n\n### VOID xshard.set_warmup(REAL probability=0.0)\n\nSet the default warmup probability. See the *warmup* parameter of [xshard.backend()](#xshard-backend). If *probability* is 0.0 (default), warmup is disabled.\n\n### VOID xshard.set_rampup(DURATION duration=0)\n\nSet the default rampup duration. See *rampup* parameter of [xshard.backend()](#xshard-backend). If *duration* is 0 (default), rampup is disabled.\n\n### VOID xshard.associate(BLOB param=0)\n\nAssociate a default [directors.shard_param()](#directors-shard-param) object or clear an association.\n\nThe value of the *param* argument must be a call to the [xshard_param.use()](#xshard-param-use) method. No argument clears the association.\n\nThe association can be changed per backend request using the *param* argument of [xshard.backend()](#xshard-backend).\n\n### BOOL xshard.add_backend(BACKEND backend, \\[STRING ident\\], \\[DURATION rampup\\], \\[REAL weight\\])\n\n``` python\nBOOL xshard.add_backend(\n      BACKEND backend,\n      [STRING ident],\n      [DURATION rampup],\n      [REAL weight]\n)\n```\n\nAdd a backend *backend* to the director.\n\n*ident*: Optionally specify an identification string for this backend, which will be hashed by [xshard.reconfigure()](#xshard-reconfigure) to construct the consistent hashing ring. The identification string defaults to the backend name.\n\n*ident* allows to add multiple instances of the same backend.\n\n*rampup*: Optionally specify a specific rampup time for this backend. Otherwise, the per-director rampup time is used (see [xshard.set_rampup()](#xshard-set-rampup)).\n\n*weight*: Optionally specify a weight to scale the [xshard.reconfigure()](#xshard-reconfigure) *replicas* parameter. *weight* is limited to at least 1. Values above 10 probably do not make much sense. The effect of *weight* is also capped such that the total number of replicas does not exceed `UINT32_MAX`.\n\n### BOOL xshard.remove_backend(\\[BACKEND backend\\], \\[STRING ident\\])\n\n``` python\nBOOL xshard.remove_backend(\n      [BACKEND backend=0],\n      [STRING ident=0]\n)\n```\n\nRemove backend(s) from the director. Either *backend* or *ident* must be specified. *ident* removes a specific instance. If *backend* is given without *ident*, all instances of this backend are removed.\n\n### BOOL xshard.clear()\n\nRemove all backends from the director.\n\n### BOOL xshard.reconfigure(INT replicas=67)\n\nExplicitly reconfigure the consistent hashing ring to reflect backend changes to become effective immediately.\n\nIf this method is not called explicitly, reconfiguration happens at the end of the current task (after `vcl_init {}` or when the current client or backend task is finished).\n\n### INT xshard.key(STRING)\n\nConvenience method to generate a sharding key for use with the *key* argument to the [xshard.backend()](#xshard-backend) method by hashing the given string with SHA256.\n\nTo generate sharding keys using other hashes, use a custom vmod like [vmod blobdigest](https://code.uplex.de/uplex-varnish/libvmod-blobdigest/blob/master/README.rst) with the *key_blob* argument of the [xshard.backend()](#xshard-backend) method.\n\n### BACKEND xshard.backend(\\[ENUM by\\], \\[INT key\\], \\[BLOB key_blob\\], \\[INT alt\\], \\[REAL warmup\\], \\[BOOL rampup\\], \\[ENUM healthy\\], \\[BLOB param\\], \\[ENUM resolve\\])\n\n``` python\nBACKEND xshard.backend(\n      [ENUM {HASH, URL, KEY, BLOB} by=HASH],\n      [INT key],\n      [BLOB key_blob],\n      [INT alt=0],\n      [REAL warmup=-1],\n      [BOOL rampup=1],\n      [ENUM {CHOSEN, IGNORE, ALL} healthy=CHOSEN],\n      [BLOB param],\n      [ENUM {NOW, LAZY} resolve]\n)\n```\n\nLookup a backend on the consistent hashing ring.\n\nThis documentation uses the notion of an order of backends for a particular shard key. This order is deterministic but seemingly random as determined by the consistent hashing algorithm and is likely to differ for different keys, depending on the number of backends and the number of replicas. In particular, the backend order referred to here is \\_not\\_ the order given when backends are added.\n\n- *by* how to determine the sharding key\n\n  - `HASH`:\n\n    - when called in backend context and in `vcl_pipe {}`: Use the varnish hash value as set by `vcl_hash{}`\n    - when called in client context other than `vcl_pipe {}`: hash `req.url`\n\n  - `URL`: hash req.url / bereq.url\n\n  - `KEY`: use the *key* argument\n\n  - `BLOB`: use the *key_blob* argument\n\n- *key* lookup key with `by=KEY`\n\n  the [xshard.key()](#xshard-key) method may come handy to generate a sharding key from custom strings.\n\n- *key_blob* lookup key with `by=BLOB`\n\n  Currently, this uses the first 4 bytes from the given blob in network byte order (big endian), left-padded with zeros for blobs smaller than 4 bytes.\n\n- *alt* alternative backend selection\n\n  Select the *alt*-th alternative backend for the given *key*.\n\n  This is particularly useful for retries / restarts due to backend errors: By setting `alt=req.restarts` or `alt=bereq.retries` with healthy=ALL, another server gets selected.\n\n  The rampup and warmup features are only active for `alt==0`\n\n- *rampup* slow start for servers which just went healthy\n\n  If `alt==0` and the chosen backend is in its rampup period, with a probability proportional to the fraction of time since the backup became healthy to the rampup period, return the next alternative backend, unless this is also in its rampup period.\n\n  The default rampup interval can be set per shard director using the [xshard.set_rampup()](#xshard-set-rampup) method or specifically per backend with the [xshard.add_backend()](#xshard-add-backend) method.\n\n- *warmup* probabilistic alternative server selection\n\n  possible values: -1, 0..1\n\n  `-1`: use the warmup probability from the director definition\n\n  Only used for `alt==0`: Sets the ratio of requests (0.0 to 1.0) that goes to the next alternate backend to warm it up when the preferred backend is healthy. Not active if any of the preferred or alternative backend are in rampup.\n\n  `warmup=0.5` is a convenient way to spread the load for each key over two backends under normal operating conditions.\n\n- *healthy*\n\n  - CHOSEN: Return a healthy backend if possible.\n\n    For `alt==0`, return the first healthy backend or none.\n\n    For `alt > 0`, ignore the health state of backends skipped for alternative backend selection, then return the next healthy backend. If this does not exist, return the last healthy backend of those skipped or none.\n\n  - IGNORE: Completely ignore backend health state\n\n    Just return the first or *alt*-th alternative backend, ignoring health state, *rampup* and *warmup*.\n\n  - ALL: Check health state also for alternative backend selection\n\n    For `alt > 0`, return the *alt*-th alternative backend of all those healthy, the last healthy backend found or none.\n\n- *resolve*\n\n  default: `LAZY` in `vcl_init{}`, `NOW` otherwise\n\n  - `NOW`: look up a backend and return it.\n\n    Can not be used in `vcl_init{}`.\n\n  - `LAZY`: return an instance of this director for later backend resolution.\n\n    `LAZY` mode is required for referencing shard director instances, for example as backends for other directors (director layering).\n\n    In `vcl_init{}` and on the client side, `LAZY` mode can not be used with any other argument.\n\n    On the backend side and in `vcl_pipe {}`, parameters from arguments or an associated parameter set affect the shard director instance for the backend request irrespective of where it is referenced.\n\n- *param*\n\n  Use or associate a parameter set. The value of the *param* argument must be a call to the [xshard_param.use()](#xshard-param-use) method.\n\n  default: as set by [xshard.associate()](#xshard-associate) or unset.\n\n  - for `resolve=NOW` take parameter defaults from the [directors.shard_param()](#directors-shard-param) parameter set\n\n  - for `resolve=LAZY` associate the [directors.shard_param()](#directors-shard-param) parameter set for this backend request\n\n    Implementation notes for use of parameter sets with `resolve=LAZY`:\n\n    - A *param* argument remains associated and any changes to the associated parameter set affect the sharding decision once the director resolves to an actual backend.\n    - If other parameter arguments are also given, they have preference and are kept even if the parameter set given by the *param* argument is subsequently changed within the same backend request.\n    - Each call to [xshard.backend()](#xshard-backend) overrides any previous call.\n\n### VOID xshard.debug(INT)\n\n*intentionally undocumented*\n\n### new xshard_param = directors.shard_param()\n\nCreate a shard parameter set.\n\nA parameter set allows for re-use of [xshard.backend()](#xshard-backend) arguments across many shard director instances and simplifies advanced use cases (e.g. shard director with custom parameters layered below other directors).\n\nParameter sets have two scopes:\n\n- per-VCL scope defined in `vcl_init{}`\n- per backend request scope\n\nThe per-VCL scope defines defaults for the per backend scope. Any changes to a parameter set in backend context and in `vcl_pipe {}` only affect the respective backend request.\n\nParameter sets can not be used in client context except for `vcl_pipe {}`.\n\nThe following example is a typical use case: A parameter set is associated with several directors. Director choice happens on the client side and parameters are changed on the backend side to implement retries on alternative backends:\n\n``` python\nsub vcl_init {\n  new shard_param = directors.shard_param();\n\n  new dir_A = directors.shard();\n  dir_A.add_backend(...);\n  dir_A.reconfigure();\n  dir_A.associate(shard_param.use()); # <-- !\n\n  new dir_B = directors.shard();\n  dir_B.add_backend(...);\n  dir_B.reconfigure();\n  dir_B.associate(shard_param.use()); # <-- !\n}\n\nsub vcl_recv {\n  if (...) {\n    set req.backend_hint = dir_A.backend(resolve=LAZY);\n  } else {\n    set req.backend_hint = dir_B.backend(resolve=LAZY);\n  }\n}\n\nsub vcl_backend_fetch {\n  # changes dir_A and dir_B behaviour\n  shard_param.set(alt=bereq.retries, by=URL);\n}\n```\n\n### VOID xshard_param.clear()\n\nReset the parameter set to default values as documented for [xshard.backend()](#xshard-backend).\n\n- in `vcl_init{}`, resets the parameter set default for this VCL in\n- backend context and in `vcl_pipe {}`, resets the parameter set for this backend request to the VCL defaults\n\nRestricted to: `vcl_pipe`, `backend`, `housekeeping`.\n\n### VOID xshard_param.set(\\[ENUM by\\], \\[INT key\\], \\[BLOB key_blob\\], \\[INT alt\\], \\[REAL warmup\\], \\[BOOL rampup\\], \\[ENUM healthy\\])\n\n``` python\nVOID xshard_param.set(\n      [ENUM {HASH, URL, KEY, BLOB} by],\n      [INT key],\n      [BLOB key_blob],\n      [INT alt],\n      [REAL warmup],\n      [BOOL rampup],\n      [ENUM {CHOSEN, IGNORE, ALL} healthy]\n)\n```\n\nChange the given parameters of a parameter set as documented for [xshard.backend()](#xshard-backend).\n\n- in `vcl_init{}`, changes the parameter set default for this VCL\n- in backend context and in `vcl_pipe {}`, changes the parameter set for this backend request, keeping the defaults set for this VCL for unspecified arguments.\n\nRestricted to: `vcl_pipe`, `backend`, `housekeeping`.\n\n### STRING xshard_param.get_by()\n\nGet a string representation of the *by* enum argument which denotes how a shard director using this parameter object would derive the shard key. See [xshard.backend()](#xshard-backend).\n\n### INT xshard_param.get_key()\n\nGet the key which a shard director using this parameter object would use. See [xshard.backend()](#xshard-backend).\n\n### INT xshard_param.get_alt()\n\nGet the *alt* parameter which a shard director using this parameter object would use. See [xshard.backend()](#xshard-backend).\n\n### REAL xshard_param.get_warmup()\n\nGet the *warmup* parameter which a shard director using this parameter object would use. See [xshard.backend()](#xshard-backend).\n\n### BOOL xshard_param.get_rampup()\n\nGet the *rampup* parameter which a shard director using this parameter object would use. See [xshard.backend()](#xshard-backend).\n\n### STRING xshard_param.get_healthy()\n\nGet a string representation of the *healthy* enum argument which a shard director using this parameter object would use. See [xshard.backend()](#xshard-backend).\n\n### BLOB xshard_param.use()\n\nFor use with the *param* argument of [xshard.backend()](#xshard-backend) to associate this shard parameter set with a shard director.\n\nRestricted to: `vcl_pipe`, `backend`, `housekeeping`.\n\n### BACKEND lookup(STRING)\n\nLookup a backend by its name.\n\nRestricted to: `housekeeping`.\n\n## ACKNOWLEDGEMENTS\n\nDevelopment of a previous version of the shard director was partly sponsored by Deutsche Telekom AG - Products & Innovation.\n\nDevelopment of a previous version of the shard director was partly sponsored by BILD GmbH & Co KG.\n\n## COPYRIGHT\n\n``` python\nThis document is licensed under the same licence as Varnish\nitself. See LICENCE for details.\n\nSPDX-License-Identifier: BSD-2-Clause\n\nCopyright (c) 2013-2015 Varnish Software AS\nCopyright 2009-2020 UPLEX - Nils Goroll Systemoptimierung\nAll rights reserved.\n\nAuthors: Poul-Henning Kamp <phk@FreeBSD.org>\n         Julian Wiesener <jw@uplex.de>\n         Nils Goroll <slink@uplex.de>\n         Geoffrey Simmons <geoff@uplex.de>\n\nSPDX-License-Identifier: BSD-2-Clause\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED.  IN NO EVENT SHALL AUTHOR OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\nOR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\nLIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\nOUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGE.\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vmod_directors.html](https://varnish-cache.org/docs/7.4/reference/vmod_directors.html)"
- name: VMOD proxy - Varnish Module to extract TLV attributes from PROXYv2
  id: reference/vmod_proxy
  summary: vmod_proxy contains functions to extract proxy-protocol-v2 TLV attributes as described in [https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)
  description: "# VMOD proxy - Varnish Module to extract TLV attributes from PROXYv2\n\n## SYNOPSIS\n\n``` literal-block\nimport proxy [as name] [from \"path\"]\n\nSTRING alpn()\n\nSTRING authority()\n\nBOOL is_ssl()\n\nBOOL client_has_cert_sess()\n\nBOOL client_has_cert_conn()\n\nINT ssl_verify_result()\n\nSTRING ssl_version()\n\nSTRING client_cert_cn()\n\nSTRING ssl_cipher()\n\nSTRING cert_sign()\n\nSTRING cert_key()\n```\n\n## DESCRIPTION\n\n*vmod_proxy* contains functions to extract proxy-protocol-v2 TLV attributes as described in [https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt).\n\n### STRING alpn()\n\nExtract ALPN attribute.\n\nExample:\n\n``` python\nset req.http.alpn = proxy.alpn();\n```\n\nRestricted to: `client`.\n\n### STRING authority()\n\nExtract authority attribute. This corresponds to SNI from a TLS connection.\n\nExample:\n\n``` python\nset req.http.authority = proxy.authority();\n```\n\nRestricted to: `client`.\n\n### BOOL is_ssl()\n\nReport if proxy-protocol-v2 has SSL TLV.\n\nExample:\n\n``` python\nif (proxy.is_ssl()) {\n        set req.http.ssl-version = proxy.ssl_version();\n}\n```\n\nRestricted to: `client`.\n\n### BOOL client_has_cert_sess()\n\nReport if the client provided a certificate at least once over the TLS session this connection belongs to.\n\nRestricted to: `client`.\n\n### BOOL client_has_cert_conn()\n\nReport if the client provided a certificate over the current connection.\n\nRestricted to: `client`.\n\n### INT ssl_verify_result()\n\nReport the SSL_get_verify_result from a TLS session. It only matters if client_has_cert_sess() is true. Per default, value is set to 0 (X509_V_OK).\n\nExample:\n\n``` python\nif (proxy.client_has_cert_sess() && proxy.ssl_verify_result() == 0) {\n        set req.http.ssl-verify = \"ok\";\n}\n```\n\nRestricted to: `client`.\n\n### STRING ssl_version()\n\nExtract SSL version attribute.\n\nExample:\n\n``` python\nset req.http.ssl-version = proxy.ssl_version();\n```\n\nRestricted to: `client`.\n\n### STRING client_cert_cn()\n\nExtract the common name attribute of the client certificate’s.\n\nExample::  \nset req.http.cert-cn = proxy.client_cert_cn();\n\nRestricted to: `client`.\n\n### STRING ssl_cipher()\n\nExtract the SSL cipher attribute.\n\nExample:\n\n``` python\nset req.http.ssl-cipher = proxy.ssl_cipher();\n```\n\nRestricted to: `client`.\n\n### STRING cert_sign()\n\nExtract the certificate signature algorithm attribute.\n\nExample:\n\n``` python\nset req.http.cert-sign = proxy.cert_sign();\n```\n\nRestricted to: `client`.\n\n### STRING cert_key()\n\nExtract the certificate key algorithm attribute.\n\nExample:\n\n``` python\nset req.http.cert-key = proxy.cert_key();\n```\n\nRestricted to: `client`.\n\n## SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [VSL](vsl#vsl-7)\n\n## COPYRIGHT\n\n``` python\nCopyright (c) 2018 GANDI SAS\nAll rights reserved.\n\nAuthor: Emmanuel Hocdet <manu@gandi.net>\n\nSPDX-License-Identifier: BSD-2-Clause\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED.  IN NO EVENT SHALL AUTHOR OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\nOR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\nLIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\nOUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGE.\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vmod_proxy.html](https://varnish-cache.org/docs/7.4/reference/vmod_proxy.html)"
- name: VMOD purge - Varnish Purge Module
  id: reference/vmod_purge
  summary: vmod_purge contains functions that offer a finer-grained control than return(purge) from vcl_recv{}. The functions can only be called from vcl_hit{} or vcl_miss{} and they should in general be used in both to ensure that all variants of a same object are taken care of
  description: "# VMOD purge - Varnish Purge Module\n\n## SYNOPSIS\n\n``` literal-block\nimport purge [as name] [from \"path\"]\n\nINT hard()\n\nINT soft(DURATION ttl, DURATION grace, DURATION keep)\n```\n\n## DESCRIPTION\n\n*vmod_purge* contains functions that offer a finer-grained control than `return(purge)` from `vcl_recv{}`. The functions can only be called from `vcl_hit{}` or `vcl_miss{}` and they should in general be used in both to ensure that all variants of a same object are taken care of.\n\n## EXAMPLE\n\n``` python\nsub vcl_recv {\n    if (req.method == \"PURGE\") {\n        if (client.ip !~ purge_acl) {\n            return (synth(405));\n        }\n        return (hash);\n    }\n}\n\nsub my_purge {\n    set req.http.purged = purge.hard();\n    if (req.http.purged == \"0\") {\n        return (synth(404));\n    }\n    else {\n        return (synth(200));\n    }\n}\n\nsub vcl_hit {\n    if (req.method == \"PURGE\") {\n        call my_purge;\n    }\n}\n\nsub vcl_miss {\n    if (req.method == \"PURGE\") {\n        call my_purge;\n    }\n}\n\nsub vcl_synth {\n    if (req.method == \"PURGE\") {\n        if (req.http.purged) {\n            set resp.http.purged = req.http.purged;\n        }\n        return (deliver);\n    }\n}\n```\n\n### INT hard()\n\nThis is equivalent to `return(purge)` but explicitly called from `vcl_hit{}` and `vcl_miss{}`. It returns the number of purged objects.\n\nExample:\n\n``` python\nset req.http.purged = purge.hard();\n```\n\nRestricted to: `vcl_hit`, `vcl_miss`.\n\n### INT soft(DURATION ttl, DURATION grace, DURATION keep)\n\n``` python\nINT soft(DURATION ttl=0, DURATION grace=-1, DURATION keep=-1)\n```\n\nSets the *ttl*, *grace* and *keep*.\n\nBy default, *ttl* is set to 0 with *grace* and *keep* periods left untouched. Setting a negative value for *grace* or *keep* periods leaves them untouched. Setting all three parameters to `0` is equivalent to a hard purge. It returns the number of soft-purged objects.\n\nRestricted to: `vcl_hit`, `vcl_miss`.\n\n## SEE ALSO\n\n- [VCL](vcl#vcl-7)\n\n## COPYRIGHT\n\n``` python\nCopyright (c) 2017 Varnish Software AS\nAll rights reserved.\n\nAuthor: Dridi Boukelmoune <dridi.boukelmoune@gmail.com>\n\nSPDX-License-Identifier: BSD-2-Clause\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED.  IN NO EVENT SHALL AUTHOR OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\nOR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\nLIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\nOUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGE.\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vmod_purge.html](https://varnish-cache.org/docs/7.4/reference/vmod_purge.html)"
- name: VMOD std - Varnish Standard Module
  id: reference/vmod_std
  summary: vmod_std contains basic functions which are part and parcel of Varnish, but which for reasons of architecture fit better in a VMOD
  description: "# VMOD std - Varnish Standard Module\n\n## SYNOPSIS\n\n``` literal-block\nimport std [as name] [from \"path\"]\n\nREAL random(REAL lo, REAL hi)\n\nREAL round(REAL r)\n\nVOID collect(HEADER hdr, STRING sep=”, “)\n\nSTRING querysort(STRING)\n\nSTRING toupper(STRING s)\n\nSTRING tolower(STRING s)\n\nSTRING strstr(STRING s1, STRING s2)\n\nBOOL fnmatch(STRING pattern, STRING subject, BOOL pathname, BOOL noescape, BOOL period)\n\nSTRING fileread(STRING)\n\nBLOB blobread(STRING)\n\nBOOL file_exists(STRING path)\n\nBOOL healthy(BACKEND be)\n\nINT port(IP ip)\n\nDURATION duration([STRING s], [DURATION fallback], [REAL real], [INT integer])\n\nBYTES bytes([STRING s], [BYTES fallback], [REAL real], [INT integer])\n\nINT integer([STRING s], [INT fallback], [BOOL bool], [BYTES bytes], [DURATION duration], [REAL real], [TIME time])\n\nIP ip(STRING s, [IP fallback], BOOL resolve=1, [STRING p])\n\nREAL real([STRING s], [REAL fallback], [INT integer], [BOOL bool], [BYTES bytes], [DURATION duration], [TIME time])\n\nTIME time([STRING s], [TIME fallback], [REAL real], [INT integer])\n\nSTRING strftime(TIME time, STRING format)\n\nVOID log(STRING s)\n\nVOID syslog(INT priority, STRING s)\n\nVOID timestamp(STRING s)\n\nBOOL syntax(REAL)\n\nSTRING getenv(STRING name)\n\nBOOL cache_req_body(BYTES size)\n\nVOID late_100_continue(BOOL late)\n\nVOID set_ip_tos(INT tos)\n\nVOID rollback(HTTP h)\n\nBOOL ban(STRING)\n\nSTRING ban_error()\n\nTIME now()\n\nDURATION timed_call(SUB)\n\nINT real2integer(REAL r, INT fallback)\n\nTIME real2time(REAL r, TIME fallback)\n\nINT time2integer(TIME t, INT fallback)\n\nREAL time2real(TIME t, REAL fallback)\n```\n\n## DESCRIPTION\n\n*vmod_std* contains basic functions which are part and parcel of Varnish, but which for reasons of architecture fit better in a VMOD.\n\n## Numeric functions\n\n### REAL random(REAL lo, REAL hi)\n\nReturns a random real number between *lo* and *hi*.\n\nThis function uses the “testable” random generator in varnishd which enables deterministic tests to be run (See `debug.srandom` CLI command). This function should not be used for cryptographic applications.\n\nExample:\n\n``` python\nset beresp.http.random-number = std.random(1, 100);\n```\n\n### REAL round(REAL r)\n\nRounds the real *r* to the nearest integer, but round halfway cases away from zero (see `round(3)`).\n\n## String functions\n\n### VOID collect(HEADER hdr, STRING sep=”, “)\n\nCollapses multiple *hdr* headers into one long header. The default separator *sep* is the standard comma separator to use when collapsing headers, with an additional whitespace for pretty printing.\n\nCare should be taken when collapsing headers. In particular collapsing `Set-Cookie` will lead to unexpected results on the browser side.\n\nUsing *hdr* from `obj.http` triggers a VCL failure.\n\nExamples:\n\n``` python\nstd.collect(req.http.accept);\nstd.collect(req.http.cookie, \"; \");\n```\n\n### STRING querysort(STRING)\n\nSorts the query string for cache normalization purposes.\n\nExample:\n\n``` python\nset req.url = std.querysort(req.url);\n```\n\n### STRING toupper(STRING s)\n\nConverts the string *s* to uppercase.\n\nExample:\n\n``` python\nset beresp.http.scream = std.toupper(\"yes!\");\n```\n\n### STRING tolower(STRING s)\n\nConverts the string *s* to lowercase.\n\nExample:\n\n``` python\nset beresp.http.nice = std.tolower(\"VerY\");\n```\n\n### STRING strstr(STRING s1, STRING s2)\n\nReturns a string beginning at the first occurrence of the string *s2* in the string *s1*, or an empty string if *s2* is not found.\n\nNote that the comparison is case sensitive.\n\nExample:\n\n``` python\nif (std.strstr(req.url, req.http.restrict)) {\n        ...\n}\n```\n\nThis will check if the content of `req.http.restrict` occurs anywhere in `req.url`.\n\n### BOOL fnmatch(STRING pattern, STRING subject, BOOL pathname, BOOL noescape, BOOL period)\n\n``` python\nBOOL fnmatch(\n   STRING pattern,\n   STRING subject,\n   BOOL pathname=1,\n   BOOL noescape=0,\n   BOOL period=0\n)\n```\n\nShell-style pattern matching; returns `true` if *subject* matches *pattern*, where *pattern* may contain wildcard characters such as `*` or `?`.\n\nThe match is executed by the implementation of `fnmatch(3)` on your system. The rules for pattern matching on most systems include the following:\n\n- `*` matches any sequence of characters\n- `?` matches a single character\n- a bracket expression such as `[abc]` or `[!0-9]` is interpreted as a character class according to the rules of basic regular expressions (*not* `pcre2(3)` regexen), except that `!` is used for character class negation instead of `^`.\n\nIf *pathname* is `true`, then the forward slash character `/` is only matched literally, and never matches `*`, `?` or a bracket expression. Otherwise, `/` may match one of those patterns. By default, *pathname* is `true`.\n\nIf *noescape* is `true`, then the backslash character `\\` is matched as an ordinary character. Otherwise, `\\` is an escape character, and matches the character that follows it in the *pattern*. For example, `\\\\` matches `\\` when *noescape* is `true`, and `\\\\` when `false`. By default, *noescape* is `false`.\n\nIf *period* is `true`, then a leading period character `.` only matches literally, and never matches `*`, `?` or a bracket expression. A period is leading if it is the first character in *subject*; if *pathname* is also `true`, then a period that immediately follows a `/` is also leading (as in `/.`). By default, *period* is `false`.\n\n[std.fnmatch()](#std-fnmatch) invokes VCL failure and returns `false` if either of *pattern* or *subject* is `NULL` – for example, if an unset header is specified.\n\nExamples:\n\n``` python\n# Matches URLs such as /foo/bar and /foo/baz\nif (std.fnmatch(\"/foo/\\*\", req.url)) { ... }\n\n# Matches URLs such as /foo/bar/baz and /foo/baz/quux\nif (std.fnmatch(\"/foo/\\*/\\*\", bereq.url)) { ... }\n\n# Matches /foo/bar/quux, but not /foo/bar/baz/quux\nif (std.fnmatch(\"/foo/\\*/quux\", req.url)) { ... }\n\n# Matches /foo/bar/quux and /foo/bar/baz/quux\nif (std.fnmatch(\"/foo/\\*/quux\", req.url, pathname=false)) { ... }\n\n# Matches /foo/bar, /foo/car and /foo/far\nif (std.fnmatch(\"/foo/?ar\", req.url)) { ... }\n\n# Matches /foo/ followed by a non-digit\nif (std.fnmatch(\"/foo/[!0-9]\", req.url)) { ... }\n```\n\n## File(system) functions\n\n### STRING fileread(STRING)\n\nReads a text file and returns a string with the content.\n\nThe entire file is cached on the first call, and subsequent calls will return this cached contents, even if the file has changed in the meantime.\n\nFor binary files, use std.blobread() instead.\n\nExample:\n\n``` python\nsynthetic(\"Response was served by \" + std.fileread(\"/etc/hostname\"));\n```\n\nConsider that the entire contents of the file appear in the string that is returned, including newlines that may result in invalid headers if [std.fileread()](#std-fileread) is used to form a header. In that case, you may need to modify the string, for example with `regsub()` (see [VCL](vcl#vcl-7)):\n\n``` python\nset beresp.http.served-by = regsub(std.fileread(\"/etc/hostname\"), \"\\R$\", \"\");\n```\n\n### BLOB blobread(STRING)\n\nReads any file and returns a blob with the content.\n\nThe entire file is cached on the first call, and subsequent calls will return this cached contents, even if the file has changed in the meantime.\n\n### BOOL file_exists(STRING path)\n\nReturns `true` if path or the file pointed to by path exists, `false` otherwise.\n\nExample:\n\n``` python\nif (std.file_exists(\"/etc/return_503\")) {\n        return (synth(503, \"Varnish is in maintenance\"));\n}\n```\n\n## Type Inspection functions\n\n### BOOL healthy(BACKEND be)\n\nReturns `true` if the backend *be* is healthy.\n\n### INT port(IP ip)\n\nReturns the port number of the IP address *ip*. Always returns `0` for a `*.ip` variable when the address is a Unix domain socket.\n\n## Type Conversion functions\n\nThese functions all have the same form:\n\n``` python\nTYPE type([arguments], [fallback TYPE])\n```\n\nPrecisely one of the *arguments* must be provided (besides the optional *fallback*), and it will be converted to *TYPE*.\n\nIf conversion fails, *fallback* will be returned and if no fallback was specified, the VCL will be failed.\n\n### DURATION duration(\\[STRING s\\], \\[DURATION fallback\\], \\[REAL real\\], \\[INT integer\\])\n\n``` python\nDURATION duration(\n   [STRING s],\n   [DURATION fallback],\n   [REAL real],\n   [INT integer]\n)\n```\n\nReturns a DURATION from a STRING, REAL or INT argument.\n\nFor a STRING *s* argument, *s* must be quantified by `ms` (milliseconds), `s` (seconds), `m` (minutes), `h` (hours),\\`\\`d\\`\\` (days), `w` (weeks) or `y` (years) units.\n\n*real* and *integer* arguments are taken as seconds.\n\nIf the conversion of an *s* argument fails, *fallback* will be returned if provided, or a VCL failure will be triggered.\n\nConversions from *real* and *integer* arguments never fail.\n\nOnly one of the *s*, *real* or *integer* arguments may be given or a VCL failure will be triggered.\n\nExamples:\n\n``` python\nset beresp.ttl = std.duration(\"1w\", 3600s);\nset beresp.ttl = std.duration(real=1.5);\nset beresp.ttl = std.duration(integer=10);\n```\n\n### BYTES bytes(\\[STRING s\\], \\[BYTES fallback\\], \\[REAL real\\], \\[INT integer\\])\n\n``` python\nBYTES bytes(\n   [STRING s],\n   [BYTES fallback],\n   [REAL real],\n   [INT integer]\n)\n```\n\nReturns BYTES from a STRING, REAL or INT argument.\n\nA STRING *s* argument can be quantified with a multiplier (`k` (kilo), `m` (mega), `g` (giga), `t` (tera) or `p` (peta)).\n\n*real* and *integer* arguments are taken as bytes.\n\nIf the conversion of an *s* argument fails, *fallback* will be returned if provided, or a VCL failure will be triggered.\n\nOther conversions may fail if the argument can not be represented, because it is negative, too small or too large. Again, *fallback* will be returned if provided, or a VCL failure will be triggered.\n\n*real* arguments will be rounded down.\n\nOnly one of the *s*, *real* or *integer* arguments may be given or a VCL failure will be triggered.\n\nExample:\n\n``` python\nstd.cache_req_body(std.bytes(something.somewhere, 10K));\nstd.cache_req_body(std.bytes(integer=10*1024));\nstd.cache_req_body(std.bytes(real=10.0*1024));\n```\n\n### INT integer(\\[STRING s\\], \\[INT fallback\\], \\[BOOL bool\\], \\[BYTES bytes\\], \\[DURATION duration\\], \\[REAL real\\], \\[TIME time\\])\n\n``` python\nINT integer(\n   [STRING s],\n   [INT fallback],\n   [BOOL bool],\n   [BYTES bytes],\n   [DURATION duration],\n   [REAL real],\n   [TIME time]\n)\n```\n\nReturns an INT from a STRING, BOOL or other quantity.\n\nIf the conversion of an *s* argument fails, *fallback* will be returned if provided, or a VCL failure will be triggered.\n\nA *bool* argument will be returned as 0 for `false` and 1 for `true`. This conversion will never fail.\n\nFor a *bytes* argument, the number of bytes will be returned. This conversion will never fail.\n\nA *duration* argument will be rounded down to the number of seconds and returned.\n\nA *real* argument will be rounded down and returned.\n\nFor a *time* argument, the number of seconds since the UNIX epoch (1970-01-01 00:00:00 UTC) will be returned.\n\n*duration*, *real* and *time* conversions may fail if the argument can not be represented because it is too small or too large. If so, *fallback* will be returned if provided, or a VCL failure will be triggered.\n\nOnly one of the *s*, *bool*, *bytes*, *duration*, *real* or *time* arguments may be given or a VCL failure will be triggered.\n\nExamples:\n\n``` python\nif (std.integer(req.http.foo, 0) > 5) {\n        ...\n}\n\nset resp.http.answer = std.integer(real=126.42/3);\n```\n\n### IP ip(STRING s, \\[IP fallback\\], BOOL resolve=1, \\[STRING p\\])\n\nConverts the string *s* to the first IP number returned by the system library function `getaddrinfo(3)`. If conversion fails, *fallback* will be returned or VCL failure will happen.\n\nThe IP address includes a port number that can be found with `std.port()` that defaults to 80. The default port can be set to a different value with the *p* argument. It will be overridden if *s* contains both an IP address and a port number or service name.\n\nWhen *s* contains both, the syntax is either `address:port` or `address port`. If the address is a numerical IPv6 address it must be enclosed between brackets, for example `[::1] 80` or `[::1]:http`. The *fallback* may also contain both an address and a port, but its default port is always 80.\n\nIf *resolve* is false, `getaddrinfo(3)` is called using `AI_NUMERICHOST` and `AI_NUMERICSERV` to avoid network lookups depending on the system’s `getaddrinfo(3)` or nsswitch configuration. This makes “numerical” IP strings and services cheaper to convert.\n\nExample:\n\n``` python\nif (std.ip(req.http.X-forwarded-for, \"0.0.0.0\") ~ my_acl) {\n        ...\n}\n```\n\n### REAL real(\\[STRING s\\], \\[REAL fallback\\], \\[INT integer\\], \\[BOOL bool\\], \\[BYTES bytes\\], \\[DURATION duration\\], \\[TIME time\\])\n\n``` python\nREAL real(\n   [STRING s],\n   [REAL fallback],\n   [INT integer],\n   [BOOL bool],\n   [BYTES bytes],\n   [DURATION duration],\n   [TIME time]\n)\n```\n\nReturns a REAL from a STRING, BOOL or other quantity.\n\nIf the conversion of an *s* argument fails, *fallback* will be returned if provided, or a VCL failure will be triggered.\n\nA *bool* argument will be returned as 0.0 for `false` and 1.0 for `true`.\n\nFor a *bytes* argument, the number of bytes will be returned.\n\nFor a *duration* argument, the number of seconds will be returned.\n\nAn *integer* argument will be returned as a REAL.\n\nFor a *time* argument, the number of seconds since the UNIX epoch (1970-01-01 00:00:00 UTC) will be returned.\n\nNone of these conversions other than *s* will fail.\n\nOnly one of the *s*, *integer*, *bool*, *bytes*, *duration* or *time* arguments may be given or a VCL failure will be triggered.\n\nExample:\n\n``` python\nif (std.real(req.http.foo, 0.0) > 5.5) {\n        ...\n}\n```\n\n### TIME time(\\[STRING s\\], \\[TIME fallback\\], \\[REAL real\\], \\[INT integer\\])\n\n``` python\nTIME time([STRING s], [TIME fallback], [REAL real], [INT integer])\n```\n\nReturns a TIME from a STRING, REAL or INT argument.\n\nFor a STRING *s* argument, the following formats are supported:\n\n``` python\n\"Sun, 06 Nov 1994 08:49:37 GMT\"\n\"Sunday, 06-Nov-94 08:49:37 GMT\"\n\"Sun Nov  6 08:49:37 1994\"\n\"1994-11-06T08:49:37\"\n\"784111777.00\"\n\"784111777\"\n```\n\n*real* and *integer* arguments are taken as seconds since the epoch.\n\nIf the conversion of an *s* argument fails or a negative *real* or *integer* argument is given, *fallback* will be returned if provided, or a VCL failure will be triggered.\n\nExamples:\n\n``` python\nif (std.time(resp.http.last-modified, now) < now - 1w) {\n        ...\n}\n\nif (std.time(int=2147483647) < now - 1w) {\n        ...\n}\n```\n\n### STRING strftime(TIME time, STRING format)\n\nFormat the *time* argument with the *format* argument using `strftime(3)` and return the result for the UTC (historically GMT) timezone.\n\nThe empty string is returned if formatting fails, but may also be returned as a valid result.\n\nExample:\n\n``` python\nset req.http.iso = std.strftime(now, \"%Y%m%dT%H%M%SZ\");\n# e.g. 20210521T175241Z\n```\n\n## LOGGING functions\n\n### VOID log(STRING s)\n\nLogs the string *s* to the shared memory log, using [VSL](vsl#vsl-7) tag `SLT_VCL_Log`.\n\nExample:\n\n``` python\nstd.log(\"Something fishy is going on with the vhost \" + req.http.host);\n```\n\n### VOID syslog(INT priority, STRING s)\n\nLogs the string *s* to syslog tagged with *priority*. *priority* is formed by ORing the facility and level values. See your system’s `syslog.h` file for possible values.\n\nNotice: Unlike VCL and other functions in the std vmod, this function will not fail VCL processing for workspace overflows: For an out of workspace condition, the [std.syslog()](#std-syslog) function has no effect.\n\nExample:\n\n``` python\nstd.syslog(9, \"Something is wrong\");\n```\n\nThis will send a message to syslog using `LOG_USER | LOG_ALERT`.\n\n### VOID timestamp(STRING s)\n\nIntroduces a timestamp in the log with the current time, using the string *s* as the label. This is useful to time the execution of lengthy VCL subroutines, and makes the timestamps inserted automatically by Varnish more accurate.\n\nExample:\n\n``` python\nstd.timestamp(\"curl-request\");\n```\n\n## CONTROL and INFORMATION functions\n\n### BOOL syntax(REAL)\n\nReturns `true` if VCL version is at least *REAL*.\n\n### STRING getenv(STRING name)\n\nReturn environment variable *name* or the empty string. See `getenv(3)`.\n\nExample:\n\n``` python\nset req.http.My-Env = std.getenv(\"MY_ENV\");\n```\n\n### BOOL cache_req_body(BYTES size)\n\nCaches the request body if it is smaller than *size*. Returns `true` if the body was cached, `false` otherwise.\n\nNormally the request body can only be sent once. Caching it enables retrying backend requests with a request body, as usually the case with `POST` and `PUT`.\n\nExample:\n\n``` python\nif (std.cache_req_body(1KB)) {\n        ...\n}\n```\n\nRestricted to: `vcl_recv`.\n\n### VOID late_100_continue(BOOL late)\n\nControls when varnish reacts to an `Expect: 100-continue` client request header.\n\nVarnish always generates a `100 Continue` response if requested by the client trough the `Expect: 100-continue` header when waiting for request body data.\n\nBut, by default, the `100 Continue` response is already generated immediately after `vcl_recv` returns to reduce latencies under the assumption that the request body will be read eventually.\n\nCalling `std.late_100_continue(true)` in `vcl_recv` will cause the `100 Continue` response to only be sent when needed. This may cause additional latencies for processing request bodies, but is the correct behavior by strict interpretation of RFC7231.\n\nThis function has no effect outside `vcl_recv` and after calling `std.cache_req_body()` or any other function consuming the request body.\n\nExample:\n\n``` python\nvcl_recv {\n        std.late_100_continue(true);\n\n        if (req.method == \"POST\") {\n                std.late_100_continue(false);\n                return (pass);\n        }\n        ...\n }\n```\n\nRestricted to: `vcl_recv`.\n\n### VOID set_ip_tos(INT tos)\n\nSets the Differentiated Services Codepoint (DSCP) / IPv4 Type of Service (TOS) / IPv6 Traffic Class (TCLASS) byte for the current session to *tos*. Silently ignored if the listen address is a Unix domain socket.\n\nPlease note that setting the traffic class affects all requests on the same http1.1 / http2 TCP connection and, in particular, is not removed at the end of the request.\n\nExample:\n\n``` python\nif (req.url ~ \"^/slow/\") {\n        std.set_ip_tos(0);\n}\n```\n\nRestricted to: `client`.\n\n### VOID rollback(HTTP h)\n\nRestores the *h* HTTP headers to their original state.\n\nExample:\n\n``` python\nstd.rollback(bereq);\n```\n\nRestricted to: `backend`, `vcl_recv`, `vcl_pass`, `vcl_hash`, `vcl_purge`, `vcl_miss`, `vcl_hit`, `vcl_deliver`, `vcl_synth`.\n\n### BOOL ban(STRING)\n\nInvalidates all objects in cache that match the given expression with the ban mechanism. Returns `true` if the ban succeeded and `false` otherwise. Error details are available via [std.ban_error()](#std-ban-error).\n\nThe format of *STRING* is:\n\n``` python\n<field> <operator> <arg> [&& <field> <oper> <arg> ...]\n```\n\n- *\\<field\\>*:\n\n  - string fields:\n\n    - `req.url`: The request url\n    - `req.http.*`: Any request header\n    - `obj.status`: The cache object status\n    - `obj.http.*`: Any cache object header\n\n    `obj.status` is treated as a string despite the fact that it is actually an integer.\n\n  - duration fields:\n\n    - `obj.ttl`: Remaining ttl at the time the ban is issued\n    - `obj.age`: Object age at the time the ban is issued\n    - `obj.grace`: The grace time of the object\n    - `obj.keep`: The keep time of the object\n\n- *\\<operator\\>*:\n\n  - for all fields:\n\n    - `==`: *\\<field\\>* and *\\<arg\\>* are equal\n    - `!=`: *\\<field\\>* and *\\<arg\\>* are unequal\n\n    strings are compared case sensitively\n\n  - for string fields:\n\n    - `~`: *\\<field\\>* matches the regular expression *\\<arg\\>*\n    - `!~`:*\\<field\\>* does not match the regular expression *\\<arg\\>*\n\n  - for duration fields:\n\n    - `>`: *\\<field\\>* is greater than *\\<arg\\>*\n    - `>=`: *\\<field\\>* is greater than or equal to *\\<arg\\>*\n    - `<`: *\\<field\\>* is less than *\\<arg\\>*\n    - `<=`: *\\<field\\>* is less than or equal to *\\<arg\\>*\n\n- *\\<arg\\>*:\n\n  - for string fields:\n\n    Either a literal string or a regular expression. Note that *\\<arg\\>* does not use any of the string delimiters like `\"` or `{\"`*…*`\"}` or `\"\"\"`*…*`\"\"\"` used elsewhere in varnish. To match against strings containing whitespace, regular expressions containing `\\s` can be used.\n\n  - for duration fields:\n\n    A VCL duration like `10s`, `5m` or `1h`, see [Durations](vcl#vcl-7-durations)\n\nExpressions can be chained using the *and* operator `&&`. For *or* semantics, use several bans.\n\nThe unset *\\<field\\>* is not equal to any string, such that, for a non-existing header, the operators `==` and `~` always evaluate as false, while the operators `!=` and `!~` always evaluate as true, respectively, for any value of *\\<arg\\>*.\n\n### STRING ban_error()\n\nReturns a textual error description of the last [std.ban()](#std-ban) call from the same task or the empty string if there either was no error or no [std.ban()](#std-ban) call.\n\n### TIME now()\n\nReturns the current time. In contrast to the `now` built-in variable, every call returns a new value.\n\n### DURATION timed_call(SUB)\n\nCall the given SUB and return a high precision measurement of the execution time.\n\n## DEPRECATED functions\n\n### INT real2integer(REAL r, INT fallback)\n\n**DEPRECATED**: This function will be removed in a future version of varnish, use [std.integer()](#std-integer) with a *real* argument and the [std.round()](#std-round) function instead, for example:\n\n``` python\nstd.integer(real=std.round(...), fallback=...)\n```\n\nRounds the real *r* to the nearest integer, but round halfway cases away from zero (see `round(3)`). If conversion fails, *fallback* will be returned.\n\nExamples:\n\n``` python\nset req.http.integer = std.real2integer(1140618699.00, 0);\nset req.http.posone = real2integer( 0.5, 0);    # =  1.0\nset req.http.negone = real2integer(-0.5, 0);    # = -1.0\n```\n\n### TIME real2time(REAL r, TIME fallback)\n\n**DEPRECATED**: This function will be removed in a future version of varnish, use [std.time()](#std-time) with a *real* argument and the [std.round()](#std-round) function instead, for example:\n\n``` python\nstd.time(real=std.round(...), fallback=...)\n```\n\nRounds the real *r* to the nearest integer (see [std.real2integer()](#std-real2integer)) and returns the corresponding time when interpreted as a unix epoch. If conversion fails, *fallback* will be returned.\n\nExample:\n\n``` python\nset req.http.time = std.real2time(1140618699.00, now);\n```\n\n### INT time2integer(TIME t, INT fallback)\n\n**DEPRECATED**: This function will be removed in a future version of varnish, use [std.integer()](#std-integer) with a *time* argument instead, for example:\n\n``` python\nstd.integer(time=..., fallback=...)\n```\n\nConverts the time *t* to a integer. If conversion fails, *fallback* will be returned.\n\nExample:\n\n``` python\nset req.http.int = std.time2integer(now, 0);\n```\n\n### REAL time2real(TIME t, REAL fallback)\n\n**DEPRECATED**: This function will be removed in a future version of varnish, use [std.real()](#std-real) with a *time* argument instead, for example:\n\n``` python\nstd.real(time=..., fallback=...)\n```\n\nConverts the time *t* to a real. If conversion fails, *fallback* will be returned.\n\nExample:\n\n``` python\nset req.http.real = std.time2real(now, 1.0);\n```\n\n## SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [VSL](vsl#vsl-7)\n- `fnmatch(3)`\n- `strftime(3)`\n\n## COPYRIGHT\n\n``` python\nCopyright (c) 2010-2017 Varnish Software AS\nAll rights reserved.\n\nAuthor: Poul-Henning Kamp <phk@FreeBSD.org>\n\nSPDX-License-Identifier: BSD-2-Clause\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED.  IN NO EVENT SHALL AUTHOR OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\nOR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\nLIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\nOUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGE.\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vmod_std.html](https://varnish-cache.org/docs/7.4/reference/vmod_std.html)"
- name: VMOD unix - Utilities for Unix domain sockets
  id: reference/vmod_unix
  summary: This VMOD provides information about the credentials of the peer process (user and group of the process owner) that is connected to a Varnish listener via a Unix domain socket, if the platform supports it
  description: "# VMOD unix - Utilities for Unix domain sockets\n\n## SYNOPSIS\n\n``` literal-block\nimport unix [as name] [from \"path\"]\n\nSTRING user()\n\nSTRING group()\n\nINT uid()\n\nINT gid()\n```\n\n## DESCRIPTION\n\nThis VMOD provides information about the credentials of the peer process (user and group of the process owner) that is connected to a Varnish listener via a Unix domain socket, if the platform supports it.\n\nExamples:\n\n``` python\nimport unix;\n\nsub vcl_recv {\n      # Return \"403 Forbidden\" if the connected peer is\n      # not running as the user \"trusteduser\".\n      if (unix.user() != \"trusteduser\") {\n              return( synth(403) );\n      }\n\n      # Require the connected peer to run in the group\n      # \"trustedgroup\".\n      if (unix.group() != \"trustedgroup\") {\n              return( synth(403) );\n      }\n\n      # Require the connected peer to run under a specific numeric\n      # user id.\n      if (unix.uid() != 4711) {\n              return( synth(403) );\n      }\n\n      # Require the connected peer to run under a numeric group id.\n      if (unix.gid() != 815) {\n              return( synth(403) );\n      }\n}\n```\n\nObtaining the peer credentials is possible on a platform that supports one of the following:\n\n- `getpeereid(3)` (such as FreeBSD and other BSD-derived systems)\n- the socket option `SO_PEERCRED` for `getsockopt(2)` (Linux)\n- `getpeerucred(3C)` (SunOS and descendants)\n\nOn SunOS and friends, the `PRIV_PROC_INFO` privilege set is added to the Varnish child process while the VMOD is loaded, see `setppriv(2)`.\n\nOn most platforms, the value returned is the effective user or group that was valid when the peer process initiated the connection.\n\n### STRING user()\n\nReturn the user name of the peer process owner.\n\nRestricted to: `client`, `backend`.\n\n### STRING group()\n\nReturn the group name of the peer process owner.\n\nRestricted to: `client`, `backend`.\n\n### INT uid()\n\nReturn the numeric user id of the peer process owner.\n\nRestricted to: `client`, `backend`.\n\n### INT gid()\n\nReturn the numeric group id of the peer process owner.\n\nRestricted to: `client`, `backend`.\n\n## ERRORS\n\nAll functions in this VMOD are subject to the following constraints:\n\n- None of them may be called in `vcl_init{}` or `vcl_fini{}`. If one of them is called in `vcl_init{}`, then the VCL program will fail to load, with an error message from the VMOD.\n\n- If called on a platform that is not supported, then VCL failure is invoked. An error message is written to the log (with the `VCL_Error` tag), and for all VCL subroutines except for `vcl_synth{}`, control is directed immediately to `vcl_synth{}`, with the response status set to 503 and the reason string set to “VCL failed”.\n\n  If the failure occurs during `vcl_synth{}`, then `vcl_synth{}` is aborted, and the the response line “503 VCL failed” is sent.\n\n- If the current listener is not a Unix domain socket, or if the attempt to read credentials fails, then a `VCL_Error` message is written to the log. The STRING functions ([unix.user()](#unix-user) and [unix.group()](#unix-group)) return `NULL`, while the INT functions ([unix.uid()](#unix-uid) and [unix.gid()](#unix-gid)) return -1.\n\n## SEE ALSO\n\n- [varnishd](varnishd#varnishd-1)\n- [VCL](vcl#vcl-7)\n- `getpeereid(3)`\n- `getsockopt(2)`\n- `getpeerucred(3C)`\n- `setppriv(2)`\n\n## COPYRIGHT\n\n``` python\nThis document is licensed under the same conditions as Varnish itself.\nSee LICENSE for details.\n\nSPDX-License-Identifier: BSD-2-Clause\n\nAuthors: Geoffrey Simmons <geoffrey.simmons@uplex.de>\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vmod_unix.html](https://varnish-cache.org/docs/7.4/reference/vmod_unix.html)"
- name: VMOD vtc - Utility module for varnishtest
  id: reference/vmod_vtc
  summary: The goal for this VMOD is to provide VCL users and VMOD authors means to test corner cases or reach certain conditions with varnishtest
  description: "# VMOD vtc - Utility module for varnishtest\n\n## SYNOPSIS\n\n``` literal-block\nimport vtc [as name] [from \"path\"]\n\nVOID barrier_sync(STRING addr, DURATION timeout=0)\n\nBACKEND no_backend()\n\nSTEVEDORE no_stevedore()\n\nIP no_ip()\n\nVOID panic(STRING)\n\nVOID sleep(DURATION)\n\nVOID workspace_alloc(ENUM, INT size)\n\nBYTES workspace_reserve(ENUM, INT size)\n\nINT workspace_free(ENUM {client, backend, session, thread})\n\nVOID workspace_snapshot(ENUM)\n\nVOID workspace_reset(ENUM)\n\nBOOL workspace_overflowed(ENUM)\n\nVOID workspace_overflow(ENUM)\n\nBLOB workspace_dump(ENUM, ENUM, BYTES off, BYTES len)\n\nINT typesize(STRING)\n\nBLOB proxy_header(ENUM version, IP client, IP server, STRING authority)\n\nVOID vsl(INT vxid, STRING tag, ENUM {c, b} side, STRING s)\n\nVOID vsl_replay(STRING s)\n```\n\n## DESCRIPTION\n\nThe goal for this VMOD is to provide VCL users and VMOD authors means to test corner cases or reach certain conditions with varnishtest.\n\n### VOID barrier_sync(STRING addr, DURATION timeout=0)\n\nWhen writing test cases, the most common pattern is to start a mock server instance, a Varnish instance, and spin up a mock client. Those entities run asynchronously, and others exist like background processes (`process`) or log readers (`logexpect`). While you can synchronize with individual entities and wait for their completion, you must use a barrier if you need to synchronize two or more entities, or wait until a certain point instead of completion.\n\nNot only is it possible to synchronize between test entities, with the `barrier_sync` function you can even synchronize VCL code:\n\n``` python\nsub vcl_recv {\n    # wait for some barrier b1 to complete\n    vtc.barrier_sync(\"${b1_sock}\");\n}\n```\n\nIf the function fails to synchronize with the barrier for some reason, or if it reaches the optional timeout, it fails the VCL transaction.\n\n## MISCELLANEOUS\n\n### BACKEND no_backend()\n\nFails at backend selection.\n\n### STEVEDORE no_stevedore()\n\nFails at storage selection.\n\n### IP no_ip()\n\nReturns a null IP address, not even a bogo_ip.\n\n### VOID panic(STRING)\n\nIt can be useful to crash the child process in order to test the robustness of a VMOD.\n\n### VOID sleep(DURATION)\n\nBlock the current worker thread.\n\n## WORKSPACES\n\nIt can be useful to put a workspace in a given state when testing corner cases like resource exhaustion for a transaction, especially for VMOD development. All functions available allow to pick which workspace you need to tamper with, available values are `client`, `backend`, `session` and `thread`.\n\n### VOID workspace_alloc(ENUM, INT size)\n\n``` python\nVOID workspace_alloc(\n   ENUM {client, backend, session, thread},\n   INT size\n)\n```\n\nAllocate and zero out memory from a workspace. A negative size will allocate as much as needed to leave that many bytes free. The actual allocation size may be higher to comply with memory alignment requirements of the CPU architecture. A failed allocation fails the transaction.\n\n### BYTES workspace_reserve(ENUM, INT size)\n\n``` python\nBYTES workspace_reserve(\n   ENUM {client, backend, session, thread},\n   INT size\n)\n```\n\nAttempt to reserve *size* bytes, zero out that memory and release the reservation right away. Return the size of the reservation.\n\nSee [vtc.workspace_alloc()](#vtc-workspace-alloc) for semantics of the *size* argument.\n\n### INT workspace_free(ENUM {client, backend, session, thread})\n\nFind how much unallocated space there is left in a workspace.\n\n### VOID workspace_snapshot(ENUM)\n\n``` python\nVOID workspace_snapshot(ENUM {client, backend, session, thread})\n```\n\nSnapshot a workspace. Only one snapshot may be active at a time and each VCL can save only one snapshot, so concurrent tasks requiring snapshots are not supported.\n\n### VOID workspace_reset(ENUM)\n\n``` python\nVOID workspace_reset(ENUM {client, backend, session, thread})\n```\n\nReset to the previous snapshot of a workspace, it must be the same workspace too.\n\n### BOOL workspace_overflowed(ENUM)\n\n``` python\nBOOL workspace_overflowed(ENUM {client, backend, session, thread})\n```\n\nFind whether the workspace overflow mark is set or not.\n\n### VOID workspace_overflow(ENUM)\n\n``` python\nVOID workspace_overflow(ENUM {client, backend, session, thread})\n```\n\nMark a workspace as overflowed.\n\n### BLOB workspace_dump(ENUM, ENUM, BYTES off, BYTES len)\n\n``` python\nBLOB workspace_dump(\n   ENUM {client, backend, session, thread},\n   ENUM {s, f, r},\n   BYTES off=0,\n   BYTES len=64\n)\n```\n\nReturn data from a workspace’s `s`, `f`, or `r` pointer as a blob. Data is copied onto the primary workspace to avoid it being subsequently overwritten.\n\nThe maximum *len* is 1KB.\n\n### INT typesize(STRING)\n\nReturns the size in bytes of a collection of C-datatypes:\n\n- `'p'`: pointer\n- `'i'`: `int`\n- `'d'`: `double`\n- `'f'`: `float`\n- `'l'`: `long`\n- `'s'`: `short`\n- `'z'`: `size_t`\n- `'o'`: `off_t`\n- `'j'`: `intmax_t`\n\nThis can be useful for VMOD authors in conjunction with workspace operations.\n\n### BLOB proxy_header(ENUM version, IP client, IP server, STRING authority)\n\n``` python\nBLOB proxy_header(\n   ENUM {v1, v2} version,\n   IP client,\n   IP server,\n   STRING authority=0\n)\n```\n\nFormat a proxy header of the given version `v1` or `v2` and addresses (The VCL IP type also contains the port number).\n\nOptionally also send an authority TLV with version `v2` (ignored for version `v1`).\n\nCandidate for moving into vmod_proxy, but there were concerns about the interface design\n\n## VSL\n\nThese functions allow to generate arbitrary log entries to test the Varnish Shared Log (VSL) implementation and readers like varnishlog.\n\n### VOID vsl(INT vxid, STRING tag, ENUM {c, b} side, STRING s)\n\nCall `VSLs()` with the given parameters.\n\nThe argument order is chosen to match VSL output.\n\nA VCL error is triggered if `tag` can not be resolved at runtime or if vxid is out of bounds.\n\n### VOID vsl_replay(STRING s)\n\nReplay literal log lines.\n\nThe parser accepts the output generated by `varnishlog -g raw` and varnishtest log `vsl|` lines.\n\nUnparsable lines are silently ignored.\n\n## SEE ALSO\n\n- [VTC](vtc#vtc-7)\n- [VCL](vcl#vcl-7)\n\n## COPYRIGHT\n\n``` python\nCopyright (c) 2017 Varnish Software AS\nAll rights reserved.\n\nAuthor: Dridi Boukelmoune <dridi.boukelmoune@gmail.com>\n\nSPDX-License-Identifier: BSD-2-Clause\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED.  IN NO EVENT SHALL AUTHOR OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\nOR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\nLIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\nOUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGE.\n\nNB: Default to strict $ABI handling, so that path is tested in vmodtool.py\n```\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vmod_vtc.html](https://varnish-cache.org/docs/7.4/reference/vmod_vtc.html)"
- name: VSL
  id: reference/vsl
  summary: This document describes the format and content of all the Varnish shared memory logging tags
  description: "# VSL\n\n## Varnish Shared Memory Logging\n\nManual section:  \n7\n\n### OVERVIEW\n\nThis document describes the format and content of all the Varnish shared memory logging tags. These tags are used by the varnishlog(1), varnishtop(1), etc. logging tools supplied with Varnish.\n\n#### VSL tags\n\nBackendClose - Backend connection closed  \nLogged when a backend connection is closed.\n\nThe format is:\n\n``` python\n%d %s %s [ %s ]\n|  |  |    |\n|  |  |    +- Optional reason\n|  |  +------ \"close\" or \"recycle\"\n|  +--------- Backend display name\n+------------ Connection file descriptor\n```\n\nBackendOpen - Backend connection opened  \nLogged when a new backend connection is opened.\n\nThe format is:\n\n``` python\n%d %s %s %s %s %s %s\n|  |  |  |  |  |  |\n|  |  |  |  |  |  +- \"connect\" or \"reuse\"\n|  |  |  |  |  +---- Local port\n|  |  |  |  +------- Local address\n|  |  |  +---------- Remote port\n|  |  +------------- Remote address\n|  +---------------- Backend display name\n+------------------- Connection file descriptor\n```\n\nBackend_health - Backend health check  \nThe result of a backend health probe.\n\nThe format is:\n\n``` python\n%s %s %s %s %u %u %u %f %f %s\n|  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  +- Probe HTTP response / error information\n|  |  |  |  |  |  |  |  +---- Average response time\n|  |  |  |  |  |  |  +------- Response time\n|  |  |  |  |  |  +---------- Probe window size\n|  |  |  |  |  +------------- Probe threshold level\n|  |  |  |  +---------------- Number of good probes in window\n|  |  |  +------------------- Probe window bits\n|  |  +---------------------- \"healthy\" or \"sick\"\n|  +------------------------- \"Back\", \"Still\" or \"Went\"\n+---------------------------- Backend name\n```\n\nProbe window bits are:\n\n``` python\n'-': Could not connect\n'4': Good IPv4\n'6': Good IPv6\n'U': Good UNIX\n'x': Error Xmit\n'X': Good Xmit\n'r': Error Recv\n'R': Good Recv\n'H': Happy\n```\n\nWhen the backend is just created, the window bits for health check slots that haven’t run yet appear as ‘-’ like failures to connect.\n\nBegin - Marks the start of a VXID  \nThe first record of a VXID transaction.\n\nThe format is:\n\n``` python\n%s %d %s [%u]\n|  |  |   |\n|  |  |   +- Task sub-level\n|  |  +----- Reason\n|  +-------- Parent vxid\n+----------- Type (\"sess\", \"req\" or \"bereq\")\n```\n\nBereqAcct - Backend request accounting  \nContains byte counters from backend request processing.\n\nThe format is:\n\n``` python\n%d %d %d %d %d %d\n|  |  |  |  |  |\n|  |  |  |  |  +- Total bytes received\n|  |  |  |  +---- Body bytes received\n|  |  |  +------- Header bytes received\n|  |  +---------- Total bytes transmitted\n|  +------------- Body bytes transmitted\n+---------------- Header bytes transmitted\n```\n\nBereqHeader - Backend request header  \nHTTP header contents.\n\nThe format is:\n\n``` python\n%s: %s\n|   |\n|   +- Header value\n+----- Header name\n```\n\nNOTE: HTTP header fields are free form records and not strictly made of 2 fields. Accessing a specific header with the prefix notation helps treating the header value as a single string.\n\nBereqMethod - Backend request method  \nThe HTTP request method used.\n\nBereqProtocol - Backend request protocol  \nThe HTTP protocol version information.\n\nBereqURL - Backend request URL  \nThe HTTP request URL.\n\nBereqUnset - Backend request unset header  \nHTTP header contents.\n\nThe format is:\n\n``` python\n%s: %s\n|   |\n|   +- Header value\n+----- Header name\n```\n\nNOTE: HTTP header fields are free form records and not strictly made of 2 fields. Accessing a specific header with the prefix notation helps treating the header value as a single string.\n\nBerespHeader - Backend response header  \nHTTP header contents.\n\nThe format is:\n\n``` python\n%s: %s\n|   |\n|   +- Header value\n+----- Header name\n```\n\nNOTE: HTTP header fields are free form records and not strictly made of 2 fields. Accessing a specific header with the prefix notation helps treating the header value as a single string.\n\nBerespProtocol - Backend response protocol  \nThe HTTP protocol version information.\n\nBerespReason - Backend response reason  \nThe HTTP response reason string.\n\nBerespStatus - Backend response status  \nThe HTTP response status code.\n\nBerespUnset - Backend response unset header  \nHTTP header contents.\n\nThe format is:\n\n``` python\n%s: %s\n|   |\n|   +- Header value\n+----- Header name\n```\n\nNOTE: HTTP header fields are free form records and not strictly made of 2 fields. Accessing a specific header with the prefix notation helps treating the header value as a single string.\n\nBogoHeader - Bogus HTTP received  \nContains the first 20 characters of received HTTP headers we could not make sense of. Applies to both req.http and beresp.http.\n\nCLI - CLI communication  \nCLI communication between varnishd master and child process.\n\nDebug - Debug messages  \nDebug messages can normally be ignored, but are sometimes helpful during trouble-shooting. Most debug messages must be explicitly enabled with parameters.\n\nDebug messages may be added, changed or removed without prior notice and shouldn’t be considered stable.\n\nNB: This log record is masked by default.\n\nESI_xmlerror - ESI parser error or warning message  \nAn error or warning was generated during parsing of an ESI object. The log record describes the problem encountered.\n\nEnd - Marks the end of a VXID  \nThe last record of a VXID transaction.\n\nError - Error messages  \nError messages are stuff you probably want to know.\n\nExpBan - Object evicted due to ban  \nLogs the VXID when an object is banned.\n\nExpKill - Object expiry event  \nLogs events related to object expiry. The events are:\n\nEXP_Rearm  \nLogged when the expiry time of an object changes.\n\nEXP_Inbox  \nLogged when the expiry thread picks an object from the inbox for processing.\n\nEXP_Kill  \nLogged when the expiry thread kills an object from the inbox.\n\nEXP_When  \nLogged when the expiry thread moves an object on the binheap.\n\nEXP_Expired  \nLogged when the expiry thread expires an object.\n\nLRU_Cand  \nLogged when an object is evaluated for LRU force expiry.\n\nLRU  \nLogged when an object is force expired due to LRU.\n\nLRU_Fail  \nLogged when no suitable candidate object is found for LRU force expiry.\n\nThe format is:\n\n``` python\nEXP_Rearm p=%p E=%f e=%f f=0x%x\nEXP_Inbox p=%p e=%f f=0x%x\nEXP_Kill p=%p e=%f f=0x%x\nEXP_When p=%p e=%f f=0x%x\nEXP_Expired x=%u t=%f\nLRU_Cand p=%p f=0x%x r=%d\nLRU x=%u\nLRU_Fail\n\nLegend:\np=%p         Objcore pointer\nt=%f         Remaining TTL (s)\ne=%f         Expiry time (unix epoch)\nE=%f         Old expiry time (unix epoch)\nf=0x%x       Objcore flags\nr=%d         Objcore refcount\nx=%u         Object VXID\n```\n\nFetchError - Error while fetching object  \nLogs the error message of a failed fetch operation.\n\nError messages should be self-explanatory, yet the http connection (HTC) class of errors is reported with these symbols:\n\n- junk (-5): Received unexpected data\n- close (-4): Connection closed\n- timeout (-3): Timed out\n- overflow (-2): Buffer/workspace too small\n- eof (-1): Unexpected end of input\n- empty (0): Empty response\n- more (1): More data required\n- complete (2): Data complete (no error)\n- idle (3): Connection was closed while idle\n\nNotice that some HTC errors are never emitted.\n\nFetch_Body - Body fetched from backend  \nReady to fetch body from backend.\n\nThe format is:\n\n``` python\n%d %s %s\n|  |  |\n|  |  +---- 'stream' or '-'\n|  +------- Text description of body fetch mode\n+---------- Body fetch mode\n```\n\nFilters - Body filters  \nList of filters applied to the body\n\nGzip - G(un)zip performed on object  \nA Gzip record is emitted for each instance of gzip or gunzip work performed. Worst case, an ESI transaction stored in gzip’ed objects but delivered gunziped, will run into many of these.\n\nThe format is:\n\n``` python\n%c %c %c %d %d %d %d %d\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  +- Bit length of compressed data\n|  |  |  |  |  |  +---- Bit location of 'last' bit\n|  |  |  |  |  +------- Bit location of first deflate block\n|  |  |  |  +---------- Bytes output\n|  |  |  +------------- Bytes input\n|  |  +---------------- 'E': ESI, '-': Plain object\n|  +------------------- 'F': Fetch, 'D': Deliver\n+---------------------- 'G': Gzip, 'U': Gunzip, 'u': Gunzip-test\n```\n\nExamples:\n\n``` python\nU F E 182 159 80 80 1392\nG F E 159 173 80 1304 1314\n```\n\nH2RxBody - Received HTTP2 frame body  \nBinary data\n\nH2RxHdr - Received HTTP2 frame header  \nBinary data\n\nH2TxBody - Transmitted HTTP2 frame body  \nBinary data\n\nH2TxHdr - Transmitted HTTP2 frame header  \nBinary data\n\nHash - Value added to hash  \nThis value was added to the object lookup hash.\n\nNB: This log record is masked by default.\n\nHit - Hit object in cache  \nObject looked up in cache.\n\nThe format is:\n\n``` python\n%u %f %f %f [%u [%u]]\n|  |  |  |   |   |\n|  |  |  |   |   +- Content length\n|  |  |  |   +----- Fetched so far\n|  |  |  +--------- Keep period\n|  |  +------------ Grace period\n|  +--------------- Remaining TTL\n+------------------ VXID of the object\n```\n\nHitMiss - Hit for miss object in cache.  \nHit-for-miss object looked up in cache.\n\nThe format is:\n\n``` python\n%u %f\n|  |\n|  +- Remaining TTL\n+---- VXID of the object\n```\n\nHitPass - Hit for pass object in cache.  \nHit-for-pass object looked up in cache.\n\nThe format is:\n\n``` python\n%u %f\n|  |\n|  +- Remaining TTL\n+---- VXID of the object\n```\n\nHttpGarbage - Unparseable HTTP request  \nLogs the content of unparseable HTTP requests.\n\nLength - Size of object body  \nLogs the size of a fetch object body.\n\nLink - Links to a child VXID  \nLinks this VXID to any child VXID it initiates.\n\nThe format is:\n\n``` python\n%s %d %s [%u]\n|  |  |   |\n|  |  |   +- Child task sub-level\n|  |  +----- Reason\n|  +-------- Child vxid\n+----------- Child type (\"sess\", \"req\" or \"bereq\")\n```\n\nLostHeader - Failed attempt to set HTTP header  \nLogs the header name of a failed HTTP header operation due to resource exhaustion or configured limits.\n\nNotice - Informational messages about request handling  \nInformational log messages on events occurred during request handling.\n\nThe format is:\n\n``` python\n%s: %s\n|   |\n|   +- Short description of the notice message\n+----- Manual page containing the detailed description\n```\n\nSee the NOTICE MESSAGES section below or the individual VMOD manual pages for detailed information of notice messages.\n\nObjHeader - Object header  \nHTTP header contents.\n\nThe format is:\n\n``` python\n%s: %s\n|   |\n|   +- Header value\n+----- Header name\n```\n\nNOTE: HTTP header fields are free form records and not strictly made of 2 fields. Accessing a specific header with the prefix notation helps treating the header value as a single string.\n\nObjProtocol - Object protocol  \nThe HTTP protocol version information.\n\nObjReason - Object reason  \nThe HTTP response reason string.\n\nObjStatus - Object status  \nThe HTTP response status code.\n\nObjUnset - Object unset header  \nHTTP header contents.\n\nThe format is:\n\n``` python\n%s: %s\n|   |\n|   +- Header value\n+----- Header name\n```\n\nNOTE: HTTP header fields are free form records and not strictly made of 2 fields. Accessing a specific header with the prefix notation helps treating the header value as a single string.\n\nPipeAcct - Pipe byte counts  \nContains byte counters for pipe sessions.\n\nThe format is:\n\n``` python\n%d %d %d %d\n|  |  |  |\n|  |  |  +------- Piped bytes to client\n|  |  +---------- Piped bytes from client\n|  +------------- Backend request headers\n+---------------- Client request headers\n```\n\nProxy - PROXY protocol information  \nPROXY protocol information.\n\nThe format is:\n\n``` python\n%d %s %d %s %d\n|  |  |  |  |\n|  |  |  |  +- server port\n|  |  |  +---- server ip\n|  |  +------- client port\n|  +---------- client ip\n+------------- PROXY protocol version\n\nAll fields are \"local\" for PROXY local connections (command 0x0)\n```\n\nProxyGarbage - Unparseable PROXY request  \nA PROXY protocol header was unparseable.\n\nReqAcct - Request handling byte counts  \nContains byte counts for the request handling. The body bytes count includes transmission overhead (ie: chunked encoding). ESI sub-requests show the body bytes this ESI fragment including any subfragments contributed to the top level request. The format is:\n\n``` python\n%d %d %d %d %d %d\n|  |  |  |  |  |\n|  |  |  |  |  +- Total bytes transmitted\n|  |  |  |  +---- Body bytes transmitted\n|  |  |  +------- Header bytes transmitted\n|  |  +---------- Total bytes received\n|  +------------- Body bytes received\n+---------------- Header bytes received\n```\n\nReqHeader - Client request header  \nHTTP header contents.\n\nThe format is:\n\n``` python\n%s: %s\n|   |\n|   +- Header value\n+----- Header name\n```\n\nNOTE: HTTP header fields are free form records and not strictly made of 2 fields. Accessing a specific header with the prefix notation helps treating the header value as a single string.\n\nReqMethod - Client request method  \nThe HTTP request method used.\n\nReqProtocol - Client request protocol  \nThe HTTP protocol version information.\n\nReqStart - Client request start  \nStart of request processing. Logs the client address, port number and listener endpoint name (from the -a command-line argument).\n\nThe format is:\n\n``` python\n%s %s %s\n|  |  |\n|  |  +-- Listener name (from -a)\n|  +----- Client Port number (0 for Unix domain sockets)\n+-------- Client IP4/6 address (0.0.0.0 for UDS)\n```\n\nReqURL - Client request URL  \nThe HTTP request URL.\n\nReqUnset - Client request unset header  \nHTTP header contents.\n\nThe format is:\n\n``` python\n%s: %s\n|   |\n|   +- Header value\n+----- Header name\n```\n\nNOTE: HTTP header fields are free form records and not strictly made of 2 fields. Accessing a specific header with the prefix notation helps treating the header value as a single string.\n\nRespHeader - Client response header  \nHTTP header contents.\n\nThe format is:\n\n``` python\n%s: %s\n|   |\n|   +- Header value\n+----- Header name\n```\n\nNOTE: HTTP header fields are free form records and not strictly made of 2 fields. Accessing a specific header with the prefix notation helps treating the header value as a single string.\n\nRespProtocol - Client response protocol  \nThe HTTP protocol version information.\n\nRespReason - Client response reason  \nThe HTTP response reason string.\n\nRespStatus - Client response status  \nThe HTTP response status code.\n\nRespUnset - Client response unset header  \nHTTP header contents.\n\nThe format is:\n\n``` python\n%s: %s\n|   |\n|   +- Header value\n+----- Header name\n```\n\nNOTE: HTTP header fields are free form records and not strictly made of 2 fields. Accessing a specific header with the prefix notation helps treating the header value as a single string.\n\nSessClose - Client connection closed  \nSessClose is the last record for any client connection.\n\nThe format is:\n\n``` python\n%s %f\n|  |\n|  +- How long the session was open\n+---- Why the connection closed\n```\n\nSessError - Client connection accept failed  \nAccepting a client connection has failed.\n\nThe format is:\n\n``` python\n%s %s %s %d %d %s\n|  |  |  |  |  |\n|  |  |  |  |  +- Detailed error message\n|  |  |  |  +---- Error Number (errno) from accept(2)\n|  |  |  +------- File descriptor number\n|  |  +---------- Local TCP port / 0 for UDS\n|  +------------- Local IPv4/6 address / 0.0.0.0 for UDS\n+---------------- Socket name (from -a argument)\n```\n\nSessOpen - Client connection opened  \nThe first record for a client connection, with the socket-endpoints of the connection.\n\nThe format is:\n\n``` python\n%s %d %s %s %s %f %d\n|  |  |  |  |  |  |\n|  |  |  |  |  |  +- File descriptor number\n|  |  |  |  |  +---- Session start time (unix epoch)\n|  |  |  |  +------- Local TCP port / 0 for UDS\n|  |  |  +---------- Local IPv4/6 address / 0.0.0.0 for UDS\n|  |  +------------- Socket name (from -a argument)\n|  +---------------- Remote TCP port / 0 for UDS\n+------------------- Remote IPv4/6 address / 0.0.0.0 for UDS\n```\n\nStorage - Where object is stored  \nType and name of the storage backend the object is stored in.\n\nThe format is:\n\n``` python\n%s %s\n|  |\n|  +- Name of storage backend\n+---- Type (\"malloc\", \"file\", \"persistent\" etc.)\n```\n\nTTL - TTL set on object  \nA TTL record is emitted whenever the ttl, grace or keep values for an object is set as well as whether the object is cacheable or not.\n\nThe format is:\n\n``` python\n%s %d %d %d %d [ %d %d %u %u ] %s\n|  |  |  |  |    |  |  |  |    |\n|  |  |  |  |    |  |  |  |    +- \"cacheable\" or \"uncacheable\"\n|  |  |  |  |    |  |  |  +------ Max-Age from Cache-Control header\n|  |  |  |  |    |  |  +--------- Expires header\n|  |  |  |  |    |  +------------ Date header\n|  |  |  |  |    +--------------- Age (incl Age: header value)\n|  |  |  |  +-------------------- Reference time for TTL\n|  |  |  +----------------------- Keep\n|  |  +-------------------------- Grace\n|  +----------------------------- TTL\n+-------------------------------- \"RFC\", \"VCL\" or \"HFP\"\n```\n\nThe four optional fields are only present in “RFC” headers.\n\nExamples:\n\n``` python\nRFC 60 10 -1 1312966109 1312966109 1312966109 0 60 cacheable\nVCL 120 10 0 1312966111 uncacheable\nHFP 2 0 0 1312966113 uncacheable\n```\n\nTimestamp - Timing information  \nContains timing information for the Varnish worker threads.\n\nTime stamps are issued by Varnish on certain events, and show the absolute time of the event, the time spent since the start of the work unit, and the time spent since the last timestamp was logged. See the TIMESTAMPS section below for information about the individual time stamps.\n\nThe format is:\n\n``` python\n%s: %f %f %f\n|   |  |  |\n|   |  |  +- Time since last timestamp\n|   |  +---- Time since start of work unit\n|   +------- Absolute time of event\n+----------- Event label\n```\n\nVCL_Error - VCL execution error message  \nLogs error messages generated during VCL execution.\n\nVCL_Log - Log statement from VCL  \nUser generated log messages insert from VCL through std.log()\n\nVCL_acl - VCL ACL check results  \nACLs with the `+log` flag emits this record with the result.\n\nThe format is:\n\n``` python\n%s %s [%s [fixed: %s]]\n|  |   |          |\n|  |   |          +- Fixed entry (see acl +pedantic flag)\n|  |   +------------ Matching entry (only for MATCH)\n|  +---------------- Name of the ACL\n+-------------------- MATCH or NO_MATCH\n```\n\nMATCH denotes an ACL match NO_MATCH denotes that a checked ACL has not matched\n\nVCL_call - VCL method called  \nLogs the VCL method name when a VCL method is called.\n\nVCL_return - VCL method return value  \nLogs the VCL method terminating statement.\n\nVCL_trace - VCL trace data  \nLogs VCL execution trace data.\n\nThe format is:\n\n``` python\n%s %u %u.%u.%u\n|  |  |  |  |\n|  |  |  |  +- VCL program line position\n|  |  |  +---- VCL program line number\n|  |  +------- VCL program source index\n|  +---------- VCL trace point index\n+------------- VCL configname\n```\n\nNB: This log record is masked by default.\n\nVCL_use - VCL in use  \nRecords the name of the VCL being used.\n\nThe format is:\n\n``` python\n%s [ %s %s ]\n|    |  |\n|    |  +- Name of label used to find it\n|    +---- \"via\"\n+--------- Name of VCL put in use\n```\n\nVSL - VSL API warnings and error message  \nWarnings and error messages generated by the VSL API while reading the shared memory log.\n\nVdpAcct - Deliver filter accounting  \nContains name of VDP and statistics.\n\nThe format is:\n\n``` python\n%s %d %d\n|  |  |\n|  |  +- Total bytes produced\n|  +---- Number of calls made\n+------- Name of filter\n```\n\nNB: This log record is masked by default.\n\nVfpAcct - Fetch filter accounting  \nContains name of VFP and statistics.\n\nThe format is:\n\n``` python\n%s %d %d\n|  |  |\n|  |  +- Total bytes produced\n|  +---- Number of calls made\n+------- Name of filter\n```\n\nNB: This log record is masked by default.\n\nWitness - Lock order witness records  \nDiagnostic recording of locking order.\n\nWorkThread - Logs thread start/stop events  \nLogs worker thread creation and termination events.\n\nThe format is:\n\n``` python\n%p %s\n|  |\n|  +- [start|end]\n+---- Worker struct pointer\n```\n\nNB: This log record is masked by default.\n\n### TIMESTAMPS\n\nTimestamps are inserted in the log on completing certain events during the worker thread’s task handling. The timestamps has a label showing which event was completed. The reported fields show the absolute time of the event, the time spent since the start of the task and the time spent since the last timestamp was logged.\n\nThe timestamps logged automatically by Varnish are inserted after completing events that are expected to have delays (e.g. network IO or spending time on a waitinglist). Timestamps can also be inserted from VCL using the std.timestamp() function. If one is doing time consuming tasks in the VCL configuration, it’s a good idea to log a timestamp after completing that task. This keeps the timing information in subsequent timestamps from including the time spent on the VCL event.\n\n#### Request handling timestamps\n\nStart  \nThe start of request processing (first byte received or restart).\n\nReq  \nComplete client request received.\n\nReqBody  \nClient request body processed (discarded, cached or passed to the backend).\n\nWaitinglist  \nCame off waitinglist.\n\nFetch  \nFetch processing finished (completely fetched or ready for streaming).\n\nProcess  \nProcessing finished, ready to deliver the client response.\n\nResp  \nDelivery of response to the client finished.\n\nRestart  \nClient request is being restarted.\n\n#### Pipe handling timestamps\n\nThe following timestamps are client timestamps specific to pipe transactions:\n\nPipe  \nOpened a pipe to the backend and forwarded the request.\n\nPipeSess  \nThe pipe session has finished.\n\nThe following timestamps change meaning in a pipe transaction:\n\nProcess  \nProcessing finished, ready to begin the pipe delivery.\n\n#### Backend fetch timestamps\n\nStart  \nStart of the backend fetch processing.\n\nFetch  \nCame off vcl_backend_fetch ready to send the backend request.\n\nConnected  \nSuccessfully established a backend connection, or selected a recycled connection from the pool.\n\nBereq  \nBackend request sent.\n\nBeresp  \nBackend response headers received.\n\nProcess  \nProcessing finished, ready to fetch the response body.\n\nBerespBody  \nBackend response body received.\n\nRetry  \nBackend request is being retried.\n\nError  \nBackend request failed to vcl_backend_error.\n\n### NOTICE MESSAGES\n\nNotice messages contain informational messages about the handling of a request. These can be exceptional circumstances encountered that causes deviation from the normal handling. The messages are prefixed with `vsl:` for core Varnish generated messages, and VMOD authors are encouraged to use `vmod_<name>:` for their own notice messages. This matches the name of the manual page where detailed descriptions of notice messages are expected.\n\nThe core messages are described below.\n\n#### Conditional fetch wait for streaming object\n\nThe backend answered 304 Not Modified on a conditional fetch using an object that has not yet been fully fetched as the stale template object. This can only happen when the TTL of the object is less than the time it takes to fetch it. The fetch is halted until the stale object is fully fetched, upon which the new object is created as normal. While waiting, any grace time on the stale object will be in effect.\n\n#### High number of variants\n\nObjects are primarily looked up from an index using the hash key computed from the `hash_data()` VCL function. When variants are involved, that is to say when a backend response was stored with a `Vary` header, a secondary lookup is performed but it is not indexed. As the number of variants for a given key increases, this can slow cache lookups down, and since this happens under a lock, this can greatly increase lock contention, even more so for frequently requested objects. Variants should therefore be used sparingly on cacheable responses, but since they can originate from either VCL or origin servers, this notice should help identify problematic resources.\n\n### HISTORY\n\nThis document was initially written by Poul-Henning Kamp, and later updated by Martin Blix Grydeland.\n\n### SEE ALSO\n\n- [varnishhist](varnishhist#varnishhist-1)\n- [varnishlog](varnishlog#varnishlog-1)\n- [varnishncsa](varnishncsa#varnishncsa-1)\n- [varnishtop](varnishtop#varnishtop-1)\n\n### COPYRIGHT\n\nThis document is licensed under the same licence as Varnish itself. See LICENCE for details.\n\n- Copyright (c) 2006 Verdens Gang AS\n- Copyright (c) 2006-2015 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vsl.html](https://varnish-cache.org/docs/7.4/reference/vsl.html)"
- name: vsl-query
  id: reference/vsl-query
  summary: The Varnish VSL Query Expressions extracts transactions from the Varnish shared memory log, and perform queries on the transactions before reporting matches
  description: "# vsl-query\n\n## Varnish VSL Query Expressions\n\nManual section:  \n7\n\n### OVERVIEW\n\nThe Varnish VSL Query Expressions extracts transactions from the Varnish shared memory log, and perform queries on the transactions before reporting matches.\n\nA transaction is a set of log lines that belongs together, e.g. a client request or a backend request. The API monitors the log, and collects all log records that make up a transaction before reporting on that transaction. Transactions can also be grouped, meaning backend transactions are reported together with the client transaction that initiated it.\n\nA query is run on a group of transactions. A query expression is true if there is a log record within the group that satisfies the condition. It is false only if none of the log records satisfies the condition. Query expressions can be combined using boolean functions. In addition to log records, it is possible to query transaction ids (vxid) in query.\n\n### GROUPING\n\nWhen grouping transactions, there is a hierarchy structure showing which transaction initiated what. The level increases by one on an ‘initiated by’ relation, so for example a backend transaction will have one higher level than the client transaction that initiated it on a cache miss. Request restart transactions don’t get their level increased to make it predictable.\n\nLevels start counting at 1, except when using raw where it will always be 0.\n\nThe grouping modes are:\n\n- `session`\n\n  All transactions initiated by a client connection are reported together. Client connections are open ended when using HTTP keep-alives, so it is undefined when the session will be reported. If the transaction timeout period is exceeded an incomplete session will be reported. Non-transactional data (vxid == 0) is not reported.\n\n- `request`\n\n  Transactions are grouped by request, where the set will include the request itself as well as any backend requests or ESI-subrequests. Session data and non-transactional data (vxid == 0) is not reported.\n\n- `vxid`\n\n  Transactions are not grouped, so each vxid is reported in its entirety. Sessions, requests, ESI-requests and backend requests are all reported individually. Non-transactional data is not reported (vxid == 0). This is the default.\n\n- `raw`\n\n  Every log record will make up a transaction of its own. All data, including non-transactional data will be reported.\n\n#### Transaction Hierarchy\n\nExample transaction hierarchy using request grouping mode\n\n``` python\nLvl 1: Client request (cache miss)\n  Lvl 2: Backend request\n  Lvl 2: ESI subrequest (cache miss)\n    Lvl 3: Backend request\n    Lvl 3: Backend request (VCL restart)\n    Lvl 3: ESI subrequest (cache miss)\n      Lvl 4: Backend request\n  Lvl 2: ESI subrequest (cache hit)\n```\n\n### MEMORY USAGE\n\nThe API will use pointers to shared memory log data as long as possible to keep memory usage at a minimum. But as the shared memory log is a ring buffer, data will get overwritten eventually, so the API creates local copies of referenced log data when varnishd comes close to overwriting still unreported content.\n\nThis process avoids loss of log data in many scenarios, but it is not failsafe: Overruns where varnishd “overtakes” the log reader process in the ring buffer can still happen when API clients cannot keep up reading and/or copying, for instance due to output blocking.\n\nThough being unrelated to grouping in principle, copying of log data is particularly relevant for session grouping together with long lasting client connections - for this grouping, the logging API client process is likely to consume relevant amounts of memory. As the vxid grouping also logs (potentially long lasting) sessions, it is also likely to require memory for copies of log entries, but far less than session grouping.\n\n### QUERY LANGUAGE\n\nA query expression consists of record selection criteria, and optionally an operator and a value to match against the selected records.\n\n``` python\n<record selection criteria> <operator> <operand>\n```\n\nAdditionally, a query expression can occur on the transaction itself rather than log records belonging to the transaction.\n\n``` python\nvxid <numerical operator> <integer>\n```\n\nA `vxid` query allows you to directly target a specific transacion, whose id can be obtained from an `X-Varnish` HTTP header, the default “guru meditation” error page, or `Begin` and `Link` log records.\n\nA query must fit on a single line, but it is possible to pass multiple queries at once, one query per line. Empty lines are ignored, and the list of queries is treated as if the ‘or’ operator was used to combine them.\n\nFor example this list of queries:\n\n``` python\n# catch varnish errors\n*Error\n\n# catch backend errors\nBerespStatus >= 500\n```\n\nis identical to this query:\n\n``` python\n(*Error) or (BerespStatus >= 500)\n```\n\nComments can be used and will be ignored, they start with the `'#'` character, which may be more useful when the query is read from a file.\n\nFor very long queries that couldn’t easily be split into multiple queries it is possible to break them into multiple lines with a backslash preceding an end of line.\n\nFor example this query:\n\n``` python\nBerespStatus >= 500\n```\n\nis identical to this query:\n\n``` python\nBerespStatus \\\n>= \\\n500\n```\n\nA backslash-newline sequence doesn’t continue a comment on the next line and isn’t allowed in a quoted string.\n\n#### Record selection criteria\n\nThe record selection criteria determines what kind records from the transaction group the expression applies to. Syntax:\n\n``` python\n{level}taglist:record-prefix[field]\n```\n\nTaglist is mandatory, the other components are optional.\n\nThe level limits the expression to a transaction at that level. If left unspecified, the expression is applied to transactions at all levels. Level is a positive integer or zero. If level is followed by a ‘+’ character, it expresses greater than or equal. If level is followed by a ‘-’, it expresses less than or equal.\n\nThe taglist is a comma-separated list of VSL record tags that this expression should be checked against. Each list element can be a tag name or a tag glob. Globs allow a ‘\\*’ either in the beginning of the name or at the end, and will select all tags that match either the prefix or subscript. A single ‘\\*’ will select all tags.\n\nThe record prefix will further limit the matches to those records that has this prefix as their first part of the record content followed by a colon. The part of the log record matched against will then be limited to what follows the prefix and colon. This is useful when matching against specific HTTP headers. The record prefix matching is done case insensitive.\n\nThe field will, if present, treat the log record as a white space separated list of fields, and only the nth part of the record will be matched against. Fields start counting at 1.\n\nAn expression using only a record selection criteria will be true if there is any record in the transaction group that is selected by the criteria.\n\n#### Operators\n\nThe following matching operators are available:\n\n- == != \\< \\<= \\> \\>=\n\n  Numerical comparison. The record contents will be converted to either an integer or a float before comparison, depending on the type of the operand.\n\n- eq ne\n\n  String comparison. ‘eq’ tests string equality, ‘ne’ tests for not equality.\n\n- ~ !~\n\n  Regular expression matching. ‘~’ is a positive match, ‘!~’ is a non-match.\n\n#### Operand\n\nThe operand is the value the selected records will be matched against.\n\nAn operand can be quoted or unquoted. Quotes can be either single or double quotes, and for quoted operands a backslash can be used to escape the quotes.\n\nUnquoted operands can only consist of the following characters:\n\n``` python\na-z A-Z 0-9 + - _ . *\n```\n\nThe following types of operands are available:\n\n- Integer\n\n  A number without any fractional part, valid for the numerical comparison operators. The integer type is used when the operand does not contain any period (.) nor exponent (e) characters. However if the record evaluates as a float, only its integral part is used for the comparison.\n\n- Float\n\n  A number with a fractional part, valid for the numerical comparison operators. The float type is used when the operand does contain a period (.) or exponent (e) character.\n\n- String\n\n  A sequence of characters, valid for the string equality operators.\n\n- Regular expression\n\n  A PCRE2 regular expression. Valid for the regular expression operators.\n\n#### Boolean functions\n\nQuery expressions can be linked together using boolean functions. The following are available, in decreasing precedence:\n\n- not \\<expr\\>\n\n  Inverts the result of \\<expr\\>\n\n- \\<expr1\\> and \\<expr2\\>\n\n  True only if both expr1 and expr2 are true\n\n- \\<expr1\\> or \\<expr2\\>\n\n  True if either of expr1 or expr2 is true\n\nExpressions can be grouped using parenthesis.\n\n### QUERY EXPRESSION EXAMPLES\n\n- Transaction group contains a request URL that equals to “/foo”\n\n  ``` python\n  ReqURL eq \"/foo\"\n  ```\n\n- Transaction group contains a request cookie header\n\n  ``` python\n  ReqHeader:cookie\n  ```\n\n- Transaction group doesn’t contain a request cookie header\n\n  ``` python\n  not ReqHeader:cookie\n  ```\n\n- Client request where internal handling took more than 800ms.:\n\n  ``` python\n  Timestamp:Process[2] > 0.8\n  ```\n\n- Transaction group contains a request user-agent header that contains “iPod” and the request delivery time exceeds 1 second\n\n  ``` python\n  ReqHeader:user-agent ~ \"iPod\" and Timestamp:Resp[2] > 1.\n  ```\n\n- Transaction group contains a backend response status larger than or equal to 500\n\n  ``` python\n  BerespStatus >= 500\n  ```\n\n- Transaction group contains a request response status of 304, but where the request did not contain an if-modified-since header\n\n  ``` python\n  RespStatus == 304 and not ReqHeader:if-modified-since\n  ```\n\n- Transactions that have had backend failures or long delivery time on their ESI subrequests. (Assumes request grouping mode).\n\n  ``` python\n  BerespStatus >= 500 or {2+}Timestamp:Process[2] > 1.\n  ```\n\n- Log non-transactional errors. (Assumes raw grouping mode).\n\n  ``` python\n  vxid == 0 and Error\n  ```\n\n### HISTORY\n\nThis document was initially written by Martin Blix Grydeland and amended by others.\n\n### COPYRIGHT\n\nThis document is licensed under the same licence as Varnish itself. See LICENCE for details.\n\n- Copyright (c) 2006 Verdens Gang AS\n- Copyright (c) 2006-2015 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vsl-query.html](https://varnish-cache.org/docs/7.4/reference/vsl-query.html)"
- name: 'VSM: Shared Memory Logging and Statistics'
  id: reference/vsm
  summary: Varnish uses shared memory to export parameters, logging and statistics, because it is faster and much more efficient than regular files
  description: "# VSM: Shared Memory Logging and Statistics\n\nVarnish uses shared memory to export parameters, logging and statistics, because it is faster and much more efficient than regular files.\n\n“Varnish Shared Memory” or VSM, is the overall mechanism maintaining sets of shared memory files, each consisting a chunk of memory identified by a two-part name (class, ident).\n\nThe Class indicates what type of data is stored in the chunk, for instance “Arg” for command line arguments useful for establishing an CLI connection to the varnishd, “Stat” for statistics counters (VSC) and “Log” for log records (VSL).\n\nThe ident name part is mostly used with stats counters, where they identify dynamic counters, such as:\n\nSMA.Transient.c_bytes\n\n## Shared memory trickery\n\nShared memory is faster than regular files, but it is also slightly tricky in ways a regular logfile is not.\n\nWhen you open a file in “append” mode, the operating system guarantees that whatever you write will not overwrite existing data in the file. The neat result of this is that multiple processes or threads writing to the same file does not even need to know about each other, it all works just as you would expect.\n\nWith a shared memory log, we get no such help from the kernel, the writers need to make sure they do not stomp on each other, and they need to make it possible and safe for the readers to access the data.\n\nThe “CS101” way to deal with that, is to introduce locks, and much time is spent examining the relative merits of the many kinds of locks available.\n\nInside the varnishd (worker) process, we use mutexes to guarantee consistency, both with respect to allocations, log entries and stats counters.\n\nWe do not want a varnishncsa trying to push data through a stalled ssh connection to stall the delivery of content, so readers like that are purely read-only, they do not get to affect the varnishd process and that means no locks for them.\n\nInstead we use “stable storage” concepts, to make sure the view seen by the readers is consistent at all times.\n\nAs long as you only add stuff, that is trivial, but taking away stuff, such as when a backend is taken out of the configuration, we need to give the readers a chance to discover this, a “cooling off” period.\n\n## The Varnish way:\n\nWhen varnishd starts, it opens locked shared memory files, advising to use different -n arguments if an attempt is made to run multiple varnishd instances with the same name.\n\nChild processes each use their own shared memory files, since a worker process restart marks a clean break in operation anyway.\n\nTo the extent possible, old shared memory files are marked as abandoned by setting the alloc_seq field to zero, which should be monitored by all readers of the VSM.\n\nProcesses subscribing to VSM files for a long time, should notice if the VSM file goes “silent” and check that the file has not been renamed due to a child restart.\n\nChunks inside the shared memory file form a linked list, and whenever that list changes, the alloc_seq field changes.\n\nThe linked list and other metadata in the VSM file, works with offsets relative to the start address of where the VSM file is memory mapped, so it need not be mapped at any particular address.\n\nWhen new chunks are allocated, for instance when a new backend is added, they are appended to the list, no matter where they are located in the VSM file.\n\nWhen a chunk is freed, it will be taken out of the linked list of allocations, its length will be set to zero and alloc_seq will be changed to indicate a change of layout. For the next 60 seconds the chunk will not be touched or reused, giving other subscribers a chance to discover the deallocation.\n\nThe include file \\<vapi/vsm.h\\> provides the supported API for accessing VSM files.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vsm.html](https://varnish-cache.org/docs/7.4/reference/vsm.html)"
- name: VTC
  id: reference/vtc
  summary: This document describes the syntax used by Varnish Test Cases files (.vtc)
  description: "# VTC\n\n## Varnish Test Case Syntax\n\nManual section:  \n7\n\n### OVERVIEW\n\nThis document describes the syntax used by Varnish Test Cases files (.vtc). A vtc file describe a scenario with different scripted HTTP-talking entities, and generally one or more Varnish instances to test.\n\n### PARSING\n\nA vtc file will be read word after word, with very little tokenization, meaning a syntax error won’t be detected until the test actually reach the relevant action in the test.\n\nA parsing error will most of the time result in an assert being triggered. If this happens, please refer yourself to the related source file and line number. However, this guide should help you avoid the most common mistakes.\n\n#### Words and strings\n\nThe parser splits words by detecting whitespace characters and a string is a word, or a series of words on the same line enclosed by double-quotes (”…”), or, for multi-line strings, enclosed in curly brackets ({…}).\n\n#### Comments\n\nThe leading whitespaces of lines are ignored. Empty lines (or ones consisting only of whitespaces) are ignored too, as are the lines starting with “#” that are comments.\n\n#### Lines and commands\n\nTest files take at most one command per line, with the first word of the line being the command and the following ones being its arguments. To continue over to a new line without breaking the argument string, you can escape the newline character (\\n) with a backslash (\\\\.\n\n### MACROS\n\nWhen a string is processed, macro expansion is performed. Macros are in the form `${<name>[,<args>...]}`, they have a name followed by an optional comma- or space-separated list of arguments. Leading and trailing spaces are ignored.\n\nThe macros `${foo,bar,baz}` and `${ foo bar baz }` are equivalent. If an argument contains a space or a comma, arguments can be quoted. For example the macro `${foo,\"bar,baz\"}` gives one argument `bar,baz` to the macro called `foo`.\n\nUnless documented otherwise, all macros are simple macros that don’t take arguments.\n\n#### Built-in macros\n\n`${bad_backend}`  \nA socket address that will reliably never accept connections.\n\n`${bad_ip}`  \nAn unlikely IPv4 address.\n\n`${date}`  \nThe current date and time formatted for HTTP.\n\n`${listen_addr}`  \nThe default listen address various components use, by default a random port on localhost.\n\n`${localhost}`  \nThe first IP address that resolves to “localhost”.\n\n`${pwd}`  \nThe working directory from which `varnishtest` was executed.\n\n`${string,<action>[,<args>...]}`  \nThe `string` macro is the entry point for text generation, it takes a specialized action with each its own set of arguments.\n\n`${string,repeat,<uint>,<str>}`  \nRepeat `uint` times the string `str`.\n\n`${testdir}`  \nThe directory containing the VTC script of the ongoing test case execution.\n\n`${tmpdir}`  \nThe dedicated working directory for the ongoing test case execution, which happens to also be the current working directory. Useful when an absolute path to the working directory is needed.\n\n`${topbuild}`  \nOnly present when the `-i` option is used, to work on Varnish itself instead of a regular installation.\n\n### SYNTAX\n\n#### barrier\n\nNOTE: This command is available everywhere commands are given.\n\nBarriers allows you to synchronize different threads to make sure events occur in the right order. It’s even possible to use them in VCL.\n\nFirst, it’s necessary to declare the barrier:\n\n``` python\nbarrier bNAME TYPE NUMBER [-cyclic]\n```\n\nWith the arguments being:\n\nbNAME  \nthis is the name of the barrier, used to identify it when you’ll create sync points. It must start with ‘b’.\n\nTYPE  \nit can be “cond” (mutex) or “sock” (socket) and sets internal behavior. If you don’t need VCL synchronization, use cond.\n\nNUMBER  \nnumber of sync point needed to go through the barrier.\n\n-cyclic  \nif present, the barrier will reset itself and be ready for another round once gotten through.\n\nThen, to add a sync point:\n\n``` python\nbarrier bNAME sync\n```\n\nThis will block the parent thread until the number of sync points for bNAME reaches the NUMBER given in the barrier declaration.\n\nIf you wish to synchronize the VCL, you need to declare a “sock” barrier. This will emit a macro definition named “bNAME_sock” that you can use in VCL (after importing the vtc vmod):\n\n``` python\nvtc.barrier_sync(\"${bNAME_sock}\");\n```\n\nThis function returns 0 if everything went well and is the equivalent of `barrier bNAME sync` at the VTC top-level.\n\n#### client/server\n\nClient and server threads are fake HTTP entities used to test your Varnish and VCL. They take any number of arguments, and the one that are not recognized, assuming they don’t start with ‘-’, are treated as specifications, laying out the actions to undertake:\n\n``` python\nclient cNAME [...]\nserver sNAME [...]\n```\n\nClients and server are identified by a string that’s the first argument, clients’ names start with ‘c’ and servers’ names start with ‘s’.\n\nAs the client and server commands share a good deal of arguments and specification actions, they are grouped in this single section, specific items will be explicitly marked as such.\n\n##### Arguments\n\n-start  \nStart the thread in background, processing the last given specification.\n\n-wait  \nBlock until the thread finishes.\n\n-run (client only)  \nEquivalent to “-start -wait”.\n\n-repeat NUMBER  \nInstead of processing the specification only once, do it NUMBER times.\n\n-keepalive  \nFor repeat, do not open new connections but rather run all iterations in the same connection\n\n-break (server only)  \nStop the server.\n\n-listen STRING (server only)  \nDictate the listening socket for the server. STRING is of the form “IP PORT”, or “/PATH/TO/SOCKET” for a Unix domain socket. In the latter case, the path must begin with ‘/’, and the server must be able to create it.\n\n-connect STRING (client only)  \nIndicate the server to connect to. STRING is also of the form “IP PORT”, or “/PATH/TO/SOCKET”. As with “server -listen”, a Unix domain socket is recognized when STRING begins with a ‘/’.\n\n-dispatch (server only, s0 only)  \nNormally, to keep things simple, server threads only handle one connection at a time, but the -dispatch switch allows to accept any number of connection and handle them following the given spec.\n\nHowever, -dispatch is only allowed for the server name “s0”.\n\n-proxy1 STRING (client only)  \nUse the PROXY protocol version 1 for this connection. STRING is of the form “CLIENTIP:PORT SERVERIP:PORT”.\n\n-proxy2 STRING (client only)  \nUse the PROXY protocol version 2 for this connection. STRING is of the form “CLIENTIP:PORT SERVERIP:PORT”.\n\n##### Macros and automatic behaviour\n\nTo make things easier in the general case, clients will connect by default to a Varnish server called v1. To connect to a different Varnish server, use ‘-connect ${vNAME_sock}’.\n\nThe -vcl+backend switch of the `varnish` command will add all the declared servers as backends. Be careful though, servers will by default listen to the 127.0.0.1 IP and will pick a random port, and publish 3 macros: sNAME_addr, sNAME_port and sNAME_sock, but only once they are started. For ‘varnish -vcl+backend’ to create the vcl with the correct values, the server must be started first.\n\n##### Specification\n\nIt’s a string, either double-quoted “like this”, but most of the time enclosed in curly brackets, allowing multilining. Write a command per line in it, empty line are ignored, and long line can be wrapped by using a backslash. For example:\n\n``` python\nclient c1 {\n    txreq -url /foo \\\n          -hdr \"bar: baz\"\n\n    rxresp\n} -run\n```\n\naccept (server only)  \nClose the current connection, if any, and accept a new one. Note that this new connection is HTTP/1.x.\n\nchunked STRING  \nSend STRING as chunked encoding.\n\n&nbsp;\n\nchunkedlen NUMBER  \nDo as `chunked` except that the string will be generated for you, with a length of NUMBER characters.\n\n&nbsp;\n\nclose (server only)  \nClose the connection. Note that if operating in HTTP/2 mode no extra (GOAWAY) frame is sent, it’s simply a TCP close.\n\n&nbsp;\n\nexpect STRING1 OP STRING2  \nTest if “STRING1 OP STRING2” is true, and if not, fails the test. OP can be ==, \\<, \\<=, \\>, \\>= when STRING1 and STRING2 represent numbers in which case it’s an order operator. If STRING1 and STRING2 are meant as strings OP is a matching operator, either == (exact match) or ~ (regex match).\n\nvarnishtest will first try to resolve STRING1 and STRING2 by looking if they have special meanings, in which case, the resolved value is use for the test. Note that this value can be a string representing a number, allowing for tests such as:\n\n``` python\nexpect req.http.x-num > 2\n```\n\nHere’s the list of recognized strings, most should be obvious as they either match VCL logic, or the txreq/txresp options:\n\n- remote.ip\n- remote.port\n- remote.path\n- req.method\n- req.url\n- req.proto\n- resp.proto\n- resp.status\n- resp.reason\n- resp.chunklen\n- req.bodylen\n- req.body\n- resp.bodylen\n- resp.body\n- req.http.NAME\n- resp.http.NAME\n\n&nbsp;\n\nexpect_close  \nReads from the connection, expecting nothing to read but an EOF.\n\n&nbsp;\n\nfatal\\|non_fatal  \nControl whether a failure of this entity should stop the test.\n\n&nbsp;\n\ngunzip  \nGunzip the body in place.\n\n&nbsp;\n\nrecv NUMBER  \nRead NUMBER bytes from the connection.\n\n&nbsp;\n\nrxchunk  \nReceive an HTTP chunk.\n\n&nbsp;\n\nrxpri (server only)  \nReceive a preface. If valid set the server to HTTP/2, abort otherwise.\n\n&nbsp;\n\nrxreq (server only)  \nReceive and parse a request’s headers and body.\n\n&nbsp;\n\nrxreqbody (server only)  \nReceive a request’s body.\n\n&nbsp;\n\nrxreqhdrs (server only)  \nReceive and parse a request’s headers (but not the body).\n\n&nbsp;\n\nrxresp \\[-no_obj\\] (client only)  \nReceive and parse a response’s headers and body. If -no_obj is present, only get the headers.\n\n&nbsp;\n\nrxrespbody (client only)  \nReceive (part of) a response’s body.\n\n-max : max length of this receive, 0 for all\n\nrxresphdrs (client only)  \nReceive and parse a response’s headers.\n\n&nbsp;\n\nsend STRING  \nPush STRING on the connection.\n\n&nbsp;\n\nsend_n NUMBER STRING  \nWrite STRING on the socket NUMBER times.\n\n&nbsp;\n\nsend_urgent STRING  \nSend string as TCP OOB urgent data. You will never need this.\n\n&nbsp;\n\nsendhex STRING  \nSend bytes as described by STRING. STRING should consist of hex pairs possibly separated by whitespace or newlines. For example: “0F EE a5 3df2”.\n\n&nbsp;\n\nsettings -dectbl INT  \nForce internal HTTP/2 settings to certain values. Currently only support setting the decoding table size.\n\n&nbsp;\n\nshell  \nSame as for the top-level shell.\n\n&nbsp;\n\nstream  \nHTTP/2 introduces the concept of streams, and these come with their own specification, and as it’s quite big, have been moved to their own chapter.\n\n&nbsp;\n\ntimeout NUMBER  \nSet the TCP timeout for this entity.\n\n&nbsp;\n\ntxpri (client only)  \nSend an HTTP/2 preface (“PRI \\* HTTP/2.0\\r\\n\\r\\nSM\\r\\n\\r\\n”) and set client to HTTP/2.\n\n&nbsp;\n\ntxreq\\|txresp \\[…\\]  \nSend a minimal request or response, but overload it if necessary.\n\ntxreq is client-specific and txresp is server-specific.\n\nThe only thing different between a request and a response, apart from who can send them is that the first line (request line vs status line), so all the options are prety much the same.\n\n-method STRING (txreq only)  \nWhat method to use (default: “GET”).\n\n-req STRING (txreq only)  \nAlias for -method.\n\n-url STRING (txreq only)  \nWhat location to use (default “/”).\n\n-proto STRING  \nWhat protocol use in the status line. (default: “HTTP/1.1”).\n\n-status NUMBER (txresp only)  \nWhat status code to return (default 200).\n\n-reason STRING (txresp only)  \nWhat message to put in the status line (default: “OK”).\n\n-noserver (txresp only)  \nDon’t include a Server header with the id of the server.\n\n-nouseragent (txreq only)  \nDon’t include a User-Agent header with the id of the client.\n\nThese three switches can appear in any order but must come before the following ones.\n\n-nohost  \nDon’t include a Host header in the request. Also Implied by the addition of a Host header with `-hdr`.\n\n-nolen  \nDon’t include a Content-Length header. Also implied by the addition of a Content-Length or Transfer-Encoding header with `-hdr`.\n\n-nodate  \nDon’t include a Date header in the response. Also implied by the addition of a Date header with `-hdr`.\n\n-hdr STRING  \nAdd STRING as a header, it must follow this format: “name: value”. It can be called multiple times.\n\n-hdrlen STRING NUMBER  \nAdd STRING as a header with NUMBER bytes of content.\n\nYou can then use the arguments related to the body:\n\n-body STRING  \nInput STRING as body.\n\n-bodyfrom FILE  \nSame as -body but content is read from FILE.\n\n-bodylen NUMBER  \nGenerate and input a body that is NUMBER bytes-long.\n\n-gziplevel NUMBER  \nSet the gzip level (call it before any of the other gzip switches).\n\n-gzipresidual NUMBER  \nAdd extra gzip bits. You should never need it.\n\n-gzipbody STRING  \nGzip STRING and send it as body.\n\n-gziplen NUMBER  \nCombine -bodylen and -gzipbody: generate a string of length NUMBER, gzip it and send as body.\n\n&nbsp;\n\nwrite_body STRING  \nWrite the body of a request or a response to a file. By using the shell command, higher-level checks on the body can be performed (eg. XML, JSON, …) provided that such checks can be delegated to an external program.\n\n#### delay\n\nNOTE: This command is available everywhere commands are given.\n\nSleep for the number of seconds specified in the argument. The number can include a fractional part, e.g. 1.5.\n\n#### feature\n\nTest that the required feature(s) for a test are available, and skip the test otherwise; or change the interpretation of the test, as documented below. feature takes any number of arguments from this list:\n\n64bit  \nThe environment is 64 bits\n\nipv4  \n127.0.0.1 works\n\nipv6  \n\\[::1\\] works\n\ndns  \nDNS lookups are working\n\ntopbuild  \nThe test has been started with ‘-i’\n\nroot  \nThe test has been invoked by the root user\n\nuser_varnish  \nThe varnish user is present\n\nuser_vcache  \nThe vcache user is present\n\ngroup_varnish  \nThe varnish group is present\n\ncmd \\<command-line\\>  \nA command line that should execute with a zero exit status\n\nignore_unknown_macro  \nDo not fail the test if a string of the form ${…} is not recognized as a macro.\n\npersistent_storage  \nVarnish was built with the deprecated persistent storage.\n\ncoverage  \nVarnish was built with code coverage enabled.\n\nasan  \nVarnish was built with the address sanitizer.\n\nmsan  \nVarnish was built with the memory sanitizer.\n\ntsan  \nVarnish was built with the thread sanitizer.\n\nubsan  \nVarnish was built with the undefined behavior sanitizer.\n\nsanitizer  \nVarnish was built with a sanitizer.\n\nworkspace_emulator  \nVarnish was built with its workspace emulator.\n\nabstract_uds  \nCreation of an abstract unix domain socket succeeded\n\nA feature name can be prefixed with an exclamation mark (!) to skip a test if the feature is present.\n\nBe careful with ignore_unknown_macro, because it may cause a test with a misspelled macro to fail silently. You should only need it if you must run a test with strings of the form “${…}”.\n\n#### filewrite\n\nWrite strings to file\n\nfilewrite \\[-a\\] /somefile “Hello” “ “ “Worldn”\n\nThe -a flag opens the file in append mode.\n\n#### haproxy\n\nDefine and interact with haproxy instances.\n\nTo define a haproxy server, you’ll use this syntax:\n\n``` python\nhaproxy hNAME -conf-OK CONFIG\nhaproxy hNAME -conf-BAD ERROR CONFIG\nhaproxy hNAME [-D] [-W] [-arg STRING] [-conf[+vcl] STRING]\n```\n\nThe first `haproxy hNAME` invocation will start the haproxy master process in the background, waiting for the `-start` switch to actually start the child.\n\nArguments:\n\nhNAME  \nIdentify the HAProxy server with a string, it must starts with ‘h’.\n\n-conf-OK CONFIG  \nRun haproxy in ‘-c’ mode to check config is OK  \nstdout/stderr should contain ‘Configuration file is valid’ The exit code should be 0.\n\n-conf-BAD ERROR CONFIG  \nRun haproxy in ‘-c’ mode to check config is BAD.  \n“ERROR” should be part of the diagnostics on stdout/stderr. The exit code should be 1.\n\n-D  \nRun HAproxy in daemon mode. If not given ‘-d’ mode used.\n\n-W  \nEnable HAproxy in Worker mode.\n\n-S  \nEnable HAproxy Master CLI in Worker mode\n\n-arg STRING  \nPass an argument to haproxy, for example “-h simple_list”.\n\n-cli STRING  \nSpecify the spec to be run by the command line interface (CLI).\n\n-mcli STRING  \nSpecify the spec to be run by the command line interface (CLI) of the Master process.\n\n-conf STRING  \nSpecify the configuration to be loaded by this HAProxy instance.\n\n-conf+backend STRING  \nSpecify the configuration to be loaded by this HAProxy instance,  \nall server instances will be automatically appended\n\n-start  \nStart this HAProxy instance.\n\n-wait  \nStop this HAProxy instance.\n\n-expectexit NUMBER  \nExpect haproxy to exit(3) with this value\n\n##### haproxy CLI Specification\n\nexpect OP STRING  \nRegex match the CLI reception buffer with STRING if OP is ~ or, on the contraty, if OP is !~ check that there is no regex match.\n\n&nbsp;\n\nsend STRING  \nPush STRING on the CLI connection. STRING will be terminated by an end of line character (n).\n\n#### logexpect\n\nReads the VSL and looks for records matching a given specification. It will process records trying to match the first pattern, and when done, will continue processing, trying to match the following pattern. If a pattern isn’t matched, the test will fail.\n\nlogexpect threads are declared this way:\n\n``` python\nlogexpect lNAME -v <id> [-g <grouping>] [-d 0|1] [-q query] \\\n        [vsl arguments] {\n                expect <skip> <vxid> <tag> <regex>\n                expect <skip> <vxid> <tag> <regex>\n                fail add <vxid> <tag> <regex>\n                fail clear\n                abort\n                ...\n        } [-start|-wait|-run]\n```\n\nAnd once declared, you can start them, or wait on them:\n\n``` python\nlogexpect lNAME <-start|-wait>\n```\n\nWith:\n\nlNAME  \nName the logexpect thread, it must start with ‘l’.\n\n-v id  \nSpecify the varnish instance to use (most of the time, id=v1).\n\n-g \\<session\\|request\\|vxid\\|raw  \nDecide how records are grouped, see -g in `man varnishlog` for more information.\n\n-d \\<0\\|1\\>  \nStart processing log records at the head of the log instead of the tail.\n\n-q query  \nFilter records using a query expression, see `man vsl-query` for more information. Multiple -q options are not supported.\n\n-m  \nAlso emit log records for misses (only for debugging)\n\n-err  \nInvert the meaning of success. Usually called once to expect the logexpect to fail\n\n-start  \nStart the logexpect thread in the background.\n\n-wait  \nWait for the logexpect thread to finish\n\n-run  \nEquivalent to “-start -wait”.\n\nVSL arguments (similar to the varnishlog options):\n\n-C  \nUse caseless regex\n\n-i \\<taglist\\>  \nInclude tags\n\n-I \\<\\[taglist:\\]regex\\>  \nInclude by regex\n\n-T \\<seconds\\>  \nTransaction end timeout\n\nexpect specification:\n\nskip: \\[uint\\|\\*\\|?\\]  \nMax number of record to skip\n\nvxid: \\[uint\\|\\*\\|=\\]  \nvxid to match\n\ntag: \\[tagname\\|\\*\\|=\\]  \nTag to match against\n\nregex:  \nregular expression to match against (optional)\n\nFor skip, vxid and tag, ‘\\*’ matches anything, ‘=’ expects the value of the previous matched record. The ‘?’ marker is equivalent to zero, expecting a match on the next record. The difference is that ‘?’ can be used when the order of individual consecutive logs is not deterministic. In other words, lines from a block of alternatives marked by ‘?’ can be matched in any order, but all need to match eventually.\n\nfail specification:\n\nadd: Add to the fail list\n\nArguments are equivalent to expect, except for skip missing\n\nclear: Clear the fail list\n\nAny number of fail specifications can be active during execution of a logexpect. All active fail specifications are matched against every log line and, if any match, the logexpect fails immediately.\n\nFor a logexpect to end successfully, there must be no specs on the fail list, so logexpects should always end with\n\nexpect \\<skip\\> \\<vxid\\> \\<tag\\> \\<termination-condition\\> fail clear\n\nabort specification:\n\nabort(3) varnishtest, intended to help debugging of the VSL client library itself.\n\n#### loop\n\nloop NUMBER STRING  \nProcess STRING as a specification, NUMBER times.\n\nThis works inside all specification strings\n\n#### process\n\nRun a process with stdin+stdout on a pseudo-terminal and stderr on a pipe.\n\nOutput from the pseudo-terminal is copied verbatim to ${pNAME_out}, and the -log/-dump/-hexdump flags will also put it in the vtc-log.\n\nThe pseudo-terminal is not in ECHO mode, but if the programs run set it to ECHO mode (“stty sane”) any input sent to the process will also appear in this stream because of the ECHO.\n\nOutput from the stderr-pipe is copied verbatim to ${pNAME_err}, and is always included in the vtc_log.\n\nprocess pNAME SPEC \\[-allow-core\\] \\[-expect-exit N\\] \\[-expect-signal N\\]  \n\\[-dump\\] \\[-hexdump\\] \\[-log\\] \\[-run\\] \\[-close\\] \\[-kill SIGNAL\\] \\[-start\\] \\[-stop\\] \\[-wait\\] \\[-write STRING\\] \\[-writeln STRING\\] \\[-writehex HEXSTRING\\] \\[-need-bytes \\[+\\]NUMBER\\] \\[-screen-dump\\] \\[-winsz LINES COLUMNSS\\] \\[-ansi-response\\] \\[-expect-cursor LINE COLUMN\\] \\[-expect-text LINE COLUMN TEXT\\] \\[-match-text LINE COLUMN REGEXP\\]\n\n&nbsp;\n\npNAME  \nName of the process. It must start with ‘p’.\n\nSPEC  \nThe command(s) to run in this process.\n\n-hexdump  \nLog output with vtc_hexdump(). Must be before -start/-run.\n\n-dump  \nLog output with vtc_dump(). Must be before -start/-run.\n\n-log  \nLog output with VLU/vtc_log(). Must be before -start/-run.\n\n-start  \nStart the process.\n\n-expect-exit N  \nExpect exit status N\n\n-expect-signal N  \nExpect signal in exit status N\n\n-allow-core  \nCore dump in exit status is OK\n\n-wait  \nWait for the process to finish.\n\n-run  \nShorthand for -start -wait.\n\nIn most cases, if you just want to start a process and wait for it to finish, you can use the `shell` command instead. The following commands are equivalent:\n\n``` python\nshell \"do --something\"\n\nprocess p1 \"do --something\" -run\n```\n\nHowever, you may use the the `process` variant to conveniently collect the standard input and output without dealing with shell redirections yourself. The `shell` command can also expect an expression from either output, consider using it if you only need to match one.\n\n-key KEYSYM  \nSend emulated key-press. KEYSYM can be one of (NPAGE, PPAGE, HOME, END)\n\n-kill SIGNAL  \nSend a signal to the process. The argument can be either the string “TERM”, “INT”, or “KILL” for SIGTERM, SIGINT or SIGKILL signals, respectively, or a hyphen (-) followed by the signal number.\n\nIf you need to use other signal names, you can use the `kill`(1) command directly:\n\n``` python\nshell \"kill -USR1 ${pNAME_pid}\"\n```\n\nNote that SIGHUP usage is discouraged in test cases.\n\n-stop  \nShorthand for -kill TERM.\n\n-close  \nAlias for “-kill HUP”\n\n-winsz LINES COLUMNS  \nChange the terminal window size to LIN lines and COL columns.\n\n-write STRING  \nWrite a string to the process’ stdin.\n\n-writeln STRING  \nSame as -write followed by a newline (\\n).\n\n-writehex HEXSTRING  \nSame as -write but interpreted as hexadecimal bytes.\n\n-need-bytes \\[+\\]NUMBER  \nWait until at least NUMBER bytes have been received in total. If ‘+’ is prefixed, NUMBER new bytes must be received.\n\n-ansi-response  \nRespond to terminal respond-back sequences\n\n-expect-cursor LINE COLUMN  \nExpect cursors location\n\n-expect-text LINE COLUMNS TEXT  \nWait for TEXT to appear at LIN,COL on the virtual screen. Lines and columns are numbered 1…N LIN==0 means “on any line” COL==0 means “anywhere on the line”\n\n-match-text LINE COLUMN REGEXP  \nWait for the PAT regular expression to match the text at LIN,COL on the virtual screen. Lines and columns are numbered 1…N LIN==0 means “on any line” COL==0 means “anywhere on the line”\n\n-screen-dump  \nDump the virtual screen into vtc_log\n\n#### setenv\n\nSet or change an environment variable:\n\n``` python\nsetenv FOO \"bar baz\"\n```\n\nThe above will set the environment variable $FOO to the value provided. There is also an `-ifunset` argument which will only set the value if the the environment variable does not already exist:\n\n``` python\nsetenv -ifunset FOO quux\n```\n\n#### shell\n\nNOTE: This command is available everywhere commands are given.\n\nPass the string given as argument to a shell. If you have multiple commands to run, you can use curly brackets to describe a multi-lines script, eg:\n\n``` python\nshell {\n        echo begin\n        cat /etc/fstab\n        echo end\n}\n```\n\nBy default a zero exit code is expected, otherwise the vtc will fail.\n\nNotice that the commandstring is prefixed with “exec 2\\>&1;” to combine stderr and stdout back to the test process.\n\nOptional arguments:\n\n-err  \nExpect non-zero exit code.\n\n-exit N  \nExpect exit code N instead of zero.\n\n-expect STRING  \nExpect string to be found in stdout+err.\n\n-match REGEXP  \nExpect regexp to match the stdout+err output.\n\n#### stream\n\n(note: this section is at the top-level for easier navigation, but it’s part of the client/server specification)\n\nStreams map roughly to a request in HTTP/2, a request is sent on stream N, the response too, then the stream is discarded. The main exception is the first stream, 0, that serves as coordinator.\n\nStream syntax follow the client/server one:\n\n``` python\nstream ID [SPEC] [ACTION]\n```\n\nID is the HTTP/2 stream number, while SPEC describes what will be done in that stream.\n\nNote that, when parsing a stream action, if the entity isn’t operating in HTTP/2 mode, these spec is ran before:\n\n``` python\ntxpri/rxpri # client/server\nstream 0 {\n    txsettings\n    rxsettings\n    txsettings -ack\n    rxsettings\n    expect settings.ack == true\n} -run\n```\n\nAnd HTTP/2 mode is then activated before parsing the specification.\n\n##### Actions\n\n-start  \nRun the specification in a thread, giving back control immediately.\n\n-wait  \nWait for the started thread to finish running the spec.\n\n-run  \nequivalent to calling `-start` then `-wait`.\n\n##### Specification\n\nThe specification of a stream follows the exact same rules as one for a client or a server.\n\n###### txreq, txresp, txcont, txpush\n\nThese four commands are about sending headers. txreq and txresp will send HEADER frames; txcont will send CONTINUATION frames; txpush PUSH frames. The only difference between txreq and txresp are the default headers set by each of them.\n\n-noadd  \nDo not add default headers. Useful to avoid duplicates when sending default headers using `-hdr`, `-idxHdr` and `-litIdxHdr`.\n\n-status INT (txresp)  \nSet the :status pseudo-header.\n\n-url STRING (txreq, txpush)  \nSet the :path pseudo-header.\n\n-method STRING (txreq, txpush)  \nSet the :method pseudo-header.\n\n-req STRING (txreq, txpush)  \nAlias for -method.\n\n-scheme STRING (txreq, txpush)  \nSet the :scheme pseudo-header.\n\n-hdr STRING1 STRING2  \nInsert a header, STRING1 being the name, and STRING2 the value.\n\n-idxHdr INT  \nInsert an indexed header, using INT as index.\n\n-litIdxHdr inc\\|not\\|never INT huf\\|plain STRING  \nInsert an literal, indexed header. The first argument specify if the header should be added to the table, shouldn’t, or mustn’t be compressed if/when retransmitted.\n\nINT is the index of the header name to use.\n\nThe third argument informs about the Huffman encoding: yes (huf) or no (plain).\n\nThe last term is the literal value of the header.\n\n-litHdr inc\\|not\\|never huf\\|plain STRING1 huf\\|plain STRING2  \nInsert a literal header, with the same first argument as `-litIdxHdr`.\n\nThe second and third terms tell what the name of the header is and if it should be Huffman-encoded, while the last two do the same regarding the value.\n\n-body STRING (txreq, txresp)  \nSpecify a body, effectively putting STRING into a DATA frame after the HEADER frame is sent.\n\n-bodyfrom FILE (txreq, txresp)  \nSame as `-body` but content is read from FILE.\n\n-bodylen INT (txreq, txresp)  \nDo the same thing as `-body` but generate a string of INT length for you.\n\n-gzipbody STRING (txreq, txresp)  \nGzip STRING and send it as body.\n\n-gziplen NUMBER (txreq, txresp)  \nCombine -bodylen and -gzipbody: generate a string of length NUMBER, gzip it and send as body.\n\n-nostrend (txreq, txresp)  \nDon’t set the END_STREAM flag automatically, making the peer expect a body after the headers.\n\n-nohdrend  \nDon’t set the END_HEADERS flag automatically, making the peer expect more HEADER frames.\n\n-dep INT (txreq, txresp)  \nTell the peer that this content depends on the stream with the INT id.\n\n-ex (txreq, txresp)  \nMake the dependency exclusive (`-dep` is still needed).\n\n-weight (txreq, txresp)  \nSet the weight for the dependency.\n\n-promised INT (txpush)  \nThe id of the promised stream.\n\n-pad STRING / -padlen INT (txreq, txresp, txpush)  \nAdd string as padding to the frame, either the one you provided with -pad, or one that is generated for you, of length INT is -padlen case.\n\n###### txdata\n\nBy default, data frames are empty. The receiving end will know the whole body has been delivered thanks to the END_STREAM flag set in the last DATA frame, and txdata automatically set it.\n\n-data STRING  \nData to be embedded into the frame.\n\n-datalen INT  \nGenerate and INT-bytes long string to be sent in the frame.\n\n-pad STRING / -padlen INT  \nAdd string as padding to the frame, either the one you provided with -pad, or one that is generated for you, of length INT is -padlen case.\n\n-nostrend  \nDon’t set the END_STREAM flag, allowing to send more data on this stream.\n\n###### rxreq, rxresp\n\nThese are two convenience functions to receive headers and body of an incoming request or response. The only difference is that rxreq can only be by a server, and rxresp by a client.\n\n###### rxhdrs\n\n`rxhdrs` will expect one HEADER frame, then, depending on the arguments, zero or more CONTINUATION frame.\n\n-all  \nKeep waiting for CONTINUATION frames until END_HEADERS flag is seen.\n\n-some INT  \nRetrieve INT - 1 CONTINUATION frames after the HEADER frame.\n\n###### rxpush\n\nThis works like `rxhdrs`, expecting a PUSH frame and then zero or more CONTINUATION frames.\n\n-all  \nKeep waiting for CONTINUATION frames until END_HEADERS flag is seen.\n\n-some INT  \nRetrieve INT - 1 CONTINUATION frames after the PUSH frame.\n\n###### rxdata\n\nReceiving data is done using the `rxdata` keywords and will retrieve one DATA frame, if you wish to receive more, you can use these two convenience arguments:\n\n-all  \nkeep waiting for DATA frame until one sets the END_STREAM flag\n\n-some INT  \nretrieve INT DATA frames.\n\nReceive a frame, any frame.\n\n###### sendhex\n\nPush bytes directly on the wire. sendhex takes exactly one argument: a string describing the bytes, in hex notation, with possible whitespaces between them. Here’s an example:\n\n``` python\nsendhex \"00 00 08 00 0900       8d\"\n```\n\n###### rxgoaway\n\nReceive a GOAWAY frame.\n\n###### txgoaway\n\nPossible options include:\n\n-err STRING\\|INT  \nset the error code to explain the termination. The second argument can be a integer or the string version of the error code as found in rfc7540#7.\n\n-laststream INT  \nthe id of the “highest-numbered stream identifier for which the sender of the GOAWAY frame might have taken some action on or might yet take action on”.\n\n-debug  \nspecify the debug data, if any to append to the frame.\n\n###### gunzip\n\nSame as the `gunzip` command for HTTP/1.\n\n###### rxping\n\nReceive a PING frame.\n\n###### txping\n\nSend PING frame.\n\n-data STRING  \nspecify the payload of the frame, with STRING being an 8-char string.\n\n-ack  \nset the ACK flag.\n\n###### rxprio\n\nReceive a PRIORITY frame.\n\n###### txprio\n\nSend a PRIORITY frame\n\n-stream INT  \nindicate the id of the stream the sender stream depends on.\n\n-ex  \nthe dependency should be made exclusive (only this streams depends on the parent stream).\n\n-weight INT  \nan 8-bits integer is used to balance priority between streams depending on the same streams.\n\n###### rxrst\n\nReceive a RST_STREAM frame.\n\n###### txrst\n\nSend a RST_STREAM frame. By default, txrst will send a 0 error code (NO_ERROR).\n\n-err STRING\\|INT  \nSets the error code to be sent. The argument can be an integer or a string describing the error, such as NO_ERROR, or CANCEL (see rfc7540#11.4 for more strings).\n\n###### rxsettings\n\nReceive a SETTINGS frame.\n\n###### txsettings\n\nSETTINGS frames must be acknowledge, arguments are as follow (most of them are from rfc7540#6.5.2):\n\n-hdrtbl INT  \nheaders table size\n\n-push BOOL  \nwhether push frames are accepted or not\n\n-maxstreams INT  \nmaximum concurrent streams allowed\n\n-winsize INT  \nsender’s initial window size\n\n-framesize INT  \nlargest frame size authorized\n\n-hdrsize INT  \nmaximum size of the header list authorized\n\n-ack  \nset the ack bit\n\n###### rxwinup\n\nReceive a WINDOW_UPDATE frame.\n\n###### txwinup\n\nTransmit a WINDOW_UPDATE frame, increasing the amount of credit of the connection (from stream 0) or of the stream (any other stream).\n\n-size INT  \ngive INT credits to the peer.\n\n&nbsp;\n\nwrite_body STRING  \nSame as the `write_body` command for HTTP/1.\n\n###### expect\n\nexpect in stream works as it does in client or server, except that the elements compared will be different.\n\nMost of these elements will be frame specific, meaning that the last frame received on that stream must of the correct type.\n\nHere the list of keywords you can look at.\n\n###### GOAWAY specific\n\ngoaway.err  \nThe error code (as integer) of the GOAWAY frame.\n\ngoaway.laststream  \nLast-Stream-ID\n\ngoaway.debug  \nDebug data, if any.\n\n###### PING specific\n\nping.data  \nThe 8-bytes string of the PING frame payload.\n\nping.ack (PING)  \n“true” if the ACK flag was set, “false” otherwise.\n\n###### PRIORITY specific\n\nprio.stream  \nThe stream ID announced.\n\nprio.exclusive  \n“true” if the priority is exclusive, else “false”.\n\nprio.weight  \nThe dependency weight.\n\n###### PUSH_PROMISE specific\n\npush.id  \nThe id of the promised stream.\n\n###### RESET_STREAM specific\n\nrst.err  \nThe error code (as integer) of the RESET_STREAM frame.\n\n###### SETTINGS specific\n\nsettings.ack  \n“true” if the ACK flag was set, else “”false.\n\nsettings.push  \n“true” if the push settings was set to yes, “false” if set to no, and \\<undef\\> if not present.\n\nsettings.hdrtbl  \nValue of HEADER_TABLE_SIZE if set, \\<undef\\> otherwise.\n\nsettings.maxstreams  \nValue of MAX_CONCURRENT_STREAMS if set, \\<undef\\> otherwise.\n\nsettings.winsize  \nValue of INITIAL_WINDOW_SIZE if set, \\<undef\\> otherwise.\n\nsetting.framesize  \nValue of MAX_FRAME_SIZE if set, \\<undef\\> otherwise.\n\nsettings.hdrsize  \nValue of MAX_HEADER_LIST_SIZE if set, \\<undef\\> otherwise.\n\n###### WINDOW_UPDATE specific\n\nwinup.size  \nThe size of the upgrade given by the WINDOW_UPDATE frame.\n\n###### Generic frame\n\nframe.data  \nPayload of the last frame\n\nframe.type  \nType of the frame, as integer.\n\nframe.size  \nSize of the frame.\n\nframe.stream  \nStream of the frame (correspond to the one you are executing this from, obviously).\n\nframe.padding (for DATA, HEADERS, PUSH_PROMISE frames)  \nNumber of padded bytes.\n\n###### Request and response\n\nNote: it’s possible to inspect a request or response while it is still being construct (in-between two frames for example).\n\nreq.bodylen / resp.bodylen  \nLength in bytes of the request/response so far.\n\nreq.body / resp.body  \nBody of the request/response so far.\n\nreq.http.STRING / resp.http.STRING  \nValue of the header STRING in the request/response.\n\nreq.status / resp.status  \n:status pseudo-header’s value.\n\nreq.url / resp.url  \n:path pseudo-header’s value.\n\nreq.method / resp.method  \n:method pseudo-header’s value.\n\nreq.authority / resp.authority  \n:method pseudo-header’s value.\n\nreq.scheme / resp.scheme  \n:method pseudo-header’s value.\n\n###### Stream\n\nstream.window  \nThe current local window size of the stream, or, if on stream 0, of the connection.\n\nstream.peer_window  \nThe current peer window size of the stream, or, if on stream 0, of the connection.\n\nstream.weight  \nWeight of the stream\n\nstream.dependency  \nId of the stream this one depends on.\n\n###### Index tables\n\ntbl.dec.size / tbl.enc.size  \nSize (bytes) of the decoding/encoding table.\n\ntbl.dec.size / tbl.enc.maxsize  \nMaximum size (bytes) of the decoding/encoding table.\n\ntbl.dec.length / tbl.enc.length  \nNumber of headers in decoding/encoding table.\n\ntbl.dec\\[INT\\].key / tbl.enc\\[INT\\].key  \nName of the header at index INT of the decoding/encoding table.\n\ntbl.dec\\[INT\\].value / tbl.enc\\[INT\\].value  \nValue of the header at index INT of the decoding/encoding table.\n\n#### syslog\n\nDefine and interact with syslog instances (for use with haproxy)\n\nTo define a syslog server, you’ll use this syntax:\n\n``` python\nsyslog SNAME\n```\n\nArguments:\n\nSNAME  \nIdentify the syslog server with a string which must start with ‘S’.\n\n-level STRING  \nSet the default syslog priority level used by any subsequent “recv” command. Any syslog dgram with a different level will be skipped by “recv” command. This default level value may be superseded by “recv” command if supplied as first argument: “recv \\<level\\>”.\n\n-start  \nStart the syslog server thread in the background.\n\n-repeat  \nInstead of processing the specification only once, do it  \nNUMBER times.\n\n-bind  \nBind the syslog socket to a local address.\n\n-wait  \nWait for that thread to terminate.\n\n-stop  \nStop the syslog server thread.\n\n#### tunnel\n\nThe goal of a tunnel is to help control the data transfer between two parties, for example to trigger socket timeouts in the middle of protocol frames, without the need to change how both parties are implemented.\n\nA tunnel accepts a connection and then connects on behalf of the source to the desired destination. Once both connections are established the tunnel will transfer bytes unchanged between the source and destination. Transfer can be interrupted, usually with the help of synchronization methods like barriers. Once the transfer is paused, it is possible to let a specific amount of bytes move in either direction.\n\n##### Arguments\n\n-start  \nStart the tunnel in background, processing the last given specification.\n\n-start+pause  \nStart the tunnel, but already paused.\n\n-wait  \nBlock until the thread finishes.\n\n-listen STRING  \nDictate the listening socket for the server. STRING is of the form “IP PORT”, or “HOST PORT”.\n\nListens by defaults to a local random port.\n\n-connect STRING  \nIndicate the server to connect to. STRING is also of the form “IP PORT”, or “HOST PORT”.\n\nConnects by default to a varnish instance called `v1`.\n\n##### Specification\n\nThe specification contains a list of tunnel commands that can be combined with barriers and delays. For example:\n\n``` python\ntunnel t1 {\n    barrier b1 sync\n    pause\n    delay 1\n    send 42\n    barrier b2 sync\n    resume\n} -start\n```\n\nIf one end of the tunnel is closed before the end of the specification the test case will fail. A specification that ends in a paused state will implicitely resume the tunnel.\n\npause  \nWait for in-flight bytes to be transferred and pause the tunnel.\n\nThe tunnel must be running.\n\n&nbsp;\n\nrecv NUMBER  \nWait until NUMBER bytes are transferred from destination to source.\n\nThe tunnel must be paused, it remains paused afterwards.\n\n&nbsp;\n\nresume  \nResume the transfer of bytes in both directions.\n\nThe tunnel must be paused.\n\n&nbsp;\n\nsend NUMBER  \nWait until NUMBER bytes are transferred from source to destination.\n\nThe tunnel must be paused, it remains paused afterwards.\n\n#### varnish\n\nDefine and interact with varnish instances.\n\nTo define a Varnish server, you’ll use this syntax:\n\n``` python\nvarnish vNAME [-arg STRING] [-vcl STRING] [-vcl+backend STRING]\n        [-errvcl STRING STRING] [-jail STRING] [-proto PROXY]\n```\n\nThe first `varnish vNAME` invocation will start the varnishd master process in the background, waiting for the `-start` switch to actually start the child.\n\nTypes used in the description below:\n\nPATTERN  \nis a ‘glob’ style pattern (ie: fnmatch(3)) as used in shell filename expansion.\n\nArguments:\n\nvNAME  \nIdentify the Varnish server with a string, it must starts with ‘v’.\n\n-arg STRING  \nPass an argument to varnishd, for example “-h simple_list”.\n\nIf the ${varnishd_args_prepend} or ${varnishd_args_append} macros are defined, they are expanded and inserted before / appended to the varnishd command line as constructed by varnishtest, before the command line itself is expanded. This enables tweaks to the varnishd command line without editing test cases. This macros can be defined using the `-D` option for varnishtest.\n\n-vcl STRING  \nSpecify the VCL to load on this Varnish instance. You’ll probably want to use multi-lines strings for this ({…}).\n\n-vcl+backend STRING  \nDo the exact same thing as -vcl, but adds the definition block of known backends (ie. already defined).\n\n-errvcl STRING1 STRING2  \nLoad STRING2 as VCL, expecting it to fail, and Varnish to send an error string matching STRING1\n\n-jail STRING  \nLook at `man varnishd` (-j) for more information.\n\n-proto PROXY  \nHave Varnish use the proxy protocol. Note that PROXY here is the actual string.\n\nYou can decide to start the Varnish instance and/or wait for several events:\n\n``` python\nvarnish vNAME [-start] [-wait] [-wait-running] [-wait-stopped]\n```\n\n-start  \nStart the child process.\n\nOnce successfully started, the following macros are available for the default listen address: `${vNAME_addr}`, `${vNAME_port}` and `${vNAME_sock}`. Additional macros are available, including the listen address name for each address vNAME listens to, like for example: `${vNAME_a0_addr}`.\n\n-stop  \nStop the child process.\n\n-syntax  \nSet the VCL syntax level for this command (default: 4.1)\n\n-wait  \nWait for that instance to terminate.\n\n-wait-running  \nWait for the Varnish child process to be started.\n\n-wait-stopped  \nWait for the Varnish child process to stop.\n\n-cleanup  \nOnce Varnish is stopped, clean everything after it. This is only used in very few tests and you should never need it.\n\n-expectexit NUMBER  \nExpect varnishd to exit(3) with this value\n\nOnce Varnish is started, you can talk to it (as you would through `varnishadm`) with these additional switches:\n\n``` python\nvarnish vNAME [-cli STRING] [-cliok STRING] [-clierr STRING]\n              [-clijson STRING]\n```\n\n-cli STRING\\|-cliok STRING\\|-clierr STATUS STRING\\|-cliexpect REGEXP STRING  \nAll four of these will send STRING to the CLI, the only difference is what they expect the result to be. -cli doesn’t expect anything, -cliok expects 200, -clierr expects STATUS, and -cliexpect expects the REGEXP to match the returned response.\n\n-clijson STRING  \nSend STRING to the CLI, expect success (CLIS_OK/200) and check that the response is parsable JSON.\n\nIt is also possible to interact with its shared memory (as you would through tools like `varnishstat`) with additional switches:\n\n-expect !PATTERN\\|PATTERN OP NUMBER\\|PATTERN OP PATTERN  \nLook into the VSM and make sure the first VSC counter identified by PATTERN has a correct value. OP can be ==, \\>, \\>=, \\<, \\<=. For example:\n\n``` python\nvarnish v1 -expect SM?.s1.g_space > 1000000\nvarnish v1 -expect cache_hit >= cache_hit_grace\n```\n\nIn the ! form the test fails if a counter matches PATTERN.\n\nThe `MAIN.` namespace can be omitted from PATTERN.\n\nThe test takes up to 5 seconds before timing out.\n\n-vsc PATTERN  \nDump VSC counters matching PATTERN.\n\n-vsl_catchup  \nWait until the logging thread has idled to make sure that all the generated log is flushed\n\n#### varnishtest\n\nAlternate name for ‘vtest’, see above.\n\n#### vtest\n\nThis should be the first command in your vtc as it will identify the test case with a short yet descriptive sentence. It takes exactly one argument, a string, eg:\n\n``` python\nvtest \"Check that vtest is actually a valid command\"\n```\n\nIt will also print that string in the log.\n\n### HISTORY\n\nThis document has been written by Guillaume Quintard.\n\n### SEE ALSO\n\n- [varnishtest](varnishtest#varnishtest-1)\n- [VMOD vtc - Utility module for varnishtest](vmod_vtc#vmod-vtc-3)\n\n### COPYRIGHT\n\nThis document is licensed under the same licence as Varnish itself. See LICENCE for details.\n\n- Copyright (c) 2006-2016 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vtc.html](https://varnish-cache.org/docs/7.4/reference/vtc.html)"
- name: VTLA - Varnish Three Letter Acronyms
  id: reference/vtla
  summary: VAV Varnish Arg Vector – Argv parsing
  description: "# VTLA - Varnish Three Letter Acronyms\n\nVery early in the project, we made a fortunate bargain on eBay, buying up a batch of 676 three letter acronyms, all starting with ‘V’.\n\nThis page tells you what we use them for, if & when we remember to add them…\n\nVAV  \nVarnish Arg Vector – Argv parsing.\n\nVBE  \nVarnish Back End – Code for contacting backends (bin/varnishd/cache_backend.c)\n\nVBP  \nVarnish Backend Polling – Health checks of backends (bin/varnishd/cache_backend_poll.c)\n\nVCA  \nVarnish Connection Acceptor – The code that receives/accepts the TCP connections (bin/varnishd/cache_acceptor.c)\n\nVCC  \nVCL to C Compiler – The code that compiles VCL to C code. (lib/libvcl)\n\nVCF  \nVarnish CatFlap\n\nVCL  \nVarnish Configuration Language – The domain-specific programming language used for configuring a varnishd.\n\nVCT  \nVarnish CType(3) – Character classification for RFC2616 and XML parsing.\n\nVDD  \nVarnish (Core) Developer Day – Quarterly invite-only meeting strictly for Varnish core (C) developers, packagers and VMOD hackers.\n\nVENC  \nVarnish ENCoding – base64 functions\n\nVEND  \nVarnish ENDianess – functions to marshall data in specified endianess\n\nVEV  \nVarnish EVent – library functions to implement a simple event-dispatcher.\n\nVEXT  \nVarnish Extension – Shared library loaded into the child process.\n\nVGB  \nVarnish Governing Board – May or may not exist. If you need to ask, you are not on it.\n\nVGC  \nVarnish Generated Code – Code generated by VCC from VCL.\n\nVIN  \nVarnish Instance Naming – Resolution of -n arguments.\n\nVLU  \nVarnish Line Up – library functions to collect stream of bytes into lines for processing. (lib/libvarnish/vlu.c)\n\nVPI  \nVCC Private Interface – functions in varnishd which only VCC is allowed to call.\n\nVRE  \nVarnish Regular Expression – library functions for regular expression based matching and substring replacement. (lib/libvarnish/vre.c)\n\nVRT  \nVarnish Run Time – functions called from compiled code. (bin/varnishd/cache_vrt.c)\n\nVRY  \nVaRY – Related to processing of Vary: HTTP headers. (bin/varnishd/cache_vary.c)\n\nVSL  \nVarnish Shared memory Log – The log written into the shared memory segment for varnish{log,ncsa,top,hist} to see.\n\nVSB  \nVarnish string Buffer – a copy of the FreeBSD “sbuf” library, for safe string handling.\n\nVSC  \nVarnish Statistics Counter – counters for various stats, exposed via varnishapi.\n\nVSS  \nVarnish Session Stuff – library functions to wrap DNS/TCP. (lib/libvarnish/vss.c)\n\nVTC  \nVarnish Test Code – a test-specification for the varnishtest program.\n\nVTE  \nVarnish Turbo Encabulator\n\nVTLA  \nVarnish Three Letter Acronym – No rule without an exception.\n\nVUG  \nVarnish User Group meeting – Half-yearly event where the users and developers of Varnish Cache gather to share experiences and plan future development.\n\nVWx  \nVarnish Waiter ‘x’ – A code module to monitor idle sessions.\n\nVWE  \nVarnish Waiter Epoll – epoll(2) (linux) based waiter module.\n\nVWK  \nVarnish Waiter Kqueue – kqueue(2) (freebsd) based waiter module.\n\nVWP  \nVarnish Waiter Poll – poll(2) based waiter module.\n\nVWS  \nVarnish Waiter Solaris – Solaris ports(2) based waiter module.\n\n## COPYRIGHT\n\nThis document is licensed under the same licence as Varnish itself. See LICENCE for details.\n\n- Copyright (c) 2019 Varnish Software AS\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/vtla.html](https://varnish-cache.org/docs/7.4/reference/vtla.html)"
- name: Who is … ?
  id: dev-guide/who
  summary: Not quite Twurp’s Peerage but a Who’s Who of the Varnish Cache project
  description: "# Who is … ?\n\nNot quite [Twurp’s Peerage](https://wiki.lspace.org/mediawiki/Twurp%27s_Peerage) but a Who’s Who of the Varnish Cache project.\n\n## Anders Berg\n\nBlame Anders! He is the one who got the crazy idea that the world needed another HTTP proxy server software, and convinced his employer, the norwegian newspaper [Verdens Gang](http://www.vg.no) to pay for the first version to be developed.\n\nHere is an interview with Anders about [how it all began](http://info.varnish-software.com/blog/celebrating-10-years-of-varnish-cache-qa-with-the-man-behind-the-idea)\n\n## Dag-Erling Smørgrav\n\nDES was working at Redpill-Linpro, a norwegian UNIX/Open Source company when Anders floated his idea for a “forward HTTP cache”, he lured PHK into joining, was one of the original developers (doing Linux), project manager and release engineer for the first three years of the project, and forced us to adopt a non-US-ASCII charset from the start.\n\n## Poul-Henning Kamp\n\nPHK, as he’s usually known, has written most of the code and come up with most of the crazy ideas in Varnish, and yet he still has trouble remembering what ‘REST’, ‘CORS’ and ‘ALPN’ means, and he flunked ‘CSS for dummies’ because he was never a webmaster or webdeveloper. He does have 30+ years of experience in systems programming, and that seems useful too.\n\nPHK’s [random outbursts](https://varnish-cache.org/docs/trunk/phk/index.html) has their own section in the Varnish documentation.\n\n## Per Buer\n\nPer also worked at Redpill-Linpro, and at some point when the impedance mismatch between Linpros “normal way of doing things” and the potential of Varnish became to steep, he convinced the company to spin off [Varnish Software](https://varnish-software.com/) with himself at the helm.\n\nDo a git blame on the Varnish documentation and you will be surprised to see how much he cares about it. Very few people notice this.\n\n## Ingvar Hagelund\n\nIngvar works as Team Leader (read very skilled sysadmin) at Redpill-Linpro, but his passion is reading books and blogging about it, as well as RPM packaging. So every Fedora and EPEL (read RedHat and CentOS) Varnish user out there owe him a thanks or two. Once in a while, he also trawls the internet checking for the rate of Varnish adoption among top web sites.\n\n## Stig Sandbeck Mathisen\n\nStig works at Redpill-Linpro and is the guy in charge of packaging Varnish for Debian, which means Ubuntu users owe him a thanks also. Besides this, he maintains VCL-mode for emacs and is generally a nice and helpful guy.\n\n## Tollef Fog Heen\n\nTollef was product owner and responsible for Varnish while working for Redpill-Linpro. later tech lead at Varnish Software and held the Varnish release manager helmet for a few years. His experience with open source (Debian, Ubuntu and many others) brought sanity to the project in ways that are hard to measure or describe.\n\n## Kristian Lyngstøl\n\nKristian was the first Varnish SuperUser, and he quite literally wrote the book, while giving Varnish courses for Redpill-Linpro, and he pushed boundaries where no boundaries had been pushed before which caused a lot of improvements and “aha!” moments in Varnish.\n\n## Artur Bergman\n\nArtur ran Wikias webservers and CDN when he discovered Varnish and eagerly adopted it, causing many bugreports, suggestions, patches and improvements. At some point, he pivoted Wikias CDN into the Varnish based startup-CDN named [Fastly](http://www.fastly.com/)\n\n## Kacper Wysocki\n\nKacper was probably the first VCL long program writer. Combine this with an interest in security and a job at Redpill-Linpro and he turned quickly into the author of security.vcl and, later, the Varnish Security Firewall. He does not have any commits in Varnish and still has managed to drive quite a few changes into the project. Similarly, he has no idea or has even thought about asking for it, and still is being added here He maintains the VCL grammar in BNF notation, which is an unexploited gold mine.\n\n## Nils Goroll\n\naka ‘slink’ is the founder of [UPLEX](http://uplex.de/), a five-head tech / consultancy company with negative to zero marketing (applied for entry into the “Earth’s worst company homepage” competition). He fell in love with Varnish when he migrated Germany’s Verdens Gang counterpart over a weekend in March 2009 and, since then, has experienced countless moments of pure joy and happiness when, after struggling for hours, he finally understood another piece of beautiful, ingenious Varnish code.\n\nNils’ primary focus are his clients and their projects. He tries to make those improvements to Varnish which matter to them.\n\n## Martin Blix Grydeland\n\nMartin was the first full-time member of the C-team at Varnish Software. He is the main responsible for the amazing revamp of the logging facilities and utilities in the 4.0 cycle and later the storage rework. Besides that he fixes lots of bugs, knows varnishtest better than most, writes vmods and is the Varnish Cache Plus architect.\n\n## Lasse Karstensen\n\nLasse is the current release manager and stable version maintainer of Varnish Cache. When not doing that, he maintains build infrastructure and runs the Varnish Software C developer team in Oslo.\n\n## Geoff Simmons\n\nGeoff started working at UPLEX in 2010 and soon learned to love Varnish as much as slink does. Since then he’s been contributing code to the project, writing up various VMODs (mostly about regular expressions, blobs, backends and directors), developing standalone applications for logging that use Martin’s VSL API, and adding custom patches to Varnish for various customer needs. He spends most of his days in customer projects as “the Varnish guy” on the operations teams.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/dev-guide/who.html](https://varnish-cache.org/docs/7.4/dev-guide/who.html)"
- name: Writing a Director
  id: reference/directors
  summary: Varnish already provides a set of general-purpose directors, and since Varnish 4, it is bundled in the built-in VMOD directors - Varnish Directors Module
  description: "# Writing a Director\n\nVarnish already provides a set of general-purpose directors, and since Varnish 4, it is bundled in the built-in [VMOD directors - Varnish Directors Module](vmod_directors#vmod-directors-3). Writing a director boils down to writing a VMOD, using the proper data structures and APIs. Not only can you write your own director if none of the built-ins fit your needs, but since Varnish 4.1 you can even write your own backends.\n\nBackends can be categorized as such:\n\n- static: native backends declared in VCL\n- dynamic: native backends created by VMODs\n- custom: backends created and fully managed by VMODs\n\n## Backends vs Directors\n\nThe intuitive classification for backend and director is an endpoint for the former and a loadbalancer for the latter, but the actual implementation is a bit more subtle. VMODs can accept backend arguments and return backends in VCL (see [VCL and C data types](vmod#ref-vmod-vcl-c-types)), but the underlying C type is `struct director` aka the `VCL_BACKEND` typedef. Under the hood director is a generic concept, and a backend is a kind of director.\n\nThe line between the two is somewhat blurry at this point, let’s look at some code instead:\n\n``` python\n// VRT interface from vrt.h\n\nstruct vdi_methods {\n    unsigned                        magic;\n#define VDI_METHODS_MAGIC           0x4ec0c4bb\n    const char                      *type;\n    vdi_http1pipe_f                 *http1pipe;\n    vdi_healthy_f                   *healthy;\n    vdi_resolve_f                   *resolve;\n    vdi_gethdrs_f                   *gethdrs;\n    vdi_getip_f                     *getip;\n    vdi_finish_f                    *finish;\n    vdi_event_f                     *event;\n    vdi_release_f                   *release;\n    vdi_destroy_f                   *destroy;\n    vdi_panic_f                     *panic;\n    vdi_list_f                      *list;\n};\n\nstruct director {\n    unsigned                        magic;\n#define DIRECTOR_MAGIC              0x3336351d\n    void                            *priv;\n    char                            *vcl_name;\n    struct vcldir                   *vdir;\n    struct lock                     *mtx;\n};\n```\n\nA director can be summed up as:\n\n- being of a specific `type` with a set of operations which is identical for all instances of that particular type\n- some instance specific attributes such as a `vcl_name` and `type`-specific private data\n\nThe difference between a *load balancing* director and a *backend* director is mainly the functions they will implement.\n\nThe fundamental steps towards a director implementation are:\n\n- implement the required functions\n\n- fill a `struct vdi_methods` with the name of your director type and your function pointers\n\n  Existence of a `healthy` callback signifies that the director has some means of dynamically determining its health state.\n\n- in your constructor or other initialization routine, allocate and initialize your director-specific configuration state (aka private data) and call `VRT_AddDirector()` with your `struct vdi_methods`, the pointer to your state and a printf format for the name of your director instance\n\n- implement methods or functions returning `VCL_BACKEND`\n\n- in your destructor or other finalizer, call `VRT_DelDirector()`\n\n- implement a `destroy` callback to destroy the actual director private state. It will be called when all references to the director are gone, until then the private state must remain intact and `vdi_methods` functions callable (but they may return errors).\n\nWhile vmods can implement functions returning directors, [Objects and methods](vmod#ref-vmod-vcl-c-objects) are usually a more natural representation with vmod object instances being or referring to the director private data.\n\n## Load Balancing Directors\n\nAs in [VMOD directors - Varnish Directors Module](vmod_directors#vmod-directors-3), you can write directors that will group backends sharing the same role, and pick them according to a strategy. If you need more than the built-in strategies (round-robin, hash, …), even though they can be stacked, it is always possible to write your own.\n\nIn this case you simply need to implement the `resolve` function for the director. Directors are walked until a leaf director is found. A leaf director doesn’t have a `resolve` function and is used to actually make the backend request, just like the backends you declare in VCL.\n\n*load balancing* directors use `VRT_Assign_Backend()` to take references to other directors. They *must* implement a `release` callback which has to release all references to other directors and ensure that none are gained after it returns.\n\n## Static Directors\n\nAs opposed to dynamic backends covered below, directors which are guaranteed to have VCL lifetime (that is, they do not get destroyed before the VCL goes cold) can call `VRT_StaticDirector()` to avoid reference counting overhead.\n\n## Dynamic Backends\n\nIf you want to speak HTTP/1 over TCP or UDS, but for some reason VCL does not fit the bill, you can instead reuse the whole backend facility. It allows you for instance to add and remove backends on-demand without the need to reload your VCL. You can then leverage your provisioning system.\n\nConsider the following snippet:\n\n``` python\nbackend default {\n    .host = \"localhost\";\n}\n```\n\nThe VCL compiler turns this declaration into a `struct vrt_backend`. When the VCL is loaded, Varnish calls `VRT_new_backend` (or rather `VRT_new_backend_clustered` for VSM efficiency) in order to create the director. Varnish doesn’t expose its data structure for actual backends, only the director abstraction and dynamic backends are built just like static backends, one *struct* at a time. You can get rid of the `struct vrt_backend` as soon as you have the `struct director`.\n\nA (dynamic) backend can’t exceed its VCL’s lifespan, because native backends are *owned* by VCLs. Though a dynamic backend can’t outlive its VCL, it can be deleted any time with `VRT_delete_backend`. The VCL will delete the remaining backends once discarded, you don’t need to take care of it.\n\nReference counting is used to ensure that backends which are no longer referenced are destroyed.\n\nFinally, Varnish will take care of event propagation for *all* native backends, but dynamic backends can only be created when the VCL is warm. If your backends are created by an independent thread (basically outside of VCL scope) you must subscribe to VCL events and watch for VCL state (see [Event functions](vmod#ref-vmod-event-functions)). Varnish will panic if you try to create a backend on a cold VCL, and `VRT_new_backend` will return `NULL` if the VCL is cooling. You are also encouraged to comply with the [VCL Temperature](varnish-cli#ref-vcl-temperature) in general.\n\n## Health Probes\n\nIt is possible in a VCL program to query the health of a director (see [BOOL healthy(BACKEND be)](vmod_std#std-healthy)). A director can report its health if it implements the `healthy` function, it is otherwise always considered healthy.\n\nUnless you are making a dynamic backend, you need to take care of the health probes yourselves. For *load balancing* directors, being healthy typically means having at least one healthy underlying backend or director.\n\nFor dynamic backends, it is just a matter of assigning the `probe` field in the `struct vrt_backend`. Once the director is created, the probe definition too is no longer needed. It is then Varnish that will take care of the health probe and disable the feature on a cold VCL (see [Event functions](vmod#ref-vmod-event-functions)).\n\nInstead of initializing your own probe definition, you can get a `VCL_PROBE` directly built from VCL (see [VCL and C data types](vmod#ref-vmod-vcl-c-types)).\n\n## Custom Backends\n\nIf you want to implement a custom backend, have a look at how Varnish implements native backends. It is the canonical implementation, and though it provides other services like connection pooling or statistics, it is essentially a director which state is a `struct backend`. Varnish native backends currently speak HTTP/1 over TCP or UDS, and as such, you need to make your own custom backend if you want Varnish to do otherwise such as connect over UDP or speak a different protocol.\n\nIf you want to leverage probes declarations in VCL, which have the advantage of being reusable since they are only specifications, you can. However, you need to implement the whole probing infrastructure from scratch.\n\nYou may also consider making your custom backend compliant with regards to the VCL state (see [Event functions](vmod#ref-vmod-event-functions)).\n\nIf you are implementing the `gethdrs` method of your backend (i.e. your backend is able to generate a backend response to be manipulated in `vcl_backend_response`), you will want to log the response code, protocol and the various headers it’ll create for easier debugging. For this, you can look at the `VSL*` family of functions, listed in `cache/cache.h`.\n\n### Data structure considerations\n\nWhen you are creating a custom backend, you may want to provide the semantics of the native backends. In this case, instead of repeating the redundant fields between data structures, you can use the macros `VRT_BACKEND_FIELDS` and `VRT_BACKEND_PROBE_FIELDS` to declare them all at once. This is the little dance Varnish uses to copy data between the `struct vrt_backend` and its internal data structure for example.\n\nThe copy can be automated with the macros `VRT_BACKEND_HANDLE` and `VRT_BACKEND_PROBE_HANDLE`. You can look at how they can be used in the Varnish code base.\n\nCopyright © 2006 Verdens Gang AS  \nCopyright © 2006–2020 Varnish Software AS  \nLicensed under the BSD-2-Clause License.  \n[https://varnish-cache.org/docs/7.4/reference/directors.html](https://varnish-cache.org/docs/7.4/reference/directors.html)"
