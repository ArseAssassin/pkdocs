---
name: scrapy
text_format: rst
generator: src:python
generator_command: src:python use "scrapy"
version: 2.11.2
description: ".. image:: https://scrapy.org/img/scrapylogo.png\n   :target: https://scrapy.org/\n   \n======\nScrapy\n======\n\n.. image:: https://img.shields.io/pypi/v/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/pypi/pyversions/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: Supported Python Versions\n\n.. image:: https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg\n   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu\n   :alt: Ubuntu\n\n.. .. image:: https://github.com/scrapy/scrapy/workflows/macOS/badge.svg\n   .. :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AmacOS\n   .. :alt: macOS\n\n\n.. image:: https://github.com/scrapy/scrapy/workflows/Windows/badge.svg\n   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows\n   :alt: Windows\n\n.. image:: https://img.shields.io/badge/wheel-yes-brightgreen.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: Wheel Status\n\n.. image:: https://img.shields.io/codecov/c/github/scrapy/scrapy/master.svg\n   :target: https://codecov.io/github/scrapy/scrapy?branch=master\n   :alt: Coverage report\n\n.. image:: https://anaconda.org/conda-forge/scrapy/badges/version.svg\n   :target: https://anaconda.org/conda-forge/scrapy\n   :alt: Conda Version\n\n\nOverview\n========\n\nScrapy is a BSD-licensed fast high-level web crawling and web scraping framework, used to\ncrawl websites and extract structured data from their pages. It can be used for\na wide range of purposes, from data mining to monitoring and automated testing.\n\nScrapy is maintained by Zyte_ (formerly Scrapinghub) and `many other\ncontributors`_.\n\n.. _many other contributors: https://github.com/scrapy/scrapy/graphs/contributors\n.. _Zyte: https://www.zyte.com/\n\nCheck the Scrapy homepage at https://scrapy.org for more information,\nincluding a list of features.\n\n\nRequirements\n============\n\n* Python 3.8+\n* Works on Linux, Windows, macOS, BSD\n\nInstall\n=======\n\nThe quick way:\n\n.. code:: bash\n\n    pip install scrapy\n\nSee the install section in the documentation at\nhttps://docs.scrapy.org/en/latest/intro/install.html for more details.\n\nDocumentation\n=============\n\nDocumentation is available online at https://docs.scrapy.org/ and in the ``docs``\ndirectory.\n\nReleases\n========\n\nYou can check https://docs.scrapy.org/en/latest/news.html for the release notes.\n\nCommunity (blog, twitter, mail list, IRC)\n=========================================\n\nSee https://scrapy.org/community/ for details.\n\nContributing\n============\n\nSee https://docs.scrapy.org/en/master/contributing.html for details.\n\nCode of Conduct\n---------------\n\nPlease note that this project is released with a Contributor `Code of Conduct <https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md>`_.\n\nBy participating in this project you agree to abide by its terms.\nPlease report unacceptable behavior to opensource@zyte.com.\n\nCompanies using Scrapy\n======================\n\nSee https://scrapy.org/companies/ for a list.\n\nCommercial Support\n==================\n\nSee https://scrapy.org/support/ for details.\n"
homepage: https://scrapy.org
license: BSD

---
- name: scrapy
  kind: module
  ns: null
  description: Scrapy - a web crawling and web scraping framework written for Python
  summary: Scrapy - a web crawling and web scraping framework written for Python
  signatures: null
  inherits_from: null
- name: version_info
  kind: const
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: twisted_version
  kind: const
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Spider
  kind: class
  ns: scrapy
  description: |-
    Base class for scrapy spiders. All spiders must inherit from this
    class.
  summary: Base class for scrapy spiders
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: None
    rest: false
  - type: Spider
  inherits_from:
  - <class 'scrapy.utils.trackref.object_ref'>
- name: Spider.close
  kind: method
  ns: scrapy
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Spider.custom_settings
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Spider.from_crawler
  kind: function
  ns: scrapy
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Spider.handles_request
  kind: function
  ns: scrapy
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Spider.log
  kind: method
  ns: scrapy
  description: |-
    Log the given message at the given log level

    This helper wraps a log call to the logger within the spider, but you
    can use it directly (e.g. Spider.logger.info('msg')) or use any other
    Python logger too.
  summary: Log the given message at the given log level
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: message
    default: null
    rest: false
  - kind: positional
    name: level
    default: '10'
    rest: false
  - type: '?'
  inherits_from: null
- name: Spider.logger
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Spider.parse
  kind: method
  ns: scrapy
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Spider.start_requests
  kind: method
  ns: scrapy
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Spider.update_settings
  kind: function
  ns: scrapy
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Request
  kind: class
  ns: scrapy
  description: |-
    Represents an HTTP request, which is usually generated in a Spider and
    executed by the Downloader, thus generating a :class:`Response`.
  summary: Represents an HTTP request, which is usually generated in a Spider and
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: callback
    default: None
    rest: false
  - kind: positional
    name: method
    default: GET
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: body
    default: None
    rest: false
  - kind: positional
    name: cookies
    default: None
    rest: false
  - kind: positional
    name: meta
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: utf-8
    rest: false
  - kind: positional
    name: priority
    default: '0'
    rest: false
  - kind: positional
    name: dont_filter
    default: 'False'
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - kind: positional
    name: flags
    default: None
    rest: false
  - kind: positional
    name: cb_kwargs
    default: None
    rest: false
  - type: Request
  inherits_from:
  - <class 'scrapy.utils.trackref.object_ref'>
- name: Request.attributes
  kind: property
  ns: scrapy
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: Request.body
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Request.cb_kwargs
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Request.copy
  kind: method
  ns: scrapy
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Request.encoding
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Request.from_curl
  kind: function
  ns: scrapy
  description: |-
    Create a Request object from a string containing a `cURL
    <https://curl.haxx.se/>`_ command. It populates the HTTP method, the
    URL, the headers, the cookies and the body. It accepts the same
    arguments as the :class:`Request` class, taking preference and
    overriding the values of the same arguments contained in the cURL
    command.

    Unrecognized options are ignored by default. To raise an error when
    finding unknown options call this method by passing
    ``ignore_unknown_options=False``.

    .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`
                 subclasses, such as :class:`~scrapy.http.JSONRequest`, or
                 :class:`~scrapy.http.XmlRpcRequest`, as well as having
                 :ref:`downloader middlewares <topics-downloader-middleware>`
                 and
                 :ref:`spider middlewares <topics-spider-middleware>`
                 enabled, such as
                 :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,
                 :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,
                 or
                 :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,
                 may modify the :class:`~scrapy.http.Request` object.

    To translate a cURL command into a Scrapy request,
    you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.
  summary: Create a Request object from a string containing a `cURL
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: curl_command
    default: null
    rest: false
  - kind: positional
    name: ignore_unknown_options
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: Request.meta
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Request.replace
  kind: method
  ns: scrapy
  description: Create a new Request with the same attributes except for those given new values
  summary: Create a new Request with the same attributes except for those given new values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Request.to_dict
  kind: method
  ns: scrapy
  description: |-
    Return a dictionary containing the Request's data.

    Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.

    If a spider is given, this method will try to find out the name of the spider methods used as callback
    and errback and include them in the output dict, raising an exception if they cannot be found.
  summary: Return a dictionary containing the Request's data
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: spider
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: Request.url
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FormRequest
  kind: class
  ns: scrapy
  description: |-
    Represents an HTTP request, which is usually generated in a Spider and
    executed by the Downloader, thus generating a :class:`Response`.
  summary: Represents an HTTP request, which is usually generated in a Spider and
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: formdata
    default: None
    rest: false
    kind: kw-only
  - type: FormRequest
  inherits_from:
  - <class 'scrapy.http.request.Request'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: FormRequest.attributes
  kind: property
  ns: scrapy
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: FormRequest.body
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FormRequest.cb_kwargs
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FormRequest.copy
  kind: method
  ns: scrapy
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormRequest.encoding
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FormRequest.from_curl
  kind: function
  ns: scrapy
  description: |-
    Create a Request object from a string containing a `cURL
    <https://curl.haxx.se/>`_ command. It populates the HTTP method, the
    URL, the headers, the cookies and the body. It accepts the same
    arguments as the :class:`Request` class, taking preference and
    overriding the values of the same arguments contained in the cURL
    command.

    Unrecognized options are ignored by default. To raise an error when
    finding unknown options call this method by passing
    ``ignore_unknown_options=False``.

    .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`
                 subclasses, such as :class:`~scrapy.http.JSONRequest`, or
                 :class:`~scrapy.http.XmlRpcRequest`, as well as having
                 :ref:`downloader middlewares <topics-downloader-middleware>`
                 and
                 :ref:`spider middlewares <topics-spider-middleware>`
                 enabled, such as
                 :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,
                 :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,
                 or
                 :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,
                 may modify the :class:`~scrapy.http.Request` object.

    To translate a cURL command into a Scrapy request,
    you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.
  summary: Create a Request object from a string containing a `cURL
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: curl_command
    default: null
    rest: false
  - kind: positional
    name: ignore_unknown_options
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: FormRequest.from_response
  kind: function
  ns: scrapy
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: formname
    default: None
    rest: false
  - kind: positional
    name: formid
    default: None
    rest: false
  - kind: positional
    name: formnumber
    default: '0'
    rest: false
  - kind: positional
    name: formdata
    default: None
    rest: false
  - kind: positional
    name: clickdata
    default: None
    rest: false
  - kind: positional
    name: dont_click
    default: 'False'
    rest: false
  - kind: positional
    name: formxpath
    default: None
    rest: false
  - kind: positional
    name: formcss
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormRequest.meta
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FormRequest.replace
  kind: method
  ns: scrapy
  description: Create a new Request with the same attributes except for those given new values
  summary: Create a new Request with the same attributes except for those given new values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormRequest.to_dict
  kind: method
  ns: scrapy
  description: |-
    Return a dictionary containing the Request's data.

    Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.

    If a spider is given, this method will try to find out the name of the spider methods used as callback
    and errback and include them in the output dict, raising an exception if they cannot be found.
  summary: Return a dictionary containing the Request's data
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: spider
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: FormRequest.url
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FormRequest.valid_form_methods
  kind: property
  ns: scrapy
  description: |-
    Built-in mutable sequence.

    If no argument is given, the constructor creates a new empty list.
    The argument must be an iterable if specified.
  summary: Built-in mutable sequence
  signatures: null
  inherits_from: null
- name: Selector
  kind: class
  ns: scrapy
  description: |-
    An instance of :class:`Selector` is a wrapper over response to select
    certain parts of its content.

    ``response`` is an :class:`~scrapy.http.HtmlResponse` or an
    :class:`~scrapy.http.XmlResponse` object that will be used for selecting
    and extracting data.

    ``text`` is a unicode string or utf-8 encoded text for cases when a
    ``response`` isn't available. Using ``text`` and ``response`` together is
    undefined behavior.

    ``type`` defines the selector type, it can be ``"html"``, ``"xml"``, ``"json"``
    or ``None`` (default).

    If ``type`` is ``None``, the selector automatically chooses the best type
    based on ``response`` type (see below), or defaults to ``"html"`` in case it
    is used together with ``text``.

    If ``type`` is ``None`` and a ``response`` is passed, the selector type is
    inferred from the response type as follows:

    * ``"html"`` for :class:`~scrapy.http.HtmlResponse` type
    * ``"xml"`` for :class:`~scrapy.http.XmlResponse` type
    * ``"html"`` for anything else

    Otherwise, if ``type`` is set, the selector type will be forced and no
    detection will occur.
  summary: An instance of :class:`Selector` is a wrapper over response to select
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: None
    rest: false
  - kind: positional
    name: text
    default: None
    rest: false
  - kind: positional
    name: type
    default: None
    rest: false
  - kind: positional
    name: root
    default: <object object at 0x7fa9743b1650>
    rest: false
  - type: Selector
  inherits_from:
  - <class 'parsel.selector.Selector'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: Selector.attrib
  kind: property
  ns: scrapy
  description: Return the attributes dictionary for underlying element.
  summary: Return the attributes dictionary for underlying element
  signatures: null
  inherits_from: null
- name: Selector.body
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Selector.css
  kind: method
  ns: scrapy
  description: |-
    Apply the given CSS selector and return a :class:`SelectorList` instance.

    ``query`` is a string containing the CSS selector to apply.

    In the background, CSS queries are translated into XPath queries using
    `cssselect`_ library and run ``.xpath()`` method.

    .. _cssselect: https://pypi.python.org/pypi/cssselect/
  summary: Apply the given CSS selector and return a :class:`SelectorList` instance
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.drop
  kind: method
  ns: scrapy
  description: Drop matched nodes from the parent element.
  summary: Drop matched nodes from the parent element
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.extract
  kind: method
  ns: scrapy
  description: |-
    Serialize and return the matched nodes.

    For HTML and XML, the result is always a string, and percent-encoded
    content is unquoted.
  summary: Serialize and return the matched nodes
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.get
  kind: method
  ns: scrapy
  description: |-
    Serialize and return the matched nodes.

    For HTML and XML, the result is always a string, and percent-encoded
    content is unquoted.
  summary: Serialize and return the matched nodes
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.getall
  kind: method
  ns: scrapy
  description: Serialize and return the matched node in a 1-element list of strings.
  summary: Serialize and return the matched node in a 1-element list of strings
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.jmespath
  kind: method
  ns: scrapy
  description: |-
    Find objects matching the JMESPath ``query`` and return the result as a
    :class:`SelectorList` instance with all elements flattened. List
    elements implement :class:`Selector` interface too.

    ``query`` is a string containing the `JMESPath
    <https://jmespath.org/>`_ query to apply.

    Any additional named arguments are passed to the underlying
    ``jmespath.search`` call, e.g.::

        selector.jmespath('author.name', options=jmespath.Options(dict_cls=collections.OrderedDict))
  summary: Find objects matching the JMESPath ``query`` and return the result as a
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.namespaces
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Selector.re
  kind: method
  ns: scrapy
  description: |-
    Apply the given regex and return a list of strings with the
    matches.

    ``regex`` can be either a compiled regular expression or a string which
    will be compiled to a regular expression using ``re.compile(regex)``.

    By default, character entity references are replaced by their
    corresponding character (except for ``&amp;`` and ``&lt;``).
    Passing ``replace_entities`` as ``False`` switches off these
    replacements.
  summary: Apply the given regex and return a list of strings with the
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: regex
    default: null
    rest: false
  - kind: positional
    name: replace_entities
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.re_first
  kind: method
  ns: scrapy
  description: |-
    Apply the given regex and return the first string which matches. If
    there is no match, return the default value (``None`` if the argument
    is not provided).

    By default, character entity references are replaced by their
    corresponding character (except for ``&amp;`` and ``&lt;``).
    Passing ``replace_entities`` as ``False`` switches off these
    replacements.
  summary: Apply the given regex and return the first string which matches
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: regex
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: replace_entities
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.register_namespace
  kind: method
  ns: scrapy
  description: |-
    Register the given namespace to be used in this :class:`Selector`.
    Without registering namespaces you can't select or extract data from
    non-standard namespaces. See :ref:`selector-examples-xml`.
  summary: Register the given namespace to be used in this :class:`Selector`
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: prefix
    default: null
    rest: false
  - kind: positional
    name: uri
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.remove
  kind: method
  ns: scrapy
  description: Remove matched nodes from the parent element.
  summary: Remove matched nodes from the parent element
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.remove_namespaces
  kind: method
  ns: scrapy
  description: |-
    Remove all namespaces, allowing to traverse the document using
    namespace-less xpaths. See :ref:`removing-namespaces`.
  summary: Remove all namespaces, allowing to traverse the document using
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.response
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Selector.root
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Selector.selectorlist_cls
  kind: class
  ns: scrapy
  description: |-
    The :class:`SelectorList` class is a subclass of the builtin ``list``
    class, which provides a few additional methods.
  summary: The :class:`SelectorList` class is a subclass of the builtin ``list``
  signatures:
  - kind: positional
    name: iterable
    default: ()
    rest: false
  - type: SelectorList
  inherits_from:
  - <class 'parsel.selector.SelectorList'>
  - <class 'list'>
  - <class 'typing.Generic'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: Selector.selectorlist_cls.append
  kind: callable
  ns: scrapy
  description: Append object to the end of the list.
  summary: Append object to the end of the list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: object
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.attrib
  kind: property
  ns: scrapy
  description: |-
    Return the attributes dictionary for the first element.
    If the list is empty, return an empty dict.
  summary: Return the attributes dictionary for the first element
  signatures: null
  inherits_from: null
- name: Selector.selectorlist_cls.clear
  kind: callable
  ns: scrapy
  description: Remove all items from list.
  summary: Remove all items from list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.copy
  kind: callable
  ns: scrapy
  description: Return a shallow copy of the list.
  summary: Return a shallow copy of the list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.count
  kind: callable
  ns: scrapy
  description: Return number of occurrences of value.
  summary: Return number of occurrences of value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.css
  kind: method
  ns: scrapy
  description: |-
    Call the ``.css()`` method for each element in this list and return
    their results flattened as another :class:`SelectorList`.

    ``query`` is the same argument as the one in :meth:`Selector.css`
  summary: Call the ``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.drop
  kind: method
  ns: scrapy
  description: Drop matched nodes from the parent for each element in this list.
  summary: Drop matched nodes from the parent for each element in this list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.extend
  kind: callable
  ns: scrapy
  description: Extend list by appending elements from the iterable.
  summary: Extend list by appending elements from the iterable
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: iterable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.extract
  kind: method
  ns: scrapy
  description: |-
    Call the ``.get()`` method for each element is this list and return
    their results flattened, as a list of strings.
  summary: Call the ``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.extract_first
  kind: method
  ns: scrapy
  description: |-
    Return the result of ``.get()`` for the first element in this list.
    If the list is empty, return the default value.
  summary: Return the result of ``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.get
  kind: method
  ns: scrapy
  description: |-
    Return the result of ``.get()`` for the first element in this list.
    If the list is empty, return the default value.
  summary: Return the result of ``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.getall
  kind: method
  ns: scrapy
  description: |-
    Call the ``.get()`` method for each element is this list and return
    their results flattened, as a list of strings.
  summary: Call the ``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.index
  kind: callable
  ns: scrapy
  description: |-
    Return first index of value.

    Raises ValueError if the value is not present.
  summary: Return first index of value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: start
    default: '0'
    rest: false
  - kind: positional
    name: stop
    default: '9223372036854775807'
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.insert
  kind: callable
  ns: scrapy
  description: Insert object before index.
  summary: Insert object before index
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: index
    default: null
    rest: false
  - kind: positional
    name: object
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.jmespath
  kind: method
  ns: scrapy
  description: |-
    Call the ``.jmespath()`` method for each element in this list and return
    their results flattened as another :class:`SelectorList`.

    ``query`` is the same argument as the one in :meth:`Selector.jmespath`.

    Any additional named arguments are passed to the underlying
    ``jmespath.search`` call, e.g.::

        selector.jmespath('author.name', options=jmespath.Options(dict_cls=collections.OrderedDict))
  summary: Call the ``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.pop
  kind: callable
  ns: scrapy
  description: |-
    Remove and return item at index (default last).

    Raises IndexError if list is empty or index is out of range.
  summary: Remove and return item at index (default last)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: index
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.re
  kind: method
  ns: scrapy
  description: |-
    Call the ``.re()`` method for each element in this list and return
    their results flattened, as a list of strings.

    By default, character entity references are replaced by their
    corresponding character (except for ``&amp;`` and ``&lt;``.
    Passing ``replace_entities`` as ``False`` switches off these
    replacements.
  summary: Call the ``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: regex
    default: null
    rest: false
  - kind: positional
    name: replace_entities
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.re_first
  kind: method
  ns: scrapy
  description: |-
    Call the ``.re()`` method for the first element in this list and
    return the result in an string. If the list is empty or the
    regex doesn't match anything, return the default value (``None`` if
    the argument is not provided).

    By default, character entity references are replaced by their
    corresponding character (except for ``&amp;`` and ``&lt;``.
    Passing ``replace_entities`` as ``False`` switches off these
    replacements.
  summary: Call the ``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: regex
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: replace_entities
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.remove
  kind: method
  ns: scrapy
  description: Remove matched nodes from the parent for each element in this list.
  summary: Remove matched nodes from the parent for each element in this list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.reverse
  kind: callable
  ns: scrapy
  description: Reverse *IN PLACE*.
  summary: Reverse *IN PLACE*
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.sort
  kind: callable
  ns: scrapy
  description: |-
    Sort the list in ascending order and return None.

    The sort is in-place (i.e. the list itself is modified) and stable (i.e. the
    order of two equal elements is maintained).

    If a key function is given, apply it once to each list item and sort them,
    ascending or descending, according to their function values.

    The reverse flag can be set to sort in descending order.
  summary: Sort the list in ascending order and return None
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: key
    default: None
    rest: false
    kind: kw-only
  - name: reverse
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: Selector.selectorlist_cls.xpath
  kind: method
  ns: scrapy
  description: |-
    Call the ``.xpath()`` method for each element in this list and return
    their results flattened as another :class:`SelectorList`.

    ``xpath`` is the same argument as the one in :meth:`Selector.xpath`

    ``namespaces`` is an optional ``prefix: namespace-uri`` mapping (dict)
    for additional prefixes to those registered with ``register_namespace(prefix, uri)``.
    Contrary to ``register_namespace()``, these prefixes are not
    saved for future calls.

    Any additional named arguments can be used to pass values for XPath
    variables in the XPath expression, e.g.::

        selector.xpath('//a[href=$url]', url="http://www.example.com")
  summary: Call the ``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Selector.type
  kind: property
  ns: scrapy
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Selector.xpath
  kind: method
  ns: scrapy
  description: |-
    Find nodes matching the xpath ``query`` and return the result as a
    :class:`SelectorList` instance with all elements flattened. List
    elements implement :class:`Selector` interface too.

    ``query`` is a string containing the XPATH query to apply.

    ``namespaces`` is an optional ``prefix: namespace-uri`` mapping (dict)
    for additional prefixes to those registered with ``register_namespace(prefix, uri)``.
    Contrary to ``register_namespace()``, these prefixes are not
    saved for future calls.

    Any additional named arguments can be used to pass values for XPath
    variables in the XPath expression, e.g.::

        selector.xpath('//a[href=$url]', url="http://www.example.com")
  summary: Find nodes matching the xpath ``query`` and return the result as a
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Item
  kind: class
  ns: scrapy
  description: |-
    Base class for scraped items.

    In Scrapy, an object is considered an ``item`` if it is an instance of either
    :class:`Item` or :class:`dict`, or any subclass. For example, when the output of a
    spider callback is evaluated, only instances of :class:`Item` or
    :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.

    If you need instances of a custom class to be considered items by Scrapy,
    you must inherit from either :class:`Item` or :class:`dict`.

    Items must declare :class:`Field` attributes, which are processed and stored
    in the ``fields`` attribute. This restricts the set of allowed field names
    and prevents typos, raising ``KeyError`` when referring to undefined fields.
    Additionally, fields can be used to define metadata and control the way
    data is processed internally. Please refer to the :ref:`documentation
    about fields <topics-items-fields>` for additional information.

    Unlike instances of :class:`dict`, instances of :class:`Item` may be
    :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.
  summary: Base class for scraped items
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: Item
  inherits_from:
  - <class 'collections.abc.MutableMapping'>
  - <class 'collections.abc.Mapping'>
  - <class 'collections.abc.Collection'>
  - <class 'collections.abc.Sized'>
  - <class 'collections.abc.Iterable'>
  - <class 'collections.abc.Container'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: Item.clear
  kind: method
  ns: scrapy
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Item.copy
  kind: method
  ns: scrapy
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Item.deepcopy
  kind: method
  ns: scrapy
  description: Return a :func:`~copy.deepcopy` of this item.
  summary: Return a :func:`~copy
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Item.fields
  kind: property
  ns: scrapy
  description: |-
    dict() -> new empty dictionary
    dict(mapping) -> new dictionary initialized from a mapping object's
        (key, value) pairs
    dict(iterable) -> new dictionary initialized as if via:
        d = {}
        for k, v in iterable:
            d[k] = v
    dict(**kwargs) -> new dictionary initialized with the name=value pairs
        in the keyword argument list.  For example:  dict(one=1, two=2)
  summary: dict() -> new empty dictionary
  signatures: null
  inherits_from: null
- name: Item.get
  kind: method
  ns: scrapy
  description: D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Item.items
  kind: method
  ns: scrapy
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Item.keys
  kind: method
  ns: scrapy
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Item.pop
  kind: method
  ns: scrapy
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
    If key is not found, d is returned if given, otherwise KeyError is raised.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: <object object at 0x7fa9743b0160>
    rest: false
  - type: '?'
  inherits_from: null
- name: Item.popitem
  kind: method
  ns: scrapy
  description: |-
    D.popitem() -> (k, v), remove and return some (key, value) pair
    as a 2-tuple; but raise KeyError if D is empty.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Item.setdefault
  kind: method
  ns: scrapy
  description: D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Item.update
  kind: method
  ns: scrapy
  description: |-
    D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.
    If E present and has a .keys() method, does:     for k in E: D[k] = E[k]
    If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v
    In either case, this is followed by: for k, v in F.items(): D[k] = v
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: ()
    rest: false
  - type: '?'
  inherits_from: null
- name: Item.values
  kind: method
  ns: scrapy
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Field
  kind: class
  ns: scrapy
  description: Container of field metadata
  summary: Container of field metadata
  signatures: null
  inherits_from:
  - <class 'dict'>
- name: Field.clear
  kind: callable
  ns: scrapy
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures: null
  inherits_from: null
- name: Field.copy
  kind: callable
  ns: scrapy
  description: D.copy() -> a shallow copy of D
  summary: D
  signatures: null
  inherits_from: null
- name: Field.get
  kind: callable
  ns: scrapy
  description: Return the value for key if key is in the dictionary, else default.
  summary: Return the value for key if key is in the dictionary, else default
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Field.items
  kind: callable
  ns: scrapy
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures: null
  inherits_from: null
- name: Field.keys
  kind: callable
  ns: scrapy
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures: null
  inherits_from: null
- name: Field.pop
  kind: callable
  ns: scrapy
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.

    If the key is not found, return the default if given; otherwise,
    raise a KeyError.
  summary: D
  signatures: null
  inherits_from: null
- name: Field.popitem
  kind: callable
  ns: scrapy
  description: |-
    Remove and return a (key, value) pair as a 2-tuple.

    Pairs are returned in LIFO (last-in, first-out) order.
    Raises KeyError if the dict is empty.
  summary: Remove and return a (key, value) pair as a 2-tuple
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Field.setdefault
  kind: callable
  ns: scrapy
  description: |-
    Insert key with a value of default if key is not in the dictionary.

    Return the value for key if key is in the dictionary, else default.
  summary: Insert key with a value of default if key is not in the dictionary
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Field.update
  kind: callable
  ns: scrapy
  description: |-
    D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.
    If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
    If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
    In either case, this is followed by: for k in F:  D[k] = F[k]
  summary: D
  signatures: null
  inherits_from: null
- name: Field.values
  kind: callable
  ns: scrapy
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures: null
  inherits_from: null
- name: scrapy.addons
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: AddonManager
  kind: class
  ns: scrapy.addons
  description: This class facilitates loading and storing :ref:`topics-addons`.
  summary: This class facilitates loading and storing :ref:`topics-addons`
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: AddonManager
  inherits_from: null
- name: AddonManager.load_settings
  kind: method
  ns: scrapy.addons
  description: |-
    Load add-ons and configurations from a settings object and apply them.

    This will load the add-on for every add-on path in the
    ``ADDONS`` setting and execute their ``update_settings`` methods.

    :param settings: The :class:`~scrapy.settings.Settings` object from             which to read the add-on configuration
    :type settings: :class:`~scrapy.settings.Settings`
  summary: Load add-ons and configurations from a settings object and apply them
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Any
  kind: class
  ns: scrapy.addons
  description: |-
    Special type indicating an unconstrained type.

    - Any is compatible with every type.
    - Any assumed to have all methods.
    - All values assumed to be instances of Any.

    Note that all the above statements are true from the point of view of
    static type checkers. At runtime, Any should not be used with instance
    checks.
  summary: Special type indicating an unconstrained type
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - type: Any
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.addons
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: NotConfigured
  kind: class
  ns: scrapy.addons
  description: Indicates a missing configuration situation
  summary: Indicates a missing configuration situation
  signatures: null
  inherits_from:
  - <class 'Exception'>
  - <class 'BaseException'>
- name: NotConfigured.add_note
  kind: callable
  ns: scrapy.addons
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: NotConfigured.args
  kind: property
  ns: scrapy.addons
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: NotConfigured.with_traceback
  kind: callable
  ns: scrapy.addons
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: Settings
  kind: class
  ns: scrapy.addons
  description: |-
    This object stores Scrapy settings for the configuration of internal
    components, and can be used for any further customization.

    It is a direct subclass and supports all methods of
    :class:`~scrapy.settings.BaseSettings`. Additionally, after instantiation
    of this class, the new object will have the global default settings
    described on :ref:`topics-settings-ref` already populated.
  summary: This object stores Scrapy settings for the configuration of internal
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: values
    default: None
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: Settings
  inherits_from:
  - <class 'scrapy.settings.BaseSettings'>
  - <class 'collections.abc.MutableMapping'>
  - <class 'collections.abc.Mapping'>
  - <class 'collections.abc.Collection'>
  - <class 'collections.abc.Sized'>
  - <class 'collections.abc.Iterable'>
  - <class 'collections.abc.Container'>
  - <class 'typing.Generic'>
- name: Settings.clear
  kind: method
  ns: scrapy.addons
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.copy
  kind: method
  ns: scrapy.addons
  description: |-
    Make a deep copy of current settings.

    This method returns a new instance of the :class:`Settings` class,
    populated with the same values and their priorities.

    Modifications to the new object won't be reflected on the original
    settings.
  summary: Make a deep copy of current settings
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.copy_to_dict
  kind: method
  ns: scrapy.addons
  description: |-
    Make a copy of current settings and convert to a dict.

    This method returns a new dict populated with the same values
    and their priorities as the current settings.

    Modifications to the returned dict won't be reflected on the original
    settings.

    This method can be useful for example for printing settings
    in Scrapy shell.
  summary: Make a copy of current settings and convert to a dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.delete
  kind: method
  ns: scrapy.addons
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.freeze
  kind: method
  ns: scrapy.addons
  description: |-
    Disable further changes to the current settings.

    After calling this method, the present state of the settings will become
    immutable. Trying to change values through the :meth:`~set` method and
    its variants won't be possible and will be alerted.
  summary: Disable further changes to the current settings
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.frozencopy
  kind: method
  ns: scrapy.addons
  description: |-
    Return an immutable copy of the current settings.

    Alias for a :meth:`~freeze` call in the object returned by :meth:`copy`.
  summary: Return an immutable copy of the current settings
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.get
  kind: method
  ns: scrapy.addons
  description: |-
    Get a setting value without affecting its original type.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value without affecting its original type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.getbool
  kind: method
  ns: scrapy.addons
  description: |-
    Get a setting value as a boolean.

    ``1``, ``'1'``, `True`` and ``'True'`` return ``True``,
    while ``0``, ``'0'``, ``False``, ``'False'`` and ``None`` return ``False``.

    For example, settings populated through environment variables set to
    ``'0'`` will return ``False`` when using this method.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value as a boolean
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.getdict
  kind: method
  ns: scrapy.addons
  description: |-
    Get a setting value as a dictionary. If the setting original type is a
    dictionary, a copy of it will be returned. If it is a string it will be
    evaluated as a JSON dictionary. In the case that it is a
    :class:`~scrapy.settings.BaseSettings` instance itself, it will be
    converted to a dictionary, containing all its current settings values
    as they would be returned by :meth:`~scrapy.settings.BaseSettings.get`,
    and losing all information about priority and mutability.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value as a dictionary
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.getdictorlist
  kind: method
  ns: scrapy.addons
  description: |-
    Get a setting value as either a :class:`dict` or a :class:`list`.

    If the setting is already a dict or a list, a copy of it will be
    returned.

    If it is a string it will be evaluated as JSON, or as a comma-separated
    list of strings as a fallback.

    For example, settings populated from the command line will return:

    -   ``{'key1': 'value1', 'key2': 'value2'}`` if set to
        ``'{"key1": "value1", "key2": "value2"}'``

    -   ``['one', 'two']`` if set to ``'["one", "two"]'`` or ``'one,two'``

    :param name: the setting name
    :type name: string

    :param default: the value to return if no setting is found
    :type default: any
  summary: Get a setting value as either a :class:`dict` or a :class:`list`
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.getfloat
  kind: method
  ns: scrapy.addons
  description: |-
    Get a setting value as a float.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value as a float
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: '0.0'
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.getint
  kind: method
  ns: scrapy.addons
  description: |-
    Get a setting value as an int.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value as an int
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.getlist
  kind: method
  ns: scrapy.addons
  description: |-
    Get a setting value as a list. If the setting original type is a list, a
    copy of it will be returned. If it's a string it will be split by ",".

    For example, settings populated through environment variables set to
    ``'one,two'`` will return a list ['one', 'two'] when using this method.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value as a list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.getpriority
  kind: method
  ns: scrapy.addons
  description: |-
    Return the current numerical priority value of a setting, or ``None`` if
    the given ``name`` does not exist.

    :param name: the setting name
    :type name: str
  summary: Return the current numerical priority value of a setting, or ``None`` if
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.getwithbase
  kind: method
  ns: scrapy.addons
  description: |-
    Get a composition of a dictionary-like setting and its `_BASE`
    counterpart.

    :param name: name of the dictionary-like setting
    :type name: str
  summary: Get a composition of a dictionary-like setting and its `_BASE`
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.items
  kind: method
  ns: scrapy.addons
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.keys
  kind: method
  ns: scrapy.addons
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.maxpriority
  kind: method
  ns: scrapy.addons
  description: |-
    Return the numerical value of the highest priority present throughout
    all settings, or the numerical value for ``default`` from
    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings
    stored.
  summary: Return the numerical value of the highest priority present throughout
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.pop
  kind: method
  ns: scrapy.addons
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
    If key is not found, d is returned if given, otherwise KeyError is raised.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: <object object at 0x7fa9743b1d40>
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.popitem
  kind: method
  ns: scrapy.addons
  description: |-
    D.popitem() -> (k, v), remove and return some (key, value) pair
    as a 2-tuple; but raise KeyError if D is empty.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.set
  kind: method
  ns: scrapy.addons
  description: |-
    Store a key/value attribute with a given priority.

    Settings should be populated *before* configuring the Crawler object
    (through the :meth:`~scrapy.crawler.Crawler.configure` method),
    otherwise they won't have any effect.

    :param name: the setting name
    :type name: str

    :param value: the value to associate with the setting
    :type value: object

    :param priority: the priority of the setting. Should be a key of
        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer
    :type priority: str or int
  summary: Store a key/value attribute with a given priority
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.setdefault
  kind: method
  ns: scrapy.addons
  description: D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.setdict
  kind: method
  ns: scrapy.addons
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: values
    default: null
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.setmodule
  kind: method
  ns: scrapy.addons
  description: |-
    Store settings from a module with a given priority.

    This is a helper function that calls
    :meth:`~scrapy.settings.BaseSettings.set` for every globally declared
    uppercase variable of ``module`` with the provided ``priority``.

    :param module: the module or the path of the module
    :type module: types.ModuleType or str

    :param priority: the priority of the settings. Should be a key of
        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer
    :type priority: str or int
  summary: Store settings from a module with a given priority
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: module
    default: null
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.update
  kind: method
  ns: scrapy.addons
  description: |-
    Store key/value pairs with a given priority.

    This is a helper function that calls
    :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``
    with the provided ``priority``.

    If ``values`` is a string, it is assumed to be JSON-encoded and parsed
    into a dict with ``json.loads()`` first. If it is a
    :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities
    will be used and the ``priority`` parameter ignored. This allows
    inserting/updating settings with different priorities with a single
    command.

    :param values: the settings names and values
    :type values: dict or string or :class:`~scrapy.settings.BaseSettings`

    :param priority: the priority of the settings. Should be a key of
        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer
    :type priority: str or int
  summary: Store key/value pairs with a given priority
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: values
    default: null
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: Settings.values
  kind: method
  ns: scrapy.addons
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.addons
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: build_component_list
  kind: function
  ns: scrapy.addons
  description: 'Compose a component list from a { class: order } dictionary.'
  summary: 'Compose a component list from a { class: order } dictionary'
  signatures:
  - kind: positional
    name: compdict
    default: null
    rest: false
  - kind: positional
    name: custom
    default: None
    rest: false
  - kind: positional
    name: convert
    default: <function update_classpath at 0x7fa9729e1a80>
    rest: false
  - type: '?'
  inherits_from: null
- name: create_instance
  kind: function
  ns: scrapy.addons
  description: |-
    Construct a class instance using its ``from_crawler`` or
    ``from_settings`` constructors, if available.

    At least one of ``settings`` and ``crawler`` needs to be different from
    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.
    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be
    tried.

    ``*args`` and ``**kwargs`` are forwarded to the constructors.

    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.

    .. versionchanged:: 2.2
       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an
       extension has not been implemented correctly).
  summary: Construct a class instance using its ``from_crawler`` or
  signatures:
  - kind: positional
    name: objcls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: load_object
  kind: function
  ns: scrapy.addons
  description: |-
    Load an object given its absolute object path, and return it.

    The object can be the import path of a class, function, variable or an
    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.

    If ``path`` is not a string, but is a callable object, such as a class or
    a function, then return it as is.
  summary: Load an object given its absolute object path, and return it
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.addons
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: scrapy.cmdline
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: BaseRunSpiderCommand
  kind: class
  ns: scrapy.cmdline
  description: Common class used to share functionality between the crawl, parse and runspider commands
  summary: Common class used to share functionality between the crawl, parse and runspider commands
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: BaseRunSpiderCommand
  inherits_from:
  - <class 'scrapy.commands.ScrapyCommand'>
- name: BaseRunSpiderCommand.add_options
  kind: method
  ns: scrapy.cmdline
  description: Populate option parse with options available for this command
  summary: Populate option parse with options available for this command
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: parser
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseRunSpiderCommand.crawler_process
  kind: property
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: BaseRunSpiderCommand.default_settings
  kind: property
  ns: scrapy.cmdline
  description: |-
    dict() -> new empty dictionary
    dict(mapping) -> new dictionary initialized from a mapping object's
        (key, value) pairs
    dict(iterable) -> new dictionary initialized as if via:
        d = {}
        for k, v in iterable:
            d[k] = v
    dict(**kwargs) -> new dictionary initialized with the name=value pairs
        in the keyword argument list.  For example:  dict(one=1, two=2)
  summary: dict() -> new empty dictionary
  signatures: null
  inherits_from: null
- name: BaseRunSpiderCommand.exitcode
  kind: property
  ns: scrapy.cmdline
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: BaseRunSpiderCommand.help
  kind: method
  ns: scrapy.cmdline
  description: |-
    An extensive help for the command. It will be shown when using the
    "help" command. It can contain newlines since no post-formatting will
    be applied to its contents.
  summary: An extensive help for the command
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseRunSpiderCommand.long_desc
  kind: method
  ns: scrapy.cmdline
  description: |-
    A long description of the command. Return short description when not
    available. It cannot contain newlines since contents will be formatted
    by optparser which removes newlines and wraps text.
  summary: A long description of the command
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseRunSpiderCommand.process_options
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: null
    rest: false
  - kind: positional
    name: opts
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseRunSpiderCommand.requires_project
  kind: property
  ns: scrapy.cmdline
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: BaseRunSpiderCommand.run
  kind: method
  ns: scrapy.cmdline
  description: Entry point for running commands
  summary: Entry point for running commands
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: null
    rest: false
  - kind: positional
    name: opts
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseRunSpiderCommand.set_crawler
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseRunSpiderCommand.short_desc
  kind: method
  ns: scrapy.cmdline
  description: A short description of the command
  summary: A short description of the command
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseRunSpiderCommand.syntax
  kind: method
  ns: scrapy.cmdline
  description: Command syntax (preferably one-line). Do not include command name.
  summary: Command syntax (preferably one-line)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlerProcess
  kind: class
  ns: scrapy.cmdline
  description: |-
    A class to run multiple scrapy crawlers in a process simultaneously.

    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support
    for starting a :mod:`~twisted.internet.reactor` and handling shutdown
    signals, like the keyboard interrupt command Ctrl-C. It also configures
    top-level logging.

    This utility should be a better fit than
    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another
    :mod:`~twisted.internet.reactor` within your application.

    The CrawlerProcess object must be instantiated with a
    :class:`~scrapy.settings.Settings` object.

    :param install_root_handler: whether to install root logging handler
        (default: True)

    This class shouldn't be needed (since Scrapy is responsible of using it
    accordingly) unless writing scripts that manually handle the crawling
    process. See :ref:`run-from-script` for an example.
  summary: A class to run multiple scrapy crawlers in a process simultaneously
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: settings
    default: None
    rest: false
  - kind: positional
    name: install_root_handler
    default: 'True'
    rest: false
  - type: CrawlerProcess
  inherits_from:
  - <class 'scrapy.crawler.CrawlerRunner'>
- name: CrawlerProcess.crawl
  kind: method
  ns: scrapy.cmdline
  description: |-
    Run a crawler with the provided arguments.

    It will call the given Crawler's :meth:`~Crawler.crawl` method, while
    keeping track of it so it can be stopped later.

    If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`
    instance, this method will try to create one using this parameter as
    the spider class given to it.

    Returns a deferred that is fired when the crawling is finished.

    :param crawler_or_spidercls: already created crawler, or a spider class
        or spider's name inside the project to create it
    :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,
        :class:`~scrapy.spiders.Spider` subclass or string

    :param args: arguments to initialize the spider

    :param kwargs: keyword arguments to initialize the spider
  summary: Run a crawler with the provided arguments
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler_or_spidercls
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlerProcess.crawlers
  kind: property
  ns: scrapy.cmdline
  description: Set of :class:`crawlers <scrapy.crawler.Crawler>` started by :meth:`crawl` and managed by this class.
  summary: Set of :class:`crawlers <scrapy
  signatures: null
  inherits_from: null
- name: CrawlerProcess.create_crawler
  kind: method
  ns: scrapy.cmdline
  description: |-
    Return a :class:`~scrapy.crawler.Crawler` object.

    * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.
    * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler
      is constructed for it.
    * If ``crawler_or_spidercls`` is a string, this function finds
      a spider with this name in a Scrapy project (using spider loader),
      then creates a Crawler instance for it.
  summary: Return a :class:`~scrapy
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler_or_spidercls
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlerProcess.join
  kind: method
  ns: scrapy.cmdline
  description: |-
    join()

    Returns a deferred that is fired when all managed :attr:`crawlers` have
    completed their executions.
  summary: join()
  signatures:
  - type: '?'
  inherits_from: null
- name: CrawlerProcess.start
  kind: method
  ns: scrapy.cmdline
  description: |-
    This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool
    size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache
    based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.

    If ``stop_after_crawl`` is True, the reactor will be stopped after all
    crawlers have finished, using :meth:`join`.

    :param bool stop_after_crawl: stop or not the reactor when all
        crawlers have finished

    :param bool install_signal_handlers: whether to install the OS signal
        handlers from Twisted and Scrapy (default: True)
  summary: This method starts a :mod:`~twisted
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: stop_after_crawl
    default: 'True'
    rest: false
  - kind: positional
    name: install_signal_handlers
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlerProcess.stop
  kind: method
  ns: scrapy.cmdline
  description: |-
    Stops simultaneously all the crawling jobs taking place.

    Returns a deferred that is fired when they all have ended.
  summary: Stops simultaneously all the crawling jobs taking place
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser
  kind: class
  ns: scrapy.cmdline
  description: |-
    Object for parsing command line strings into Python objects.

    Keyword Arguments:
        - prog -- The name of the program (default:
            ``os.path.basename(sys.argv[0])``)
        - usage -- A usage message (default: auto-generated from arguments)
        - description -- A description of what the program does
        - epilog -- Text following the argument descriptions
        - parents -- Parsers whose arguments should be copied into this one
        - formatter_class -- HelpFormatter class for printing help messages
        - prefix_chars -- Characters that prefix optional arguments
        - fromfile_prefix_chars -- Characters that prefix files containing
            additional arguments
        - argument_default -- The default value for all arguments
        - conflict_handler -- String indicating how to handle conflicts
        - add_help -- Add a -h/-help option
        - allow_abbrev -- Allow long options to be abbreviated unambiguously
        - exit_on_error -- Determines whether or not ArgumentParser exits with
            error info when an error occurs
  summary: Object for parsing command line strings into Python objects
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: prog
    default: None
    rest: false
  - kind: positional
    name: usage
    default: None
    rest: false
  - kind: positional
    name: description
    default: None
    rest: false
  - kind: positional
    name: epilog
    default: None
    rest: false
  - kind: positional
    name: parents
    default: '[]'
    rest: false
  - kind: positional
    name: formatter_class
    default: <class 'argparse.HelpFormatter'>
    rest: false
  - kind: positional
    name: prefix_chars
    default: '-'
    rest: false
  - kind: positional
    name: fromfile_prefix_chars
    default: None
    rest: false
  - kind: positional
    name: argument_default
    default: None
    rest: false
  - kind: positional
    name: conflict_handler
    default: error
    rest: false
  - kind: positional
    name: add_help
    default: 'True'
    rest: false
  - kind: positional
    name: allow_abbrev
    default: 'True'
    rest: false
  - kind: positional
    name: exit_on_error
    default: 'True'
    rest: false
  - type: ScrapyArgumentParser
  inherits_from:
  - <class 'argparse.ArgumentParser'>
  - <class 'argparse._AttributeHolder'>
  - <class 'argparse._ActionsContainer'>
- name: ScrapyArgumentParser.add_argument
  kind: method
  ns: scrapy.cmdline
  description: |-
    add_argument(dest, ..., name=value, ...)
    add_argument(option_string, option_string, ..., name=value, ...)
  summary: 'add_argument(dest, '
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.add_argument_group
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.add_mutually_exclusive_group
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.add_subparsers
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.convert_arg_line_to_args
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: arg_line
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.error
  kind: method
  ns: scrapy.cmdline
  description: |-
    error(message: string)

    Prints a usage message incorporating the message to stderr and
    exits.

    If you override this in a subclass, it should not return -- it
    should either exit or raise an exception.
  summary: 'error(message: string)'
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: message
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.exit
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: status
    default: '0'
    rest: false
  - kind: positional
    name: message
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.format_help
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.format_usage
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.get_default
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: dest
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.parse_args
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: None
    rest: false
  - kind: positional
    name: namespace
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.parse_intermixed_args
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: None
    rest: false
  - kind: positional
    name: namespace
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.parse_known_args
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: None
    rest: false
  - kind: positional
    name: namespace
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.parse_known_intermixed_args
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: None
    rest: false
  - kind: positional
    name: namespace
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.print_help
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.print_usage
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.register
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: registry_name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: object
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyArgumentParser.set_defaults
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyCommand
  kind: class
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: ScrapyCommand
  inherits_from: null
- name: ScrapyCommand.add_options
  kind: method
  ns: scrapy.cmdline
  description: Populate option parse with options available for this command
  summary: Populate option parse with options available for this command
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: parser
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyCommand.crawler_process
  kind: property
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ScrapyCommand.default_settings
  kind: property
  ns: scrapy.cmdline
  description: |-
    dict() -> new empty dictionary
    dict(mapping) -> new dictionary initialized from a mapping object's
        (key, value) pairs
    dict(iterable) -> new dictionary initialized as if via:
        d = {}
        for k, v in iterable:
            d[k] = v
    dict(**kwargs) -> new dictionary initialized with the name=value pairs
        in the keyword argument list.  For example:  dict(one=1, two=2)
  summary: dict() -> new empty dictionary
  signatures: null
  inherits_from: null
- name: ScrapyCommand.exitcode
  kind: property
  ns: scrapy.cmdline
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: ScrapyCommand.help
  kind: method
  ns: scrapy.cmdline
  description: |-
    An extensive help for the command. It will be shown when using the
    "help" command. It can contain newlines since no post-formatting will
    be applied to its contents.
  summary: An extensive help for the command
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyCommand.long_desc
  kind: method
  ns: scrapy.cmdline
  description: |-
    A long description of the command. Return short description when not
    available. It cannot contain newlines since contents will be formatted
    by optparser which removes newlines and wraps text.
  summary: A long description of the command
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyCommand.process_options
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: null
    rest: false
  - kind: positional
    name: opts
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyCommand.requires_project
  kind: property
  ns: scrapy.cmdline
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: ScrapyCommand.run
  kind: method
  ns: scrapy.cmdline
  description: Entry point for running commands
  summary: Entry point for running commands
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: null
    rest: false
  - kind: positional
    name: opts
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyCommand.set_crawler
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyCommand.short_desc
  kind: method
  ns: scrapy.cmdline
  description: A short description of the command
  summary: A short description of the command
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyCommand.syntax
  kind: method
  ns: scrapy.cmdline
  description: Command syntax (preferably one-line). Do not include command name.
  summary: Command syntax (preferably one-line)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyHelpFormatter
  kind: class
  ns: scrapy.cmdline
  description: Help Formatter for scrapy command line help messages.
  summary: Help Formatter for scrapy command line help messages
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: prog
    default: null
    rest: false
  - kind: positional
    name: indent_increment
    default: '2'
    rest: false
  - kind: positional
    name: max_help_position
    default: '24'
    rest: false
  - kind: positional
    name: width
    default: None
    rest: false
  - type: ScrapyHelpFormatter
  inherits_from:
  - <class 'argparse.HelpFormatter'>
- name: ScrapyHelpFormatter.add_argument
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: action
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyHelpFormatter.add_arguments
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: actions
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyHelpFormatter.add_text
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: text
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyHelpFormatter.add_usage
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: usage
    default: null
    rest: false
  - kind: positional
    name: actions
    default: null
    rest: false
  - kind: positional
    name: groups
    default: null
    rest: false
  - kind: positional
    name: prefix
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyHelpFormatter.end_section
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyHelpFormatter.format_help
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyHelpFormatter.format_part_strings
  kind: method
  ns: scrapy.cmdline
  description: Underline and title case command line help message headers.
  summary: Underline and title case command line help message headers
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: part_strings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyHelpFormatter.start_section
  kind: method
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: heading
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: UsageError
  kind: class
  ns: scrapy.cmdline
  description: To indicate a command-line usage error
  summary: To indicate a command-line usage error
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: UsageError
  inherits_from:
  - <class 'Exception'>
  - <class 'BaseException'>
- name: UsageError.add_note
  kind: callable
  ns: scrapy.cmdline
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: UsageError.args
  kind: property
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: UsageError.with_traceback
  kind: callable
  ns: scrapy.cmdline
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: entry_points
  kind: function
  ns: scrapy.cmdline
  description: |-
    Return EntryPoint objects for all installed packages.

    Pass selection parameters (group or name) to filter the
    result to entry points matching those properties (see
    EntryPoints.select()).

    For compatibility, returns ``SelectableGroups`` object unless
    selection parameters are supplied. In the future, this function
    will return ``EntryPoints`` instead of ``SelectableGroups``
    even when no selection parameters are supplied.

    For maximum future compatibility, pass selection parameters
    or invoke ``.select`` with parameters on the result.

    :return: EntryPoints or SelectableGroups for all installed packages.
  summary: Return EntryPoint objects for all installed packages
  signatures:
  - type: '?'
  inherits_from: null
- name: execute
  kind: function
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: argv
    default: None
    rest: false
  - kind: positional
    name: settings
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: garbage_collect
  kind: function
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: get_project_settings
  kind: function
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: inside_project
  kind: function
  ns: scrapy.cmdline
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: walk_modules
  kind: function
  ns: scrapy.cmdline
  description: |-
    Loads a module and all its submodules from the given module path and
    returns them. If *any* module throws an exception while importing, that
    exception is thrown back.

    For example: walk_modules('scrapy.utils')
  summary: Loads a module and all its submodules from the given module path and
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.commands
  kind: module
  ns: null
  description: Base class for Scrapy commands
  summary: Base class for Scrapy commands
  signatures: null
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.commands
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.commands
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.commands
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path
  kind: class
  ns: scrapy.commands
  description: |-
    PurePath subclass that can make system calls.

    Path represents a filesystem path but unlike PurePath, also offers
    methods to do system calls on path objects. Depending on your system,
    instantiating a Path will return either a PosixPath or a WindowsPath
    object. You can also instantiate a PosixPath or WindowsPath directly,
    but cannot instantiate a WindowsPath on a POSIX system or vice versa.
  summary: PurePath subclass that can make system calls
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - type: Path
  inherits_from:
  - <class 'pathlib.PurePath'>
- name: Path.absolute
  kind: method
  ns: scrapy.commands
  description: |-
    Return an absolute version of this path by prepending the current
    working directory. No normalization or symlink resolution is performed.

    Use resolve() to get the canonical path to a file.
  summary: Return an absolute version of this path by prepending the current
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.anchor
  kind: property
  ns: scrapy.commands
  description: The concatenation of the drive and root, or ''.
  summary: The concatenation of the drive and root, or ''
  signatures: null
  inherits_from: null
- name: Path.as_posix
  kind: method
  ns: scrapy.commands
  description: |-
    Return the string representation of the path with forward (/)
    slashes.
  summary: Return the string representation of the path with forward (/)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.as_uri
  kind: method
  ns: scrapy.commands
  description: Return the path as a 'file' URI.
  summary: Return the path as a 'file' URI
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.chmod
  kind: method
  ns: scrapy.commands
  description: Change the permissions of the path, like os.chmod().
  summary: Change the permissions of the path, like os
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: mode
    default: null
    rest: false
  - name: follow_symlinks
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: Path.cwd
  kind: function
  ns: scrapy.commands
  description: |-
    Return a new path pointing to the current working directory
    (as returned by os.getcwd()).
  summary: Return a new path pointing to the current working directory
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.drive
  kind: property
  ns: scrapy.commands
  description: The drive prefix (letter or UNC path), if any.
  summary: The drive prefix (letter or UNC path), if any
  signatures: null
  inherits_from: null
- name: Path.exists
  kind: method
  ns: scrapy.commands
  description: Whether this path exists.
  summary: Whether this path exists
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.expanduser
  kind: method
  ns: scrapy.commands
  description: |-
    Return a new path with expanded ~ and ~user constructs
    (as returned by os.path.expanduser)
  summary: Return a new path with expanded ~ and ~user constructs
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.glob
  kind: method
  ns: scrapy.commands
  description: |-
    Iterate over this subtree and yield all existing files (of any
    kind, including directories) matching the given relative pattern.
  summary: Iterate over this subtree and yield all existing files (of any
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: pattern
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.group
  kind: method
  ns: scrapy.commands
  description: Return the group name of the file gid.
  summary: Return the group name of the file gid
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.hardlink_to
  kind: method
  ns: scrapy.commands
  description: |-
    Make this path a hard link pointing to the same file as *target*.

    Note the order of arguments (self, target) is the reverse of os.link's.
  summary: Make this path a hard link pointing to the same file as *target*
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: target
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.home
  kind: function
  ns: scrapy.commands
  description: |-
    Return a new path pointing to the user's home directory (as
    returned by os.path.expanduser('~')).
  summary: Return a new path pointing to the user's home directory (as
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.is_absolute
  kind: method
  ns: scrapy.commands
  description: |-
    True if the path is absolute (has both a root and, if applicable,
    a drive).
  summary: True if the path is absolute (has both a root and, if applicable,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.is_block_device
  kind: method
  ns: scrapy.commands
  description: Whether this path is a block device.
  summary: Whether this path is a block device
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.is_char_device
  kind: method
  ns: scrapy.commands
  description: Whether this path is a character device.
  summary: Whether this path is a character device
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.is_dir
  kind: method
  ns: scrapy.commands
  description: Whether this path is a directory.
  summary: Whether this path is a directory
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.is_fifo
  kind: method
  ns: scrapy.commands
  description: Whether this path is a FIFO.
  summary: Whether this path is a FIFO
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.is_file
  kind: method
  ns: scrapy.commands
  description: |-
    Whether this path is a regular file (also True for symlinks pointing
    to regular files).
  summary: Whether this path is a regular file (also True for symlinks pointing
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.is_mount
  kind: method
  ns: scrapy.commands
  description: Check if this path is a POSIX mount point
  summary: Check if this path is a POSIX mount point
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.is_relative_to
  kind: method
  ns: scrapy.commands
  description: "Return True if the path is relative to another path or False.\n        "
  summary: Return True if the path is relative to another path or False
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.is_reserved
  kind: method
  ns: scrapy.commands
  description: |-
    Return True if the path contains one of the special names reserved
    by the system, if any.
  summary: Return True if the path contains one of the special names reserved
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.is_socket
  kind: method
  ns: scrapy.commands
  description: Whether this path is a socket.
  summary: Whether this path is a socket
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.is_symlink
  kind: method
  ns: scrapy.commands
  description: Whether this path is a symbolic link.
  summary: Whether this path is a symbolic link
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.iterdir
  kind: method
  ns: scrapy.commands
  description: |-
    Iterate over the files in this directory.  Does not yield any
    result for the special paths '.' and '..'.
  summary: Iterate over the files in this directory
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.joinpath
  kind: method
  ns: scrapy.commands
  description: |-
    Combine this path with one or several arguments, and return a
    new path representing either a subpath (if all arguments are relative
    paths) or a totally different path (if one of the arguments is
    anchored).
  summary: Combine this path with one or several arguments, and return a
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.lchmod
  kind: method
  ns: scrapy.commands
  description: |-
    Like chmod(), except if the path points to a symlink, the symlink's
    permissions are changed, rather than its target's.
  summary: Like chmod(), except if the path points to a symlink, the symlink's
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: mode
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.link_to
  kind: method
  ns: scrapy.commands
  description: |-
    Make the target path a hard link pointing to this path.

    Note this function does not make this path a hard link to *target*,
    despite the implication of the function and argument names. The order
    of arguments (target, link) is the reverse of Path.symlink_to, but
    matches that of os.link.

    Deprecated since Python 3.10 and scheduled for removal in Python 3.12.
    Use `hardlink_to()` instead.
  summary: Make the target path a hard link pointing to this path
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: target
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.lstat
  kind: method
  ns: scrapy.commands
  description: |-
    Like stat(), except if the path points to a symlink, the symlink's
    status information is returned, rather than its target's.
  summary: Like stat(), except if the path points to a symlink, the symlink's
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.match
  kind: method
  ns: scrapy.commands
  description: Return True if this path matches the given pattern.
  summary: Return True if this path matches the given pattern
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path_pattern
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.mkdir
  kind: method
  ns: scrapy.commands
  description: Create a new directory at this given path.
  summary: Create a new directory at this given path
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: mode
    default: '511'
    rest: false
  - kind: positional
    name: parents
    default: 'False'
    rest: false
  - kind: positional
    name: exist_ok
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.name
  kind: property
  ns: scrapy.commands
  description: The final path component, if any.
  summary: The final path component, if any
  signatures: null
  inherits_from: null
- name: Path.open
  kind: method
  ns: scrapy.commands
  description: |-
    Open the file pointed by this path and return a file object, as
    the built-in open() function does.
  summary: Open the file pointed by this path and return a file object, as
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: mode
    default: r
    rest: false
  - kind: positional
    name: buffering
    default: '-1'
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: None
    rest: false
  - kind: positional
    name: newline
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.owner
  kind: method
  ns: scrapy.commands
  description: Return the login name of the file owner.
  summary: Return the login name of the file owner
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.parent
  kind: property
  ns: scrapy.commands
  description: The logical parent of the path.
  summary: The logical parent of the path
  signatures: null
  inherits_from: null
- name: Path.parents
  kind: property
  ns: scrapy.commands
  description: A sequence of this path's logical parents.
  summary: A sequence of this path's logical parents
  signatures: null
  inherits_from: null
- name: Path.parts
  kind: property
  ns: scrapy.commands
  description: |-
    An object providing sequence-like access to the
    components in the filesystem path.
  summary: An object providing sequence-like access to the
  signatures: null
  inherits_from: null
- name: Path.read_bytes
  kind: method
  ns: scrapy.commands
  description: Open the file in bytes mode, read it, and close the file.
  summary: Open the file in bytes mode, read it, and close the file
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.read_text
  kind: method
  ns: scrapy.commands
  description: Open the file in text mode, read it, and close the file.
  summary: Open the file in text mode, read it, and close the file
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.readlink
  kind: method
  ns: scrapy.commands
  description: Return the path to which the symbolic link points.
  summary: Return the path to which the symbolic link points
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.relative_to
  kind: method
  ns: scrapy.commands
  description: |-
    Return the relative path to another path identified by the passed
    arguments.  If the operation is not possible (because this is not
    a subpath of the other path), raise ValueError.
  summary: Return the relative path to another path identified by the passed
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.rename
  kind: method
  ns: scrapy.commands
  description: |-
    Rename this path to the target path.

    The target path may be absolute or relative. Relative paths are
    interpreted relative to the current working directory, *not* the
    directory of the Path object.

    Returns the new Path instance pointing to the target path.
  summary: Rename this path to the target path
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: target
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.replace
  kind: method
  ns: scrapy.commands
  description: |-
    Rename this path to the target path, overwriting if that path exists.

    The target path may be absolute or relative. Relative paths are
    interpreted relative to the current working directory, *not* the
    directory of the Path object.

    Returns the new Path instance pointing to the target path.
  summary: Rename this path to the target path, overwriting if that path exists
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: target
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.resolve
  kind: method
  ns: scrapy.commands
  description: |-
    Make the path absolute, resolving all symlinks on the way and also
    normalizing it.
  summary: Make the path absolute, resolving all symlinks on the way and also
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: strict
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.rglob
  kind: method
  ns: scrapy.commands
  description: |-
    Recursively yield all existing files (of any kind, including
    directories) matching the given relative pattern, anywhere in
    this subtree.
  summary: Recursively yield all existing files (of any kind, including
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: pattern
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.rmdir
  kind: method
  ns: scrapy.commands
  description: Remove this directory.  The directory must be empty.
  summary: Remove this directory
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.root
  kind: property
  ns: scrapy.commands
  description: The root of the path, if any.
  summary: The root of the path, if any
  signatures: null
  inherits_from: null
- name: Path.samefile
  kind: method
  ns: scrapy.commands
  description: |-
    Return whether other_path is the same or not as this file
    (as returned by os.path.samefile()).
  summary: Return whether other_path is the same or not as this file
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other_path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.stat
  kind: method
  ns: scrapy.commands
  description: |-
    Return the result of the stat() system call on this path, like
    os.stat() does.
  summary: Return the result of the stat() system call on this path, like
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: follow_symlinks
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: Path.stem
  kind: property
  ns: scrapy.commands
  description: The final path component, minus its last suffix.
  summary: The final path component, minus its last suffix
  signatures: null
  inherits_from: null
- name: Path.suffix
  kind: property
  ns: scrapy.commands
  description: |-
    The final component's last suffix, if any.

    This includes the leading period. For example: '.txt'
  summary: The final component's last suffix, if any
  signatures: null
  inherits_from: null
- name: Path.suffixes
  kind: property
  ns: scrapy.commands
  description: |-
    A list of the final component's suffixes, if any.

    These include the leading periods. For example: ['.tar', '.gz']
  summary: A list of the final component's suffixes, if any
  signatures: null
  inherits_from: null
- name: Path.symlink_to
  kind: method
  ns: scrapy.commands
  description: |-
    Make this path a symlink pointing to the target path.
    Note the order of arguments (link, target) is the reverse of os.symlink.
  summary: Make this path a symlink pointing to the target path
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: target
    default: null
    rest: false
  - kind: positional
    name: target_is_directory
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.touch
  kind: method
  ns: scrapy.commands
  description: Create this file with the given access mode, if it doesn't exist.
  summary: Create this file with the given access mode, if it doesn't exist
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: mode
    default: '438'
    rest: false
  - kind: positional
    name: exist_ok
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.unlink
  kind: method
  ns: scrapy.commands
  description: |-
    Remove this file or link.
    If the path is a directory, use rmdir() instead.
  summary: Remove this file or link
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: missing_ok
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.with_name
  kind: method
  ns: scrapy.commands
  description: Return a new path with the file name changed.
  summary: Return a new path with the file name changed
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.with_stem
  kind: method
  ns: scrapy.commands
  description: Return a new path with the stem changed.
  summary: Return a new path with the stem changed
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: stem
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.with_suffix
  kind: method
  ns: scrapy.commands
  description: |-
    Return a new path with the file suffix changed.  If the path
    has no suffix, add given suffix.  If the given suffix is an empty
    string, remove the suffix from the path.
  summary: Return a new path with the file suffix changed
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: suffix
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.write_bytes
  kind: method
  ns: scrapy.commands
  description: Open the file in bytes mode, write to it, and close the file.
  summary: Open the file in bytes mode, write to it, and close the file
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: data
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Path.write_text
  kind: method
  ns: scrapy.commands
  description: Open the file in text mode, write to it, and close the file.
  summary: Open the file in text mode, write to it, and close the file
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: data
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: None
    rest: false
  - kind: positional
    name: newline
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: arglist_to_dict
  kind: function
  ns: scrapy.commands
  description: |-
    Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a
    dict
  summary: 'Convert a list of arguments like [''arg1=val1'', ''arg2=val2'', '
  signatures:
  - kind: positional
    name: arglist
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: feed_process_params_from_cli
  kind: function
  ns: scrapy.commands
  description: |-
    Receives feed export params (from the 'crawl' or 'runspider' commands),
    checks for inconsistencies in their quantities and returns a dictionary
    suitable to be used as the FEEDS setting.
  summary: Receives feed export params (from the 'crawl' or 'runspider' commands),
  signatures:
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: output
    default: null
    rest: false
  - kind: positional
    name: output_format
    default: None
    rest: false
  - kind: positional
    name: overwrite_output
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.contracts
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: AsyncGenerator
  kind: callable
  ns: scrapy.contracts
  description: A generic version of collections.abc.AsyncGenerator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Contract
  kind: class
  ns: scrapy.contracts
  description: Abstract class for contracts
  summary: Abstract class for contracts
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: method
    default: null
    rest: false
  - type: Contract
  inherits_from: null
- name: Contract.add_post_hook
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: results
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Contract.add_pre_hook
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: results
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Contract.adjust_request_args
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Contract.request_cls
  kind: property
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ContractsManager
  kind: class
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: contracts
    default: null
    rest: false
  - type: ContractsManager
  inherits_from: null
- name: ContractsManager.contracts
  kind: property
  ns: scrapy.contracts
  description: |-
    dict() -> new empty dictionary
    dict(mapping) -> new dictionary initialized from a mapping object's
        (key, value) pairs
    dict(iterable) -> new dictionary initialized as if via:
        d = {}
        for k, v in iterable:
            d[k] = v
    dict(**kwargs) -> new dictionary initialized with the name=value pairs
        in the keyword argument list.  For example:  dict(one=1, two=2)
  summary: dict() -> new empty dictionary
  signatures: null
  inherits_from: null
- name: ContractsManager.extract_contracts
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: method
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ContractsManager.from_method
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: method
    default: null
    rest: false
  - kind: positional
    name: results
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ContractsManager.from_spider
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: results
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ContractsManager.tested_methods_from_spidercls
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spidercls
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CoroutineType
  kind: class
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: coroutine
  inherits_from: null
- name: CoroutineType.close
  kind: callable
  ns: scrapy.contracts
  description: close() -> raise GeneratorExit inside coroutine.
  summary: close() -> raise GeneratorExit inside coroutine
  signatures: null
  inherits_from: null
- name: CoroutineType.cr_await
  kind: property
  ns: scrapy.contracts
  description: object being awaited on, or None
  summary: object being awaited on, or None
  signatures: null
  inherits_from: null
- name: CoroutineType.cr_code
  kind: property
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CoroutineType.cr_frame
  kind: property
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CoroutineType.cr_origin
  kind: property
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CoroutineType.cr_running
  kind: property
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CoroutineType.cr_suspended
  kind: property
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CoroutineType.send
  kind: callable
  ns: scrapy.contracts
  description: |-
    send(arg) -> send 'arg' into coroutine,
    return next iterated value or raise StopIteration.
  summary: send(arg) -> send 'arg' into coroutine,
  signatures: null
  inherits_from: null
- name: CoroutineType.throw
  kind: callable
  ns: scrapy.contracts
  description: |-
    throw(value)
    throw(type[,value[,traceback]])

    Raise exception in coroutine, return next iterated value or raise
    StopIteration.
  summary: throw(value)
  signatures: null
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.contracts
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase
  kind: class
  ns: scrapy.contracts
  description: |-
    A class whose instances are single test cases.

    By default, the test code itself should be placed in a method named
    'runTest'.

    If the fixture may be used for many test cases, create as
    many test methods as are needed. When instantiating such a TestCase
    subclass, specify in the constructor arguments the name of the test method
    that the instance is to execute.

    Test authors should subclass TestCase for their own tests. Construction
    and deconstruction of the test's environment ('fixture') can be
    implemented by overriding the 'setUp' and 'tearDown' methods respectively.

    If it is necessary to override the __init__ method, the base class
    __init__ method must always be called. It is important that subclasses
    should not change the signature of their __init__ method, since instances
    of the classes are instantiated automatically by parts of the framework
    in order to be run.

    When subclassing TestCase, you can set these attributes:
    * failureException: determines which exception will be raised when
        the instance's assertion methods fail; test methods raising this
        exception will be deemed to have 'failed' rather than 'errored'.
    * longMessage: determines whether long messages (including repr of
        objects used in assert methods) will be printed on failure in *addition*
        to any explicit message passed.
    * maxDiff: sets the maximum length of a diff in failure messages
        by assert methods using difflib. It is looked up as an instance
        attribute so can be configured by individual tests if required.
  summary: A class whose instances are single test cases
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: methodName
    default: runTest
    rest: false
  - type: TestCase
  inherits_from: null
- name: TestCase.addClassCleanup
  kind: function
  ns: scrapy.contracts
  description: |-
    Same as addCleanup, except the cleanup items are called even if
    setUpClass fails (unlike tearDownClass).
  summary: Same as addCleanup, except the cleanup items are called even if
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: function
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.addCleanup
  kind: method
  ns: scrapy.contracts
  description: |-
    Add a function, with arguments, to be called when the test is
    completed. Functions added are called on a LIFO basis and are
    called after tearDown on test failure or success.

    Cleanup items are called even if setUp fails (unlike tearDown).
  summary: Add a function, with arguments, to be called when the test is
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: function
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.addTypeEqualityFunc
  kind: method
  ns: scrapy.contracts
  description: |-
    Add a type specific assertEqual style function to compare a type.

    This method is for use by TestCase subclasses that need to register
    their own type equality functions to provide nicer error messages.

    Args:
        typeobj: The data type to call this function on when both values
                are of the same type in assertEqual().
        function: The callable taking two arguments and an optional
                msg= argument that raises self.failureException with a
                useful error message when the two arguments are not equal.
  summary: Add a type specific assertEqual style function to compare a type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: typeobj
    default: null
    rest: false
  - kind: positional
    name: function
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertAlmostEqual
  kind: method
  ns: scrapy.contracts
  description: |-
    Fail if the two objects are unequal as determined by their
    difference rounded to the given number of decimal places
    (default 7) and comparing to zero, or by comparing that the
    difference between the two objects is more than the given
    delta.

    Note that decimal places (from zero) are usually not the same
    as significant digits (measured from the most significant digit).

    If the two objects compare equal then they will automatically
    compare almost equal.
  summary: Fail if the two objects are unequal as determined by their
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: first
    default: null
    rest: false
  - kind: positional
    name: second
    default: null
    rest: false
  - kind: positional
    name: places
    default: None
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - kind: positional
    name: delta
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertAlmostEquals
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.assertCountEqual
  kind: method
  ns: scrapy.contracts
  description: |-
    Asserts that two iterables have the same elements, the same number of
    times, without regard to order.

        self.assertEqual(Counter(list(first)),
                         Counter(list(second)))

     Example:
        - [0, 1, 1] and [1, 0, 1] compare equal.
        - [0, 0, 1] and [0, 1] compare unequal.
  summary: Asserts that two iterables have the same elements, the same number of
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: first
    default: null
    rest: false
  - kind: positional
    name: second
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertDictContainsSubset
  kind: method
  ns: scrapy.contracts
  description: Checks whether dictionary is a superset of subset.
  summary: Checks whether dictionary is a superset of subset
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: subset
    default: null
    rest: false
  - kind: positional
    name: dictionary
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertDictEqual
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: d1
    default: null
    rest: false
  - kind: positional
    name: d2
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertEqual
  kind: method
  ns: scrapy.contracts
  description: |-
    Fail if the two objects are unequal as determined by the '=='
    operator.
  summary: Fail if the two objects are unequal as determined by the '=='
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: first
    default: null
    rest: false
  - kind: positional
    name: second
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertEquals
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.assertFalse
  kind: method
  ns: scrapy.contracts
  description: Check that the expression is false.
  summary: Check that the expression is false
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expr
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertGreater
  kind: method
  ns: scrapy.contracts
  description: Just like self.assertTrue(a > b), but with a nicer default message.
  summary: Just like self
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: a
    default: null
    rest: false
  - kind: positional
    name: b
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertGreaterEqual
  kind: method
  ns: scrapy.contracts
  description: Just like self.assertTrue(a >= b), but with a nicer default message.
  summary: Just like self
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: a
    default: null
    rest: false
  - kind: positional
    name: b
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertIn
  kind: method
  ns: scrapy.contracts
  description: Just like self.assertTrue(a in b), but with a nicer default message.
  summary: Just like self
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: member
    default: null
    rest: false
  - kind: positional
    name: container
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertIs
  kind: method
  ns: scrapy.contracts
  description: Just like self.assertTrue(a is b), but with a nicer default message.
  summary: Just like self
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expr1
    default: null
    rest: false
  - kind: positional
    name: expr2
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertIsInstance
  kind: method
  ns: scrapy.contracts
  description: |-
    Same as self.assertTrue(isinstance(obj, cls)), with a nicer
    default message.
  summary: Same as self
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: obj
    default: null
    rest: false
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertIsNone
  kind: method
  ns: scrapy.contracts
  description: Same as self.assertTrue(obj is None), with a nicer default message.
  summary: Same as self
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: obj
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertIsNot
  kind: method
  ns: scrapy.contracts
  description: Just like self.assertTrue(a is not b), but with a nicer default message.
  summary: Just like self
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expr1
    default: null
    rest: false
  - kind: positional
    name: expr2
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertIsNotNone
  kind: method
  ns: scrapy.contracts
  description: Included for symmetry with assertIsNone.
  summary: Included for symmetry with assertIsNone
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: obj
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertLess
  kind: method
  ns: scrapy.contracts
  description: Just like self.assertTrue(a < b), but with a nicer default message.
  summary: Just like self
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: a
    default: null
    rest: false
  - kind: positional
    name: b
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertLessEqual
  kind: method
  ns: scrapy.contracts
  description: Just like self.assertTrue(a <= b), but with a nicer default message.
  summary: Just like self
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: a
    default: null
    rest: false
  - kind: positional
    name: b
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertListEqual
  kind: method
  ns: scrapy.contracts
  description: |-
    A list-specific equality assertion.

    Args:
        list1: The first list to compare.
        list2: The second list to compare.
        msg: Optional message to use on failure instead of a list of
                differences.
  summary: A list-specific equality assertion
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: list1
    default: null
    rest: false
  - kind: positional
    name: list2
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertLogs
  kind: method
  ns: scrapy.contracts
  description: |-
    Fail unless a log message of level *level* or higher is emitted
    on *logger_name* or its children.  If omitted, *level* defaults to
    INFO and *logger* defaults to the root logger.

    This method must be used as a context manager, and will yield
    a recording object with two attributes: `output` and `records`.
    At the end of the context manager, the `output` attribute will
    be a list of the matching formatted log messages and the
    `records` attribute will be a list of the corresponding LogRecord
    objects.

    Example::

        with self.assertLogs('foo', level='INFO') as cm:
            logging.getLogger('foo').info('first message')
            logging.getLogger('foo.bar').error('second message')
        self.assertEqual(cm.output, ['INFO:foo:first message',
                                     'ERROR:foo.bar:second message'])
  summary: Fail unless a log message of level *level* or higher is emitted
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: logger
    default: None
    rest: false
  - kind: positional
    name: level
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertMultiLineEqual
  kind: method
  ns: scrapy.contracts
  description: Assert that two multi-line strings are equal.
  summary: Assert that two multi-line strings are equal
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: first
    default: null
    rest: false
  - kind: positional
    name: second
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertNoLogs
  kind: method
  ns: scrapy.contracts
  description: |-
    Fail unless no log messages of level *level* or higher are emitted
    on *logger_name* or its children.

    This method must be used as a context manager.
  summary: Fail unless no log messages of level *level* or higher are emitted
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: logger
    default: None
    rest: false
  - kind: positional
    name: level
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertNotAlmostEqual
  kind: method
  ns: scrapy.contracts
  description: |-
    Fail if the two objects are equal as determined by their
    difference rounded to the given number of decimal places
    (default 7) and comparing to zero, or by comparing that the
    difference between the two objects is less than the given delta.

    Note that decimal places (from zero) are usually not the same
    as significant digits (measured from the most significant digit).

    Objects that are equal automatically fail.
  summary: Fail if the two objects are equal as determined by their
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: first
    default: null
    rest: false
  - kind: positional
    name: second
    default: null
    rest: false
  - kind: positional
    name: places
    default: None
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - kind: positional
    name: delta
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertNotAlmostEquals
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.assertNotEqual
  kind: method
  ns: scrapy.contracts
  description: |-
    Fail if the two objects are equal as determined by the '!='
    operator.
  summary: Fail if the two objects are equal as determined by the '!='
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: first
    default: null
    rest: false
  - kind: positional
    name: second
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertNotEquals
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.assertNotIn
  kind: method
  ns: scrapy.contracts
  description: Just like self.assertTrue(a not in b), but with a nicer default message.
  summary: Just like self
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: member
    default: null
    rest: false
  - kind: positional
    name: container
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertNotIsInstance
  kind: method
  ns: scrapy.contracts
  description: Included for symmetry with assertIsInstance.
  summary: Included for symmetry with assertIsInstance
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: obj
    default: null
    rest: false
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertNotRegex
  kind: method
  ns: scrapy.contracts
  description: Fail the test if the text matches the regular expression.
  summary: Fail the test if the text matches the regular expression
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: unexpected_regex
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertNotRegexpMatches
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.assertRaises
  kind: method
  ns: scrapy.contracts
  description: |-
    Fail unless an exception of class expected_exception is raised
    by the callable when invoked with specified positional and
    keyword arguments. If a different type of exception is
    raised, it will not be caught, and the test case will be
    deemed to have suffered an error, exactly as for an
    unexpected exception.

    If called with the callable and arguments omitted, will return a
    context object used like this::

         with self.assertRaises(SomeException):
             do_something()

    An optional keyword argument 'msg' can be provided when assertRaises
    is used as a context object.

    The context manager keeps a reference to the exception as
    the 'exception' attribute. This allows you to inspect the
    exception after the assertion::

        with self.assertRaises(SomeException) as cm:
            do_something()
        the_exception = cm.exception
        self.assertEqual(the_exception.error_code, 3)
  summary: Fail unless an exception of class expected_exception is raised
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expected_exception
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertRaisesRegex
  kind: method
  ns: scrapy.contracts
  description: |-
    Asserts that the message in a raised exception matches a regex.

    Args:
        expected_exception: Exception class expected to be raised.
        expected_regex: Regex (re.Pattern object or string) expected
                to be found in error message.
        args: Function to be called and extra positional args.
        kwargs: Extra kwargs.
        msg: Optional message used in case of failure. Can only be used
                when assertRaisesRegex is used as a context manager.
  summary: Asserts that the message in a raised exception matches a regex
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expected_exception
    default: null
    rest: false
  - kind: positional
    name: expected_regex
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertRaisesRegexp
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.assertRegex
  kind: method
  ns: scrapy.contracts
  description: Fail the test unless the text matches the regular expression.
  summary: Fail the test unless the text matches the regular expression
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: expected_regex
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertRegexpMatches
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.assertSequenceEqual
  kind: method
  ns: scrapy.contracts
  description: |-
    An equality assertion for ordered sequences (like lists and tuples).

    For the purposes of this function, a valid ordered sequence type is one
    which can be indexed, has a length, and has an equality operator.

    Args:
        seq1: The first sequence to compare.
        seq2: The second sequence to compare.
        seq_type: The expected datatype of the sequences, or None if no
                datatype should be enforced.
        msg: Optional message to use on failure instead of a list of
                differences.
  summary: An equality assertion for ordered sequences (like lists and tuples)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: seq1
    default: null
    rest: false
  - kind: positional
    name: seq2
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - kind: positional
    name: seq_type
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertSetEqual
  kind: method
  ns: scrapy.contracts
  description: |-
    A set-specific equality assertion.

    Args:
        set1: The first set to compare.
        set2: The second set to compare.
        msg: Optional message to use on failure instead of a list of
                differences.

    assertSetEqual uses ducktyping to support different types of sets, and
    is optimized for sets specifically (parameters must support a
    difference method).
  summary: A set-specific equality assertion
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: set1
    default: null
    rest: false
  - kind: positional
    name: set2
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertTrue
  kind: method
  ns: scrapy.contracts
  description: Check that the expression is true.
  summary: Check that the expression is true
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expr
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertTupleEqual
  kind: method
  ns: scrapy.contracts
  description: |-
    A tuple-specific equality assertion.

    Args:
        tuple1: The first tuple to compare.
        tuple2: The second tuple to compare.
        msg: Optional message to use on failure instead of a list of
                differences.
  summary: A tuple-specific equality assertion
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tuple1
    default: null
    rest: false
  - kind: positional
    name: tuple2
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertWarns
  kind: method
  ns: scrapy.contracts
  description: |-
    Fail unless a warning of class warnClass is triggered
    by the callable when invoked with specified positional and
    keyword arguments.  If a different type of warning is
    triggered, it will not be handled: depending on the other
    warning filtering rules in effect, it might be silenced, printed
    out, or raised as an exception.

    If called with the callable and arguments omitted, will return a
    context object used like this::

         with self.assertWarns(SomeWarning):
             do_something()

    An optional keyword argument 'msg' can be provided when assertWarns
    is used as a context object.

    The context manager keeps a reference to the first matching
    warning as the 'warning' attribute; similarly, the 'filename'
    and 'lineno' attributes give you information about the line
    of Python code from which the warning was triggered.
    This allows you to inspect the warning after the assertion::

        with self.assertWarns(SomeWarning) as cm:
            do_something()
        the_warning = cm.warning
        self.assertEqual(the_warning.some_attribute, 147)
  summary: Fail unless a warning of class warnClass is triggered
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expected_warning
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assertWarnsRegex
  kind: method
  ns: scrapy.contracts
  description: |-
    Asserts that the message in a triggered warning matches a regexp.
    Basic functioning is similar to assertWarns() with the addition
    that only warnings whose messages also match the regular expression
    are considered successful matches.

    Args:
        expected_warning: Warning class expected to be triggered.
        expected_regex: Regex (re.Pattern object or string) expected
                to be found in error message.
        args: Function to be called and extra positional args.
        kwargs: Extra kwargs.
        msg: Optional message used in case of failure. Can only be used
                when assertWarnsRegex is used as a context manager.
  summary: Asserts that the message in a triggered warning matches a regexp
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expected_warning
    default: null
    rest: false
  - kind: positional
    name: expected_regex
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.assert_
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.countTestCases
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.debug
  kind: method
  ns: scrapy.contracts
  description: Run the test without collecting errors in a TestResult
  summary: Run the test without collecting errors in a TestResult
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.defaultTestResult
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.doClassCleanups
  kind: function
  ns: scrapy.contracts
  description: |-
    Execute all class cleanup functions. Normally called for you after
    tearDownClass.
  summary: Execute all class cleanup functions
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.doCleanups
  kind: method
  ns: scrapy.contracts
  description: |-
    Execute all cleanup functions. Normally called for you after
    tearDown.
  summary: Execute all cleanup functions
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.enterClassContext
  kind: function
  ns: scrapy.contracts
  description: Same as enterContext, but class-wide.
  summary: Same as enterContext, but class-wide
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: cm
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.enterContext
  kind: method
  ns: scrapy.contracts
  description: |-
    Enters the supplied context manager.

    If successful, also adds its __exit__ method as a cleanup
    function and returns the result of the __enter__ method.
  summary: Enters the supplied context manager
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cm
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.fail
  kind: method
  ns: scrapy.contracts
  description: Fail immediately, with the given message.
  summary: Fail immediately, with the given message
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.failIf
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.failIfAlmostEqual
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.failIfEqual
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.failUnless
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.failUnlessAlmostEqual
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.failUnlessEqual
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.failUnlessRaises
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.failureException
  kind: class
  ns: scrapy.contracts
  description: Assertion failed.
  summary: Assertion failed
  signatures: null
  inherits_from:
  - <class 'Exception'>
  - <class 'BaseException'>
- name: TestCase.failureException.add_note
  kind: callable
  ns: scrapy.contracts
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: TestCase.failureException.args
  kind: property
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TestCase.failureException.with_traceback
  kind: callable
  ns: scrapy.contracts
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: TestCase.id
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.longMessage
  kind: property
  ns: scrapy.contracts
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: TestCase.maxDiff
  kind: property
  ns: scrapy.contracts
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: TestCase.run
  kind: method
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: result
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.setUp
  kind: method
  ns: scrapy.contracts
  description: Hook method for setting up the test fixture before exercising it.
  summary: Hook method for setting up the test fixture before exercising it
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.setUpClass
  kind: function
  ns: scrapy.contracts
  description: Hook method for setting up class fixture before running tests in the class.
  summary: Hook method for setting up class fixture before running tests in the class
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.shortDescription
  kind: method
  ns: scrapy.contracts
  description: |-
    Returns a one-line description of the test, or None if no
    description has been provided.

    The default implementation of this method returns the first line of
    the specified test method's docstring.
  summary: Returns a one-line description of the test, or None if no
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.skipTest
  kind: method
  ns: scrapy.contracts
  description: Skip this test.
  summary: Skip this test
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.subTest
  kind: method
  ns: scrapy.contracts
  description: |-
    Return a context manager that will return the enclosed block
    of code in a subtest identified by the optional message and
    keyword parameters.  A failure in the subtest marks the test
    case as failed but resumes execution at the end of the enclosed
    block, allowing further test code to be executed.
  summary: Return a context manager that will return the enclosed block
  signatures:
  - type: '?'
  inherits_from: null
- name: TestCase.tearDown
  kind: method
  ns: scrapy.contracts
  description: Hook method for deconstructing the test fixture after testing it.
  summary: Hook method for deconstructing the test fixture after testing it
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TestCase.tearDownClass
  kind: function
  ns: scrapy.contracts
  description: Hook method for deconstructing the class fixture after running all tests in the class.
  summary: Hook method for deconstructing the class fixture after running all tests in the class
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: get_spec
  kind: function
  ns: scrapy.contracts
  description: |-
    Returns (args, kwargs) tuple for a function
    >>> import re
    >>> get_spec(re.match)
    (['pattern', 'string'], {'flags': 0})

    >>> class Test:
    ...     def __call__(self, val):
    ...         pass
    ...     def method(self, val, flags=0):
    ...         pass

    >>> get_spec(Test)
    (['self', 'val'], {})

    >>> get_spec(Test.method)
    (['self', 'val'], {'flags': 0})

    >>> get_spec(Test().method)
    (['self', 'val'], {'flags': 0})
  summary: Returns (args, kwargs) tuple for a function
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: getmembers
  kind: function
  ns: scrapy.contracts
  description: |-
    Return all members of an object as (name, value) pairs sorted by name.
    Optionally, only return members that satisfy a given predicate.
  summary: Return all members of an object as (name, value) pairs sorted by name
  signatures:
  - kind: positional
    name: object
    default: null
    rest: false
  - kind: positional
    name: predicate
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: iterate_spider_output
  kind: function
  ns: scrapy.contracts
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: wraps
  kind: function
  ns: scrapy.contracts
  description: |-
    Decorator factory to apply update_wrapper() to a wrapper function

    Returns a decorator that invokes update_wrapper() with the decorated
    function as the wrapper argument and the arguments to wraps() as the
    remaining arguments. Default arguments are as for update_wrapper().
    This is a convenience function to simplify applying partial() to
    update_wrapper().
  summary: Decorator factory to apply update_wrapper() to a wrapper function
  signatures:
  - kind: positional
    name: wrapped
    default: null
    rest: false
  - kind: positional
    name: assigned
    default: ('__module__', '__name__', '__qualname__', '__doc__', '__annotations__')
    rest: false
  - kind: positional
    name: updated
    default: ('__dict__',)
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.core
  kind: module
  ns: null
  description: Scrapy core library classes and functions.
  summary: Scrapy core library classes and functions
  signatures: null
  inherits_from: null
- name: downloader
  kind: module
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: BaseSettings
  kind: class
  ns: scrapy.core
  description: |-
    Instances of this class behave like dictionaries, but store priorities
    along with their ``(key, value)`` pairs, and can be frozen (i.e. marked
    immutable).

    Key-value entries can be passed on initialization with the ``values``
    argument, and they would take the ``priority`` level (unless ``values`` is
    already an instance of :class:`~scrapy.settings.BaseSettings`, in which
    case the existing priority levels will be kept).  If the ``priority``
    argument is a string, the priority name will be looked up in
    :attr:`~scrapy.settings.SETTINGS_PRIORITIES`. Otherwise, a specific integer
    should be provided.

    Once the object is created, new settings can be loaded or updated with the
    :meth:`~scrapy.settings.BaseSettings.set` method, and can be accessed with
    the square bracket notation of dictionaries, or with the
    :meth:`~scrapy.settings.BaseSettings.get` method of the instance and its
    value conversion variants. When requesting a stored key, the value with the
    highest priority will be retrieved.
  summary: Instances of this class behave like dictionaries, but store priorities
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: values
    default: None
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: BaseSettings
  inherits_from:
  - <class 'collections.abc.MutableMapping'>
  - <class 'collections.abc.Mapping'>
  - <class 'collections.abc.Collection'>
  - <class 'collections.abc.Sized'>
  - <class 'collections.abc.Iterable'>
  - <class 'collections.abc.Container'>
  - <class 'typing.Generic'>
- name: BaseSettings.clear
  kind: method
  ns: scrapy.core
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.copy
  kind: method
  ns: scrapy.core
  description: |-
    Make a deep copy of current settings.

    This method returns a new instance of the :class:`Settings` class,
    populated with the same values and their priorities.

    Modifications to the new object won't be reflected on the original
    settings.
  summary: Make a deep copy of current settings
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.copy_to_dict
  kind: method
  ns: scrapy.core
  description: |-
    Make a copy of current settings and convert to a dict.

    This method returns a new dict populated with the same values
    and their priorities as the current settings.

    Modifications to the returned dict won't be reflected on the original
    settings.

    This method can be useful for example for printing settings
    in Scrapy shell.
  summary: Make a copy of current settings and convert to a dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.delete
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.freeze
  kind: method
  ns: scrapy.core
  description: |-
    Disable further changes to the current settings.

    After calling this method, the present state of the settings will become
    immutable. Trying to change values through the :meth:`~set` method and
    its variants won't be possible and will be alerted.
  summary: Disable further changes to the current settings
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.frozencopy
  kind: method
  ns: scrapy.core
  description: |-
    Return an immutable copy of the current settings.

    Alias for a :meth:`~freeze` call in the object returned by :meth:`copy`.
  summary: Return an immutable copy of the current settings
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.get
  kind: method
  ns: scrapy.core
  description: |-
    Get a setting value without affecting its original type.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value without affecting its original type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.getbool
  kind: method
  ns: scrapy.core
  description: |-
    Get a setting value as a boolean.

    ``1``, ``'1'``, `True`` and ``'True'`` return ``True``,
    while ``0``, ``'0'``, ``False``, ``'False'`` and ``None`` return ``False``.

    For example, settings populated through environment variables set to
    ``'0'`` will return ``False`` when using this method.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value as a boolean
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.getdict
  kind: method
  ns: scrapy.core
  description: |-
    Get a setting value as a dictionary. If the setting original type is a
    dictionary, a copy of it will be returned. If it is a string it will be
    evaluated as a JSON dictionary. In the case that it is a
    :class:`~scrapy.settings.BaseSettings` instance itself, it will be
    converted to a dictionary, containing all its current settings values
    as they would be returned by :meth:`~scrapy.settings.BaseSettings.get`,
    and losing all information about priority and mutability.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value as a dictionary
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.getdictorlist
  kind: method
  ns: scrapy.core
  description: |-
    Get a setting value as either a :class:`dict` or a :class:`list`.

    If the setting is already a dict or a list, a copy of it will be
    returned.

    If it is a string it will be evaluated as JSON, or as a comma-separated
    list of strings as a fallback.

    For example, settings populated from the command line will return:

    -   ``{'key1': 'value1', 'key2': 'value2'}`` if set to
        ``'{"key1": "value1", "key2": "value2"}'``

    -   ``['one', 'two']`` if set to ``'["one", "two"]'`` or ``'one,two'``

    :param name: the setting name
    :type name: string

    :param default: the value to return if no setting is found
    :type default: any
  summary: Get a setting value as either a :class:`dict` or a :class:`list`
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.getfloat
  kind: method
  ns: scrapy.core
  description: |-
    Get a setting value as a float.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value as a float
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: '0.0'
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.getint
  kind: method
  ns: scrapy.core
  description: |-
    Get a setting value as an int.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value as an int
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.getlist
  kind: method
  ns: scrapy.core
  description: |-
    Get a setting value as a list. If the setting original type is a list, a
    copy of it will be returned. If it's a string it will be split by ",".

    For example, settings populated through environment variables set to
    ``'one,two'`` will return a list ['one', 'two'] when using this method.

    :param name: the setting name
    :type name: str

    :param default: the value to return if no setting is found
    :type default: object
  summary: Get a setting value as a list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.getpriority
  kind: method
  ns: scrapy.core
  description: |-
    Return the current numerical priority value of a setting, or ``None`` if
    the given ``name`` does not exist.

    :param name: the setting name
    :type name: str
  summary: Return the current numerical priority value of a setting, or ``None`` if
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.getwithbase
  kind: method
  ns: scrapy.core
  description: |-
    Get a composition of a dictionary-like setting and its `_BASE`
    counterpart.

    :param name: name of the dictionary-like setting
    :type name: str
  summary: Get a composition of a dictionary-like setting and its `_BASE`
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.items
  kind: method
  ns: scrapy.core
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.keys
  kind: method
  ns: scrapy.core
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.maxpriority
  kind: method
  ns: scrapy.core
  description: |-
    Return the numerical value of the highest priority present throughout
    all settings, or the numerical value for ``default`` from
    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings
    stored.
  summary: Return the numerical value of the highest priority present throughout
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.pop
  kind: method
  ns: scrapy.core
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
    If key is not found, d is returned if given, otherwise KeyError is raised.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: <object object at 0x7fa9743b1d40>
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.popitem
  kind: method
  ns: scrapy.core
  description: |-
    D.popitem() -> (k, v), remove and return some (key, value) pair
    as a 2-tuple; but raise KeyError if D is empty.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.set
  kind: method
  ns: scrapy.core
  description: |-
    Store a key/value attribute with a given priority.

    Settings should be populated *before* configuring the Crawler object
    (through the :meth:`~scrapy.crawler.Crawler.configure` method),
    otherwise they won't have any effect.

    :param name: the setting name
    :type name: str

    :param value: the value to associate with the setting
    :type value: object

    :param priority: the priority of the setting. Should be a key of
        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer
    :type priority: str or int
  summary: Store a key/value attribute with a given priority
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.setdefault
  kind: method
  ns: scrapy.core
  description: D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.setdict
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: values
    default: null
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.setmodule
  kind: method
  ns: scrapy.core
  description: |-
    Store settings from a module with a given priority.

    This is a helper function that calls
    :meth:`~scrapy.settings.BaseSettings.set` for every globally declared
    uppercase variable of ``module`` with the provided ``priority``.

    :param module: the module or the path of the module
    :type module: types.ModuleType or str

    :param priority: the priority of the settings. Should be a key of
        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer
    :type priority: str or int
  summary: Store settings from a module with a given priority
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: module
    default: null
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.update
  kind: method
  ns: scrapy.core
  description: |-
    Store key/value pairs with a given priority.

    This is a helper function that calls
    :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``
    with the provided ``priority``.

    If ``values`` is a string, it is assumed to be JSON-encoded and parsed
    into a dict with ``json.loads()`` first. If it is a
    :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities
    will be used and the ``priority`` parameter ignored. This allows
    inserting/updating settings with different priorities with a single
    command.

    :param values: the settings names and values
    :type values: dict or string or :class:`~scrapy.settings.BaseSettings`

    :param priority: the priority of the settings. Should be a key of
        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer
    :type priority: str or int
  summary: Store key/value pairs with a given priority
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: values
    default: null
    rest: false
  - kind: positional
    name: priority
    default: project
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseSettings.values
  kind: method
  ns: scrapy.core
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred
  kind: class
  ns: scrapy.core
  description: |-
    This is a callback which will be put off until later.

    Why do we want this? Well, in cases where a function in a threaded
    program would block until it gets a result, for Twisted it should
    not block. Instead, it should return a L{Deferred}.

    This can be implemented for protocols that run over the network by
    writing an asynchronous protocol for L{twisted.internet}. For methods
    that come from outside packages that are not under our control, we use
    threads (see for example L{twisted.enterprise.adbapi}).

    For more information about Deferreds, see doc/core/howto/defer.html or
    U{http://twistedmatrix.com/documents/current/core/howto/defer.html}

    When creating a Deferred, you may provide a canceller function, which
    will be called by d.cancel() to let you do any clean-up necessary if the
    user decides not to wait for the deferred to complete.

    @ivar called: A flag which is C{False} until either C{callback} or
        C{errback} is called and afterwards always C{True}.
    @ivar paused: A counter of how many unmatched C{pause} calls have been made
        on this instance.
    @ivar _suppressAlreadyCalled: A flag used by the cancellation mechanism
        which is C{True} if the Deferred has no canceller and has been
        cancelled, C{False} otherwise.  If C{True}, it can be expected that
        C{callback} or C{errback} will eventually be called and the result
        should be silently discarded.
    @ivar _runningCallbacks: A flag which is C{True} while this instance is
        executing its callback chain, used to stop recursive execution of
        L{_runCallbacks}
    @ivar _chainedTo: If this L{Deferred} is waiting for the result of another
        L{Deferred}, this is a reference to the other Deferred.  Otherwise,
        L{None}.
  summary: This is a callback which will be put off until later
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: canceller
    default: None
    rest: false
  - type: Deferred
  inherits_from:
  - <class 'collections.abc.Awaitable'>
  - <class 'typing.Generic'>
- name: Deferred.addBoth
  kind: method
  ns: scrapy.core
  description: |-
    Convenience method for adding a single callable as both a callback
    and an errback.

    See L{addCallbacks}.
  summary: Convenience method for adding a single callable as both a callback
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.addCallback
  kind: method
  ns: scrapy.core
  description: |-
    Convenience method for adding just a callback.

    See L{addCallbacks}.
  summary: Convenience method for adding just a callback
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.addCallbacks
  kind: method
  ns: scrapy.core
  description: |-
    Add a pair of callbacks (success and error) to this L{Deferred}.

    These will be executed when the 'master' callback is run.

    @note: The signature of this function was designed many years before
        PEP 612; ParamSpec provides no mechanism to annotate parameters
        like C{callbackArgs}; this is therefore inherently less type-safe
        than calling C{addCallback} and C{addErrback} separately.

    @return: C{self}.
  summary: Add a pair of callbacks (success and error) to this L{Deferred}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - kind: positional
    name: callbackArgs
    default: ()
    rest: false
  - kind: positional
    name: callbackKeywords
    default: '{}'
    rest: false
  - kind: positional
    name: errbackArgs
    default: ()
    rest: false
  - kind: positional
    name: errbackKeywords
    default: '{}'
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.addErrback
  kind: method
  ns: scrapy.core
  description: |-
    Convenience method for adding just an errback.

    See L{addCallbacks}.
  summary: Convenience method for adding just an errback
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: errback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.addTimeout
  kind: method
  ns: scrapy.core
  description: |-
    Time out this L{Deferred} by scheduling it to be cancelled after
    C{timeout} seconds.

    The timeout encompasses all the callbacks and errbacks added to this
    L{defer.Deferred} before the call to L{addTimeout}, and none added
    after the call.

    If this L{Deferred} gets timed out, it errbacks with a L{TimeoutError},
    unless a cancelable function was passed to its initialization or unless
    a different C{onTimeoutCancel} callable is provided.

    @param timeout: number of seconds to wait before timing out this
        L{Deferred}
    @param clock: The object which will be used to schedule the timeout.
    @param onTimeoutCancel: A callable which is called immediately after
        this L{Deferred} times out, and not if this L{Deferred} is
        otherwise cancelled before the timeout. It takes an arbitrary
        value, which is the value of this L{Deferred} at that exact point
        in time (probably a L{CancelledError} L{Failure}), and the
        C{timeout}.  The default callable (if C{None} is provided) will
        translate a L{CancelledError} L{Failure} into a L{TimeoutError}.

    @return: C{self}.

    @since: 16.5
  summary: Time out this L{Deferred} by scheduling it to be cancelled after
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: timeout
    default: null
    rest: false
  - kind: positional
    name: clock
    default: null
    rest: false
  - kind: positional
    name: onTimeoutCancel
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.asFuture
  kind: method
  ns: scrapy.core
  description: |-
    Adapt this L{Deferred} into a L{Future} which is bound to C{loop}.

    @note: converting a L{Deferred} to an L{Future} consumes both
        its result and its errors, so this method implicitly converts
        C{self} into a L{Deferred} firing with L{None}, regardless of what
        its result previously would have been.

    @since: Twisted 17.5.0

    @param loop: The L{asyncio} event loop to bind the L{Future} to.

    @return: A L{Future} which will fire when the L{Deferred} fires.
  summary: Adapt this L{Deferred} into a L{Future} which is bound to C{loop}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: loop
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.callback
  kind: method
  ns: scrapy.core
  description: |-
    Run all success callbacks that have been added to this L{Deferred}.

    Each callback will have its result passed as the first argument to
    the next; this way, the callbacks act as a 'processing chain'.  If
    the success-callback returns a L{Failure} or raises an L{Exception},
    processing will continue on the *error* callback chain.  If a
    callback (or errback) returns another L{Deferred}, this L{Deferred}
    will be chained to it (and further callbacks will not run until that
    L{Deferred} has a result).

    An instance of L{Deferred} may only have either L{callback} or
    L{errback} called on it, and only once.

    @param result: The object which will be passed to the first callback
        added to this L{Deferred} (via L{addCallback}), unless C{result} is
        a L{Failure}, in which case the behavior is the same as calling
        C{errback(result)}.

    @raise AlreadyCalledError: If L{callback} or L{errback} has already been
        called on this L{Deferred}.
  summary: Run all success callbacks that have been added to this L{Deferred}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.called
  kind: property
  ns: scrapy.core
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: Deferred.cancel
  kind: method
  ns: scrapy.core
  description: |-
    Cancel this L{Deferred}.

    If the L{Deferred} has not yet had its C{errback} or C{callback} method
    invoked, call the canceller function provided to the constructor. If
    that function does not invoke C{callback} or C{errback}, or if no
    canceller function was provided, errback with L{CancelledError}.

    If this L{Deferred} is waiting on another L{Deferred}, forward the
    cancellation to the other L{Deferred}.
  summary: Cancel this L{Deferred}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.chainDeferred
  kind: method
  ns: scrapy.core
  description: |-
    Chain another L{Deferred} to this L{Deferred}.

    This method adds callbacks to this L{Deferred} to call C{d}'s callback
    or errback, as appropriate. It is merely a shorthand way of performing
    the following::

        d1.addCallbacks(d2.callback, d2.errback)

    When you chain a deferred C{d2} to another deferred C{d1} with
    C{d1.chainDeferred(d2)}, you are making C{d2} participate in the
    callback chain of C{d1}.
    Thus any event that fires C{d1} will also fire C{d2}.
    However, the converse is B{not} true; if C{d2} is fired, C{d1} will not
    be affected.

    Note that unlike the case where chaining is caused by a L{Deferred}
    being returned from a callback, it is possible to cause the call
    stack size limit to be exceeded by chaining many L{Deferred}s
    together with C{chainDeferred}.

    @return: C{self}.
  summary: Chain another L{Deferred} to this L{Deferred}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: d
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.debug
  kind: property
  ns: scrapy.core
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: Deferred.errback
  kind: method
  ns: scrapy.core
  description: |-
    Run all error callbacks that have been added to this L{Deferred}.

    Each callback will have its result passed as the first
    argument to the next; this way, the callbacks act as a
    'processing chain'. Also, if the error-callback returns a non-Failure
    or doesn't raise an L{Exception}, processing will continue on the
    *success*-callback chain.

    If the argument that's passed to me is not a L{Failure} instance,
    it will be embedded in one. If no argument is passed, a
    L{Failure} instance will be created based on the current
    traceback stack.

    Passing a string as `fail' is deprecated, and will be punished with
    a warning message.

    An instance of L{Deferred} may only have either L{callback} or
    L{errback} called on it, and only once.

    @param fail: The L{Failure} object which will be passed to the first
        errback added to this L{Deferred} (via L{addErrback}).
        Alternatively, a L{Exception} instance from which a L{Failure} will
        be constructed (with no traceback) or L{None} to create a L{Failure}
        instance from the current exception state (with a traceback).

    @raise AlreadyCalledError: If L{callback} or L{errback} has already been
        called on this L{Deferred}.
    @raise NoCurrentExceptionError: If C{fail} is L{None} but there is
        no current exception state.
  summary: Run all error callbacks that have been added to this L{Deferred}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: fail
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.fromCoroutine
  kind: function
  ns: scrapy.core
  description: |-
    Schedule the execution of a coroutine that awaits on L{Deferred}s,
    wrapping it in a L{Deferred} that will fire on success/failure of the
    coroutine.

    Coroutine functions return a coroutine object, similar to how
    generators work. This function turns that coroutine into a Deferred,
    meaning that it can be used in regular Twisted code. For example::

        import treq
        from twisted.internet.defer import Deferred
        from twisted.internet.task import react

        async def crawl(pages):
            results = {}
            for page in pages:
                results[page] = await treq.content(await treq.get(page))
            return results

        def main(reactor):
            pages = [
                "http://localhost:8080"
            ]
            d = Deferred.fromCoroutine(crawl(pages))
            d.addCallback(print)
            return d

        react(main)

    @since: Twisted 21.2.0

    @param coro: The coroutine object to schedule.

    @raise ValueError: If C{coro} is not a coroutine or generator.
  summary: Schedule the execution of a coroutine that awaits on L{Deferred}s,
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: coro
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.fromFuture
  kind: function
  ns: scrapy.core
  description: |-
    Adapt a L{Future} to a L{Deferred}.

    @note: This creates a L{Deferred} from a L{Future}, I{not} from
        a C{coroutine}; in other words, you will need to call
        L{asyncio.ensure_future}, L{asyncio.loop.create_task} or create an
        L{asyncio.Task} yourself to get from a C{coroutine} to a
        L{Future} if what you have is an awaitable coroutine and
        not a L{Future}.  (The length of this list of techniques is
        exactly why we have left it to the caller!)

    @since: Twisted 17.5.0

    @param future: The L{Future} to adapt.

    @return: A L{Deferred} which will fire when the L{Future} fires.
  summary: Adapt a L{Future} to a L{Deferred}
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: future
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.pause
  kind: method
  ns: scrapy.core
  description: Stop processing on a L{Deferred} until L{unpause}() is called.
  summary: Stop processing on a L{Deferred} until L{unpause}() is called
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.paused
  kind: property
  ns: scrapy.core
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: Deferred.send
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: value
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Deferred.unpause
  kind: method
  ns: scrapy.core
  description: Process all callbacks made since L{pause}() was called.
  summary: Process all callbacks made since L{pause}() was called
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deque
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.deque.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.core
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloadHandlers
  kind: class
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: DownloadHandlers
  inherits_from: null
- name: DownloadHandlers.download_request
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Downloader
  kind: class
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: Downloader
  inherits_from: null
- name: Downloader.DOWNLOAD_SLOT
  kind: property
  ns: scrapy.core
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: Downloader.close
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Downloader.fetch
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Downloader.needs_backout
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderMiddlewareManager
  kind: class
  ns: scrapy.core
  description: Base class for implementing middleware managers
  summary: Base class for implementing middleware managers
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: DownloaderMiddlewareManager
  inherits_from:
  - <class 'scrapy.middleware.MiddlewareManager'>
- name: DownloaderMiddlewareManager.close_spider
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderMiddlewareManager.component_name
  kind: property
  ns: scrapy.core
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: DownloaderMiddlewareManager.download
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: download_func
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderMiddlewareManager.from_crawler
  kind: function
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderMiddlewareManager.from_settings
  kind: function
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderMiddlewareManager.open_spider
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Response
  kind: class
  ns: scrapy.core
  description: |-
    An object that represents an HTTP response, which is usually
    downloaded (by the Downloader) and fed to the Spiders for processing.
  summary: An object that represents an HTTP response, which is usually
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: status
    default: '200'
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: body
    default: b''
    rest: false
  - kind: positional
    name: flags
    default: None
    rest: false
  - kind: positional
    name: request
    default: None
    rest: false
  - kind: positional
    name: certificate
    default: None
    rest: false
  - kind: positional
    name: ip_address
    default: None
    rest: false
  - kind: positional
    name: protocol
    default: None
    rest: false
  - type: Response
  inherits_from:
  - <class 'scrapy.utils.trackref.object_ref'>
- name: Response.attributes
  kind: property
  ns: scrapy.core
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: Response.body
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Response.cb_kwargs
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Response.copy
  kind: method
  ns: scrapy.core
  description: Return a copy of this Response
  summary: Return a copy of this Response
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Response.css
  kind: method
  ns: scrapy.core
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Response.follow
  kind: method
  ns: scrapy.core
  description: |-
    Return a :class:`~.Request` instance to follow a link ``url``.
    It accepts the same arguments as ``Request.__init__`` method,
    but ``url`` can be a relative URL or a ``scrapy.link.Link`` object,
    not only an absolute URL.

    :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow`
    method which supports selectors in addition to absolute/relative URLs
    and Link objects.

    .. versionadded:: 2.0
       The *flags* parameter.
  summary: Return a :class:`~
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: callback
    default: None
    rest: false
  - kind: positional
    name: method
    default: GET
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: body
    default: None
    rest: false
  - kind: positional
    name: cookies
    default: None
    rest: false
  - kind: positional
    name: meta
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: utf-8
    rest: false
  - kind: positional
    name: priority
    default: '0'
    rest: false
  - kind: positional
    name: dont_filter
    default: 'False'
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - kind: positional
    name: cb_kwargs
    default: None
    rest: false
  - kind: positional
    name: flags
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Response.follow_all
  kind: method
  ns: scrapy.core
  description: |-
    .. versionadded:: 2.0

    Return an iterable of :class:`~.Request` instances to follow all links
    in ``urls``. It accepts the same arguments as ``Request.__init__`` method,
    but elements of ``urls`` can be relative URLs or :class:`~scrapy.link.Link` objects,
    not only absolute URLs.

    :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow_all`
    method which supports selectors in addition to absolute/relative URLs
    and Link objects.
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: urls
    default: null
    rest: false
  - kind: positional
    name: callback
    default: None
    rest: false
  - kind: positional
    name: method
    default: GET
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: body
    default: None
    rest: false
  - kind: positional
    name: cookies
    default: None
    rest: false
  - kind: positional
    name: meta
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: utf-8
    rest: false
  - kind: positional
    name: priority
    default: '0'
    rest: false
  - kind: positional
    name: dont_filter
    default: 'False'
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - kind: positional
    name: cb_kwargs
    default: None
    rest: false
  - kind: positional
    name: flags
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Response.jmespath
  kind: method
  ns: scrapy.core
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Response.meta
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Response.replace
  kind: method
  ns: scrapy.core
  description: Create a new Response with the same attributes except for those given new values
  summary: Create a new Response with the same attributes except for those given new values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Response.text
  kind: property
  ns: scrapy.core
  description: |-
    For subclasses of TextResponse, this will return the body
    as str
  summary: For subclasses of TextResponse, this will return the body
  signatures: null
  inherits_from: null
- name: Response.url
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Response.urljoin
  kind: method
  ns: scrapy.core
  description: |-
    Join this Response's url with a possible relative url to form an
    absolute interpretation of the latter.
  summary: Join this Response's url with a possible relative url to form an
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Response.xpath
  kind: method
  ns: scrapy.core
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Set
  kind: callable
  ns: scrapy.core
  description: A generic version of set.
  summary: A generic version of set
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SignalManager
  kind: class
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sender
    default: _Anonymous
    rest: false
  - type: SignalManager
  inherits_from: null
- name: SignalManager.connect
  kind: method
  ns: scrapy.core
  description: |-
    Connect a receiver function to a signal.

    The signal can be any object, although Scrapy comes with some
    predefined signals that are documented in the :ref:`topics-signals`
    section.

    :param receiver: the function to be connected
    :type receiver: collections.abc.Callable

    :param signal: the signal to connect to
    :type signal: object
  summary: Connect a receiver function to a signal
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: receiver
    default: null
    rest: false
  - kind: positional
    name: signal
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SignalManager.disconnect
  kind: method
  ns: scrapy.core
  description: |-
    Disconnect a receiver function from a signal. This has the
    opposite effect of the :meth:`connect` method, and the arguments
    are the same.
  summary: Disconnect a receiver function from a signal
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: receiver
    default: null
    rest: false
  - kind: positional
    name: signal
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SignalManager.disconnect_all
  kind: method
  ns: scrapy.core
  description: |-
    Disconnect all receivers from the given signal.

    :param signal: the signal to disconnect from
    :type signal: object
  summary: Disconnect all receivers from the given signal
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: signal
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SignalManager.send_catch_log
  kind: method
  ns: scrapy.core
  description: |-
    Send a signal, catch exceptions and log them.

    The keyword arguments are passed to the signal handlers (connected
    through the :meth:`connect` method).
  summary: Send a signal, catch exceptions and log them
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: signal
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SignalManager.send_catch_log_deferred
  kind: method
  ns: scrapy.core
  description: |-
    Like :meth:`send_catch_log` but supports returning
    :class:`~twisted.internet.defer.Deferred` objects from signal handlers.

    Returns a Deferred that gets fired once all signal handlers
    deferreds were fired. Send a signal, catch exceptions and log them.

    The keyword arguments are passed to the signal handlers (connected
    through the :meth:`connect` method).
  summary: Like :meth:`send_catch_log` but supports returning
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: signal
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Slot
  kind: class
  ns: scrapy.core
  description: Downloader slot
  summary: Downloader slot
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: concurrency
    default: null
    rest: false
  - kind: positional
    name: delay
    default: null
    rest: false
  - kind: positional
    name: randomize_delay
    default: null
    rest: false
  - type: Slot
  inherits_from: null
- name: Slot.close
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Slot.download_delay
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Slot.free_transfer_slots
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.core
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.core
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: datetime
  kind: class
  ns: scrapy.core
  description: |-
    datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])

    The year, month and day arguments are required. tzinfo may be None, or an
    instance of a tzinfo subclass. The remaining arguments may be ints.
  summary: datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])
  signatures: null
  inherits_from:
  - <class 'datetime.date'>
- name: datetime.astimezone
  kind: callable
  ns: scrapy.core
  description: tz -> convert to local time in new timezone tz
  summary: tz -> convert to local time in new timezone tz
  signatures: null
  inherits_from: null
- name: datetime.ctime
  kind: callable
  ns: scrapy.core
  description: Return ctime() style string.
  summary: Return ctime() style string
  signatures: null
  inherits_from: null
- name: datetime.date
  kind: callable
  ns: scrapy.core
  description: Return date object with same year, month and day.
  summary: Return date object with same year, month and day
  signatures: null
  inherits_from: null
- name: datetime.day
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: datetime.dst
  kind: callable
  ns: scrapy.core
  description: Return self.tzinfo.dst(self).
  summary: Return self
  signatures: null
  inherits_from: null
- name: datetime.fold
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: datetime.hour
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: datetime.isocalendar
  kind: callable
  ns: scrapy.core
  description: Return a named tuple containing ISO year, week number, and weekday.
  summary: Return a named tuple containing ISO year, week number, and weekday
  signatures: null
  inherits_from: null
- name: datetime.isoformat
  kind: callable
  ns: scrapy.core
  description: |-
    [sep] -> string in ISO 8601 format, YYYY-MM-DDT[HH[:MM[:SS[.mmm[uuu]]]]][+HH:MM].
    sep is used to separate the year from the time, and defaults to 'T'.
    The optional argument timespec specifies the number of additional terms
    of the time to include. Valid options are 'auto', 'hours', 'minutes',
    'seconds', 'milliseconds' and 'microseconds'.
  summary: '[sep] -> string in ISO 8601 format, YYYY-MM-DDT[HH[:MM[:SS['
  signatures: null
  inherits_from: null
- name: datetime.isoweekday
  kind: callable
  ns: scrapy.core
  description: |-
    Return the day of the week represented by the date.
    Monday == 1 ... Sunday == 7
  summary: Return the day of the week represented by the date
  signatures: null
  inherits_from: null
- name: datetime.max
  kind: property
  ns: scrapy.core
  description: |-
    datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])

    The year, month and day arguments are required. tzinfo may be None, or an
    instance of a tzinfo subclass. The remaining arguments may be ints.
  summary: datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])
  signatures: null
  inherits_from: null
- name: datetime.microsecond
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: datetime.min
  kind: property
  ns: scrapy.core
  description: |-
    datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])

    The year, month and day arguments are required. tzinfo may be None, or an
    instance of a tzinfo subclass. The remaining arguments may be ints.
  summary: datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])
  signatures: null
  inherits_from: null
- name: datetime.minute
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: datetime.month
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: datetime.replace
  kind: callable
  ns: scrapy.core
  description: Return datetime with new specified fields.
  summary: Return datetime with new specified fields
  signatures: null
  inherits_from: null
- name: datetime.resolution
  kind: property
  ns: scrapy.core
  description: |-
    Difference between two datetime values.

    timedelta(days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0)

    All arguments are optional and default to 0.
    Arguments may be integers or floats, and may be positive or negative.
  summary: Difference between two datetime values
  signatures: null
  inherits_from: null
- name: datetime.second
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: datetime.strftime
  kind: callable
  ns: scrapy.core
  description: format -> strftime() style string.
  summary: format -> strftime() style string
  signatures: null
  inherits_from: null
- name: datetime.time
  kind: callable
  ns: scrapy.core
  description: Return time object with same time but with tzinfo=None.
  summary: Return time object with same time but with tzinfo=None
  signatures: null
  inherits_from: null
- name: datetime.timestamp
  kind: callable
  ns: scrapy.core
  description: Return POSIX timestamp as float.
  summary: Return POSIX timestamp as float
  signatures: null
  inherits_from: null
- name: datetime.timetuple
  kind: callable
  ns: scrapy.core
  description: Return time tuple, compatible with time.localtime().
  summary: Return time tuple, compatible with time
  signatures: null
  inherits_from: null
- name: datetime.timetz
  kind: callable
  ns: scrapy.core
  description: Return time object with same time and tzinfo.
  summary: Return time object with same time and tzinfo
  signatures: null
  inherits_from: null
- name: datetime.toordinal
  kind: callable
  ns: scrapy.core
  description: Return proleptic Gregorian ordinal.  January 1 of year 1 is day 1.
  summary: Return proleptic Gregorian ordinal
  signatures: null
  inherits_from: null
- name: datetime.tzinfo
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: datetime.tzname
  kind: callable
  ns: scrapy.core
  description: Return self.tzinfo.tzname(self).
  summary: Return self
  signatures: null
  inherits_from: null
- name: datetime.utcoffset
  kind: callable
  ns: scrapy.core
  description: Return self.tzinfo.utcoffset(self).
  summary: Return self
  signatures: null
  inherits_from: null
- name: datetime.utctimetuple
  kind: callable
  ns: scrapy.core
  description: Return UTC time tuple, compatible with time.localtime().
  summary: Return UTC time tuple, compatible with time
  signatures: null
  inherits_from: null
- name: datetime.weekday
  kind: callable
  ns: scrapy.core
  description: |-
    Return the day of the week represented by the date.
    Monday == 0 ... Sunday == 6
  summary: Return the day of the week represented by the date
  signatures: null
  inherits_from: null
- name: datetime.year
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: deque
  kind: class
  ns: scrapy.core
  description: |-
    deque([iterable[, maxlen]]) --> deque object

    A list-like sequence optimized for data accesses near its endpoints.
  summary: deque([iterable[, maxlen]]) --> deque object
  signatures: null
  inherits_from: null
- name: deque.append
  kind: callable
  ns: scrapy.core
  description: Add an element to the right side of the deque.
  summary: Add an element to the right side of the deque
  signatures: null
  inherits_from: null
- name: deque.appendleft
  kind: callable
  ns: scrapy.core
  description: Add an element to the left side of the deque.
  summary: Add an element to the left side of the deque
  signatures: null
  inherits_from: null
- name: deque.clear
  kind: callable
  ns: scrapy.core
  description: Remove all elements from the deque.
  summary: Remove all elements from the deque
  signatures: null
  inherits_from: null
- name: deque.copy
  kind: callable
  ns: scrapy.core
  description: Return a shallow copy of a deque.
  summary: Return a shallow copy of a deque
  signatures: null
  inherits_from: null
- name: deque.count
  kind: callable
  ns: scrapy.core
  description: D.count(value) -- return number of occurrences of value
  summary: D
  signatures: null
  inherits_from: null
- name: deque.extend
  kind: callable
  ns: scrapy.core
  description: Extend the right side of the deque with elements from the iterable
  summary: Extend the right side of the deque with elements from the iterable
  signatures: null
  inherits_from: null
- name: deque.extendleft
  kind: callable
  ns: scrapy.core
  description: Extend the left side of the deque with elements from the iterable
  summary: Extend the left side of the deque with elements from the iterable
  signatures: null
  inherits_from: null
- name: deque.index
  kind: callable
  ns: scrapy.core
  description: |-
    D.index(value, [start, [stop]]) -- return first index of value.
    Raises ValueError if the value is not present.
  summary: D
  signatures: null
  inherits_from: null
- name: deque.insert
  kind: callable
  ns: scrapy.core
  description: D.insert(index, object) -- insert object before index
  summary: D
  signatures: null
  inherits_from: null
- name: deque.maxlen
  kind: property
  ns: scrapy.core
  description: maximum size of a deque or None if unbounded
  summary: maximum size of a deque or None if unbounded
  signatures: null
  inherits_from: null
- name: deque.pop
  kind: callable
  ns: scrapy.core
  description: Remove and return the rightmost element.
  summary: Remove and return the rightmost element
  signatures: null
  inherits_from: null
- name: deque.popleft
  kind: callable
  ns: scrapy.core
  description: Remove and return the leftmost element.
  summary: Remove and return the leftmost element
  signatures: null
  inherits_from: null
- name: deque.remove
  kind: callable
  ns: scrapy.core
  description: D.remove(value) -- remove first occurrence of value.
  summary: D
  signatures: null
  inherits_from: null
- name: deque.reverse
  kind: callable
  ns: scrapy.core
  description: D.reverse() -- reverse *IN PLACE*
  summary: D
  signatures: null
  inherits_from: null
- name: deque.rotate
  kind: callable
  ns: scrapy.core
  description: Rotate the deque n steps to the right (default n=1).  If n is negative, rotates left.
  summary: Rotate the deque n steps to the right (default n=1)
  signatures: null
  inherits_from: null
- name: dnscache
  kind: const
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: handlers
  kind: module
  ns: scrapy.core
  description: Download handlers for different schemes
  summary: Download handlers for different schemes
  signatures: null
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.core
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.core
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: NotSupported
  kind: class
  ns: scrapy.core
  description: Indicates a feature or method is not supported
  summary: Indicates a feature or method is not supported
  signatures: null
  inherits_from:
  - <class 'Exception'>
  - <class 'BaseException'>
- name: NotSupported.add_note
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: NotSupported.args
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: NotSupported.with_traceback
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.core
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.core
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: create_instance
  kind: function
  ns: scrapy.core
  description: |-
    Construct a class instance using its ``from_crawler`` or
    ``from_settings`` constructors, if available.

    At least one of ``settings`` and ``crawler`` needs to be different from
    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.
    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be
    tried.

    ``*args`` and ``**kwargs`` are forwarded to the constructors.

    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.

    .. versionchanged:: 2.2
       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an
       extension has not been implemented correctly).
  summary: Construct a class instance using its ``from_crawler`` or
  signatures:
  - kind: positional
    name: objcls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: load_object
  kind: function
  ns: scrapy.core
  description: |-
    Load an object given its absolute object path, and return it.

    The object can be the import path of a class, function, variable or an
    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.

    If ``path`` is not a string, but is a callable object, such as a class or
    a function, then return it as is.
  summary: Load an object given its absolute object path, and return it
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: urlparse_cached
  kind: function
  ns: scrapy.core
  description: |-
    Return urlparse.urlparse caching the result, where the argument can be a
    Request or Response object
  summary: Return urlparse
  signatures:
  - kind: positional
    name: request_or_response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: without_none_values
  kind: function
  ns: scrapy.core
  description: |-
    Return a copy of ``iterable`` with all ``None`` entries removed.

    If ``iterable`` is a mapping, return a dictionary where all pairs that have
    value ``None`` have been removed.
  summary: Return a copy of ``iterable`` with all ``None`` entries removed
  signatures:
  - kind: positional
    name: iterable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: middleware
  kind: module
  ns: scrapy.core
  description: |-
    Downloader Middleware manager

    See documentation in docs/topics/downloader-middleware.rst
  summary: Downloader Middleware manager
  signatures: null
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.core
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure
  kind: class
  ns: scrapy.core
  description: |-
    A basic abstraction for an error that has occurred.

    This is necessary because Python's built-in error mechanisms are
    inconvenient for asynchronous communication.

    The C{stack} and C{frame} attributes contain frames.  Each frame is a tuple
    of (funcName, fileName, lineNumber, localsItems, globalsItems), where
    localsItems and globalsItems are the contents of
    C{locals().items()}/C{globals().items()} for that frame, or an empty tuple
    if those details were not captured.

    @ivar value: The exception instance responsible for this failure.
    @ivar type: The exception's class.
    @ivar stack: list of frames, innermost last, excluding C{Failure.__init__}.
    @ivar frames: list of frames, innermost first.
  summary: A basic abstraction for an error that has occurred
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: exc_value
    default: None
    rest: false
  - kind: positional
    name: exc_type
    default: None
    rest: false
  - kind: positional
    name: exc_tb
    default: None
    rest: false
  - kind: positional
    name: captureVars
    default: 'False'
    rest: false
  - type: Failure
  inherits_from:
  - <class 'BaseException'>
- name: Failure.add_note
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: Failure.args
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Failure.check
  kind: method
  ns: scrapy.core
  description: |-
    Check if this failure's type is in a predetermined list.

    @type errorTypes: list of L{Exception} classes or
                      fully-qualified class names.
    @returns: the matching L{Exception} type, or None if no match.
  summary: Check if this failure's type is in a predetermined list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.cleanFailure
  kind: method
  ns: scrapy.core
  description: |-
    Remove references to other objects, replacing them with strings.

    On Python 3, this will also set the C{__traceback__} attribute of the
    exception instance to L{None}.
  summary: Remove references to other objects, replacing them with strings
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.getBriefTraceback
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.getErrorMessage
  kind: method
  ns: scrapy.core
  description: Get a string of the exception which caused this Failure.
  summary: Get a string of the exception which caused this Failure
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.getTraceback
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: elideFrameworkCode
    default: '0'
    rest: false
  - kind: positional
    name: detail
    default: default
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.getTracebackObject
  kind: method
  ns: scrapy.core
  description: |-
    Get an object that represents this Failure's stack that can be passed
    to traceback.extract_tb.

    If the original traceback object is still present, return that. If this
    traceback object has been lost but we still have the information,
    return a fake traceback object (see L{_Traceback}). If there is no
    traceback information at all, return None.
  summary: Get an object that represents this Failure's stack that can be passed
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.pickled
  kind: property
  ns: scrapy.core
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: Failure.printBriefTraceback
  kind: method
  ns: scrapy.core
  description: Print a traceback as densely as possible.
  summary: Print a traceback as densely as possible
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: None
    rest: false
  - kind: positional
    name: elideFrameworkCode
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.printDetailedTraceback
  kind: method
  ns: scrapy.core
  description: Print a traceback with detailed locals and globals information.
  summary: Print a traceback with detailed locals and globals information
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: None
    rest: false
  - kind: positional
    name: elideFrameworkCode
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.printTraceback
  kind: method
  ns: scrapy.core
  description: |-
    Emulate Python's standard error reporting mechanism.

    @param file: If specified, a file-like object to which to write the
        traceback.

    @param elideFrameworkCode: A flag indicating whether to attempt to
        remove uninteresting frames from within Twisted itself from the
        output.

    @param detail: A string indicating how much information to include
        in the traceback.  Must be one of C{'brief'}, C{'default'}, or
        C{'verbose'}.
  summary: Emulate Python's standard error reporting mechanism
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: None
    rest: false
  - kind: positional
    name: elideFrameworkCode
    default: 'False'
    rest: false
  - kind: positional
    name: detail
    default: default
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.raiseException
  kind: method
  ns: scrapy.core
  description: |-
    raise the original exception, preserving traceback
    information if available.
  summary: raise the original exception, preserving traceback
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.stack
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Failure.throwExceptionIntoGenerator
  kind: method
  ns: scrapy.core
  description: |-
    Throw the original exception into the given generator,
    preserving traceback information if available.

    @return: The next value yielded from the generator.
    @raise StopIteration: If there are no more values in the generator.
    @raise anything else: Anything that the generator raises.
  summary: Throw the original exception into the given generator,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: g
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.trap
  kind: method
  ns: scrapy.core
  description: |-
    Trap this failure if its type is in a predetermined list.

    This allows you to trap a Failure in an error callback.  It will be
    automatically re-raised if it is not a type that you expect.

    The reason for having this particular API is because it's very useful
    in Deferred errback chains::

        def _ebFoo(self, failure):
            r = failure.trap(Spam, Eggs)
            print('The Failure is due to either Spam or Eggs!')
            if r == Spam:
                print('Spam did it!')
            elif r == Eggs:
                print('Eggs did it!')

    If the failure is not a Spam or an Eggs, then the Failure will be
    'passed on' to the next errback. In Python 2 the Failure will be
    raised; in Python 3 the underlying exception will be re-raised.

    @type errorTypes: L{Exception}
  summary: Trap this failure if its type is in a predetermined list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Failure.with_traceback
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.core
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MiddlewareManager
  kind: class
  ns: scrapy.core
  description: Base class for implementing middleware managers
  summary: Base class for implementing middleware managers
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: MiddlewareManager
  inherits_from: null
- name: MiddlewareManager.close_spider
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MiddlewareManager.component_name
  kind: property
  ns: scrapy.core
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: MiddlewareManager.from_crawler
  kind: function
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MiddlewareManager.from_settings
  kind: function
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MiddlewareManager.open_spider
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.core
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: build_component_list
  kind: function
  ns: scrapy.core
  description: 'Compose a component list from a { class: order } dictionary.'
  summary: 'Compose a component list from a { class: order } dictionary'
  signatures:
  - kind: positional
    name: compdict
    default: null
    rest: false
  - kind: positional
    name: custom
    default: None
    rest: false
  - kind: positional
    name: convert
    default: <function update_classpath at 0x7fa9729e1a80>
    rest: false
  - type: '?'
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.core
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: deferred_from_coro
  kind: function
  ns: scrapy.core
  description: Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine
  summary: Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine
  signatures:
  - kind: positional
    name: o
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: inlineCallbacks
  kind: function
  ns: scrapy.core
  description: |-
    L{inlineCallbacks} helps you write L{Deferred}-using code that looks like a
    regular sequential function. For example::

        @inlineCallbacks
        def thingummy():
            thing = yield makeSomeRequestResultingInDeferred()
            print(thing)  # the result! hoorj!

    When you call anything that results in a L{Deferred}, you can simply yield it;
    your generator will automatically be resumed when the Deferred's result is
    available. The generator will be sent the result of the L{Deferred} with the
    'send' method on generators, or if the result was a failure, 'throw'.

    Things that are not L{Deferred}s may also be yielded, and your generator
    will be resumed with the same object sent back. This means C{yield}
    performs an operation roughly equivalent to L{maybeDeferred}.

    Your inlineCallbacks-enabled generator will return a L{Deferred} object, which
    will result in the return value of the generator (or will fail with a
    failure object if your generator raises an unhandled exception). Note that
    you can't use C{return result} to return a value; use C{returnValue(result)}
    instead. Falling off the end of the generator, or simply using C{return}
    will cause the L{Deferred} to have a result of L{None}.

    Be aware that L{returnValue} will not accept a L{Deferred} as a parameter.
    If you believe the thing you'd like to return could be a L{Deferred}, do
    this::

        result = yield result
        returnValue(result)

    The L{Deferred} returned from your deferred generator may errback if your
    generator raised an exception::

        @inlineCallbacks
        def thingummy():
            thing = yield makeSomeRequestResultingInDeferred()
            if thing == 'I love Twisted':
                # will become the result of the Deferred
                returnValue('TWISTED IS GREAT!')
            else:
                # will trigger an errback
                raise Exception('DESTROY ALL LIFE')

    It is possible to use the C{return} statement instead of L{returnValue}::

        @inlineCallbacks
        def loadData(url):
            response = yield makeRequest(url)
            return json.loads(response)

    You can cancel the L{Deferred} returned from your L{inlineCallbacks}
    generator before it is fired by your generator completing (either by
    reaching its end, a C{return} statement, or by calling L{returnValue}).
    A C{CancelledError} will be raised from the C{yield}ed L{Deferred} that
    has been cancelled if that C{Deferred} does not otherwise suppress it.
  summary: L{inlineCallbacks} helps you write L{Deferred}-using code that looks like a
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: mustbe_deferred
  kind: function
  ns: scrapy.core
  description: |-
    Same as twisted.internet.defer.maybeDeferred, but delay calling
    callback/errback to next reactor loop
  summary: Same as twisted
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: mustbe_deferred
  kind: function
  ns: scrapy.core
  description: |-
    Same as twisted.internet.defer.maybeDeferred, but delay calling
    callback/errback to next reactor loop
  summary: Same as twisted
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: urlparse_cached
  kind: function
  ns: scrapy.core
  description: |-
    Return urlparse.urlparse caching the result, where the argument can be a
    Request or Response object
  summary: Return urlparse
  signatures:
  - kind: positional
    name: request_or_response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: engine
  kind: module
  ns: scrapy.core
  description: |-
    This is the Scrapy engine which controls the Scheduler, Downloader and Spider.

    For more information see docs/topics/architecture.rst
  summary: This is the Scrapy engine which controls the Scheduler, Downloader and Spider
  signatures: null
  inherits_from: null
- name: CallLaterOnce
  kind: class
  ns: scrapy.core
  description: |-
    Schedule a function to be called in the next reactor loop, but only if
    it hasn't been already scheduled since the last time it ran.
  summary: Schedule a function to be called in the next reactor loop, but only if
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: func
    default: null
    rest: false
  - type: CallLaterOnce
  inherits_from: null
- name: CallLaterOnce.cancel
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CallLaterOnce.schedule
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: delay
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.core
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CloseSpider
  kind: class
  ns: scrapy.core
  description: Raise this from callbacks to request the spider to be closed
  summary: Raise this from callbacks to request the spider to be closed
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: reason
    default: cancelled
    rest: false
  - type: CloseSpider
  inherits_from:
  - <class 'Exception'>
  - <class 'BaseException'>
- name: CloseSpider.add_note
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: CloseSpider.args
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CloseSpider.with_traceback
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: DontCloseSpider
  kind: class
  ns: scrapy.core
  description: Request the spider not to be closed yet
  summary: Request the spider not to be closed yet
  signatures: null
  inherits_from:
  - <class 'Exception'>
  - <class 'BaseException'>
- name: DontCloseSpider.add_note
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: DontCloseSpider.args
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DontCloseSpider.with_traceback
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: ExecutionEngine
  kind: class
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: spider_closed_callback
    default: null
    rest: false
  - type: ExecutionEngine
  inherits_from: null
- name: ExecutionEngine.close
  kind: method
  ns: scrapy.core
  description: |-
    Gracefully close the execution engine.
    If it has already been started, stop it. In all cases, close the spider and the downloader.
  summary: Gracefully close the execution engine
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ExecutionEngine.close_spider
  kind: method
  ns: scrapy.core
  description: Close (cancel) spider and clear all its outstanding requests
  summary: Close (cancel) spider and clear all its outstanding requests
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: reason
    default: cancelled
    rest: false
  - type: '?'
  inherits_from: null
- name: ExecutionEngine.crawl
  kind: method
  ns: scrapy.core
  description: Inject the request into the spider <-> downloader pipeline
  summary: Inject the request into the spider <-> downloader pipeline
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ExecutionEngine.download
  kind: method
  ns: scrapy.core
  description: Return a Deferred which fires with a Response as result, only downloader middlewares are applied
  summary: Return a Deferred which fires with a Response as result, only downloader middlewares are applied
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ExecutionEngine.open_spider
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: ExecutionEngine.pause
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ExecutionEngine.spider_is_idle
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ExecutionEngine.start
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: ExecutionEngine.stop
  kind: method
  ns: scrapy.core
  description: Gracefully stop the execution engine
  summary: Gracefully stop the execution engine
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ExecutionEngine.unpause
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IgnoreRequest
  kind: class
  ns: scrapy.core
  description: Indicates a decision was made not to process a request
  summary: Indicates a decision was made not to process a request
  signatures: null
  inherits_from:
  - <class 'Exception'>
  - <class 'BaseException'>
- name: IgnoreRequest.add_note
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: IgnoreRequest.args
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: IgnoreRequest.with_traceback
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterator
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.Iterator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogFormatter
  kind: class
  ns: scrapy.core
  description: |-
    Class for generating log messages for different actions.

    All methods must return a dictionary listing the parameters ``level``, ``msg``
    and ``args`` which are going to be used for constructing the log message when
    calling ``logging.log``.

    Dictionary keys for the method outputs:

    *   ``level`` is the log level for that action, you can use those from the
        `python logging library <https://docs.python.org/3/library/logging.html>`_ :
        ``logging.DEBUG``, ``logging.INFO``, ``logging.WARNING``, ``logging.ERROR``
        and ``logging.CRITICAL``.
    *   ``msg`` should be a string that can contain different formatting placeholders.
        This string, formatted with the provided ``args``, is going to be the long message
        for that action.
    *   ``args`` should be a tuple or dict with the formatting placeholders for ``msg``.
        The final log message is computed as ``msg % args``.

    Users can define their own ``LogFormatter`` class if they want to customize how
    each action is logged or if they want to omit it entirely. In order to omit
    logging an action the method must return ``None``.

    Here is an example on how to create a custom log formatter to lower the severity level of
    the log message when an item is dropped from the pipeline::

            class PoliteLogFormatter(logformatter.LogFormatter):
                def dropped(self, item, exception, response, spider):
                    return {
                        'level': logging.INFO, # lowering the level from logging.WARNING
                        'msg': "Dropped: %(exception)s" + os.linesep + "%(item)s",
                        'args': {
                            'exception': exception,
                            'item': item,
                        }
                    }
  summary: Class for generating log messages for different actions
  signatures:
  - type: LogFormatter
  inherits_from: null
- name: LogFormatter.crawled
  kind: method
  ns: scrapy.core
  description: Logs a message when the crawler finds a webpage.
  summary: Logs a message when the crawler finds a webpage
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogFormatter.download_error
  kind: method
  ns: scrapy.core
  description: |-
    Logs a download error message from a spider (typically coming from
    the engine).

    .. versionadded:: 2.0
  summary: Logs a download error message from a spider (typically coming from
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failure
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: errmsg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: LogFormatter.dropped
  kind: method
  ns: scrapy.core
  description: Logs a message when an item is dropped while it is passing through the item pipeline.
  summary: Logs a message when an item is dropped while it is passing through the item pipeline
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - kind: positional
    name: exception
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogFormatter.from_crawler
  kind: function
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogFormatter.item_error
  kind: method
  ns: scrapy.core
  description: |-
    Logs a message when an item causes an error while it is passing
    through the item pipeline.

    .. versionadded:: 2.0
  summary: Logs a message when an item causes an error while it is passing
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - kind: positional
    name: exception
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogFormatter.scraped
  kind: method
  ns: scrapy.core
  description: Logs a message when an item is scraped by a spider.
  summary: Logs a message when an item is scraped by a spider
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogFormatter.spider_error
  kind: method
  ns: scrapy.core
  description: |-
    Logs an error message from a spider.

    .. versionadded:: 2.0
  summary: Logs an error message from a spider
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failure
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LoopingCall
  kind: class
  ns: scrapy.core
  description: |-
    Call a function repeatedly.

    If C{f} returns a deferred, rescheduling will not take place until the
    deferred has fired. The result value is ignored.

    @ivar f: The function to call.
    @ivar a: A tuple of arguments to pass the function.
    @ivar kw: A dictionary of keyword arguments to pass to the function.
    @ivar clock: A provider of
        L{twisted.internet.interfaces.IReactorTime}.  The default is
        L{twisted.internet.reactor}. Feel free to set this to
        something else, but it probably ought to be set *before*
        calling L{start}.

    @ivar running: A flag which is C{True} while C{f} is scheduled to be called
        (or is currently being called). It is set to C{True} when L{start} is
        called and set to C{False} when L{stop} is called or if C{f} raises an
        exception. In either case, it will be C{False} by the time the
        C{Deferred} returned by L{start} fires its callback or errback.

    @ivar _realLastTime: When counting skips, the time at which the skip
        counter was last invoked.

    @ivar _runAtStart: A flag indicating whether the 'now' argument was passed
        to L{LoopingCall.start}.
  summary: Call a function repeatedly
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: f
    default: null
    rest: false
  - type: LoopingCall
  inherits_from: null
- name: LoopingCall.call
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LoopingCall.deferred
  kind: property
  ns: scrapy.core
  description: |-
    DEPRECATED. L{Deferred} fired when loop stops or fails.

    Use the L{Deferred} returned by L{LoopingCall.start}.
  summary: DEPRECATED
  signatures: null
  inherits_from: null
- name: LoopingCall.interval
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LoopingCall.reset
  kind: method
  ns: scrapy.core
  description: |-
    Skip the next iteration and reset the timer.

    @since: 11.1
  summary: Skip the next iteration and reset the timer
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LoopingCall.running
  kind: property
  ns: scrapy.core
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: LoopingCall.start
  kind: method
  ns: scrapy.core
  description: |-
    Start running function every interval seconds.

    @param interval: The number of seconds between calls.  May be
    less than one.  Precision will depend on the underlying
    platform, the available hardware, and the load on the system.

    @param now: If True, run this call right now.  Otherwise, wait
    until the interval has elapsed before beginning.

    @return: A Deferred whose callback will be invoked with
    C{self} when C{self.stop} is called, or whose errback will be
    invoked when the function raises an exception or returned a
    deferred that has its errback invoked.
  summary: Start running function every interval seconds
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: interval
    default: null
    rest: false
  - kind: positional
    name: now
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: LoopingCall.starttime
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LoopingCall.stop
  kind: method
  ns: scrapy.core
  description: Stop running function.
  summary: Stop running function
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LoopingCall.withCount
  kind: function
  ns: scrapy.core
  description: |-
    An alternate constructor for L{LoopingCall} that makes available the
    number of calls which should have occurred since it was last invoked.

    Note that this number is an C{int} value; It represents the discrete
    number of calls that should have been made.  For example, if you are
    using a looping call to display an animation with discrete frames, this
    number would be the number of frames to advance.

    The count is normally 1, but can be higher. For example, if the reactor
    is blocked and takes too long to invoke the L{LoopingCall}, a Deferred
    returned from a previous call is not fired before an interval has
    elapsed, or if the callable itself blocks for longer than an interval,
    preventing I{itself} from being called.

    When running with an interval of 0, count will be always 1.

    @param countCallable: A callable that will be invoked each time the
        resulting LoopingCall is run, with an integer specifying the number
        of calls that should have been invoked.

    @return: An instance of L{LoopingCall} with call counting enabled,
        which provides the count as the first positional argument.

    @since: 9.0
  summary: An alternate constructor for L{LoopingCall} that makes available the
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: countCallable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.core
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Scraper
  kind: class
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: Scraper
  inherits_from: null
- name: Scraper.call_spider
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: result
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Scraper.close_spider
  kind: method
  ns: scrapy.core
  description: Close a spider being scraped and release its resources
  summary: Close a spider being scraped and release its resources
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Scraper.enqueue_scrape
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: result
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Scraper.handle_spider_error
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _failure
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Scraper.handle_spider_output
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: result
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Scraper.is_idle
  kind: method
  ns: scrapy.core
  description: Return True if there isn't any more spiders to process
  summary: Return True if there isn't any more spiders to process
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Scraper.open_spider
  kind: method
  ns: scrapy.core
  description: Open the given spider for scraping and allocate resources for it
  summary: Open the given spider for scraping and allocate resources for it
  signatures:
  - type: '?'
  inherits_from: null
- name: Set
  kind: callable
  ns: scrapy.core
  description: A generic version of set.
  summary: A generic version of set
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Slot
  kind: class
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: start_requests
    default: null
    rest: false
  - kind: positional
    name: close_if_idle
    default: null
    rest: false
  - kind: positional
    name: nextcall
    default: null
    rest: false
  - kind: positional
    name: scheduler
    default: null
    rest: false
  - type: Slot
  inherits_from: null
- name: Slot.add_request
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Slot.close
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Slot.remove_request
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.core
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.core
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.core
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: create_instance
  kind: function
  ns: scrapy.core
  description: |-
    Construct a class instance using its ``from_crawler`` or
    ``from_settings`` constructors, if available.

    At least one of ``settings`` and ``crawler`` needs to be different from
    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.
    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be
    tried.

    ``*args`` and ``**kwargs`` are forwarded to the constructors.

    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.

    .. versionchanged:: 2.2
       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an
       extension has not been implemented correctly).
  summary: Construct a class instance using its ``from_crawler`` or
  signatures:
  - kind: positional
    name: objcls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: failure_to_exc_info
  kind: function
  ns: scrapy.core
  description: Extract exc_info from Failure instances
  summary: Extract exc_info from Failure instances
  signatures:
  - kind: positional
    name: failure
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: global_object_name
  kind: function
  ns: scrapy.core
  description: |-
    Return full name of a global object.

    >>> from scrapy import Request
    >>> global_object_name(Request)
    'scrapy.http.request.Request'
  summary: Return full name of a global object
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: inlineCallbacks
  kind: function
  ns: scrapy.core
  description: |-
    L{inlineCallbacks} helps you write L{Deferred}-using code that looks like a
    regular sequential function. For example::

        @inlineCallbacks
        def thingummy():
            thing = yield makeSomeRequestResultingInDeferred()
            print(thing)  # the result! hoorj!

    When you call anything that results in a L{Deferred}, you can simply yield it;
    your generator will automatically be resumed when the Deferred's result is
    available. The generator will be sent the result of the L{Deferred} with the
    'send' method on generators, or if the result was a failure, 'throw'.

    Things that are not L{Deferred}s may also be yielded, and your generator
    will be resumed with the same object sent back. This means C{yield}
    performs an operation roughly equivalent to L{maybeDeferred}.

    Your inlineCallbacks-enabled generator will return a L{Deferred} object, which
    will result in the return value of the generator (or will fail with a
    failure object if your generator raises an unhandled exception). Note that
    you can't use C{return result} to return a value; use C{returnValue(result)}
    instead. Falling off the end of the generator, or simply using C{return}
    will cause the L{Deferred} to have a result of L{None}.

    Be aware that L{returnValue} will not accept a L{Deferred} as a parameter.
    If you believe the thing you'd like to return could be a L{Deferred}, do
    this::

        result = yield result
        returnValue(result)

    The L{Deferred} returned from your deferred generator may errback if your
    generator raised an exception::

        @inlineCallbacks
        def thingummy():
            thing = yield makeSomeRequestResultingInDeferred()
            if thing == 'I love Twisted':
                # will become the result of the Deferred
                returnValue('TWISTED IS GREAT!')
            else:
                # will trigger an errback
                raise Exception('DESTROY ALL LIFE')

    It is possible to use the C{return} statement instead of L{returnValue}::

        @inlineCallbacks
        def loadData(url):
            response = yield makeRequest(url)
            return json.loads(response)

    You can cancel the L{Deferred} returned from your L{inlineCallbacks}
    generator before it is fired by your generator completing (either by
    reaching its end, a C{return} statement, or by calling L{returnValue}).
    A C{CancelledError} will be raised from the C{yield}ed L{Deferred} that
    has been cancelled if that C{Deferred} does not otherwise suppress it.
  summary: L{inlineCallbacks} helps you write L{Deferred}-using code that looks like a
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: load_object
  kind: function
  ns: scrapy.core
  description: |-
    Load an object given its absolute object path, and return it.

    The object can be the import path of a class, function, variable or an
    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.

    If ``path`` is not a string, but is a callable object, such as a class or
    a function, then return it as is.
  summary: Load an object given its absolute object path, and return it
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logformatter_adapter
  kind: function
  ns: scrapy.core
  description: |-
    Helper that takes the dictionary output from the methods in LogFormatter
    and adapts it into a tuple of positional arguments for logger.log calls,
    handling backward compatibility as well.
  summary: Helper that takes the dictionary output from the methods in LogFormatter
  signatures:
  - kind: positional
    name: logkws
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: succeed
  kind: function
  ns: scrapy.core
  description: |-
    Return a L{Deferred} that has already had C{.callback(result)} called.

    This is useful when you're writing synchronous code to an
    asynchronous interface: i.e., some code is calling you expecting a
    L{Deferred} result, but you don't actually need to do anything
    asynchronous. Just return C{defer.succeed(theResult)}.

    See L{fail} for a version of this function that uses a failing
    L{Deferred} rather than a successful one.

    @param result: The result to give to the Deferred's 'callback'
           method.
  summary: Return a L{Deferred} that has already had C{
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scraper
  kind: module
  ns: scrapy.core
  description: |-
    This module implements the Scraper component which parses responses and
    extracts information from them
  summary: This module implements the Scraper component which parses responses and
  signatures: null
  inherits_from: null
- name: AsyncGenerator
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.AsyncGenerator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AsyncIterable
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.AsyncIterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deque
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.deque.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DropItem
  kind: class
  ns: scrapy.core
  description: Drop item from the item pipeline
  summary: Drop item from the item pipeline
  signatures: null
  inherits_from:
  - <class 'Exception'>
  - <class 'BaseException'>
- name: DropItem.add_note
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: DropItem.args
  kind: property
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DropItem.with_traceback
  kind: callable
  ns: scrapy.core
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemPipelineManager
  kind: class
  ns: scrapy.core
  description: Base class for implementing middleware managers
  summary: Base class for implementing middleware managers
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: ItemPipelineManager
  inherits_from:
  - <class 'scrapy.middleware.MiddlewareManager'>
- name: ItemPipelineManager.close_spider
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemPipelineManager.component_name
  kind: property
  ns: scrapy.core
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: ItemPipelineManager.from_crawler
  kind: function
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemPipelineManager.from_settings
  kind: function
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemPipelineManager.open_spider
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemPipelineManager.process_item
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.core
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: QueueTuple
  kind: callable
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Set
  kind: callable
  ns: scrapy.core
  description: A generic version of set.
  summary: A generic version of set
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Slot
  kind: class
  ns: scrapy.core
  description: Scraper slot (one per running spider)
  summary: Scraper slot (one per running spider)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: max_active_size
    default: '5000000'
    rest: false
  - type: Slot
  inherits_from: null
- name: Slot.MIN_RESPONSE_SIZE
  kind: property
  ns: scrapy.core
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: Slot.add_response_request
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: result
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Slot.finish_response
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: result
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Slot.is_idle
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Slot.needs_backout
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Slot.next_response_request_deferred
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderMiddlewareManager
  kind: class
  ns: scrapy.core
  description: Base class for implementing middleware managers
  summary: Base class for implementing middleware managers
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: SpiderMiddlewareManager
  inherits_from:
  - <class 'scrapy.middleware.MiddlewareManager'>
- name: SpiderMiddlewareManager.close_spider
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderMiddlewareManager.component_name
  kind: property
  ns: scrapy.core
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: SpiderMiddlewareManager.from_crawler
  kind: function
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderMiddlewareManager.from_settings
  kind: function
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderMiddlewareManager.open_spider
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderMiddlewareManager.process_start_requests
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: start_requests
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderMiddlewareManager.scrape_response
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: scrape_func
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.core
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.core
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.core
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: aiter_errback
  kind: function
  ns: scrapy.core
  description: |-
    Wraps an async iterable calling an errback if an error is caught while
    iterating it. Similar to scrapy.utils.defer.iter_errback()
  summary: Wraps an async iterable calling an errback if an error is caught while
  signatures:
  - kind: positional
    name: aiterable
    default: null
    rest: false
  - kind: positional
    name: errback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: annotations
  kind: const
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: defer_fail
  kind: function
  ns: scrapy.core
  description: |-
    Same as twisted.internet.defer.fail but delay calling errback until
    next reactor loop

    It delays by 100ms so reactor has a chance to go through readers and writers
    before attending pending delayed calls, so do not set delay to zero.
  summary: Same as twisted
  signatures:
  - kind: positional
    name: _failure
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: defer_succeed
  kind: function
  ns: scrapy.core
  description: |-
    Same as twisted.internet.defer.succeed but delay calling callback until
    next reactor loop

    It delays by 100ms so reactor has a chance to go through readers and writers
    before attending pending delayed calls, so do not set delay to zero.
  summary: Same as twisted
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: failure_to_exc_info
  kind: function
  ns: scrapy.core
  description: Extract exc_info from Failure instances
  summary: Extract exc_info from Failure instances
  signatures:
  - kind: positional
    name: failure
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: inlineCallbacks
  kind: function
  ns: scrapy.core
  description: |-
    L{inlineCallbacks} helps you write L{Deferred}-using code that looks like a
    regular sequential function. For example::

        @inlineCallbacks
        def thingummy():
            thing = yield makeSomeRequestResultingInDeferred()
            print(thing)  # the result! hoorj!

    When you call anything that results in a L{Deferred}, you can simply yield it;
    your generator will automatically be resumed when the Deferred's result is
    available. The generator will be sent the result of the L{Deferred} with the
    'send' method on generators, or if the result was a failure, 'throw'.

    Things that are not L{Deferred}s may also be yielded, and your generator
    will be resumed with the same object sent back. This means C{yield}
    performs an operation roughly equivalent to L{maybeDeferred}.

    Your inlineCallbacks-enabled generator will return a L{Deferred} object, which
    will result in the return value of the generator (or will fail with a
    failure object if your generator raises an unhandled exception). Note that
    you can't use C{return result} to return a value; use C{returnValue(result)}
    instead. Falling off the end of the generator, or simply using C{return}
    will cause the L{Deferred} to have a result of L{None}.

    Be aware that L{returnValue} will not accept a L{Deferred} as a parameter.
    If you believe the thing you'd like to return could be a L{Deferred}, do
    this::

        result = yield result
        returnValue(result)

    The L{Deferred} returned from your deferred generator may errback if your
    generator raised an exception::

        @inlineCallbacks
        def thingummy():
            thing = yield makeSomeRequestResultingInDeferred()
            if thing == 'I love Twisted':
                # will become the result of the Deferred
                returnValue('TWISTED IS GREAT!')
            else:
                # will trigger an errback
                raise Exception('DESTROY ALL LIFE')

    It is possible to use the C{return} statement instead of L{returnValue}::

        @inlineCallbacks
        def loadData(url):
            response = yield makeRequest(url)
            return json.loads(response)

    You can cancel the L{Deferred} returned from your L{inlineCallbacks}
    generator before it is fired by your generator completing (either by
    reaching its end, a C{return} statement, or by calling L{returnValue}).
    A C{CancelledError} will be raised from the C{yield}ed L{Deferred} that
    has been cancelled if that C{Deferred} does not otherwise suppress it.
  summary: L{inlineCallbacks} helps you write L{Deferred}-using code that looks like a
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: is_item
  kind: function
  ns: scrapy.core
  description: |-
    Return True if the given object belongs to one of the supported types, False otherwise.

    Alias for ItemAdapter.is_item
  summary: Return True if the given object belongs to one of the supported types, False otherwise
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: iter_errback
  kind: function
  ns: scrapy.core
  description: |-
    Wraps an iterable calling an errback if an error is caught while
    iterating it.
  summary: Wraps an iterable calling an errback if an error is caught while
  signatures:
  - kind: positional
    name: iterable
    default: null
    rest: false
  - kind: positional
    name: errback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: iterate_spider_output
  kind: function
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: load_object
  kind: function
  ns: scrapy.core
  description: |-
    Load an object given its absolute object path, and return it.

    The object can be the import path of a class, function, variable or an
    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.

    If ``path`` is not a string, but is a callable object, such as a class or
    a function, then return it as is.
  summary: Load an object given its absolute object path, and return it
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logformatter_adapter
  kind: function
  ns: scrapy.core
  description: |-
    Helper that takes the dictionary output from the methods in LogFormatter
    and adapts it into a tuple of positional arguments for logger.log calls,
    handling backward compatibility as well.
  summary: Helper that takes the dictionary output from the methods in LogFormatter
  signatures:
  - kind: positional
    name: logkws
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: parallel
  kind: function
  ns: scrapy.core
  description: |-
    Execute a callable over the objects in the given iterable, in parallel,
    using no more than ``count`` concurrent calls.

    Taken from: https://jcalderone.livejournal.com/24285.html
  summary: Execute a callable over the objects in the given iterable, in parallel,
  signatures:
  - kind: positional
    name: iterable
    default: null
    rest: false
  - kind: positional
    name: count
    default: null
    rest: false
  - kind: positional
    name: callable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: parallel_async
  kind: function
  ns: scrapy.core
  description: Like parallel but for async iterators
  summary: Like parallel but for async iterators
  signatures:
  - kind: positional
    name: async_iterable
    default: null
    rest: false
  - kind: positional
    name: count
    default: null
    rest: false
  - kind: positional
    name: callable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: warn_on_generator_with_return_value
  kind: function
  ns: scrapy.core
  description: |-
    Logs a warning if a callable is a generator function and includes
    a 'return' statement with a value different than None
  summary: Logs a warning if a callable is a generator function and includes
  signatures:
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: callable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: spidermw
  kind: module
  ns: scrapy.core
  description: |-
    Spider Middleware manager

    See documentation in docs/topics/spider-middleware.rst
  summary: Spider Middleware manager
  signatures: null
  inherits_from: null
- name: AsyncGenerator
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.AsyncGenerator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AsyncIterable
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.AsyncIterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.core
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.core
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.core
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableAsyncChain
  kind: class
  ns: scrapy.core
  description: Similar to MutableChain but for async iterables
  summary: Similar to MutableChain but for async iterables
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: MutableAsyncChain
  inherits_from:
  - <class 'collections.abc.AsyncIterable'>
  - <class 'typing.Generic'>
- name: MutableAsyncChain.extend
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableChain
  kind: class
  ns: scrapy.core
  description: Thin wrapper around itertools.chain, allowing to add iterables "in-place"
  summary: Thin wrapper around itertools
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: MutableChain
  inherits_from:
  - <class 'collections.abc.Iterable'>
  - <class 'typing.Generic'>
- name: MutableChain.extend
  kind: method
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.core
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapeFunc
  kind: callable
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.core
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.core
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: as_async_generator
  kind: function
  ns: scrapy.core
  description: Wraps an iterable (sync or async) into an async generator.
  summary: Wraps an iterable (sync or async) into an async generator
  signatures:
  - kind: positional
    name: it
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: build_component_list
  kind: function
  ns: scrapy.core
  description: 'Compose a component list from a { class: order } dictionary.'
  summary: 'Compose a component list from a { class: order } dictionary'
  signatures:
  - kind: positional
    name: compdict
    default: null
    rest: false
  - kind: positional
    name: custom
    default: None
    rest: false
  - kind: positional
    name: convert
    default: <function update_classpath at 0x7fa9729e1a80>
    rest: false
  - type: '?'
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.core
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: collect_asyncgen
  kind: function
  ns: scrapy.core
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: deferred_f_from_coro_f
  kind: function
  ns: scrapy.core
  description: |-
    Converts a coroutine function into a function that returns a Deferred.

    The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.
    This is useful for callback chains, as callback functions are called with the previous callback result.
  summary: Converts a coroutine function into a function that returns a Deferred
  signatures:
  - kind: positional
    name: coro_f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: deferred_from_coro
  kind: function
  ns: scrapy.core
  description: Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine
  summary: Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine
  signatures:
  - kind: positional
    name: o
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: inlineCallbacks
  kind: function
  ns: scrapy.core
  description: |-
    L{inlineCallbacks} helps you write L{Deferred}-using code that looks like a
    regular sequential function. For example::

        @inlineCallbacks
        def thingummy():
            thing = yield makeSomeRequestResultingInDeferred()
            print(thing)  # the result! hoorj!

    When you call anything that results in a L{Deferred}, you can simply yield it;
    your generator will automatically be resumed when the Deferred's result is
    available. The generator will be sent the result of the L{Deferred} with the
    'send' method on generators, or if the result was a failure, 'throw'.

    Things that are not L{Deferred}s may also be yielded, and your generator
    will be resumed with the same object sent back. This means C{yield}
    performs an operation roughly equivalent to L{maybeDeferred}.

    Your inlineCallbacks-enabled generator will return a L{Deferred} object, which
    will result in the return value of the generator (or will fail with a
    failure object if your generator raises an unhandled exception). Note that
    you can't use C{return result} to return a value; use C{returnValue(result)}
    instead. Falling off the end of the generator, or simply using C{return}
    will cause the L{Deferred} to have a result of L{None}.

    Be aware that L{returnValue} will not accept a L{Deferred} as a parameter.
    If you believe the thing you'd like to return could be a L{Deferred}, do
    this::

        result = yield result
        returnValue(result)

    The L{Deferred} returned from your deferred generator may errback if your
    generator raised an exception::

        @inlineCallbacks
        def thingummy():
            thing = yield makeSomeRequestResultingInDeferred()
            if thing == 'I love Twisted':
                # will become the result of the Deferred
                returnValue('TWISTED IS GREAT!')
            else:
                # will trigger an errback
                raise Exception('DESTROY ALL LIFE')

    It is possible to use the C{return} statement instead of L{returnValue}::

        @inlineCallbacks
        def loadData(url):
            response = yield makeRequest(url)
            return json.loads(response)

    You can cancel the L{Deferred} returned from your L{inlineCallbacks}
    generator before it is fired by your generator completing (either by
    reaching its end, a C{return} statement, or by calling L{returnValue}).
    A C{CancelledError} will be raised from the C{yield}ed L{Deferred} that
    has been cancelled if that C{Deferred} does not otherwise suppress it.
  summary: L{inlineCallbacks} helps you write L{Deferred}-using code that looks like a
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: isasyncgenfunction
  kind: function
  ns: scrapy.core
  description: |-
    Return true if the object is an asynchronous generator function.

    Asynchronous generator functions are defined with "async def"
    syntax and have "yield" expressions in their body.
  summary: Return true if the object is an asynchronous generator function
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: iscoroutine
  kind: function
  ns: scrapy.core
  description: Return true if the object is a coroutine.
  summary: Return true if the object is a coroutine
  signatures:
  - kind: positional
    name: object
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: islice
  kind: class
  ns: scrapy.core
  description: |-
    islice(iterable, stop) --> islice object
    islice(iterable, start, stop[, step]) --> islice object

    Return an iterator whose next() method returns selected values from an
    iterable.  If start is specified, will skip all preceding elements;
    otherwise, start defaults to zero.  Step defaults to one.  If
    specified as another value, step determines how many values are
    skipped between successive calls.  Works like a slice() on a list
    but returns an iterator.
  summary: islice(iterable, stop) --> islice object
  signatures: null
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.core
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: maybe_deferred_to_future
  kind: function
  ns: scrapy.core
  description: |-
    .. versionadded:: 2.6.0

    Return *d* as an object that can be awaited from a :ref:`Scrapy callable
    defined as a coroutine <coroutine-support>`.

    What you can await in Scrapy callables defined as coroutines depends on the
    value of :setting:`TWISTED_REACTOR`:

    -   When not using the asyncio reactor, you can only await on
        :class:`~twisted.internet.defer.Deferred` objects.

    -   When :ref:`using the asyncio reactor <install-asyncio>`, you can only
        await on :class:`asyncio.Future` objects.

    If you want to write code that uses ``Deferred`` objects but works with any
    reactor, use this function on all ``Deferred`` objects::

        class MySpider(Spider):
            ...
            async def parse(self, response):
                additional_request = scrapy.Request('https://example.org/price')
                deferred = self.crawler.engine.download(additional_request)
                additional_response = await maybe_deferred_to_future(deferred)
  summary: ''
  signatures:
  - kind: positional
    name: d
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: mustbe_deferred
  kind: function
  ns: scrapy.core
  description: |-
    Same as twisted.internet.defer.maybeDeferred, but delay calling
    callback/errback to next reactor loop
  summary: Same as twisted
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.crawler
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Crawler
  kind: class
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spidercls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: None
    rest: false
  - kind: positional
    name: init_reactor
    default: 'False'
    rest: false
  - type: Crawler
  inherits_from: null
- name: Crawler.crawl
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: Crawler.stop
  kind: method
  ns: scrapy.crawler
  description: |-
    Starts a graceful stop of the crawler and returns a deferred that is
    fired when the crawler is stopped.
  summary: Starts a graceful stop of the crawler and returns a deferred that is
  signatures:
  - type: '?'
  inherits_from: null
- name: CrawlerRunner
  kind: class
  ns: scrapy.crawler
  description: |-
    This is a convenient helper class that keeps track of, manages and runs
    crawlers inside an already setup :mod:`~twisted.internet.reactor`.

    The CrawlerRunner object must be instantiated with a
    :class:`~scrapy.settings.Settings` object.

    This class shouldn't be needed (since Scrapy is responsible of using it
    accordingly) unless writing scripts that manually handle the crawling
    process. See :ref:`run-from-script` for an example.
  summary: This is a convenient helper class that keeps track of, manages and runs
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: settings
    default: None
    rest: false
  - type: CrawlerRunner
  inherits_from: null
- name: CrawlerRunner.crawl
  kind: method
  ns: scrapy.crawler
  description: |-
    Run a crawler with the provided arguments.

    It will call the given Crawler's :meth:`~Crawler.crawl` method, while
    keeping track of it so it can be stopped later.

    If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`
    instance, this method will try to create one using this parameter as
    the spider class given to it.

    Returns a deferred that is fired when the crawling is finished.

    :param crawler_or_spidercls: already created crawler, or a spider class
        or spider's name inside the project to create it
    :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,
        :class:`~scrapy.spiders.Spider` subclass or string

    :param args: arguments to initialize the spider

    :param kwargs: keyword arguments to initialize the spider
  summary: Run a crawler with the provided arguments
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler_or_spidercls
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlerRunner.crawlers
  kind: property
  ns: scrapy.crawler
  description: Set of :class:`crawlers <scrapy.crawler.Crawler>` started by :meth:`crawl` and managed by this class.
  summary: Set of :class:`crawlers <scrapy
  signatures: null
  inherits_from: null
- name: CrawlerRunner.create_crawler
  kind: method
  ns: scrapy.crawler
  description: |-
    Return a :class:`~scrapy.crawler.Crawler` object.

    * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.
    * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler
      is constructed for it.
    * If ``crawler_or_spidercls`` is a string, this function finds
      a spider with this name in a Scrapy project (using spider loader),
      then creates a Crawler instance for it.
  summary: Return a :class:`~scrapy
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler_or_spidercls
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlerRunner.join
  kind: method
  ns: scrapy.crawler
  description: |-
    join()

    Returns a deferred that is fired when all managed :attr:`crawlers` have
    completed their executions.
  summary: join()
  signatures:
  - type: '?'
  inherits_from: null
- name: CrawlerRunner.stop
  kind: method
  ns: scrapy.crawler
  description: |-
    Stops simultaneously all the crawling jobs taking place.

    Returns a deferred that is fired when they all have ended.
  summary: Stops simultaneously all the crawling jobs taking place
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList
  kind: class
  ns: scrapy.crawler
  description: |-
    L{DeferredList} is a tool for collecting the results of several Deferreds.

    This tracks a list of L{Deferred}s for their results, and makes a single
    callback when they have all completed.  By default, the ultimate result is a
    list of (success, result) tuples, 'success' being a boolean.
    L{DeferredList} exposes the same API that L{Deferred} does, so callbacks and
    errbacks can be added to it in the same way.

    L{DeferredList} is implemented by adding callbacks and errbacks to each
    L{Deferred} in the list passed to it.  This means callbacks and errbacks
    added to the Deferreds before they are passed to L{DeferredList} will change
    the result that L{DeferredList} sees (i.e., L{DeferredList} is not special).
    Callbacks and errbacks can also be added to the Deferreds after they are
    passed to L{DeferredList} and L{DeferredList} may change the result that
    they see.

    See the documentation for the C{__init__} arguments for more information.

    @ivar _deferredList: The L{list} of L{Deferred}s to track.
  summary: L{DeferredList} is a tool for collecting the results of several Deferreds
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: deferredList
    default: null
    rest: false
  - kind: positional
    name: fireOnOneCallback
    default: 'False'
    rest: false
  - kind: positional
    name: fireOnOneErrback
    default: 'False'
    rest: false
  - kind: positional
    name: consumeErrors
    default: 'False'
    rest: false
  - type: DeferredList
  inherits_from:
  - <class 'twisted.internet.defer.Deferred'>
  - <class 'collections.abc.Awaitable'>
  - <class 'typing.Generic'>
- name: DeferredList.addBoth
  kind: method
  ns: scrapy.crawler
  description: |-
    Convenience method for adding a single callable as both a callback
    and an errback.

    See L{addCallbacks}.
  summary: Convenience method for adding a single callable as both a callback
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.addCallback
  kind: method
  ns: scrapy.crawler
  description: |-
    Convenience method for adding just a callback.

    See L{addCallbacks}.
  summary: Convenience method for adding just a callback
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.addCallbacks
  kind: method
  ns: scrapy.crawler
  description: |-
    Add a pair of callbacks (success and error) to this L{Deferred}.

    These will be executed when the 'master' callback is run.

    @note: The signature of this function was designed many years before
        PEP 612; ParamSpec provides no mechanism to annotate parameters
        like C{callbackArgs}; this is therefore inherently less type-safe
        than calling C{addCallback} and C{addErrback} separately.

    @return: C{self}.
  summary: Add a pair of callbacks (success and error) to this L{Deferred}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - kind: positional
    name: callbackArgs
    default: ()
    rest: false
  - kind: positional
    name: callbackKeywords
    default: '{}'
    rest: false
  - kind: positional
    name: errbackArgs
    default: ()
    rest: false
  - kind: positional
    name: errbackKeywords
    default: '{}'
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.addErrback
  kind: method
  ns: scrapy.crawler
  description: |-
    Convenience method for adding just an errback.

    See L{addCallbacks}.
  summary: Convenience method for adding just an errback
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: errback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.addTimeout
  kind: method
  ns: scrapy.crawler
  description: |-
    Time out this L{Deferred} by scheduling it to be cancelled after
    C{timeout} seconds.

    The timeout encompasses all the callbacks and errbacks added to this
    L{defer.Deferred} before the call to L{addTimeout}, and none added
    after the call.

    If this L{Deferred} gets timed out, it errbacks with a L{TimeoutError},
    unless a cancelable function was passed to its initialization or unless
    a different C{onTimeoutCancel} callable is provided.

    @param timeout: number of seconds to wait before timing out this
        L{Deferred}
    @param clock: The object which will be used to schedule the timeout.
    @param onTimeoutCancel: A callable which is called immediately after
        this L{Deferred} times out, and not if this L{Deferred} is
        otherwise cancelled before the timeout. It takes an arbitrary
        value, which is the value of this L{Deferred} at that exact point
        in time (probably a L{CancelledError} L{Failure}), and the
        C{timeout}.  The default callable (if C{None} is provided) will
        translate a L{CancelledError} L{Failure} into a L{TimeoutError}.

    @return: C{self}.

    @since: 16.5
  summary: Time out this L{Deferred} by scheduling it to be cancelled after
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: timeout
    default: null
    rest: false
  - kind: positional
    name: clock
    default: null
    rest: false
  - kind: positional
    name: onTimeoutCancel
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.asFuture
  kind: method
  ns: scrapy.crawler
  description: |-
    Adapt this L{Deferred} into a L{Future} which is bound to C{loop}.

    @note: converting a L{Deferred} to an L{Future} consumes both
        its result and its errors, so this method implicitly converts
        C{self} into a L{Deferred} firing with L{None}, regardless of what
        its result previously would have been.

    @since: Twisted 17.5.0

    @param loop: The L{asyncio} event loop to bind the L{Future} to.

    @return: A L{Future} which will fire when the L{Deferred} fires.
  summary: Adapt this L{Deferred} into a L{Future} which is bound to C{loop}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: loop
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.callback
  kind: method
  ns: scrapy.crawler
  description: |-
    Run all success callbacks that have been added to this L{Deferred}.

    Each callback will have its result passed as the first argument to
    the next; this way, the callbacks act as a 'processing chain'.  If
    the success-callback returns a L{Failure} or raises an L{Exception},
    processing will continue on the *error* callback chain.  If a
    callback (or errback) returns another L{Deferred}, this L{Deferred}
    will be chained to it (and further callbacks will not run until that
    L{Deferred} has a result).

    An instance of L{Deferred} may only have either L{callback} or
    L{errback} called on it, and only once.

    @param result: The object which will be passed to the first callback
        added to this L{Deferred} (via L{addCallback}), unless C{result} is
        a L{Failure}, in which case the behavior is the same as calling
        C{errback(result)}.

    @raise AlreadyCalledError: If L{callback} or L{errback} has already been
        called on this L{Deferred}.
  summary: Run all success callbacks that have been added to this L{Deferred}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.called
  kind: property
  ns: scrapy.crawler
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: DeferredList.cancel
  kind: method
  ns: scrapy.crawler
  description: |-
    Cancel this L{DeferredList}.

    If the L{DeferredList} hasn't fired yet, cancel every L{Deferred} in
    the list.

    If the L{DeferredList} has fired, including the case where the
    C{fireOnOneCallback}/C{fireOnOneErrback} flag is set and the
    L{DeferredList} fires because one L{Deferred} in the list fires with a
    non-failure/failure result, do nothing in the C{cancel} method.
  summary: Cancel this L{DeferredList}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.chainDeferred
  kind: method
  ns: scrapy.crawler
  description: |-
    Chain another L{Deferred} to this L{Deferred}.

    This method adds callbacks to this L{Deferred} to call C{d}'s callback
    or errback, as appropriate. It is merely a shorthand way of performing
    the following::

        d1.addCallbacks(d2.callback, d2.errback)

    When you chain a deferred C{d2} to another deferred C{d1} with
    C{d1.chainDeferred(d2)}, you are making C{d2} participate in the
    callback chain of C{d1}.
    Thus any event that fires C{d1} will also fire C{d2}.
    However, the converse is B{not} true; if C{d2} is fired, C{d1} will not
    be affected.

    Note that unlike the case where chaining is caused by a L{Deferred}
    being returned from a callback, it is possible to cause the call
    stack size limit to be exceeded by chaining many L{Deferred}s
    together with C{chainDeferred}.

    @return: C{self}.
  summary: Chain another L{Deferred} to this L{Deferred}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: d
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.debug
  kind: property
  ns: scrapy.crawler
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: DeferredList.errback
  kind: method
  ns: scrapy.crawler
  description: |-
    Run all error callbacks that have been added to this L{Deferred}.

    Each callback will have its result passed as the first
    argument to the next; this way, the callbacks act as a
    'processing chain'. Also, if the error-callback returns a non-Failure
    or doesn't raise an L{Exception}, processing will continue on the
    *success*-callback chain.

    If the argument that's passed to me is not a L{Failure} instance,
    it will be embedded in one. If no argument is passed, a
    L{Failure} instance will be created based on the current
    traceback stack.

    Passing a string as `fail' is deprecated, and will be punished with
    a warning message.

    An instance of L{Deferred} may only have either L{callback} or
    L{errback} called on it, and only once.

    @param fail: The L{Failure} object which will be passed to the first
        errback added to this L{Deferred} (via L{addErrback}).
        Alternatively, a L{Exception} instance from which a L{Failure} will
        be constructed (with no traceback) or L{None} to create a L{Failure}
        instance from the current exception state (with a traceback).

    @raise AlreadyCalledError: If L{callback} or L{errback} has already been
        called on this L{Deferred}.
    @raise NoCurrentExceptionError: If C{fail} is L{None} but there is
        no current exception state.
  summary: Run all error callbacks that have been added to this L{Deferred}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: fail
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.fireOnOneCallback
  kind: property
  ns: scrapy.crawler
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: DeferredList.fireOnOneErrback
  kind: property
  ns: scrapy.crawler
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: DeferredList.fromCoroutine
  kind: function
  ns: scrapy.crawler
  description: |-
    Schedule the execution of a coroutine that awaits on L{Deferred}s,
    wrapping it in a L{Deferred} that will fire on success/failure of the
    coroutine.

    Coroutine functions return a coroutine object, similar to how
    generators work. This function turns that coroutine into a Deferred,
    meaning that it can be used in regular Twisted code. For example::

        import treq
        from twisted.internet.defer import Deferred
        from twisted.internet.task import react

        async def crawl(pages):
            results = {}
            for page in pages:
                results[page] = await treq.content(await treq.get(page))
            return results

        def main(reactor):
            pages = [
                "http://localhost:8080"
            ]
            d = Deferred.fromCoroutine(crawl(pages))
            d.addCallback(print)
            return d

        react(main)

    @since: Twisted 21.2.0

    @param coro: The coroutine object to schedule.

    @raise ValueError: If C{coro} is not a coroutine or generator.
  summary: Schedule the execution of a coroutine that awaits on L{Deferred}s,
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: coro
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.fromFuture
  kind: function
  ns: scrapy.crawler
  description: |-
    Adapt a L{Future} to a L{Deferred}.

    @note: This creates a L{Deferred} from a L{Future}, I{not} from
        a C{coroutine}; in other words, you will need to call
        L{asyncio.ensure_future}, L{asyncio.loop.create_task} or create an
        L{asyncio.Task} yourself to get from a C{coroutine} to a
        L{Future} if what you have is an awaitable coroutine and
        not a L{Future}.  (The length of this list of techniques is
        exactly why we have left it to the caller!)

    @since: Twisted 17.5.0

    @param future: The L{Future} to adapt.

    @return: A L{Deferred} which will fire when the L{Future} fires.
  summary: Adapt a L{Future} to a L{Deferred}
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: future
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.pause
  kind: method
  ns: scrapy.crawler
  description: Stop processing on a L{Deferred} until L{unpause}() is called.
  summary: Stop processing on a L{Deferred} until L{unpause}() is called
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.paused
  kind: property
  ns: scrapy.crawler
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: DeferredList.send
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: value
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: DeferredList.unpause
  kind: method
  ns: scrapy.crawler
  description: Process all callbacks made since L{pause}() was called.
  summary: Process all callbacks made since L{pause}() was called
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.crawler
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DoesNotImplement
  kind: class
  ns: scrapy.crawler
  description: |-
    DoesNotImplement(interface[, target])

    The *target* (optional) does not implement the *interface*.

    .. versionchanged:: 5.0.0
       Add the *target* argument and attribute, and change the resulting
       string value of this object accordingly.
  summary: DoesNotImplement(interface[, target])
  signatures: null
  inherits_from:
  - <class 'zope.interface.exceptions._TargetInvalid'>
  - <class 'zope.interface.exceptions.Invalid'>
  - <class 'Exception'>
  - <class 'BaseException'>
- name: DoesNotImplement.add_note
  kind: callable
  ns: scrapy.crawler
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: DoesNotImplement.args
  kind: property
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DoesNotImplement.interface
  kind: property
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DoesNotImplement.target
  kind: property
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DoesNotImplement.with_traceback
  kind: callable
  ns: scrapy.crawler
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: ExtensionManager
  kind: class
  ns: scrapy.crawler
  description: Base class for implementing middleware managers
  summary: Base class for implementing middleware managers
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: ExtensionManager
  inherits_from:
  - <class 'scrapy.middleware.MiddlewareManager'>
- name: ExtensionManager.close_spider
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ExtensionManager.component_name
  kind: property
  ns: scrapy.crawler
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: ExtensionManager.from_crawler
  kind: function
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ExtensionManager.from_settings
  kind: function
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ExtensionManager.open_spider
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.crawler
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ISpiderLoader
  kind: callable
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler
  kind: class
  ns: scrapy.crawler
  description: Record log levels count into a crawler stats
  summary: Record log levels count into a crawler stats
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: LogCounterHandler
  inherits_from:
  - <class 'logging.Handler'>
  - <class 'logging.Filterer'>
- name: LogCounterHandler.acquire
  kind: method
  ns: scrapy.crawler
  description: Acquire the I/O thread lock.
  summary: Acquire the I/O thread lock
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.addFilter
  kind: method
  ns: scrapy.crawler
  description: Add the specified filter to this handler.
  summary: Add the specified filter to this handler
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: filter
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.close
  kind: method
  ns: scrapy.crawler
  description: |-
    Tidy up any resources used by the handler.

    This version removes the handler from an internal map of handlers,
    _handlers, which is used for handler lookup by name. Subclasses
    should ensure that this gets called from overridden close()
    methods.
  summary: Tidy up any resources used by the handler
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.createLock
  kind: method
  ns: scrapy.crawler
  description: Acquire a thread lock for serializing access to the underlying I/O.
  summary: Acquire a thread lock for serializing access to the underlying I/O
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.emit
  kind: method
  ns: scrapy.crawler
  description: |-
    Do whatever it takes to actually log the specified logging record.

    This version is intended to be implemented by subclasses and so
    raises a NotImplementedError.
  summary: Do whatever it takes to actually log the specified logging record
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: record
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.filter
  kind: method
  ns: scrapy.crawler
  description: |-
    Determine if a record is loggable by consulting all the filters.

    The default is to allow the record to be logged; any filter can veto
    this and the record is then dropped. Returns a zero value if a record
    is to be dropped, else non-zero.

    .. versionchanged:: 3.2

       Allow filters to be just callables.
  summary: Determine if a record is loggable by consulting all the filters
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: record
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.flush
  kind: method
  ns: scrapy.crawler
  description: |-
    Ensure all logging output has been flushed.

    This version does nothing and is intended to be implemented by
    subclasses.
  summary: Ensure all logging output has been flushed
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.format
  kind: method
  ns: scrapy.crawler
  description: |-
    Format the specified record.

    If a formatter is set, use it. Otherwise, use the default formatter
    for the module.
  summary: Format the specified record
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: record
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.get_name
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.handle
  kind: method
  ns: scrapy.crawler
  description: |-
    Conditionally emit the specified logging record.

    Emission depends on filters which may have been added to the handler.
    Wrap the actual emission of the record with acquisition/release of
    the I/O thread lock. Returns whether the filter passed the record for
    emission.
  summary: Conditionally emit the specified logging record
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: record
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.handleError
  kind: method
  ns: scrapy.crawler
  description: |-
    Handle errors which occur during an emit() call.

    This method should be called from handlers when an exception is
    encountered during an emit() call. If raiseExceptions is false,
    exceptions get silently ignored. This is what is mostly wanted
    for a logging system - most users will not care about errors in
    the logging system, they are more interested in application errors.
    You could, however, replace this with a custom handler if you wish.
    The record which was being processed is passed in to this method.
  summary: Handle errors which occur during an emit() call
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: record
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.name
  kind: property
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LogCounterHandler.release
  kind: method
  ns: scrapy.crawler
  description: Release the I/O thread lock.
  summary: Release the I/O thread lock
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.removeFilter
  kind: method
  ns: scrapy.crawler
  description: Remove the specified filter from this handler.
  summary: Remove the specified filter from this handler
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: filter
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.setFormatter
  kind: method
  ns: scrapy.crawler
  description: Set the formatter for this handler.
  summary: Set the formatter for this handler
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: fmt
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.setLevel
  kind: method
  ns: scrapy.crawler
  description: Set the logging level of this handler.  level must be an int or a str.
  summary: Set the logging level of this handler
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: level
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LogCounterHandler.set_name
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleInvalid
  kind: class
  ns: scrapy.crawler
  description: |-
    The *target* has failed to implement the *interface* in
    multiple ways.

    The failures are described by *exceptions*, a collection of
    other `Invalid` instances.

    .. versionadded:: 5.0
  summary: The *target* has failed to implement the *interface* in
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: interface
    default: null
    rest: false
  - kind: positional
    name: target
    default: null
    rest: false
  - kind: positional
    name: exceptions
    default: null
    rest: false
  - type: MultipleInvalid
  inherits_from:
  - <class 'zope.interface.exceptions._TargetInvalid'>
  - <class 'zope.interface.exceptions.Invalid'>
  - <class 'Exception'>
  - <class 'BaseException'>
- name: MultipleInvalid.add_note
  kind: callable
  ns: scrapy.crawler
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: MultipleInvalid.args
  kind: property
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MultipleInvalid.exceptions
  kind: property
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MultipleInvalid.interface
  kind: property
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MultipleInvalid.target
  kind: property
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MultipleInvalid.with_traceback
  kind: callable
  ns: scrapy.crawler
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.crawler
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyDeprecationWarning
  kind: class
  ns: scrapy.crawler
  description: |-
    Warning category for deprecated features, since the default
    DeprecationWarning is silenced on Python 2.7+
  summary: Warning category for deprecated features, since the default
  signatures: null
  inherits_from:
  - <class 'Warning'>
  - <class 'Exception'>
  - <class 'BaseException'>
- name: ScrapyDeprecationWarning.add_note
  kind: callable
  ns: scrapy.crawler
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: ScrapyDeprecationWarning.args
  kind: property
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ScrapyDeprecationWarning.with_traceback
  kind: callable
  ns: scrapy.crawler
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: Set
  kind: callable
  ns: scrapy.crawler
  description: A generic version of set.
  summary: A generic version of set
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StatsCollector
  kind: class
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: StatsCollector
  inherits_from: null
- name: StatsCollector.clear_stats
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: StatsCollector.close_spider
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StatsCollector.get_stats
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: StatsCollector.get_value
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: StatsCollector.inc_value
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: count
    default: '1'
    rest: false
  - kind: positional
    name: start
    default: '0'
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: StatsCollector.max_value
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: StatsCollector.min_value
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: StatsCollector.open_spider
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StatsCollector.set_stats
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: stats
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: StatsCollector.set_value
  kind: method
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.crawler
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.crawler
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: annotations
  kind: const
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.crawler
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: configure_logging
  kind: function
  ns: scrapy.crawler
  description: |-
    Initialize logging defaults for Scrapy.

    :param settings: settings used to create and configure a handler for the
        root logger (default: None).
    :type settings: dict, :class:`~scrapy.settings.Settings` object or ``None``

    :param install_root_handler: whether to install root logging handler
        (default: True)
    :type install_root_handler: bool

    This function does:

    - Route warnings and twisted logging through Python standard logging
    - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively
    - Route stdout to log if LOG_STDOUT setting is True

    When ``install_root_handler`` is True (default), this function also
    creates a handler for the root logger according to given settings
    (see :ref:`topics-logging-settings`). You can override default options
    using ``settings`` argument. When ``settings`` is empty or None, defaults
    are used.
  summary: Initialize logging defaults for Scrapy
  signatures:
  - kind: positional
    name: settings
    default: None
    rest: false
  - kind: positional
    name: install_root_handler
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: create_instance
  kind: function
  ns: scrapy.crawler
  description: |-
    Construct a class instance using its ``from_crawler`` or
    ``from_settings`` constructors, if available.

    At least one of ``settings`` and ``crawler`` needs to be different from
    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.
    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be
    tried.

    ``*args`` and ``**kwargs`` are forwarded to the constructors.

    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.

    .. versionchanged:: 2.2
       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an
       extension has not been implemented correctly).
  summary: Construct a class instance using its ``from_crawler`` or
  signatures:
  - kind: positional
    name: objcls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: get_scrapy_root_handler
  kind: function
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: inlineCallbacks
  kind: function
  ns: scrapy.crawler
  description: |-
    L{inlineCallbacks} helps you write L{Deferred}-using code that looks like a
    regular sequential function. For example::

        @inlineCallbacks
        def thingummy():
            thing = yield makeSomeRequestResultingInDeferred()
            print(thing)  # the result! hoorj!

    When you call anything that results in a L{Deferred}, you can simply yield it;
    your generator will automatically be resumed when the Deferred's result is
    available. The generator will be sent the result of the L{Deferred} with the
    'send' method on generators, or if the result was a failure, 'throw'.

    Things that are not L{Deferred}s may also be yielded, and your generator
    will be resumed with the same object sent back. This means C{yield}
    performs an operation roughly equivalent to L{maybeDeferred}.

    Your inlineCallbacks-enabled generator will return a L{Deferred} object, which
    will result in the return value of the generator (or will fail with a
    failure object if your generator raises an unhandled exception). Note that
    you can't use C{return result} to return a value; use C{returnValue(result)}
    instead. Falling off the end of the generator, or simply using C{return}
    will cause the L{Deferred} to have a result of L{None}.

    Be aware that L{returnValue} will not accept a L{Deferred} as a parameter.
    If you believe the thing you'd like to return could be a L{Deferred}, do
    this::

        result = yield result
        returnValue(result)

    The L{Deferred} returned from your deferred generator may errback if your
    generator raised an exception::

        @inlineCallbacks
        def thingummy():
            thing = yield makeSomeRequestResultingInDeferred()
            if thing == 'I love Twisted':
                # will become the result of the Deferred
                returnValue('TWISTED IS GREAT!')
            else:
                # will trigger an errback
                raise Exception('DESTROY ALL LIFE')

    It is possible to use the C{return} statement instead of L{returnValue}::

        @inlineCallbacks
        def loadData(url):
            response = yield makeRequest(url)
            return json.loads(response)

    You can cancel the L{Deferred} returned from your L{inlineCallbacks}
    generator before it is fired by your generator completing (either by
    reaching its end, a C{return} statement, or by calling L{returnValue}).
    A C{CancelledError} will be raised from the C{yield}ed L{Deferred} that
    has been cancelled if that C{Deferred} does not otherwise suppress it.
  summary: L{inlineCallbacks} helps you write L{Deferred}-using code that looks like a
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: install_reactor
  kind: function
  ns: scrapy.crawler
  description: |-
    Installs the :mod:`~twisted.internet.reactor` with the specified
    import path. Also installs the asyncio event loop with the specified import
    path if the asyncio reactor is enabled
  summary: Installs the :mod:`~twisted
  signatures:
  - kind: positional
    name: reactor_path
    default: null
    rest: false
  - kind: positional
    name: event_loop_path
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: install_scrapy_root_handler
  kind: function
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: install_shutdown_handlers
  kind: function
  ns: scrapy.crawler
  description: |-
    Install the given function as a signal handler for all common shutdown
    signals (such as SIGINT, SIGTERM, etc). If ``override_sigint`` is ``False`` the
    SIGINT handler won't be installed if there is already a handler in place
    (e.g. Pdb)
  summary: Install the given function as a signal handler for all common shutdown
  signatures:
  - kind: positional
    name: function
    default: null
    rest: false
  - kind: positional
    name: override_sigint
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: is_asyncio_reactor_installed
  kind: function
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: load_object
  kind: function
  ns: scrapy.crawler
  description: |-
    Load an object given its absolute object path, and return it.

    The object can be the import path of a class, function, variable or an
    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.

    If ``path`` is not a string, but is a callable object, such as a class or
    a function, then return it as is.
  summary: Load an object given its absolute object path, and return it
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: log_reactor_info
  kind: function
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: log_scrapy_info
  kind: function
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: maybeDeferred
  kind: function
  ns: scrapy.crawler
  description: |-
    Invoke a function that may or may not return a L{Deferred} or coroutine.

    Call the given function with the given arguments.  Then:

      - If the returned object is a L{Deferred}, return it.

      - If the returned object is a L{Failure}, wrap it with L{fail} and
        return it.

      - If the returned object is a L{types.CoroutineType}, wrap it with
        L{Deferred.fromCoroutine} and return it.

      - Otherwise, wrap it in L{succeed} and return it.

      - If an exception is raised, convert it to a L{Failure}, wrap it in
        L{fail}, and then return it.

    @param f: The callable to invoke
    @param args: The arguments to pass to C{f}
    @param kwargs: The keyword arguments to pass to C{f}

    @return: The result of the function call, wrapped in a L{Deferred} if
    necessary.
  summary: Invoke a function that may or may not return a L{Deferred} or coroutine
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: overridden_settings
  kind: function
  ns: scrapy.crawler
  description: Return an iterable of the settings that have been overridden
  summary: Return an iterable of the settings that have been overridden
  signatures:
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: signal_names
  kind: const
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: verifyClass
  kind: function
  ns: scrapy.crawler
  description: Verify that the *candidate* might correctly provide *iface*.
  summary: Verify that the *candidate* might correctly provide *iface*
  signatures:
  - kind: positional
    name: iface
    default: null
    rest: false
  - kind: positional
    name: candidate
    default: null
    rest: false
  - kind: positional
    name: tentative
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: verify_installed_asyncio_event_loop
  kind: function
  ns: scrapy.crawler
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: loop_path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: verify_installed_reactor
  kind: function
  ns: scrapy.crawler
  description: |-
    Raises :exc:`Exception` if the installed
    :mod:`~twisted.internet.reactor` does not match the specified import
    path.
  summary: Raises :exc:`Exception` if the installed
  signatures:
  - kind: positional
    name: reactor_path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.downloadermiddlewares
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: scrapy.dupefilters
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: BaseDupeFilter
  kind: class
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - type: BaseDupeFilter
  inherits_from: null
- name: BaseDupeFilter.close
  kind: method
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseDupeFilter.from_settings
  kind: function
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseDupeFilter.log
  kind: method
  ns: scrapy.dupefilters
  description: Log that a request has been filtered
  summary: Log that a request has been filtered
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseDupeFilter.open
  kind: method
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseDupeFilter.request_seen
  kind: method
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.dupefilters
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RFPDupeFilter
  kind: class
  ns: scrapy.dupefilters
  description: Request Fingerprint duplicates filter
  summary: Request Fingerprint duplicates filter
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: None
    rest: false
  - kind: positional
    name: debug
    default: 'False'
    rest: false
  - name: fingerprinter
    default: None
    rest: false
    kind: kw-only
  - type: RFPDupeFilter
  inherits_from:
  - <class 'scrapy.dupefilters.BaseDupeFilter'>
- name: RFPDupeFilter.close
  kind: method
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RFPDupeFilter.from_crawler
  kind: function
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RFPDupeFilter.from_settings
  kind: function
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - name: fingerprinter
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: RFPDupeFilter.log
  kind: method
  ns: scrapy.dupefilters
  description: Log that a request has been filtered
  summary: Log that a request has been filtered
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RFPDupeFilter.open
  kind: method
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RFPDupeFilter.request_fingerprint
  kind: method
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RFPDupeFilter.request_seen
  kind: method
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RequestFingerprinter
  kind: class
  ns: scrapy.dupefilters
  description: |-
    Default fingerprinter.

    It takes into account a canonical version
    (:func:`w3lib.url.canonicalize_url`) of :attr:`request.url
    <scrapy.http.Request.url>` and the values of :attr:`request.method
    <scrapy.http.Request.method>` and :attr:`request.body
    <scrapy.http.Request.body>`. It then generates an `SHA1
    <https://en.wikipedia.org/wiki/SHA-1>`_ hash.

    .. seealso:: :setting:`REQUEST_FINGERPRINTER_IMPLEMENTATION`.
  summary: Default fingerprinter
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: None
    rest: false
  - type: RequestFingerprinter
  inherits_from: null
- name: RequestFingerprinter.fingerprint
  kind: method
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RequestFingerprinter.from_crawler
  kind: function
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RequestFingerprinterProtocol
  kind: class
  ns: scrapy.dupefilters
  description: |-
    Base class for protocol classes.

    Protocol classes are defined as::

        class Proto(Protocol):
            def meth(self) -> int:
                ...

    Such classes are primarily used with static type checkers that recognize
    structural subtyping (static duck-typing).

    For example::

        class C:
            def meth(self) -> int:
                return 0

        def func(x: Proto) -> int:
            return x.meth()

        func(C())  # Passes static type check

    See PEP 544 for details. Protocol classes decorated with
    @typing.runtime_checkable act as simple-minded runtime protocols that check
    only the presence of given attributes, ignoring their type signatures.
    Protocol classes can be generic, they are defined as::

        class GenProto(Protocol[T]):
            def meth(self) -> T:
                ...
  summary: Base class for protocol classes
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: RequestFingerprinterProtocol
  inherits_from:
  - <class 'typing.Protocol'>
  - <class 'typing.Generic'>
- name: RequestFingerprinterProtocol.fingerprint
  kind: method
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Set
  kind: callable
  ns: scrapy.dupefilters
  description: A generic version of set.
  summary: A generic version of set
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: annotations
  kind: const
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: job_dir
  kind: function
  ns: scrapy.dupefilters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: referer_str
  kind: function
  ns: scrapy.dupefilters
  description: Return Referer HTTP header suitable for logging.
  summary: Return Referer HTTP header suitable for logging
  signatures:
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.exceptions
  kind: module
  ns: null
  description: |-
    Scrapy core exceptions

    These exceptions are documented in docs/topics/exceptions.rst. Please don't add
    new exceptions here without documenting them there.
  summary: Scrapy core exceptions
  signatures: null
  inherits_from: null
- name: ContractFail
  kind: class
  ns: scrapy.exceptions
  description: Error raised in case of a failing contract
  summary: Error raised in case of a failing contract
  signatures: null
  inherits_from:
  - <class 'AssertionError'>
  - <class 'Exception'>
  - <class 'BaseException'>
- name: ContractFail.add_note
  kind: callable
  ns: scrapy.exceptions
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: ContractFail.args
  kind: property
  ns: scrapy.exceptions
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ContractFail.with_traceback
  kind: callable
  ns: scrapy.exceptions
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: StopDownload
  kind: class
  ns: scrapy.exceptions
  description: |-
    Stop the download of the body for a given response.
    The 'fail' boolean parameter indicates whether or not the resulting partial response
    should be handled by the request errback. Note that 'fail' is a keyword-only argument.
  summary: Stop the download of the body for a given response
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: fail
    default: 'True'
    rest: false
    kind: kw-only
  - type: StopDownload
  inherits_from:
  - <class 'Exception'>
  - <class 'BaseException'>
- name: StopDownload.add_note
  kind: callable
  ns: scrapy.exceptions
  description: |-
    Exception.add_note(note) --
    add a note to the exception
  summary: Exception
  signatures: null
  inherits_from: null
- name: StopDownload.args
  kind: property
  ns: scrapy.exceptions
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: StopDownload.with_traceback
  kind: callable
  ns: scrapy.exceptions
  description: |-
    Exception.with_traceback(tb) --
    set self.__traceback__ to tb and return self.
  summary: Exception
  signatures: null
  inherits_from: null
- name: scrapy.exporters
  kind: module
  ns: null
  description: Item Exporters are used to export/serialize items into different formats.
  summary: Item Exporters are used to export/serialize items into different formats
  signatures: null
  inherits_from: null
- name: BaseItemExporter
  kind: class
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: dont_fail
    default: 'False'
    rest: false
    kind: kw-only
  - type: BaseItemExporter
  inherits_from: null
- name: BaseItemExporter.export_item
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseItemExporter.finish_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseItemExporter.serialize_field
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BaseItemExporter.start_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PprintItemExporter
  kind: class
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: null
    rest: false
  - type: PprintItemExporter
  inherits_from:
  - <class 'scrapy.exporters.BaseItemExporter'>
- name: PprintItemExporter.export_item
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PprintItemExporter.finish_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PprintItemExporter.serialize_field
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PprintItemExporter.start_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleItemExporter
  kind: class
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: null
    rest: false
  - kind: positional
    name: protocol
    default: '4'
    rest: false
  - type: PickleItemExporter
  inherits_from:
  - <class 'scrapy.exporters.BaseItemExporter'>
- name: PickleItemExporter.export_item
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleItemExporter.finish_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleItemExporter.serialize_field
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleItemExporter.start_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CsvItemExporter
  kind: class
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: null
    rest: false
  - kind: positional
    name: include_headers_line
    default: 'True'
    rest: false
  - kind: positional
    name: join_multivalued
    default: ','
    rest: false
  - kind: positional
    name: errors
    default: None
    rest: false
  - type: CsvItemExporter
  inherits_from:
  - <class 'scrapy.exporters.BaseItemExporter'>
- name: CsvItemExporter.export_item
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CsvItemExporter.finish_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CsvItemExporter.serialize_field
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CsvItemExporter.start_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlItemExporter
  kind: class
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: null
    rest: false
  - type: XmlItemExporter
  inherits_from:
  - <class 'scrapy.exporters.BaseItemExporter'>
- name: XmlItemExporter.export_item
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlItemExporter.finish_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlItemExporter.serialize_field
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlItemExporter.start_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonLinesItemExporter
  kind: class
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: null
    rest: false
  - type: JsonLinesItemExporter
  inherits_from:
  - <class 'scrapy.exporters.BaseItemExporter'>
- name: JsonLinesItemExporter.export_item
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonLinesItemExporter.finish_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonLinesItemExporter.serialize_field
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonLinesItemExporter.start_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonItemExporter
  kind: class
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: null
    rest: false
  - type: JsonItemExporter
  inherits_from:
  - <class 'scrapy.exporters.BaseItemExporter'>
- name: JsonItemExporter.export_item
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonItemExporter.finish_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonItemExporter.serialize_field
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonItemExporter.start_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalItemExporter
  kind: class
  ns: scrapy.exporters
  description: |-
    Exports items in a Python-specific binary format (see
    :mod:`marshal`).

    :param file: The file-like object to use for exporting the data. Its
                 ``write`` method should accept :class:`bytes` (a disk file
                 opened in binary mode, a :class:`~io.BytesIO` object, etc)
  summary: Exports items in a Python-specific binary format (see
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: null
    rest: false
  - type: MarshalItemExporter
  inherits_from:
  - <class 'scrapy.exporters.BaseItemExporter'>
- name: MarshalItemExporter.export_item
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalItemExporter.finish_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalItemExporter.serialize_field
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalItemExporter.start_exporting
  kind: method
  ns: scrapy.exporters
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.extension
  kind: module
  ns: null
  description: |-
    The Extension Manager

    See documentation in docs/topics/extensions.rst
  summary: The Extension Manager
  signatures: null
  inherits_from: null
- name: build_component_list
  kind: function
  ns: scrapy.extension
  description: 'Compose a component list from a { class: order } dictionary.'
  summary: 'Compose a component list from a { class: order } dictionary'
  signatures:
  - kind: positional
    name: compdict
    default: null
    rest: false
  - kind: positional
    name: custom
    default: None
    rest: false
  - kind: positional
    name: convert
    default: <function update_classpath at 0x7fa9729e1a80>
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.extensions
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: scrapy.http
  kind: module
  ns: null
  description: |-
    Module containing all HTTP related classes

    Use this module (instead of the more specific ones) when importing Headers,
    Request and Response outside this module.
  summary: Module containing all HTTP related classes
  signatures: null
  inherits_from: null
- name: Headers
  kind: class
  ns: scrapy.http
  description: Case insensitive http headers dictionary
  summary: Case insensitive http headers dictionary
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: seq
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: utf-8
    rest: false
  - type: Headers
  inherits_from:
  - <class 'scrapy.utils.datatypes.CaselessDict'>
  - <class 'dict'>
- name: Headers.appendlist
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.clear
  kind: callable
  ns: scrapy.http
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures: null
  inherits_from: null
- name: Headers.copy
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.fromkeys
  kind: function
  ns: scrapy.http
  description: Create a new dictionary with keys from iterable and values set to value.
  summary: Create a new dictionary with keys from iterable and values set to value
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: keys
    default: null
    rest: false
  - kind: positional
    name: value
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.get
  kind: method
  ns: scrapy.http
  description: Return the value for key if key is in the dictionary, else default.
  summary: Return the value for key if key is in the dictionary, else default
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: def_val
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.getlist
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: def_val
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.has_key
  kind: method
  ns: scrapy.http
  description: True if the dictionary has the specified key, else False.
  summary: True if the dictionary has the specified key, else False
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.items
  kind: method
  ns: scrapy.http
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.keys
  kind: callable
  ns: scrapy.http
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures: null
  inherits_from: null
- name: Headers.normkey
  kind: method
  ns: scrapy.http
  description: Normalize key to bytes
  summary: Normalize key to bytes
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.normvalue
  kind: method
  ns: scrapy.http
  description: Normalize values to bytes
  summary: Normalize values to bytes
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.pop
  kind: method
  ns: scrapy.http
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.

    If the key is not found, return the default if given; otherwise,
    raise a KeyError.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.popitem
  kind: callable
  ns: scrapy.http
  description: |-
    Remove and return a (key, value) pair as a 2-tuple.

    Pairs are returned in LIFO (last-in, first-out) order.
    Raises KeyError if the dict is empty.
  summary: Remove and return a (key, value) pair as a 2-tuple
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.setdefault
  kind: method
  ns: scrapy.http
  description: |-
    Insert key with a value of default if key is not in the dictionary.

    Return the value for key if key is in the dictionary, else default.
  summary: Insert key with a value of default if key is not in the dictionary
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: def_val
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.setlist
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: list_
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.setlistdefault
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default_list
    default: ()
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.to_string
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.to_unicode_dict
  kind: method
  ns: scrapy.http
  description: |-
    Return headers as a CaselessDict with unicode keys
    and unicode values. Multiple values are joined with ','.
  summary: Return headers as a CaselessDict with unicode keys
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.update
  kind: method
  ns: scrapy.http
  description: |-
    D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.
    If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
    If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
    In either case, this is followed by: for k in F:  D[k] = F[k]
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: seq
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Headers.values
  kind: method
  ns: scrapy.http
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HtmlResponse
  kind: class
  ns: scrapy.http
  description: |-
    An object that represents an HTTP response, which is usually
    downloaded (by the Downloader) and fed to the Spiders for processing.
  summary: An object that represents an HTTP response, which is usually
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: HtmlResponse
  inherits_from:
  - <class 'scrapy.http.response.text.TextResponse'>
  - <class 'scrapy.http.response.Response'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: HtmlResponse.attributes
  kind: property
  ns: scrapy.http
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: HtmlResponse.body
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HtmlResponse.cb_kwargs
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HtmlResponse.copy
  kind: method
  ns: scrapy.http
  description: Return a copy of this Response
  summary: Return a copy of this Response
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HtmlResponse.css
  kind: method
  ns: scrapy.http
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HtmlResponse.encoding
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HtmlResponse.follow
  kind: method
  ns: scrapy.http
  description: |-
    Return a :class:`~.Request` instance to follow a link ``url``.
    It accepts the same arguments as ``Request.__init__`` method,
    but ``url`` can be not only an absolute URL, but also

    * a relative URL
    * a :class:`~scrapy.link.Link` object, e.g. the result of
      :ref:`topics-link-extractors`
    * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.
      ``response.css('a.my_link')[0]``
    * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.
      ``response.css('a::attr(href)')[0]`` or
      ``response.xpath('//img/@src')[0]``

    See :ref:`response-follow-example` for usage examples.
  summary: Return a :class:`~
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: callback
    default: None
    rest: false
  - kind: positional
    name: method
    default: GET
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: body
    default: None
    rest: false
  - kind: positional
    name: cookies
    default: None
    rest: false
  - kind: positional
    name: meta
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: priority
    default: '0'
    rest: false
  - kind: positional
    name: dont_filter
    default: 'False'
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - kind: positional
    name: cb_kwargs
    default: None
    rest: false
  - kind: positional
    name: flags
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: HtmlResponse.follow_all
  kind: method
  ns: scrapy.http
  description: |-
    A generator that produces :class:`~.Request` instances to follow all
    links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s
    ``__init__`` method, except that each ``urls`` element does not need to be
    an absolute URL, it can be any of the following:

    * a relative URL
    * a :class:`~scrapy.link.Link` object, e.g. the result of
      :ref:`topics-link-extractors`
    * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.
      ``response.css('a.my_link')[0]``
    * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.
      ``response.css('a::attr(href)')[0]`` or
      ``response.xpath('//img/@src')[0]``

    In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction
    within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).

    Note that when passing a ``SelectorList`` as argument for the ``urls`` parameter or
    using the ``css`` or ``xpath`` parameters, this method will not produce requests for
    selectors from which links cannot be obtained (for instance, anchor tags without an
    ``href`` attribute)
  summary: A generator that produces :class:`~
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: urls
    default: None
    rest: false
  - kind: positional
    name: callback
    default: None
    rest: false
  - kind: positional
    name: method
    default: GET
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: body
    default: None
    rest: false
  - kind: positional
    name: cookies
    default: None
    rest: false
  - kind: positional
    name: meta
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: priority
    default: '0'
    rest: false
  - kind: positional
    name: dont_filter
    default: 'False'
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - kind: positional
    name: cb_kwargs
    default: None
    rest: false
  - kind: positional
    name: flags
    default: None
    rest: false
  - kind: positional
    name: css
    default: None
    rest: false
  - kind: positional
    name: xpath
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: HtmlResponse.jmespath
  kind: method
  ns: scrapy.http
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HtmlResponse.json
  kind: method
  ns: scrapy.http
  description: |-
    .. versionadded:: 2.2

    Deserialize a JSON document to a Python object.
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HtmlResponse.meta
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HtmlResponse.replace
  kind: method
  ns: scrapy.http
  description: Create a new Response with the same attributes except for those given new values
  summary: Create a new Response with the same attributes except for those given new values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HtmlResponse.selector
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HtmlResponse.text
  kind: property
  ns: scrapy.http
  description: Body as unicode
  summary: Body as unicode
  signatures: null
  inherits_from: null
- name: HtmlResponse.url
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HtmlResponse.urljoin
  kind: method
  ns: scrapy.http
  description: |-
    Join this Response's url with a possible relative url to form an
    absolute interpretation of the latter.
  summary: Join this Response's url with a possible relative url to form an
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HtmlResponse.xpath
  kind: method
  ns: scrapy.http
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonRequest
  kind: class
  ns: scrapy.http
  description: |-
    Represents an HTTP request, which is usually generated in a Spider and
    executed by the Downloader, thus generating a :class:`Response`.
  summary: Represents an HTTP request, which is usually generated in a Spider and
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: dumps_kwargs
    default: None
    rest: false
    kind: kw-only
  - type: JsonRequest
  inherits_from:
  - <class 'scrapy.http.request.Request'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: JsonRequest.attributes
  kind: property
  ns: scrapy.http
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: JsonRequest.body
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: JsonRequest.cb_kwargs
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: JsonRequest.copy
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonRequest.dumps_kwargs
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: JsonRequest.encoding
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: JsonRequest.from_curl
  kind: function
  ns: scrapy.http
  description: |-
    Create a Request object from a string containing a `cURL
    <https://curl.haxx.se/>`_ command. It populates the HTTP method, the
    URL, the headers, the cookies and the body. It accepts the same
    arguments as the :class:`Request` class, taking preference and
    overriding the values of the same arguments contained in the cURL
    command.

    Unrecognized options are ignored by default. To raise an error when
    finding unknown options call this method by passing
    ``ignore_unknown_options=False``.

    .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`
                 subclasses, such as :class:`~scrapy.http.JSONRequest`, or
                 :class:`~scrapy.http.XmlRpcRequest`, as well as having
                 :ref:`downloader middlewares <topics-downloader-middleware>`
                 and
                 :ref:`spider middlewares <topics-spider-middleware>`
                 enabled, such as
                 :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,
                 :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,
                 or
                 :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,
                 may modify the :class:`~scrapy.http.Request` object.

    To translate a cURL command into a Scrapy request,
    you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.
  summary: Create a Request object from a string containing a `cURL
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: curl_command
    default: null
    rest: false
  - kind: positional
    name: ignore_unknown_options
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonRequest.meta
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: JsonRequest.replace
  kind: method
  ns: scrapy.http
  description: Create a new Request with the same attributes except for those given new values
  summary: Create a new Request with the same attributes except for those given new values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JsonRequest.to_dict
  kind: method
  ns: scrapy.http
  description: |-
    Return a dictionary containing the Request's data.

    Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.

    If a spider is given, this method will try to find out the name of the spider methods used as callback
    and errback and include them in the output dict, raising an exception if they cannot be found.
  summary: Return a dictionary containing the Request's data
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: spider
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: JsonRequest.url
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TextResponse
  kind: class
  ns: scrapy.http
  description: |-
    An object that represents an HTTP response, which is usually
    downloaded (by the Downloader) and fed to the Spiders for processing.
  summary: An object that represents an HTTP response, which is usually
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: TextResponse
  inherits_from:
  - <class 'scrapy.http.response.Response'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: TextResponse.attributes
  kind: property
  ns: scrapy.http
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: TextResponse.body
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TextResponse.cb_kwargs
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TextResponse.copy
  kind: method
  ns: scrapy.http
  description: Return a copy of this Response
  summary: Return a copy of this Response
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextResponse.css
  kind: method
  ns: scrapy.http
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextResponse.encoding
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TextResponse.follow
  kind: method
  ns: scrapy.http
  description: |-
    Return a :class:`~.Request` instance to follow a link ``url``.
    It accepts the same arguments as ``Request.__init__`` method,
    but ``url`` can be not only an absolute URL, but also

    * a relative URL
    * a :class:`~scrapy.link.Link` object, e.g. the result of
      :ref:`topics-link-extractors`
    * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.
      ``response.css('a.my_link')[0]``
    * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.
      ``response.css('a::attr(href)')[0]`` or
      ``response.xpath('//img/@src')[0]``

    See :ref:`response-follow-example` for usage examples.
  summary: Return a :class:`~
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: callback
    default: None
    rest: false
  - kind: positional
    name: method
    default: GET
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: body
    default: None
    rest: false
  - kind: positional
    name: cookies
    default: None
    rest: false
  - kind: positional
    name: meta
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: priority
    default: '0'
    rest: false
  - kind: positional
    name: dont_filter
    default: 'False'
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - kind: positional
    name: cb_kwargs
    default: None
    rest: false
  - kind: positional
    name: flags
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextResponse.follow_all
  kind: method
  ns: scrapy.http
  description: |-
    A generator that produces :class:`~.Request` instances to follow all
    links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s
    ``__init__`` method, except that each ``urls`` element does not need to be
    an absolute URL, it can be any of the following:

    * a relative URL
    * a :class:`~scrapy.link.Link` object, e.g. the result of
      :ref:`topics-link-extractors`
    * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.
      ``response.css('a.my_link')[0]``
    * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.
      ``response.css('a::attr(href)')[0]`` or
      ``response.xpath('//img/@src')[0]``

    In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction
    within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).

    Note that when passing a ``SelectorList`` as argument for the ``urls`` parameter or
    using the ``css`` or ``xpath`` parameters, this method will not produce requests for
    selectors from which links cannot be obtained (for instance, anchor tags without an
    ``href`` attribute)
  summary: A generator that produces :class:`~
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: urls
    default: None
    rest: false
  - kind: positional
    name: callback
    default: None
    rest: false
  - kind: positional
    name: method
    default: GET
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: body
    default: None
    rest: false
  - kind: positional
    name: cookies
    default: None
    rest: false
  - kind: positional
    name: meta
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: priority
    default: '0'
    rest: false
  - kind: positional
    name: dont_filter
    default: 'False'
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - kind: positional
    name: cb_kwargs
    default: None
    rest: false
  - kind: positional
    name: flags
    default: None
    rest: false
  - kind: positional
    name: css
    default: None
    rest: false
  - kind: positional
    name: xpath
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextResponse.jmespath
  kind: method
  ns: scrapy.http
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextResponse.json
  kind: method
  ns: scrapy.http
  description: |-
    .. versionadded:: 2.2

    Deserialize a JSON document to a Python object.
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextResponse.meta
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TextResponse.replace
  kind: method
  ns: scrapy.http
  description: Create a new Response with the same attributes except for those given new values
  summary: Create a new Response with the same attributes except for those given new values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextResponse.selector
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TextResponse.text
  kind: property
  ns: scrapy.http
  description: Body as unicode
  summary: Body as unicode
  signatures: null
  inherits_from: null
- name: TextResponse.url
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TextResponse.urljoin
  kind: method
  ns: scrapy.http
  description: |-
    Join this Response's url with a possible relative url to form an
    absolute interpretation of the latter.
  summary: Join this Response's url with a possible relative url to form an
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextResponse.xpath
  kind: method
  ns: scrapy.http
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlResponse
  kind: class
  ns: scrapy.http
  description: |-
    An object that represents an HTTP response, which is usually
    downloaded (by the Downloader) and fed to the Spiders for processing.
  summary: An object that represents an HTTP response, which is usually
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: XmlResponse
  inherits_from:
  - <class 'scrapy.http.response.text.TextResponse'>
  - <class 'scrapy.http.response.Response'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: XmlResponse.attributes
  kind: property
  ns: scrapy.http
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: XmlResponse.body
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XmlResponse.cb_kwargs
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XmlResponse.copy
  kind: method
  ns: scrapy.http
  description: Return a copy of this Response
  summary: Return a copy of this Response
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlResponse.css
  kind: method
  ns: scrapy.http
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlResponse.encoding
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XmlResponse.follow
  kind: method
  ns: scrapy.http
  description: |-
    Return a :class:`~.Request` instance to follow a link ``url``.
    It accepts the same arguments as ``Request.__init__`` method,
    but ``url`` can be not only an absolute URL, but also

    * a relative URL
    * a :class:`~scrapy.link.Link` object, e.g. the result of
      :ref:`topics-link-extractors`
    * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.
      ``response.css('a.my_link')[0]``
    * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.
      ``response.css('a::attr(href)')[0]`` or
      ``response.xpath('//img/@src')[0]``

    See :ref:`response-follow-example` for usage examples.
  summary: Return a :class:`~
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: callback
    default: None
    rest: false
  - kind: positional
    name: method
    default: GET
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: body
    default: None
    rest: false
  - kind: positional
    name: cookies
    default: None
    rest: false
  - kind: positional
    name: meta
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: priority
    default: '0'
    rest: false
  - kind: positional
    name: dont_filter
    default: 'False'
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - kind: positional
    name: cb_kwargs
    default: None
    rest: false
  - kind: positional
    name: flags
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlResponse.follow_all
  kind: method
  ns: scrapy.http
  description: |-
    A generator that produces :class:`~.Request` instances to follow all
    links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s
    ``__init__`` method, except that each ``urls`` element does not need to be
    an absolute URL, it can be any of the following:

    * a relative URL
    * a :class:`~scrapy.link.Link` object, e.g. the result of
      :ref:`topics-link-extractors`
    * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.
      ``response.css('a.my_link')[0]``
    * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.
      ``response.css('a::attr(href)')[0]`` or
      ``response.xpath('//img/@src')[0]``

    In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction
    within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).

    Note that when passing a ``SelectorList`` as argument for the ``urls`` parameter or
    using the ``css`` or ``xpath`` parameters, this method will not produce requests for
    selectors from which links cannot be obtained (for instance, anchor tags without an
    ``href`` attribute)
  summary: A generator that produces :class:`~
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: urls
    default: None
    rest: false
  - kind: positional
    name: callback
    default: None
    rest: false
  - kind: positional
    name: method
    default: GET
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: body
    default: None
    rest: false
  - kind: positional
    name: cookies
    default: None
    rest: false
  - kind: positional
    name: meta
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: priority
    default: '0'
    rest: false
  - kind: positional
    name: dont_filter
    default: 'False'
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - kind: positional
    name: cb_kwargs
    default: None
    rest: false
  - kind: positional
    name: flags
    default: None
    rest: false
  - kind: positional
    name: css
    default: None
    rest: false
  - kind: positional
    name: xpath
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlResponse.jmespath
  kind: method
  ns: scrapy.http
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlResponse.json
  kind: method
  ns: scrapy.http
  description: |-
    .. versionadded:: 2.2

    Deserialize a JSON document to a Python object.
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlResponse.meta
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XmlResponse.replace
  kind: method
  ns: scrapy.http
  description: Create a new Response with the same attributes except for those given new values
  summary: Create a new Response with the same attributes except for those given new values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlResponse.selector
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XmlResponse.text
  kind: property
  ns: scrapy.http
  description: Body as unicode
  summary: Body as unicode
  signatures: null
  inherits_from: null
- name: XmlResponse.url
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XmlResponse.urljoin
  kind: method
  ns: scrapy.http
  description: |-
    Join this Response's url with a possible relative url to form an
    absolute interpretation of the latter.
  summary: Join this Response's url with a possible relative url to form an
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlResponse.xpath
  kind: method
  ns: scrapy.http
  description: |-
    Shortcut method implemented only by responses whose content
    is text (subclasses of TextResponse).
  summary: Shortcut method implemented only by responses whose content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlRpcRequest
  kind: class
  ns: scrapy.http
  description: |-
    Represents an HTTP request, which is usually generated in a Spider and
    executed by the Downloader, thus generating a :class:`Response`.
  summary: Represents an HTTP request, which is usually generated in a Spider and
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: encoding
    default: None
    rest: false
    kind: kw-only
  - type: XmlRpcRequest
  inherits_from:
  - <class 'scrapy.http.request.Request'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: XmlRpcRequest.attributes
  kind: property
  ns: scrapy.http
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: XmlRpcRequest.body
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XmlRpcRequest.cb_kwargs
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XmlRpcRequest.copy
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlRpcRequest.encoding
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XmlRpcRequest.from_curl
  kind: function
  ns: scrapy.http
  description: |-
    Create a Request object from a string containing a `cURL
    <https://curl.haxx.se/>`_ command. It populates the HTTP method, the
    URL, the headers, the cookies and the body. It accepts the same
    arguments as the :class:`Request` class, taking preference and
    overriding the values of the same arguments contained in the cURL
    command.

    Unrecognized options are ignored by default. To raise an error when
    finding unknown options call this method by passing
    ``ignore_unknown_options=False``.

    .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`
                 subclasses, such as :class:`~scrapy.http.JSONRequest`, or
                 :class:`~scrapy.http.XmlRpcRequest`, as well as having
                 :ref:`downloader middlewares <topics-downloader-middleware>`
                 and
                 :ref:`spider middlewares <topics-spider-middleware>`
                 enabled, such as
                 :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,
                 :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,
                 or
                 :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,
                 may modify the :class:`~scrapy.http.Request` object.

    To translate a cURL command into a Scrapy request,
    you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.
  summary: Create a Request object from a string containing a `cURL
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: curl_command
    default: null
    rest: false
  - kind: positional
    name: ignore_unknown_options
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlRpcRequest.meta
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XmlRpcRequest.replace
  kind: method
  ns: scrapy.http
  description: Create a new Request with the same attributes except for those given new values
  summary: Create a new Request with the same attributes except for those given new values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XmlRpcRequest.to_dict
  kind: method
  ns: scrapy.http
  description: |-
    Return a dictionary containing the Request's data.

    Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.

    If a spider is given, this method will try to find out the name of the spider methods used as callback
    and errback and include them in the output dict, raising an exception if they cannot be found.
  summary: Return a dictionary containing the Request's data
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: spider
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: XmlRpcRequest.url
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: common
  kind: module
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: obsolete_setter
  kind: function
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: setter
    default: null
    rest: false
  - kind: positional
    name: attrname
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: headers
  kind: module
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CaseInsensitiveDict
  kind: class
  ns: scrapy.http
  description: |-
    A dict-like structure that accepts strings or bytes
    as keys and allows case-insensitive lookups.
  summary: A dict-like structure that accepts strings or bytes
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: CaseInsensitiveDict
  inherits_from:
  - <class 'collections.UserDict'>
  - <class 'collections.abc.MutableMapping'>
  - <class 'collections.abc.Mapping'>
  - <class 'collections.abc.Collection'>
  - <class 'collections.abc.Sized'>
  - <class 'collections.abc.Iterable'>
  - <class 'collections.abc.Container'>
- name: CaseInsensitiveDict.clear
  kind: method
  ns: scrapy.http
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaseInsensitiveDict.copy
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaseInsensitiveDict.fromkeys
  kind: function
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: iterable
    default: null
    rest: false
  - kind: positional
    name: value
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CaseInsensitiveDict.get
  kind: method
  ns: scrapy.http
  description: D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CaseInsensitiveDict.items
  kind: method
  ns: scrapy.http
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaseInsensitiveDict.keys
  kind: method
  ns: scrapy.http
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaseInsensitiveDict.pop
  kind: method
  ns: scrapy.http
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
    If key is not found, d is returned if given, otherwise KeyError is raised.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: <object object at 0x7fa9743b0160>
    rest: false
  - type: '?'
  inherits_from: null
- name: CaseInsensitiveDict.popitem
  kind: method
  ns: scrapy.http
  description: |-
    D.popitem() -> (k, v), remove and return some (key, value) pair
    as a 2-tuple; but raise KeyError if D is empty.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaseInsensitiveDict.setdefault
  kind: method
  ns: scrapy.http
  description: D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CaseInsensitiveDict.update
  kind: method
  ns: scrapy.http
  description: |-
    D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.
    If E present and has a .keys() method, does:     for k in E: D[k] = E[k]
    If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v
    In either case, this is followed by: for k, v in F.items(): D[k] = v
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: ()
    rest: false
  - type: '?'
  inherits_from: null
- name: CaseInsensitiveDict.values
  kind: method
  ns: scrapy.http
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaselessDict
  kind: class
  ns: scrapy.http
  description: |-
    dict() -> new empty dictionary
    dict(mapping) -> new dictionary initialized from a mapping object's
        (key, value) pairs
    dict(iterable) -> new dictionary initialized as if via:
        d = {}
        for k, v in iterable:
            d[k] = v
    dict(**kwargs) -> new dictionary initialized with the name=value pairs
        in the keyword argument list.  For example:  dict(one=1, two=2)
  summary: dict() -> new empty dictionary
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - type: CaselessDict
  inherits_from:
  - <class 'dict'>
- name: CaselessDict.clear
  kind: callable
  ns: scrapy.http
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures: null
  inherits_from: null
- name: CaselessDict.copy
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaselessDict.fromkeys
  kind: function
  ns: scrapy.http
  description: Create a new dictionary with keys from iterable and values set to value.
  summary: Create a new dictionary with keys from iterable and values set to value
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: keys
    default: null
    rest: false
  - kind: positional
    name: value
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CaselessDict.get
  kind: method
  ns: scrapy.http
  description: Return the value for key if key is in the dictionary, else default.
  summary: Return the value for key if key is in the dictionary, else default
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: def_val
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CaselessDict.has_key
  kind: method
  ns: scrapy.http
  description: True if the dictionary has the specified key, else False.
  summary: True if the dictionary has the specified key, else False
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaselessDict.items
  kind: callable
  ns: scrapy.http
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures: null
  inherits_from: null
- name: CaselessDict.keys
  kind: callable
  ns: scrapy.http
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures: null
  inherits_from: null
- name: CaselessDict.normkey
  kind: method
  ns: scrapy.http
  description: Method to normalize dictionary key access
  summary: Method to normalize dictionary key access
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaselessDict.normvalue
  kind: method
  ns: scrapy.http
  description: Method to normalize values prior to be set
  summary: Method to normalize values prior to be set
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaselessDict.pop
  kind: method
  ns: scrapy.http
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.

    If the key is not found, return the default if given; otherwise,
    raise a KeyError.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaselessDict.popitem
  kind: callable
  ns: scrapy.http
  description: |-
    Remove and return a (key, value) pair as a 2-tuple.

    Pairs are returned in LIFO (last-in, first-out) order.
    Raises KeyError if the dict is empty.
  summary: Remove and return a (key, value) pair as a 2-tuple
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaselessDict.setdefault
  kind: method
  ns: scrapy.http
  description: |-
    Insert key with a value of default if key is not in the dictionary.

    Return the value for key if key is in the dictionary, else default.
  summary: Insert key with a value of default if key is not in the dictionary
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: def_val
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CaselessDict.update
  kind: method
  ns: scrapy.http
  description: |-
    D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.
    If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
    If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
    In either case, this is followed by: for k in F:  D[k] = F[k]
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: seq
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CaselessDict.values
  kind: callable
  ns: scrapy.http
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures: null
  inherits_from: null
- name: Mapping
  kind: class
  ns: scrapy.http
  description: |-
    A Mapping is a generic container for associating key/value
    pairs.

    This class provides concrete generic implementations of all
    methods except for __getitem__, __iter__, and __len__.
  summary: A Mapping is a generic container for associating key/value
  signatures:
  - type: Mapping
  inherits_from:
  - <class 'collections.abc.Collection'>
  - <class 'collections.abc.Sized'>
  - <class 'collections.abc.Iterable'>
  - <class 'collections.abc.Container'>
- name: Mapping.get
  kind: method
  ns: scrapy.http
  description: D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Mapping.items
  kind: method
  ns: scrapy.http
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Mapping.keys
  kind: method
  ns: scrapy.http
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Mapping.values
  kind: method
  ns: scrapy.http
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: headers_dict_to_raw
  kind: function
  ns: scrapy.http
  description: |-
    Returns a raw HTTP headers representation of headers

    For example:

    >>> import w3lib.http
    >>> w3lib.http.headers_dict_to_raw({b'Content-type': b'text/html', b'Accept': b'gzip'}) # doctest: +SKIP
    'Content-type: text/html\\r\\nAccept: gzip'
    >>>

    Note that keys and values must be bytes.

    Argument is ``None`` (returns ``None``):

    >>> w3lib.http.headers_dict_to_raw(None)
    >>>
  summary: Returns a raw HTTP headers representation of headers
  signatures:
  - kind: positional
    name: headers_dict
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: to_unicode
  kind: function
  ns: scrapy.http
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: request
  kind: module
  ns: scrapy.http
  description: |-
    This module implements the Request class which is used to represent HTTP
    requests in Scrapy.

    See documentation in docs/topics/request-response.rst
  summary: This module implements the Request class which is used to represent HTTP
  signatures: null
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.http
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.http
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: NO_CALLBACK
  kind: function
  ns: scrapy.http
  description: |-
    When assigned to the ``callback`` parameter of
    :class:`~scrapy.http.Request`, it indicates that the request is not meant
    to have a spider callback at all.

    For example:

    .. code-block:: python

       Request("https://example.com", callback=NO_CALLBACK)

    This value should be used by :ref:`components <topics-components>` that
    create and handle their own requests, e.g. through
    :meth:`scrapy.core.engine.ExecutionEngine.download`, so that downloader
    middlewares handling such requests can treat them differently from requests
    intended for the :meth:`~scrapy.Spider.parse` callback.
  summary: When assigned to the ``callback`` parameter of
  signatures:
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.http
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RequestTypeVar
  kind: const
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.http
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.http
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TypeVar
  kind: class
  ns: scrapy.http
  description: |-
    Type variable.

    Usage::

      T = TypeVar('T')  # Can be anything
      A = TypeVar('A', str, bytes)  # Must be str or bytes

    Type variables exist primarily for the benefit of static type
    checkers.  They serve as the parameters for generic types as well
    as for generic function definitions.  See class Generic for more
    information on generic types.  Generic functions work as follows:

      def repeat(x: T, n: int) -> List[T]:
          '''Return a list containing n references to x.'''
          return [x]*n

      def longest(x: A, y: A) -> A:
          '''Return the longest of two strings.'''
          return x if len(x) >= len(y) else y

    The latter example's signature is essentially the overloading
    of (str, str) -> str and (bytes, bytes) -> bytes.  Also note
    that if the arguments are instances of some subclass of str,
    the return type is still plain str.

    At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.

    Type variables defined with covariant=True or contravariant=True
    can be used to declare covariant or contravariant generic types.
    See PEP 484 for more details. By default generic types are invariant
    in all type variables.

    Type variables can be introspected. e.g.:

      T.__name__ == 'T'
      T.__constraints__ == ()
      T.__covariant__ == False
      T.__contravariant__ = False
      A.__constraints__ == (str, bytes)

    Note that only type variables defined in global scope can be pickled.
  summary: Type variable
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - name: bound
    default: None
    rest: false
    kind: kw-only
  - name: covariant
    default: 'False'
    rest: false
    kind: kw-only
  - name: contravariant
    default: 'False'
    rest: false
    kind: kw-only
  - type: TypeVar
  inherits_from:
  - <class 'typing._Final'>
  - <class 'typing._Immutable'>
  - <class 'typing._BoundVarianceMixin'>
  - <class 'typing._PickleUsingNameMixin'>
- name: Union
  kind: callable
  ns: scrapy.http
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: curl_to_request_kwargs
  kind: function
  ns: scrapy.http
  description: |-
    Convert a cURL command syntax to Request kwargs.

    :param str curl_command: string containing the curl command
    :param bool ignore_unknown_options: If true, only a warning is emitted when
                                        cURL options are unknown. Otherwise
                                        raises an error. (default: True)
    :return: dictionary of Request kwargs
  summary: Convert a cURL command syntax to Request kwargs
  signatures:
  - kind: positional
    name: curl_command
    default: null
    rest: false
  - kind: positional
    name: ignore_unknown_options
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: escape_ajax
  kind: function
  ns: scrapy.http
  description: |-
    Return the crawlable url according to:
    https://developers.google.com/webmasters/ajax-crawling/docs/getting-started

    >>> escape_ajax("www.example.com/ajax.html#!key=value")
    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'
    >>> escape_ajax("www.example.com/ajax.html?k1=v1&k2=v2#!key=value")
    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'
    >>> escape_ajax("www.example.com/ajax.html?#!key=value")
    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'
    >>> escape_ajax("www.example.com/ajax.html#!")
    'www.example.com/ajax.html?_escaped_fragment_='

    URLs that are not "AJAX crawlable" (according to Google) returned as-is:

    >>> escape_ajax("www.example.com/ajax.html#key=value")
    'www.example.com/ajax.html#key=value'
    >>> escape_ajax("www.example.com/ajax.html#")
    'www.example.com/ajax.html#'
    >>> escape_ajax("www.example.com/ajax.html")
    'www.example.com/ajax.html'
  summary: 'Return the crawlable url according to:'
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: form
  kind: module
  ns: scrapy.http
  description: |-
    This module implements the FormRequest class which is a more convenient class
    (than Request) to generate Requests based on form data.

    See documentation in docs/topics/request-response.rst
  summary: This module implements the FormRequest class which is a more convenient class
  signatures: null
  inherits_from: null
- name: FormElement
  kind: class
  ns: scrapy.http
  description: Represents a <form> element.
  summary: Represents a <form> element
  signatures: null
  inherits_from:
  - <class 'lxml.html.HtmlElement'>
  - <class 'lxml.html.HtmlMixin'>
  - <class 'lxml.etree.ElementBase'>
  - <class 'lxml.etree._Element'>
- name: FormElement.action
  kind: property
  ns: scrapy.http
  description: Get/set the form's ``action`` attribute.
  summary: Get/set the form's ``action`` attribute
  signatures: null
  inherits_from: null
- name: FormElement.addnext
  kind: callable
  ns: scrapy.http
  description: |-
    addnext(self, element)

    Adds the element as a following sibling directly after this
    element.

    This is normally used to set a processing instruction or comment after
    the root node of a document.  Note that tail text is automatically
    discarded when adding at the root level.
  summary: addnext(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.addprevious
  kind: callable
  ns: scrapy.http
  description: |-
    addprevious(self, element)

    Adds the element as a preceding sibling directly before this
    element.

    This is normally used to set a processing instruction or comment
    before the root node of a document.  Note that tail text is
    automatically discarded when adding at the root level.
  summary: addprevious(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.append
  kind: callable
  ns: scrapy.http
  description: |-
    append(self, element)

    Adds a subelement to the end of this element.
  summary: append(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.attrib
  kind: property
  ns: scrapy.http
  description: |-
    Element attribute dictionary. Where possible, use get(), set(),
    keys(), values() and items() to access element attributes.
  summary: Element attribute dictionary
  signatures: null
  inherits_from: null
- name: FormElement.base
  kind: property
  ns: scrapy.http
  description: |-
    The base URI of the Element (xml:base or HTML base URL).
    None if the base URI is unknown.

    Note that the value depends on the URL of the document that
    holds the Element if there is no xml:base attribute on the
    Element or its ancestors.

    Setting this property will set an xml:base attribute on the
    Element, regardless of the document type (XML or HTML).
  summary: The base URI of the Element (xml:base or HTML base URL)
  signatures: null
  inherits_from: null
- name: FormElement.base_url
  kind: property
  ns: scrapy.http
  description: |-
    Returns the base URL, given when the page was parsed.

    Use with ``urlparse.urljoin(el.base_url, href)`` to get
    absolute URLs.
  summary: Returns the base URL, given when the page was parsed
  signatures: null
  inherits_from: null
- name: FormElement.body
  kind: property
  ns: scrapy.http
  description: |-
    Return the <body> element.  Can be called from a child element
    to get the document's head.
  summary: Return the <body> element
  signatures: null
  inherits_from: null
- name: FormElement.classes
  kind: property
  ns: scrapy.http
  description: A set-like wrapper around the 'class' attribute.
  summary: A set-like wrapper around the 'class' attribute
  signatures: null
  inherits_from: null
- name: FormElement.clear
  kind: callable
  ns: scrapy.http
  description: |-
    clear(self, keep_tail=False)

    Resets an element.  This function removes all subelements, clears
    all attributes and sets the text and tail properties to None.

    Pass ``keep_tail=True`` to leave the tail text untouched.
  summary: clear(self, keep_tail=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: keep_tail
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.cssselect
  kind: method
  ns: scrapy.http
  description: |-
    Run the CSS expression on this element and its children,
    returning a list of the results.

    Equivalent to lxml.cssselect.CSSSelect(expr, translator='html')(self)
    -- note that pre-compiling the expression can provide a substantial
    speedup.
  summary: Run the CSS expression on this element and its children,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expr
    default: null
    rest: false
  - kind: positional
    name: translator
    default: html
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.drop_tag
  kind: method
  ns: scrapy.http
  description: |-
    Remove the tag, but not its children or text.  The children and text
    are merged into the parent.

    Example::

        >>> h = fragment_fromstring('<div>Hello <b>World!</b></div>')
        >>> h.find('.//b').drop_tag()
        >>> print(tostring(h, encoding='unicode'))
        <div>Hello World!</div>
  summary: Remove the tag, but not its children or text
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.drop_tree
  kind: method
  ns: scrapy.http
  description: |-
    Removes this element from the tree, including its children and
    text.  The tail text is joined to the previous element or
    parent.
  summary: Removes this element from the tree, including its children and
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.extend
  kind: callable
  ns: scrapy.http
  description: |-
    extend(self, elements)

    Extends the current children by the elements in the iterable.
  summary: extend(self, elements)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: elements
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.fields
  kind: property
  ns: scrapy.http
  description: |-
    Dictionary-like object that represents all the fields in this
    form.  You can set values in this dictionary to effect the
    form.
  summary: Dictionary-like object that represents all the fields in this
  signatures: null
  inherits_from: null
- name: FormElement.find
  kind: callable
  ns: scrapy.http
  description: |-
    find(self, path, namespaces=None)

    Finds the first matching subelement, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: find(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.find_class
  kind: method
  ns: scrapy.http
  description: Find any elements with the given class name.
  summary: Find any elements with the given class name
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: class_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.find_rel_links
  kind: method
  ns: scrapy.http
  description: Find any links like ``<a rel="{rel}">...</a>``; returns a list of elements.
  summary: Find any links like ``<a rel="{rel}">
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: rel
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.findall
  kind: callable
  ns: scrapy.http
  description: |-
    findall(self, path, namespaces=None)

    Finds all matching subelements, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: findall(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.findtext
  kind: callable
  ns: scrapy.http
  description: |-
    findtext(self, path, default=None, namespaces=None)

    Finds text for the first matching subelement, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: findtext(self, path, default=None, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.form_values
  kind: method
  ns: scrapy.http
  description: |-
    Return a list of tuples of the field values for the form.
    This is suitable to be passed to ``urllib.urlencode()``.
  summary: Return a list of tuples of the field values for the form
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.forms
  kind: property
  ns: scrapy.http
  description: Return a list of all the forms
  summary: Return a list of all the forms
  signatures: null
  inherits_from: null
- name: FormElement.get
  kind: callable
  ns: scrapy.http
  description: |-
    get(self, key, default=None)

    Gets an element attribute.
  summary: get(self, key, default=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.get_element_by_id
  kind: method
  ns: scrapy.http
  description: |-
    Get the first element in a document with the given id.  If none is
    found, return the default argument if provided or raise KeyError
    otherwise.

    Note that there can be more than one element with the same id,
    and this isn't uncommon in HTML documents found in the wild.
    Browsers return only the first match, and this function does
    the same.
  summary: Get the first element in a document with the given id
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: id
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.getchildren
  kind: callable
  ns: scrapy.http
  description: |-
    getchildren(self)

    Returns all direct children.  The elements are returned in document
    order.

    :deprecated: Note that this method has been deprecated as of
      ElementTree 1.3 and lxml 2.0.  New code should use
      ``list(element)`` or simply iterate over elements.
  summary: getchildren(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.getiterator
  kind: callable
  ns: scrapy.http
  description: |-
    getiterator(self, tag=None, *tags)

    Returns a sequence or iterator of all elements in the subtree in
    document order (depth first pre-order), starting with this
    element.

    Can be restricted to find only elements with specific tags,
    see `iter`.

    :deprecated: Note that this method is deprecated as of
      ElementTree 1.3 and lxml 2.0.  It returns an iterator in
      lxml, which diverges from the original ElementTree
      behaviour.  If you want an efficient iterator, use the
      ``element.iter()`` method instead.  You should only use this
      method in new code if you require backwards compatibility
      with older versions of lxml or ElementTree.
  summary: getiterator(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.getnext
  kind: callable
  ns: scrapy.http
  description: |-
    getnext(self)

    Returns the following sibling of this element or None.
  summary: getnext(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.getparent
  kind: callable
  ns: scrapy.http
  description: |-
    getparent(self)

    Returns the parent of this element or None for the root element.
  summary: getparent(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.getprevious
  kind: callable
  ns: scrapy.http
  description: |-
    getprevious(self)

    Returns the preceding sibling of this element or None.
  summary: getprevious(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.getroottree
  kind: callable
  ns: scrapy.http
  description: |-
    getroottree(self)

    Return an ElementTree for the root node of the document that
    contains this element.

    This is the same as following element.getparent() up the tree until it
    returns None (for the root element) and then build an ElementTree for
    the last parent that was returned.
  summary: getroottree(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.head
  kind: property
  ns: scrapy.http
  description: |-
    Returns the <head> element.  Can be called from a child
    element to get the document's head.
  summary: Returns the <head> element
  signatures: null
  inherits_from: null
- name: FormElement.index
  kind: callable
  ns: scrapy.http
  description: |-
    index(self, child, start=None, stop=None)

    Find the position of the child within the parent.

    This method is not part of the original ElementTree API.
  summary: index(self, child, start=None, stop=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: child
    default: null
    rest: false
  - kind: positional
    name: start
    default: None
    rest: false
  - kind: positional
    name: stop
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.inputs
  kind: property
  ns: scrapy.http
  description: |-
    Returns an accessor for all the input elements in the form.

    See `InputGetter` for more information about the object.
  summary: Returns an accessor for all the input elements in the form
  signatures: null
  inherits_from: null
- name: FormElement.insert
  kind: callable
  ns: scrapy.http
  description: |-
    insert(self, index, element)

    Inserts a subelement at the given position in this element
  summary: insert(self, index, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: index
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.items
  kind: callable
  ns: scrapy.http
  description: |-
    items(self)

    Gets element attributes, as a sequence. The attributes are returned in
    an arbitrary order.
  summary: items(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.iter
  kind: callable
  ns: scrapy.http
  description: |-
    iter(self, tag=None, *tags)

    Iterate over all elements in the subtree in document order (depth
    first pre-order), starting with this element.

    Can be restricted to find only elements with specific tags:
    pass ``"{ns}localname"`` as tag. Either or both of ``ns`` and
    ``localname`` can be ``*`` for a wildcard; ``ns`` can be empty
    for no namespace. ``"localname"`` is equivalent to ``"{}localname"``
    (i.e. no namespace) but ``"*"`` is ``"{*}*"`` (any or no namespace),
    not ``"{}*"``.

    You can also pass the Element, Comment, ProcessingInstruction and
    Entity factory functions to look only for the specific element type.

    Passing multiple tags (or a sequence of tags) instead of a single tag
    will let the iterator return all elements matching any of these tags,
    in document order.
  summary: iter(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.iterancestors
  kind: callable
  ns: scrapy.http
  description: |-
    iterancestors(self, tag=None, *tags)

    Iterate over the ancestors of this element (from parent to parent).

    Can be restricted to find only elements with specific tags,
    see `iter`.
  summary: iterancestors(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.iterchildren
  kind: callable
  ns: scrapy.http
  description: |-
    iterchildren(self, tag=None, *tags, reversed=False)

    Iterate over the children of this element.

    As opposed to using normal iteration on this element, the returned
    elements can be reversed with the 'reversed' keyword and restricted
    to find only elements with specific tags, see `iter`.
  summary: iterchildren(self, tag=None, *tags, reversed=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: reversed
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: FormElement.iterdescendants
  kind: callable
  ns: scrapy.http
  description: |-
    iterdescendants(self, tag=None, *tags)

    Iterate over the descendants of this element in document order.

    As opposed to ``el.iter()``, this iterator does not yield the element
    itself.  The returned elements can be restricted to find only elements
    with specific tags, see `iter`.
  summary: iterdescendants(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.iterfind
  kind: callable
  ns: scrapy.http
  description: |-
    iterfind(self, path, namespaces=None)

    Iterates over all matching subelements, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: iterfind(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.iterlinks
  kind: method
  ns: scrapy.http
  description: |-
    Yield (element, attribute, link, pos), where attribute may be None
    (indicating the link is in the text).  ``pos`` is the position
    where the link occurs; often 0, but sometimes something else in
    the case of links in stylesheets or style tags.

    Note: <base href> is *not* taken into account in any way.  The
    link you get is exactly the link in the document.

    Note: multiple links inside of a single text string or
    attribute value are returned in reversed order.  This makes it
    possible to replace or delete them from the text string value
    based on their reported text positions.  Otherwise, a
    modification at one text position can change the positions of
    links reported later on.
  summary: Yield (element, attribute, link, pos), where attribute may be None
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.itersiblings
  kind: callable
  ns: scrapy.http
  description: |-
    itersiblings(self, tag=None, *tags, preceding=False)

    Iterate over the following or preceding siblings of this element.

    The direction is determined by the 'preceding' keyword which
    defaults to False, i.e. forward iteration over the following
    siblings.  When True, the iterator yields the preceding
    siblings in reverse document order, i.e. starting right before
    the current element and going backwards.

    Can be restricted to find only elements with specific tags,
    see `iter`.
  summary: itersiblings(self, tag=None, *tags, preceding=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: preceding
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: FormElement.itertext
  kind: callable
  ns: scrapy.http
  description: |-
    itertext(self, tag=None, *tags, with_tail=True)

    Iterates over the text content of a subtree.

    You can pass tag names to restrict text content to specific elements,
    see `iter`.

    You can set the ``with_tail`` keyword argument to ``False`` to skip
    over tail text.
  summary: itertext(self, tag=None, *tags, with_tail=True)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: with_tail
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: FormElement.keys
  kind: callable
  ns: scrapy.http
  description: |-
    keys(self)

    Gets a list of attribute names.  The names are returned in an
    arbitrary order (just like for an ordinary Python dictionary).
  summary: keys(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.label
  kind: property
  ns: scrapy.http
  description: Get or set any <label> element associated with this element.
  summary: Get or set any <label> element associated with this element
  signatures: null
  inherits_from: null
- name: FormElement.make_links_absolute
  kind: method
  ns: scrapy.http
  description: |-
    Make all links in the document absolute, given the
    ``base_url`` for the document (the full URL where the document
    came from), or if no ``base_url`` is given, then the ``.base_url``
    of the document.

    If ``resolve_base_href`` is true, then any ``<base href>``
    tags in the document are used *and* removed from the document.
    If it is false then any such tag is ignored.

    If ``handle_failures`` is None (default), a failure to process
    a URL will abort the processing.  If set to 'ignore', errors
    are ignored.  If set to 'discard', failing URLs will be removed.
  summary: Make all links in the document absolute, given the
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: base_url
    default: None
    rest: false
  - kind: positional
    name: resolve_base_href
    default: 'True'
    rest: false
  - kind: positional
    name: handle_failures
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.makeelement
  kind: callable
  ns: scrapy.http
  description: |-
    makeelement(self, _tag, attrib=None, nsmap=None, **_extra)

    Creates a new element associated with the same document.
  summary: makeelement(self, _tag, attrib=None, nsmap=None, **_extra)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _tag
    default: null
    rest: false
  - kind: positional
    name: attrib
    default: None
    rest: false
  - kind: positional
    name: nsmap
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.method
  kind: property
  ns: scrapy.http
  description: |-
    Get/set the form's method.  Always returns a capitalized
    string, and defaults to ``'GET'``
  summary: Get/set the form's method
  signatures: null
  inherits_from: null
- name: FormElement.nsmap
  kind: property
  ns: scrapy.http
  description: |-
    Namespace prefix->URI mapping known in the context of this
    Element.  This includes all namespace declarations of the
    parents.

    Note that changing the returned dict has no effect on the Element.
  summary: Namespace prefix->URI mapping known in the context of this
  signatures: null
  inherits_from: null
- name: FormElement.prefix
  kind: property
  ns: scrapy.http
  description: "Namespace prefix or None.\n        "
  summary: Namespace prefix or None
  signatures: null
  inherits_from: null
- name: FormElement.remove
  kind: callable
  ns: scrapy.http
  description: |-
    remove(self, element)

    Removes a matching subelement. Unlike the find methods, this
    method compares elements based on identity, not on tag value
    or contents.
  summary: remove(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.replace
  kind: callable
  ns: scrapy.http
  description: |-
    replace(self, old_element, new_element)

    Replaces a subelement with the element passed as second argument.
  summary: replace(self, old_element, new_element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: old_element
    default: null
    rest: false
  - kind: positional
    name: new_element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.resolve_base_href
  kind: method
  ns: scrapy.http
  description: |-
    Find any ``<base href>`` tag in the document, and apply its
    values to all links found in the document.  Also remove the
    tag once it has been applied.

    If ``handle_failures`` is None (default), a failure to process
    a URL will abort the processing.  If set to 'ignore', errors
    are ignored.  If set to 'discard', failing URLs will be removed.
  summary: Find any ``<base href>`` tag in the document, and apply its
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: handle_failures
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.rewrite_links
  kind: method
  ns: scrapy.http
  description: |-
    Rewrite all the links in the document.  For each link
    ``link_repl_func(link)`` will be called, and the return value
    will replace the old link.

    Note that links may not be absolute (unless you first called
    ``make_links_absolute()``), and may be internal (e.g.,
    ``'#anchor'``).  They can also be values like
    ``'mailto:email'`` or ``'javascript:expr'``.

    If you give ``base_href`` then all links passed to
    ``link_repl_func()`` will take that into account.

    If the ``link_repl_func`` returns None, the attribute or
    tag text will be removed completely.
  summary: Rewrite all the links in the document
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: link_repl_func
    default: null
    rest: false
  - kind: positional
    name: resolve_base_href
    default: 'True'
    rest: false
  - kind: positional
    name: base_href
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.set
  kind: method
  ns: scrapy.http
  description: |-
    set(self, key, value=None)

    Sets an element attribute.  If no value is provided, or if the value is None,
    creates a 'boolean' attribute without value, e.g. "<form novalidate></form>"
    for ``form.set('novalidate')``.
  summary: set(self, key, value=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.sourceline
  kind: property
  ns: scrapy.http
  description: "Original line number as found by the parser or None if unknown.\n        "
  summary: Original line number as found by the parser or None if unknown
  signatures: null
  inherits_from: null
- name: FormElement.tag
  kind: property
  ns: scrapy.http
  description: "Element tag\n        "
  summary: Element tag
  signatures: null
  inherits_from: null
- name: FormElement.tail
  kind: property
  ns: scrapy.http
  description: |-
    Text after this element's end tag, but before the next sibling
    element's start tag. This is either a string or the value None, if
    there was no text.
  summary: Text after this element's end tag, but before the next sibling
  signatures: null
  inherits_from: null
- name: FormElement.text
  kind: property
  ns: scrapy.http
  description: |-
    Text before the first subelement. This is either a string or
    the value None, if there was no text.
  summary: Text before the first subelement
  signatures: null
  inherits_from: null
- name: FormElement.text_content
  kind: method
  ns: scrapy.http
  description: Return the text content of the tag (and the text in any children).
  summary: Return the text content of the tag (and the text in any children)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.values
  kind: callable
  ns: scrapy.http
  description: |-
    values(self)

    Gets element attribute values as a sequence of strings.  The
    attributes are returned in an arbitrary order.
  summary: values(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormElement.xpath
  kind: callable
  ns: scrapy.http
  description: |-
    xpath(self, _path, namespaces=None, extensions=None, smart_strings=True, **_variables)

    Evaluate an xpath expression using the element as context node.
  summary: xpath(self, _path, namespaces=None, extensions=None, smart_strings=True, **_variables)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _path
    default: null
    rest: false
  - name: namespaces
    default: None
    rest: false
    kind: kw-only
  - name: extensions
    default: None
    rest: false
    kind: kw-only
  - name: smart_strings
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: FormRequestTypeVar
  kind: const
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FormdataKVType
  kind: callable
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FormdataType
  kind: callable
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement
  kind: class
  ns: scrapy.http
  description: |-
    Represents an ``<input>`` element.

    You can get the type with ``.type`` (which is lower-cased and
    defaults to ``'text'``).

    Also you can get and set the value with ``.value``

    Checkboxes and radios have the attribute ``input.checkable ==
    True`` (for all others it is false) and a boolean attribute
    ``.checked``.
  summary: Represents an ``<input>`` element
  signatures: null
  inherits_from:
  - <class 'lxml.html.InputMixin'>
  - <class 'lxml.html.HtmlElement'>
  - <class 'lxml.html.HtmlMixin'>
  - <class 'lxml.etree.ElementBase'>
  - <class 'lxml.etree._Element'>
- name: InputElement.addnext
  kind: callable
  ns: scrapy.http
  description: |-
    addnext(self, element)

    Adds the element as a following sibling directly after this
    element.

    This is normally used to set a processing instruction or comment after
    the root node of a document.  Note that tail text is automatically
    discarded when adding at the root level.
  summary: addnext(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.addprevious
  kind: callable
  ns: scrapy.http
  description: |-
    addprevious(self, element)

    Adds the element as a preceding sibling directly before this
    element.

    This is normally used to set a processing instruction or comment
    before the root node of a document.  Note that tail text is
    automatically discarded when adding at the root level.
  summary: addprevious(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.append
  kind: callable
  ns: scrapy.http
  description: |-
    append(self, element)

    Adds a subelement to the end of this element.
  summary: append(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.attrib
  kind: property
  ns: scrapy.http
  description: |-
    Element attribute dictionary. Where possible, use get(), set(),
    keys(), values() and items() to access element attributes.
  summary: Element attribute dictionary
  signatures: null
  inherits_from: null
- name: InputElement.base
  kind: property
  ns: scrapy.http
  description: |-
    The base URI of the Element (xml:base or HTML base URL).
    None if the base URI is unknown.

    Note that the value depends on the URL of the document that
    holds the Element if there is no xml:base attribute on the
    Element or its ancestors.

    Setting this property will set an xml:base attribute on the
    Element, regardless of the document type (XML or HTML).
  summary: The base URI of the Element (xml:base or HTML base URL)
  signatures: null
  inherits_from: null
- name: InputElement.base_url
  kind: property
  ns: scrapy.http
  description: |-
    Returns the base URL, given when the page was parsed.

    Use with ``urlparse.urljoin(el.base_url, href)`` to get
    absolute URLs.
  summary: Returns the base URL, given when the page was parsed
  signatures: null
  inherits_from: null
- name: InputElement.body
  kind: property
  ns: scrapy.http
  description: |-
    Return the <body> element.  Can be called from a child element
    to get the document's head.
  summary: Return the <body> element
  signatures: null
  inherits_from: null
- name: InputElement.checkable
  kind: property
  ns: scrapy.http
  description: 'Boolean: can this element be checked?'
  summary: 'Boolean: can this element be checked?'
  signatures: null
  inherits_from: null
- name: InputElement.checked
  kind: property
  ns: scrapy.http
  description: |-
    Boolean attribute to get/set the presence of the ``checked``
    attribute.

    You can only use this on checkable input types.
  summary: Boolean attribute to get/set the presence of the ``checked``
  signatures: null
  inherits_from: null
- name: InputElement.classes
  kind: property
  ns: scrapy.http
  description: A set-like wrapper around the 'class' attribute.
  summary: A set-like wrapper around the 'class' attribute
  signatures: null
  inherits_from: null
- name: InputElement.clear
  kind: callable
  ns: scrapy.http
  description: |-
    clear(self, keep_tail=False)

    Resets an element.  This function removes all subelements, clears
    all attributes and sets the text and tail properties to None.

    Pass ``keep_tail=True`` to leave the tail text untouched.
  summary: clear(self, keep_tail=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: keep_tail
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.cssselect
  kind: method
  ns: scrapy.http
  description: |-
    Run the CSS expression on this element and its children,
    returning a list of the results.

    Equivalent to lxml.cssselect.CSSSelect(expr, translator='html')(self)
    -- note that pre-compiling the expression can provide a substantial
    speedup.
  summary: Run the CSS expression on this element and its children,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expr
    default: null
    rest: false
  - kind: positional
    name: translator
    default: html
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.drop_tag
  kind: method
  ns: scrapy.http
  description: |-
    Remove the tag, but not its children or text.  The children and text
    are merged into the parent.

    Example::

        >>> h = fragment_fromstring('<div>Hello <b>World!</b></div>')
        >>> h.find('.//b').drop_tag()
        >>> print(tostring(h, encoding='unicode'))
        <div>Hello World!</div>
  summary: Remove the tag, but not its children or text
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.drop_tree
  kind: method
  ns: scrapy.http
  description: |-
    Removes this element from the tree, including its children and
    text.  The tail text is joined to the previous element or
    parent.
  summary: Removes this element from the tree, including its children and
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.extend
  kind: callable
  ns: scrapy.http
  description: |-
    extend(self, elements)

    Extends the current children by the elements in the iterable.
  summary: extend(self, elements)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: elements
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.find
  kind: callable
  ns: scrapy.http
  description: |-
    find(self, path, namespaces=None)

    Finds the first matching subelement, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: find(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.find_class
  kind: method
  ns: scrapy.http
  description: Find any elements with the given class name.
  summary: Find any elements with the given class name
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: class_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.find_rel_links
  kind: method
  ns: scrapy.http
  description: Find any links like ``<a rel="{rel}">...</a>``; returns a list of elements.
  summary: Find any links like ``<a rel="{rel}">
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: rel
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.findall
  kind: callable
  ns: scrapy.http
  description: |-
    findall(self, path, namespaces=None)

    Finds all matching subelements, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: findall(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.findtext
  kind: callable
  ns: scrapy.http
  description: |-
    findtext(self, path, default=None, namespaces=None)

    Finds text for the first matching subelement, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: findtext(self, path, default=None, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.forms
  kind: property
  ns: scrapy.http
  description: Return a list of all the forms
  summary: Return a list of all the forms
  signatures: null
  inherits_from: null
- name: InputElement.get
  kind: callable
  ns: scrapy.http
  description: |-
    get(self, key, default=None)

    Gets an element attribute.
  summary: get(self, key, default=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.get_element_by_id
  kind: method
  ns: scrapy.http
  description: |-
    Get the first element in a document with the given id.  If none is
    found, return the default argument if provided or raise KeyError
    otherwise.

    Note that there can be more than one element with the same id,
    and this isn't uncommon in HTML documents found in the wild.
    Browsers return only the first match, and this function does
    the same.
  summary: Get the first element in a document with the given id
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: id
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.getchildren
  kind: callable
  ns: scrapy.http
  description: |-
    getchildren(self)

    Returns all direct children.  The elements are returned in document
    order.

    :deprecated: Note that this method has been deprecated as of
      ElementTree 1.3 and lxml 2.0.  New code should use
      ``list(element)`` or simply iterate over elements.
  summary: getchildren(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.getiterator
  kind: callable
  ns: scrapy.http
  description: |-
    getiterator(self, tag=None, *tags)

    Returns a sequence or iterator of all elements in the subtree in
    document order (depth first pre-order), starting with this
    element.

    Can be restricted to find only elements with specific tags,
    see `iter`.

    :deprecated: Note that this method is deprecated as of
      ElementTree 1.3 and lxml 2.0.  It returns an iterator in
      lxml, which diverges from the original ElementTree
      behaviour.  If you want an efficient iterator, use the
      ``element.iter()`` method instead.  You should only use this
      method in new code if you require backwards compatibility
      with older versions of lxml or ElementTree.
  summary: getiterator(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.getnext
  kind: callable
  ns: scrapy.http
  description: |-
    getnext(self)

    Returns the following sibling of this element or None.
  summary: getnext(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.getparent
  kind: callable
  ns: scrapy.http
  description: |-
    getparent(self)

    Returns the parent of this element or None for the root element.
  summary: getparent(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.getprevious
  kind: callable
  ns: scrapy.http
  description: |-
    getprevious(self)

    Returns the preceding sibling of this element or None.
  summary: getprevious(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.getroottree
  kind: callable
  ns: scrapy.http
  description: |-
    getroottree(self)

    Return an ElementTree for the root node of the document that
    contains this element.

    This is the same as following element.getparent() up the tree until it
    returns None (for the root element) and then build an ElementTree for
    the last parent that was returned.
  summary: getroottree(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.head
  kind: property
  ns: scrapy.http
  description: |-
    Returns the <head> element.  Can be called from a child
    element to get the document's head.
  summary: Returns the <head> element
  signatures: null
  inherits_from: null
- name: InputElement.index
  kind: callable
  ns: scrapy.http
  description: |-
    index(self, child, start=None, stop=None)

    Find the position of the child within the parent.

    This method is not part of the original ElementTree API.
  summary: index(self, child, start=None, stop=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: child
    default: null
    rest: false
  - kind: positional
    name: start
    default: None
    rest: false
  - kind: positional
    name: stop
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.insert
  kind: callable
  ns: scrapy.http
  description: |-
    insert(self, index, element)

    Inserts a subelement at the given position in this element
  summary: insert(self, index, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: index
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.items
  kind: callable
  ns: scrapy.http
  description: |-
    items(self)

    Gets element attributes, as a sequence. The attributes are returned in
    an arbitrary order.
  summary: items(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.iter
  kind: callable
  ns: scrapy.http
  description: |-
    iter(self, tag=None, *tags)

    Iterate over all elements in the subtree in document order (depth
    first pre-order), starting with this element.

    Can be restricted to find only elements with specific tags:
    pass ``"{ns}localname"`` as tag. Either or both of ``ns`` and
    ``localname`` can be ``*`` for a wildcard; ``ns`` can be empty
    for no namespace. ``"localname"`` is equivalent to ``"{}localname"``
    (i.e. no namespace) but ``"*"`` is ``"{*}*"`` (any or no namespace),
    not ``"{}*"``.

    You can also pass the Element, Comment, ProcessingInstruction and
    Entity factory functions to look only for the specific element type.

    Passing multiple tags (or a sequence of tags) instead of a single tag
    will let the iterator return all elements matching any of these tags,
    in document order.
  summary: iter(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.iterancestors
  kind: callable
  ns: scrapy.http
  description: |-
    iterancestors(self, tag=None, *tags)

    Iterate over the ancestors of this element (from parent to parent).

    Can be restricted to find only elements with specific tags,
    see `iter`.
  summary: iterancestors(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.iterchildren
  kind: callable
  ns: scrapy.http
  description: |-
    iterchildren(self, tag=None, *tags, reversed=False)

    Iterate over the children of this element.

    As opposed to using normal iteration on this element, the returned
    elements can be reversed with the 'reversed' keyword and restricted
    to find only elements with specific tags, see `iter`.
  summary: iterchildren(self, tag=None, *tags, reversed=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: reversed
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: InputElement.iterdescendants
  kind: callable
  ns: scrapy.http
  description: |-
    iterdescendants(self, tag=None, *tags)

    Iterate over the descendants of this element in document order.

    As opposed to ``el.iter()``, this iterator does not yield the element
    itself.  The returned elements can be restricted to find only elements
    with specific tags, see `iter`.
  summary: iterdescendants(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.iterfind
  kind: callable
  ns: scrapy.http
  description: |-
    iterfind(self, path, namespaces=None)

    Iterates over all matching subelements, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: iterfind(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.iterlinks
  kind: method
  ns: scrapy.http
  description: |-
    Yield (element, attribute, link, pos), where attribute may be None
    (indicating the link is in the text).  ``pos`` is the position
    where the link occurs; often 0, but sometimes something else in
    the case of links in stylesheets or style tags.

    Note: <base href> is *not* taken into account in any way.  The
    link you get is exactly the link in the document.

    Note: multiple links inside of a single text string or
    attribute value are returned in reversed order.  This makes it
    possible to replace or delete them from the text string value
    based on their reported text positions.  Otherwise, a
    modification at one text position can change the positions of
    links reported later on.
  summary: Yield (element, attribute, link, pos), where attribute may be None
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.itersiblings
  kind: callable
  ns: scrapy.http
  description: |-
    itersiblings(self, tag=None, *tags, preceding=False)

    Iterate over the following or preceding siblings of this element.

    The direction is determined by the 'preceding' keyword which
    defaults to False, i.e. forward iteration over the following
    siblings.  When True, the iterator yields the preceding
    siblings in reverse document order, i.e. starting right before
    the current element and going backwards.

    Can be restricted to find only elements with specific tags,
    see `iter`.
  summary: itersiblings(self, tag=None, *tags, preceding=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: preceding
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: InputElement.itertext
  kind: callable
  ns: scrapy.http
  description: |-
    itertext(self, tag=None, *tags, with_tail=True)

    Iterates over the text content of a subtree.

    You can pass tag names to restrict text content to specific elements,
    see `iter`.

    You can set the ``with_tail`` keyword argument to ``False`` to skip
    over tail text.
  summary: itertext(self, tag=None, *tags, with_tail=True)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: with_tail
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: InputElement.keys
  kind: callable
  ns: scrapy.http
  description: |-
    keys(self)

    Gets a list of attribute names.  The names are returned in an
    arbitrary order (just like for an ordinary Python dictionary).
  summary: keys(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.label
  kind: property
  ns: scrapy.http
  description: Get or set any <label> element associated with this element.
  summary: Get or set any <label> element associated with this element
  signatures: null
  inherits_from: null
- name: InputElement.make_links_absolute
  kind: method
  ns: scrapy.http
  description: |-
    Make all links in the document absolute, given the
    ``base_url`` for the document (the full URL where the document
    came from), or if no ``base_url`` is given, then the ``.base_url``
    of the document.

    If ``resolve_base_href`` is true, then any ``<base href>``
    tags in the document are used *and* removed from the document.
    If it is false then any such tag is ignored.

    If ``handle_failures`` is None (default), a failure to process
    a URL will abort the processing.  If set to 'ignore', errors
    are ignored.  If set to 'discard', failing URLs will be removed.
  summary: Make all links in the document absolute, given the
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: base_url
    default: None
    rest: false
  - kind: positional
    name: resolve_base_href
    default: 'True'
    rest: false
  - kind: positional
    name: handle_failures
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.makeelement
  kind: callable
  ns: scrapy.http
  description: |-
    makeelement(self, _tag, attrib=None, nsmap=None, **_extra)

    Creates a new element associated with the same document.
  summary: makeelement(self, _tag, attrib=None, nsmap=None, **_extra)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _tag
    default: null
    rest: false
  - kind: positional
    name: attrib
    default: None
    rest: false
  - kind: positional
    name: nsmap
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.name
  kind: property
  ns: scrapy.http
  description: Get/set the name of the element
  summary: Get/set the name of the element
  signatures: null
  inherits_from: null
- name: InputElement.nsmap
  kind: property
  ns: scrapy.http
  description: |-
    Namespace prefix->URI mapping known in the context of this
    Element.  This includes all namespace declarations of the
    parents.

    Note that changing the returned dict has no effect on the Element.
  summary: Namespace prefix->URI mapping known in the context of this
  signatures: null
  inherits_from: null
- name: InputElement.prefix
  kind: property
  ns: scrapy.http
  description: "Namespace prefix or None.\n        "
  summary: Namespace prefix or None
  signatures: null
  inherits_from: null
- name: InputElement.remove
  kind: callable
  ns: scrapy.http
  description: |-
    remove(self, element)

    Removes a matching subelement. Unlike the find methods, this
    method compares elements based on identity, not on tag value
    or contents.
  summary: remove(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.replace
  kind: callable
  ns: scrapy.http
  description: |-
    replace(self, old_element, new_element)

    Replaces a subelement with the element passed as second argument.
  summary: replace(self, old_element, new_element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: old_element
    default: null
    rest: false
  - kind: positional
    name: new_element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.resolve_base_href
  kind: method
  ns: scrapy.http
  description: |-
    Find any ``<base href>`` tag in the document, and apply its
    values to all links found in the document.  Also remove the
    tag once it has been applied.

    If ``handle_failures`` is None (default), a failure to process
    a URL will abort the processing.  If set to 'ignore', errors
    are ignored.  If set to 'discard', failing URLs will be removed.
  summary: Find any ``<base href>`` tag in the document, and apply its
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: handle_failures
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.rewrite_links
  kind: method
  ns: scrapy.http
  description: |-
    Rewrite all the links in the document.  For each link
    ``link_repl_func(link)`` will be called, and the return value
    will replace the old link.

    Note that links may not be absolute (unless you first called
    ``make_links_absolute()``), and may be internal (e.g.,
    ``'#anchor'``).  They can also be values like
    ``'mailto:email'`` or ``'javascript:expr'``.

    If you give ``base_href`` then all links passed to
    ``link_repl_func()`` will take that into account.

    If the ``link_repl_func`` returns None, the attribute or
    tag text will be removed completely.
  summary: Rewrite all the links in the document
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: link_repl_func
    default: null
    rest: false
  - kind: positional
    name: resolve_base_href
    default: 'True'
    rest: false
  - kind: positional
    name: base_href
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.set
  kind: method
  ns: scrapy.http
  description: |-
    set(self, key, value=None)

    Sets an element attribute.  If no value is provided, or if the value is None,
    creates a 'boolean' attribute without value, e.g. "<form novalidate></form>"
    for ``form.set('novalidate')``.
  summary: set(self, key, value=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.sourceline
  kind: property
  ns: scrapy.http
  description: "Original line number as found by the parser or None if unknown.\n        "
  summary: Original line number as found by the parser or None if unknown
  signatures: null
  inherits_from: null
- name: InputElement.tag
  kind: property
  ns: scrapy.http
  description: "Element tag\n        "
  summary: Element tag
  signatures: null
  inherits_from: null
- name: InputElement.tail
  kind: property
  ns: scrapy.http
  description: |-
    Text after this element's end tag, but before the next sibling
    element's start tag. This is either a string or the value None, if
    there was no text.
  summary: Text after this element's end tag, but before the next sibling
  signatures: null
  inherits_from: null
- name: InputElement.text
  kind: property
  ns: scrapy.http
  description: |-
    Text before the first subelement. This is either a string or
    the value None, if there was no text.
  summary: Text before the first subelement
  signatures: null
  inherits_from: null
- name: InputElement.text_content
  kind: method
  ns: scrapy.http
  description: Return the text content of the tag (and the text in any children).
  summary: Return the text content of the tag (and the text in any children)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.type
  kind: property
  ns: scrapy.http
  description: Return the type of this element (using the type attribute).
  summary: Return the type of this element (using the type attribute)
  signatures: null
  inherits_from: null
- name: InputElement.value
  kind: property
  ns: scrapy.http
  description: |-
    Get/set the value of this element, using the ``value`` attribute.

    Also, if this is a checkbox and it has no value, this defaults
    to ``'on'``.  If it is a checkbox or radio that is not
    checked, this returns None.
  summary: Get/set the value of this element, using the ``value`` attribute
  signatures: null
  inherits_from: null
- name: InputElement.values
  kind: callable
  ns: scrapy.http
  description: |-
    values(self)

    Gets element attribute values as a sequence of strings.  The
    attributes are returned in an arbitrary order.
  summary: values(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: InputElement.xpath
  kind: callable
  ns: scrapy.http
  description: |-
    xpath(self, _path, namespaces=None, extensions=None, smart_strings=True, **_variables)

    Evaluate an xpath expression using the element as context node.
  summary: xpath(self, _path, namespaces=None, extensions=None, smart_strings=True, **_variables)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _path
    default: null
    rest: false
  - name: namespaces
    default: None
    rest: false
    kind: kw-only
  - name: extensions
    default: None
    rest: false
    kind: kw-only
  - name: smart_strings
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.http
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.http
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions
  kind: class
  ns: scrapy.http
  description: |-
    Represents all the selected options in a ``<select multiple>`` element.

    You can add to this set-like option to select an option, or remove
    to unselect the option.
  summary: Represents all the selected options in a ``<select multiple>`` element
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: select
    default: null
    rest: false
  - type: MultipleSelectOptions
  inherits_from:
  - <class 'lxml.html._setmixin.SetMixin'>
  - <class 'collections.abc.MutableSet'>
  - <class 'collections.abc.Set'>
  - <class 'collections.abc.Collection'>
  - <class 'collections.abc.Sized'>
  - <class 'collections.abc.Iterable'>
  - <class 'collections.abc.Container'>
- name: MultipleSelectOptions.add
  kind: method
  ns: scrapy.http
  description: Add an element.
  summary: Add an element
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.clear
  kind: method
  ns: scrapy.http
  description: This is slow (creates N new iterators!) but effective.
  summary: This is slow (creates N new iterators!) but effective
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.copy
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.difference
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.difference_update
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.discard
  kind: method
  ns: scrapy.http
  description: Remove an element.  Do not raise an exception if absent.
  summary: Remove an element
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.intersection
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.intersection_update
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.isdisjoint
  kind: method
  ns: scrapy.http
  description: Return True if two sets have a null intersection.
  summary: Return True if two sets have a null intersection
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.issubset
  kind: method
  ns: scrapy.http
  description: Return self<=value.
  summary: Return self<=value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.issuperset
  kind: method
  ns: scrapy.http
  description: Return self>=value.
  summary: Return self>=value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.options
  kind: property
  ns: scrapy.http
  description: Iterator of all the ``<option>`` elements.
  summary: Iterator of all the ``<option>`` elements
  signatures: null
  inherits_from: null
- name: MultipleSelectOptions.pop
  kind: method
  ns: scrapy.http
  description: Return the popped value.  Raise KeyError if empty.
  summary: Return the popped value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.remove
  kind: method
  ns: scrapy.http
  description: Remove an element. If not a member, raise a KeyError.
  summary: Remove an element
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.symmetric_difference
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.symmetric_difference_update
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.union
  kind: method
  ns: scrapy.http
  description: Return self|value.
  summary: Return self|value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MultipleSelectOptions.update
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.http
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement
  kind: class
  ns: scrapy.http
  description: |-
    ``<select>`` element.  You can get the name with ``.name``.

    ``.value`` will be the value of the selected option, unless this
    is a multi-select element (``<select multiple>``), in which case
    it will be a set-like object.  In either case ``.value_options``
    gives the possible values.

    The boolean attribute ``.multiple`` shows if this is a
    multi-select.
  summary: '``<select>`` element'
  signatures: null
  inherits_from:
  - <class 'lxml.html.InputMixin'>
  - <class 'lxml.html.HtmlElement'>
  - <class 'lxml.html.HtmlMixin'>
  - <class 'lxml.etree.ElementBase'>
  - <class 'lxml.etree._Element'>
- name: SelectElement.addnext
  kind: callable
  ns: scrapy.http
  description: |-
    addnext(self, element)

    Adds the element as a following sibling directly after this
    element.

    This is normally used to set a processing instruction or comment after
    the root node of a document.  Note that tail text is automatically
    discarded when adding at the root level.
  summary: addnext(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.addprevious
  kind: callable
  ns: scrapy.http
  description: |-
    addprevious(self, element)

    Adds the element as a preceding sibling directly before this
    element.

    This is normally used to set a processing instruction or comment
    before the root node of a document.  Note that tail text is
    automatically discarded when adding at the root level.
  summary: addprevious(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.append
  kind: callable
  ns: scrapy.http
  description: |-
    append(self, element)

    Adds a subelement to the end of this element.
  summary: append(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.attrib
  kind: property
  ns: scrapy.http
  description: |-
    Element attribute dictionary. Where possible, use get(), set(),
    keys(), values() and items() to access element attributes.
  summary: Element attribute dictionary
  signatures: null
  inherits_from: null
- name: SelectElement.base
  kind: property
  ns: scrapy.http
  description: |-
    The base URI of the Element (xml:base or HTML base URL).
    None if the base URI is unknown.

    Note that the value depends on the URL of the document that
    holds the Element if there is no xml:base attribute on the
    Element or its ancestors.

    Setting this property will set an xml:base attribute on the
    Element, regardless of the document type (XML or HTML).
  summary: The base URI of the Element (xml:base or HTML base URL)
  signatures: null
  inherits_from: null
- name: SelectElement.base_url
  kind: property
  ns: scrapy.http
  description: |-
    Returns the base URL, given when the page was parsed.

    Use with ``urlparse.urljoin(el.base_url, href)`` to get
    absolute URLs.
  summary: Returns the base URL, given when the page was parsed
  signatures: null
  inherits_from: null
- name: SelectElement.body
  kind: property
  ns: scrapy.http
  description: |-
    Return the <body> element.  Can be called from a child element
    to get the document's head.
  summary: Return the <body> element
  signatures: null
  inherits_from: null
- name: SelectElement.classes
  kind: property
  ns: scrapy.http
  description: A set-like wrapper around the 'class' attribute.
  summary: A set-like wrapper around the 'class' attribute
  signatures: null
  inherits_from: null
- name: SelectElement.clear
  kind: callable
  ns: scrapy.http
  description: |-
    clear(self, keep_tail=False)

    Resets an element.  This function removes all subelements, clears
    all attributes and sets the text and tail properties to None.

    Pass ``keep_tail=True`` to leave the tail text untouched.
  summary: clear(self, keep_tail=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: keep_tail
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.cssselect
  kind: method
  ns: scrapy.http
  description: |-
    Run the CSS expression on this element and its children,
    returning a list of the results.

    Equivalent to lxml.cssselect.CSSSelect(expr, translator='html')(self)
    -- note that pre-compiling the expression can provide a substantial
    speedup.
  summary: Run the CSS expression on this element and its children,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expr
    default: null
    rest: false
  - kind: positional
    name: translator
    default: html
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.drop_tag
  kind: method
  ns: scrapy.http
  description: |-
    Remove the tag, but not its children or text.  The children and text
    are merged into the parent.

    Example::

        >>> h = fragment_fromstring('<div>Hello <b>World!</b></div>')
        >>> h.find('.//b').drop_tag()
        >>> print(tostring(h, encoding='unicode'))
        <div>Hello World!</div>
  summary: Remove the tag, but not its children or text
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.drop_tree
  kind: method
  ns: scrapy.http
  description: |-
    Removes this element from the tree, including its children and
    text.  The tail text is joined to the previous element or
    parent.
  summary: Removes this element from the tree, including its children and
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.extend
  kind: callable
  ns: scrapy.http
  description: |-
    extend(self, elements)

    Extends the current children by the elements in the iterable.
  summary: extend(self, elements)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: elements
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.find
  kind: callable
  ns: scrapy.http
  description: |-
    find(self, path, namespaces=None)

    Finds the first matching subelement, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: find(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.find_class
  kind: method
  ns: scrapy.http
  description: Find any elements with the given class name.
  summary: Find any elements with the given class name
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: class_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.find_rel_links
  kind: method
  ns: scrapy.http
  description: Find any links like ``<a rel="{rel}">...</a>``; returns a list of elements.
  summary: Find any links like ``<a rel="{rel}">
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: rel
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.findall
  kind: callable
  ns: scrapy.http
  description: |-
    findall(self, path, namespaces=None)

    Finds all matching subelements, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: findall(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.findtext
  kind: callable
  ns: scrapy.http
  description: |-
    findtext(self, path, default=None, namespaces=None)

    Finds text for the first matching subelement, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: findtext(self, path, default=None, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.forms
  kind: property
  ns: scrapy.http
  description: Return a list of all the forms
  summary: Return a list of all the forms
  signatures: null
  inherits_from: null
- name: SelectElement.get
  kind: callable
  ns: scrapy.http
  description: |-
    get(self, key, default=None)

    Gets an element attribute.
  summary: get(self, key, default=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.get_element_by_id
  kind: method
  ns: scrapy.http
  description: |-
    Get the first element in a document with the given id.  If none is
    found, return the default argument if provided or raise KeyError
    otherwise.

    Note that there can be more than one element with the same id,
    and this isn't uncommon in HTML documents found in the wild.
    Browsers return only the first match, and this function does
    the same.
  summary: Get the first element in a document with the given id
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: id
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.getchildren
  kind: callable
  ns: scrapy.http
  description: |-
    getchildren(self)

    Returns all direct children.  The elements are returned in document
    order.

    :deprecated: Note that this method has been deprecated as of
      ElementTree 1.3 and lxml 2.0.  New code should use
      ``list(element)`` or simply iterate over elements.
  summary: getchildren(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.getiterator
  kind: callable
  ns: scrapy.http
  description: |-
    getiterator(self, tag=None, *tags)

    Returns a sequence or iterator of all elements in the subtree in
    document order (depth first pre-order), starting with this
    element.

    Can be restricted to find only elements with specific tags,
    see `iter`.

    :deprecated: Note that this method is deprecated as of
      ElementTree 1.3 and lxml 2.0.  It returns an iterator in
      lxml, which diverges from the original ElementTree
      behaviour.  If you want an efficient iterator, use the
      ``element.iter()`` method instead.  You should only use this
      method in new code if you require backwards compatibility
      with older versions of lxml or ElementTree.
  summary: getiterator(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.getnext
  kind: callable
  ns: scrapy.http
  description: |-
    getnext(self)

    Returns the following sibling of this element or None.
  summary: getnext(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.getparent
  kind: callable
  ns: scrapy.http
  description: |-
    getparent(self)

    Returns the parent of this element or None for the root element.
  summary: getparent(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.getprevious
  kind: callable
  ns: scrapy.http
  description: |-
    getprevious(self)

    Returns the preceding sibling of this element or None.
  summary: getprevious(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.getroottree
  kind: callable
  ns: scrapy.http
  description: |-
    getroottree(self)

    Return an ElementTree for the root node of the document that
    contains this element.

    This is the same as following element.getparent() up the tree until it
    returns None (for the root element) and then build an ElementTree for
    the last parent that was returned.
  summary: getroottree(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.head
  kind: property
  ns: scrapy.http
  description: |-
    Returns the <head> element.  Can be called from a child
    element to get the document's head.
  summary: Returns the <head> element
  signatures: null
  inherits_from: null
- name: SelectElement.index
  kind: callable
  ns: scrapy.http
  description: |-
    index(self, child, start=None, stop=None)

    Find the position of the child within the parent.

    This method is not part of the original ElementTree API.
  summary: index(self, child, start=None, stop=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: child
    default: null
    rest: false
  - kind: positional
    name: start
    default: None
    rest: false
  - kind: positional
    name: stop
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.insert
  kind: callable
  ns: scrapy.http
  description: |-
    insert(self, index, element)

    Inserts a subelement at the given position in this element
  summary: insert(self, index, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: index
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.items
  kind: callable
  ns: scrapy.http
  description: |-
    items(self)

    Gets element attributes, as a sequence. The attributes are returned in
    an arbitrary order.
  summary: items(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.iter
  kind: callable
  ns: scrapy.http
  description: |-
    iter(self, tag=None, *tags)

    Iterate over all elements in the subtree in document order (depth
    first pre-order), starting with this element.

    Can be restricted to find only elements with specific tags:
    pass ``"{ns}localname"`` as tag. Either or both of ``ns`` and
    ``localname`` can be ``*`` for a wildcard; ``ns`` can be empty
    for no namespace. ``"localname"`` is equivalent to ``"{}localname"``
    (i.e. no namespace) but ``"*"`` is ``"{*}*"`` (any or no namespace),
    not ``"{}*"``.

    You can also pass the Element, Comment, ProcessingInstruction and
    Entity factory functions to look only for the specific element type.

    Passing multiple tags (or a sequence of tags) instead of a single tag
    will let the iterator return all elements matching any of these tags,
    in document order.
  summary: iter(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.iterancestors
  kind: callable
  ns: scrapy.http
  description: |-
    iterancestors(self, tag=None, *tags)

    Iterate over the ancestors of this element (from parent to parent).

    Can be restricted to find only elements with specific tags,
    see `iter`.
  summary: iterancestors(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.iterchildren
  kind: callable
  ns: scrapy.http
  description: |-
    iterchildren(self, tag=None, *tags, reversed=False)

    Iterate over the children of this element.

    As opposed to using normal iteration on this element, the returned
    elements can be reversed with the 'reversed' keyword and restricted
    to find only elements with specific tags, see `iter`.
  summary: iterchildren(self, tag=None, *tags, reversed=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: reversed
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: SelectElement.iterdescendants
  kind: callable
  ns: scrapy.http
  description: |-
    iterdescendants(self, tag=None, *tags)

    Iterate over the descendants of this element in document order.

    As opposed to ``el.iter()``, this iterator does not yield the element
    itself.  The returned elements can be restricted to find only elements
    with specific tags, see `iter`.
  summary: iterdescendants(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.iterfind
  kind: callable
  ns: scrapy.http
  description: |-
    iterfind(self, path, namespaces=None)

    Iterates over all matching subelements, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: iterfind(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.iterlinks
  kind: method
  ns: scrapy.http
  description: |-
    Yield (element, attribute, link, pos), where attribute may be None
    (indicating the link is in the text).  ``pos`` is the position
    where the link occurs; often 0, but sometimes something else in
    the case of links in stylesheets or style tags.

    Note: <base href> is *not* taken into account in any way.  The
    link you get is exactly the link in the document.

    Note: multiple links inside of a single text string or
    attribute value are returned in reversed order.  This makes it
    possible to replace or delete them from the text string value
    based on their reported text positions.  Otherwise, a
    modification at one text position can change the positions of
    links reported later on.
  summary: Yield (element, attribute, link, pos), where attribute may be None
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.itersiblings
  kind: callable
  ns: scrapy.http
  description: |-
    itersiblings(self, tag=None, *tags, preceding=False)

    Iterate over the following or preceding siblings of this element.

    The direction is determined by the 'preceding' keyword which
    defaults to False, i.e. forward iteration over the following
    siblings.  When True, the iterator yields the preceding
    siblings in reverse document order, i.e. starting right before
    the current element and going backwards.

    Can be restricted to find only elements with specific tags,
    see `iter`.
  summary: itersiblings(self, tag=None, *tags, preceding=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: preceding
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: SelectElement.itertext
  kind: callable
  ns: scrapy.http
  description: |-
    itertext(self, tag=None, *tags, with_tail=True)

    Iterates over the text content of a subtree.

    You can pass tag names to restrict text content to specific elements,
    see `iter`.

    You can set the ``with_tail`` keyword argument to ``False`` to skip
    over tail text.
  summary: itertext(self, tag=None, *tags, with_tail=True)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: with_tail
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: SelectElement.keys
  kind: callable
  ns: scrapy.http
  description: |-
    keys(self)

    Gets a list of attribute names.  The names are returned in an
    arbitrary order (just like for an ordinary Python dictionary).
  summary: keys(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.label
  kind: property
  ns: scrapy.http
  description: Get or set any <label> element associated with this element.
  summary: Get or set any <label> element associated with this element
  signatures: null
  inherits_from: null
- name: SelectElement.make_links_absolute
  kind: method
  ns: scrapy.http
  description: |-
    Make all links in the document absolute, given the
    ``base_url`` for the document (the full URL where the document
    came from), or if no ``base_url`` is given, then the ``.base_url``
    of the document.

    If ``resolve_base_href`` is true, then any ``<base href>``
    tags in the document are used *and* removed from the document.
    If it is false then any such tag is ignored.

    If ``handle_failures`` is None (default), a failure to process
    a URL will abort the processing.  If set to 'ignore', errors
    are ignored.  If set to 'discard', failing URLs will be removed.
  summary: Make all links in the document absolute, given the
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: base_url
    default: None
    rest: false
  - kind: positional
    name: resolve_base_href
    default: 'True'
    rest: false
  - kind: positional
    name: handle_failures
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.makeelement
  kind: callable
  ns: scrapy.http
  description: |-
    makeelement(self, _tag, attrib=None, nsmap=None, **_extra)

    Creates a new element associated with the same document.
  summary: makeelement(self, _tag, attrib=None, nsmap=None, **_extra)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _tag
    default: null
    rest: false
  - kind: positional
    name: attrib
    default: None
    rest: false
  - kind: positional
    name: nsmap
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.multiple
  kind: property
  ns: scrapy.http
  description: 'Boolean attribute: is there a ``multiple`` attribute on this element.'
  summary: 'Boolean attribute: is there a ``multiple`` attribute on this element'
  signatures: null
  inherits_from: null
- name: SelectElement.name
  kind: property
  ns: scrapy.http
  description: Get/set the name of the element
  summary: Get/set the name of the element
  signatures: null
  inherits_from: null
- name: SelectElement.nsmap
  kind: property
  ns: scrapy.http
  description: |-
    Namespace prefix->URI mapping known in the context of this
    Element.  This includes all namespace declarations of the
    parents.

    Note that changing the returned dict has no effect on the Element.
  summary: Namespace prefix->URI mapping known in the context of this
  signatures: null
  inherits_from: null
- name: SelectElement.prefix
  kind: property
  ns: scrapy.http
  description: "Namespace prefix or None.\n        "
  summary: Namespace prefix or None
  signatures: null
  inherits_from: null
- name: SelectElement.remove
  kind: callable
  ns: scrapy.http
  description: |-
    remove(self, element)

    Removes a matching subelement. Unlike the find methods, this
    method compares elements based on identity, not on tag value
    or contents.
  summary: remove(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.replace
  kind: callable
  ns: scrapy.http
  description: |-
    replace(self, old_element, new_element)

    Replaces a subelement with the element passed as second argument.
  summary: replace(self, old_element, new_element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: old_element
    default: null
    rest: false
  - kind: positional
    name: new_element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.resolve_base_href
  kind: method
  ns: scrapy.http
  description: |-
    Find any ``<base href>`` tag in the document, and apply its
    values to all links found in the document.  Also remove the
    tag once it has been applied.

    If ``handle_failures`` is None (default), a failure to process
    a URL will abort the processing.  If set to 'ignore', errors
    are ignored.  If set to 'discard', failing URLs will be removed.
  summary: Find any ``<base href>`` tag in the document, and apply its
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: handle_failures
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.rewrite_links
  kind: method
  ns: scrapy.http
  description: |-
    Rewrite all the links in the document.  For each link
    ``link_repl_func(link)`` will be called, and the return value
    will replace the old link.

    Note that links may not be absolute (unless you first called
    ``make_links_absolute()``), and may be internal (e.g.,
    ``'#anchor'``).  They can also be values like
    ``'mailto:email'`` or ``'javascript:expr'``.

    If you give ``base_href`` then all links passed to
    ``link_repl_func()`` will take that into account.

    If the ``link_repl_func`` returns None, the attribute or
    tag text will be removed completely.
  summary: Rewrite all the links in the document
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: link_repl_func
    default: null
    rest: false
  - kind: positional
    name: resolve_base_href
    default: 'True'
    rest: false
  - kind: positional
    name: base_href
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.set
  kind: method
  ns: scrapy.http
  description: |-
    set(self, key, value=None)

    Sets an element attribute.  If no value is provided, or if the value is None,
    creates a 'boolean' attribute without value, e.g. "<form novalidate></form>"
    for ``form.set('novalidate')``.
  summary: set(self, key, value=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.sourceline
  kind: property
  ns: scrapy.http
  description: "Original line number as found by the parser or None if unknown.\n        "
  summary: Original line number as found by the parser or None if unknown
  signatures: null
  inherits_from: null
- name: SelectElement.tag
  kind: property
  ns: scrapy.http
  description: "Element tag\n        "
  summary: Element tag
  signatures: null
  inherits_from: null
- name: SelectElement.tail
  kind: property
  ns: scrapy.http
  description: |-
    Text after this element's end tag, but before the next sibling
    element's start tag. This is either a string or the value None, if
    there was no text.
  summary: Text after this element's end tag, but before the next sibling
  signatures: null
  inherits_from: null
- name: SelectElement.text
  kind: property
  ns: scrapy.http
  description: |-
    Text before the first subelement. This is either a string or
    the value None, if there was no text.
  summary: Text before the first subelement
  signatures: null
  inherits_from: null
- name: SelectElement.text_content
  kind: method
  ns: scrapy.http
  description: Return the text content of the tag (and the text in any children).
  summary: Return the text content of the tag (and the text in any children)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.value
  kind: property
  ns: scrapy.http
  description: |-
    Get/set the value of this select (the selected option).

    If this is a multi-select, this is a set-like object that
    represents all the selected options.
  summary: Get/set the value of this select (the selected option)
  signatures: null
  inherits_from: null
- name: SelectElement.value_options
  kind: property
  ns: scrapy.http
  description: |-
    All the possible values this select can have (the ``value``
    attribute of all the ``<option>`` elements.
  summary: All the possible values this select can have (the ``value``
  signatures: null
  inherits_from: null
- name: SelectElement.values
  kind: callable
  ns: scrapy.http
  description: |-
    values(self)

    Gets element attribute values as a sequence of strings.  The
    attributes are returned in an arbitrary order.
  summary: values(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SelectElement.xpath
  kind: callable
  ns: scrapy.http
  description: |-
    xpath(self, _path, namespaces=None, extensions=None, smart_strings=True, **_variables)

    Evaluate an xpath expression using the element as context node.
  summary: xpath(self, _path, namespaces=None, extensions=None, smart_strings=True, **_variables)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _path
    default: null
    rest: false
  - name: namespaces
    default: None
    rest: false
    kind: kw-only
  - name: extensions
    default: None
    rest: false
    kind: kw-only
  - name: smart_strings
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: TextareaElement
  kind: class
  ns: scrapy.http
  description: |-
    ``<textarea>`` element.  You can get the name with ``.name`` and
    get/set the value with ``.value``
  summary: '``<textarea>`` element'
  signatures: null
  inherits_from:
  - <class 'lxml.html.InputMixin'>
  - <class 'lxml.html.HtmlElement'>
  - <class 'lxml.html.HtmlMixin'>
  - <class 'lxml.etree.ElementBase'>
  - <class 'lxml.etree._Element'>
- name: TextareaElement.addnext
  kind: callable
  ns: scrapy.http
  description: |-
    addnext(self, element)

    Adds the element as a following sibling directly after this
    element.

    This is normally used to set a processing instruction or comment after
    the root node of a document.  Note that tail text is automatically
    discarded when adding at the root level.
  summary: addnext(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.addprevious
  kind: callable
  ns: scrapy.http
  description: |-
    addprevious(self, element)

    Adds the element as a preceding sibling directly before this
    element.

    This is normally used to set a processing instruction or comment
    before the root node of a document.  Note that tail text is
    automatically discarded when adding at the root level.
  summary: addprevious(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.append
  kind: callable
  ns: scrapy.http
  description: |-
    append(self, element)

    Adds a subelement to the end of this element.
  summary: append(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.attrib
  kind: property
  ns: scrapy.http
  description: |-
    Element attribute dictionary. Where possible, use get(), set(),
    keys(), values() and items() to access element attributes.
  summary: Element attribute dictionary
  signatures: null
  inherits_from: null
- name: TextareaElement.base
  kind: property
  ns: scrapy.http
  description: |-
    The base URI of the Element (xml:base or HTML base URL).
    None if the base URI is unknown.

    Note that the value depends on the URL of the document that
    holds the Element if there is no xml:base attribute on the
    Element or its ancestors.

    Setting this property will set an xml:base attribute on the
    Element, regardless of the document type (XML or HTML).
  summary: The base URI of the Element (xml:base or HTML base URL)
  signatures: null
  inherits_from: null
- name: TextareaElement.base_url
  kind: property
  ns: scrapy.http
  description: |-
    Returns the base URL, given when the page was parsed.

    Use with ``urlparse.urljoin(el.base_url, href)`` to get
    absolute URLs.
  summary: Returns the base URL, given when the page was parsed
  signatures: null
  inherits_from: null
- name: TextareaElement.body
  kind: property
  ns: scrapy.http
  description: |-
    Return the <body> element.  Can be called from a child element
    to get the document's head.
  summary: Return the <body> element
  signatures: null
  inherits_from: null
- name: TextareaElement.classes
  kind: property
  ns: scrapy.http
  description: A set-like wrapper around the 'class' attribute.
  summary: A set-like wrapper around the 'class' attribute
  signatures: null
  inherits_from: null
- name: TextareaElement.clear
  kind: callable
  ns: scrapy.http
  description: |-
    clear(self, keep_tail=False)

    Resets an element.  This function removes all subelements, clears
    all attributes and sets the text and tail properties to None.

    Pass ``keep_tail=True`` to leave the tail text untouched.
  summary: clear(self, keep_tail=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: keep_tail
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.cssselect
  kind: method
  ns: scrapy.http
  description: |-
    Run the CSS expression on this element and its children,
    returning a list of the results.

    Equivalent to lxml.cssselect.CSSSelect(expr, translator='html')(self)
    -- note that pre-compiling the expression can provide a substantial
    speedup.
  summary: Run the CSS expression on this element and its children,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: expr
    default: null
    rest: false
  - kind: positional
    name: translator
    default: html
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.drop_tag
  kind: method
  ns: scrapy.http
  description: |-
    Remove the tag, but not its children or text.  The children and text
    are merged into the parent.

    Example::

        >>> h = fragment_fromstring('<div>Hello <b>World!</b></div>')
        >>> h.find('.//b').drop_tag()
        >>> print(tostring(h, encoding='unicode'))
        <div>Hello World!</div>
  summary: Remove the tag, but not its children or text
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.drop_tree
  kind: method
  ns: scrapy.http
  description: |-
    Removes this element from the tree, including its children and
    text.  The tail text is joined to the previous element or
    parent.
  summary: Removes this element from the tree, including its children and
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.extend
  kind: callable
  ns: scrapy.http
  description: |-
    extend(self, elements)

    Extends the current children by the elements in the iterable.
  summary: extend(self, elements)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: elements
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.find
  kind: callable
  ns: scrapy.http
  description: |-
    find(self, path, namespaces=None)

    Finds the first matching subelement, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: find(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.find_class
  kind: method
  ns: scrapy.http
  description: Find any elements with the given class name.
  summary: Find any elements with the given class name
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: class_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.find_rel_links
  kind: method
  ns: scrapy.http
  description: Find any links like ``<a rel="{rel}">...</a>``; returns a list of elements.
  summary: Find any links like ``<a rel="{rel}">
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: rel
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.findall
  kind: callable
  ns: scrapy.http
  description: |-
    findall(self, path, namespaces=None)

    Finds all matching subelements, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: findall(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.findtext
  kind: callable
  ns: scrapy.http
  description: |-
    findtext(self, path, default=None, namespaces=None)

    Finds text for the first matching subelement, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: findtext(self, path, default=None, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.forms
  kind: property
  ns: scrapy.http
  description: Return a list of all the forms
  summary: Return a list of all the forms
  signatures: null
  inherits_from: null
- name: TextareaElement.get
  kind: callable
  ns: scrapy.http
  description: |-
    get(self, key, default=None)

    Gets an element attribute.
  summary: get(self, key, default=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.get_element_by_id
  kind: method
  ns: scrapy.http
  description: |-
    Get the first element in a document with the given id.  If none is
    found, return the default argument if provided or raise KeyError
    otherwise.

    Note that there can be more than one element with the same id,
    and this isn't uncommon in HTML documents found in the wild.
    Browsers return only the first match, and this function does
    the same.
  summary: Get the first element in a document with the given id
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: id
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.getchildren
  kind: callable
  ns: scrapy.http
  description: |-
    getchildren(self)

    Returns all direct children.  The elements are returned in document
    order.

    :deprecated: Note that this method has been deprecated as of
      ElementTree 1.3 and lxml 2.0.  New code should use
      ``list(element)`` or simply iterate over elements.
  summary: getchildren(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.getiterator
  kind: callable
  ns: scrapy.http
  description: |-
    getiterator(self, tag=None, *tags)

    Returns a sequence or iterator of all elements in the subtree in
    document order (depth first pre-order), starting with this
    element.

    Can be restricted to find only elements with specific tags,
    see `iter`.

    :deprecated: Note that this method is deprecated as of
      ElementTree 1.3 and lxml 2.0.  It returns an iterator in
      lxml, which diverges from the original ElementTree
      behaviour.  If you want an efficient iterator, use the
      ``element.iter()`` method instead.  You should only use this
      method in new code if you require backwards compatibility
      with older versions of lxml or ElementTree.
  summary: getiterator(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.getnext
  kind: callable
  ns: scrapy.http
  description: |-
    getnext(self)

    Returns the following sibling of this element or None.
  summary: getnext(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.getparent
  kind: callable
  ns: scrapy.http
  description: |-
    getparent(self)

    Returns the parent of this element or None for the root element.
  summary: getparent(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.getprevious
  kind: callable
  ns: scrapy.http
  description: |-
    getprevious(self)

    Returns the preceding sibling of this element or None.
  summary: getprevious(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.getroottree
  kind: callable
  ns: scrapy.http
  description: |-
    getroottree(self)

    Return an ElementTree for the root node of the document that
    contains this element.

    This is the same as following element.getparent() up the tree until it
    returns None (for the root element) and then build an ElementTree for
    the last parent that was returned.
  summary: getroottree(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.head
  kind: property
  ns: scrapy.http
  description: |-
    Returns the <head> element.  Can be called from a child
    element to get the document's head.
  summary: Returns the <head> element
  signatures: null
  inherits_from: null
- name: TextareaElement.index
  kind: callable
  ns: scrapy.http
  description: |-
    index(self, child, start=None, stop=None)

    Find the position of the child within the parent.

    This method is not part of the original ElementTree API.
  summary: index(self, child, start=None, stop=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: child
    default: null
    rest: false
  - kind: positional
    name: start
    default: None
    rest: false
  - kind: positional
    name: stop
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.insert
  kind: callable
  ns: scrapy.http
  description: |-
    insert(self, index, element)

    Inserts a subelement at the given position in this element
  summary: insert(self, index, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: index
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.items
  kind: callable
  ns: scrapy.http
  description: |-
    items(self)

    Gets element attributes, as a sequence. The attributes are returned in
    an arbitrary order.
  summary: items(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.iter
  kind: callable
  ns: scrapy.http
  description: |-
    iter(self, tag=None, *tags)

    Iterate over all elements in the subtree in document order (depth
    first pre-order), starting with this element.

    Can be restricted to find only elements with specific tags:
    pass ``"{ns}localname"`` as tag. Either or both of ``ns`` and
    ``localname`` can be ``*`` for a wildcard; ``ns`` can be empty
    for no namespace. ``"localname"`` is equivalent to ``"{}localname"``
    (i.e. no namespace) but ``"*"`` is ``"{*}*"`` (any or no namespace),
    not ``"{}*"``.

    You can also pass the Element, Comment, ProcessingInstruction and
    Entity factory functions to look only for the specific element type.

    Passing multiple tags (or a sequence of tags) instead of a single tag
    will let the iterator return all elements matching any of these tags,
    in document order.
  summary: iter(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.iterancestors
  kind: callable
  ns: scrapy.http
  description: |-
    iterancestors(self, tag=None, *tags)

    Iterate over the ancestors of this element (from parent to parent).

    Can be restricted to find only elements with specific tags,
    see `iter`.
  summary: iterancestors(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.iterchildren
  kind: callable
  ns: scrapy.http
  description: |-
    iterchildren(self, tag=None, *tags, reversed=False)

    Iterate over the children of this element.

    As opposed to using normal iteration on this element, the returned
    elements can be reversed with the 'reversed' keyword and restricted
    to find only elements with specific tags, see `iter`.
  summary: iterchildren(self, tag=None, *tags, reversed=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: reversed
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: TextareaElement.iterdescendants
  kind: callable
  ns: scrapy.http
  description: |-
    iterdescendants(self, tag=None, *tags)

    Iterate over the descendants of this element in document order.

    As opposed to ``el.iter()``, this iterator does not yield the element
    itself.  The returned elements can be restricted to find only elements
    with specific tags, see `iter`.
  summary: iterdescendants(self, tag=None, *tags)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.iterfind
  kind: callable
  ns: scrapy.http
  description: |-
    iterfind(self, path, namespaces=None)

    Iterates over all matching subelements, by tag name or path.

    The optional ``namespaces`` argument accepts a
    prefix-to-namespace mapping that allows the usage of XPath
    prefixes in the path expression.
  summary: iterfind(self, path, namespaces=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: namespaces
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.iterlinks
  kind: method
  ns: scrapy.http
  description: |-
    Yield (element, attribute, link, pos), where attribute may be None
    (indicating the link is in the text).  ``pos`` is the position
    where the link occurs; often 0, but sometimes something else in
    the case of links in stylesheets or style tags.

    Note: <base href> is *not* taken into account in any way.  The
    link you get is exactly the link in the document.

    Note: multiple links inside of a single text string or
    attribute value are returned in reversed order.  This makes it
    possible to replace or delete them from the text string value
    based on their reported text positions.  Otherwise, a
    modification at one text position can change the positions of
    links reported later on.
  summary: Yield (element, attribute, link, pos), where attribute may be None
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.itersiblings
  kind: callable
  ns: scrapy.http
  description: |-
    itersiblings(self, tag=None, *tags, preceding=False)

    Iterate over the following or preceding siblings of this element.

    The direction is determined by the 'preceding' keyword which
    defaults to False, i.e. forward iteration over the following
    siblings.  When True, the iterator yields the preceding
    siblings in reverse document order, i.e. starting right before
    the current element and going backwards.

    Can be restricted to find only elements with specific tags,
    see `iter`.
  summary: itersiblings(self, tag=None, *tags, preceding=False)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: preceding
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: TextareaElement.itertext
  kind: callable
  ns: scrapy.http
  description: |-
    itertext(self, tag=None, *tags, with_tail=True)

    Iterates over the text content of a subtree.

    You can pass tag names to restrict text content to specific elements,
    see `iter`.

    You can set the ``with_tail`` keyword argument to ``False`` to skip
    over tail text.
  summary: itertext(self, tag=None, *tags, with_tail=True)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: None
    rest: false
  - name: with_tail
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: TextareaElement.keys
  kind: callable
  ns: scrapy.http
  description: |-
    keys(self)

    Gets a list of attribute names.  The names are returned in an
    arbitrary order (just like for an ordinary Python dictionary).
  summary: keys(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.label
  kind: property
  ns: scrapy.http
  description: Get or set any <label> element associated with this element.
  summary: Get or set any <label> element associated with this element
  signatures: null
  inherits_from: null
- name: TextareaElement.make_links_absolute
  kind: method
  ns: scrapy.http
  description: |-
    Make all links in the document absolute, given the
    ``base_url`` for the document (the full URL where the document
    came from), or if no ``base_url`` is given, then the ``.base_url``
    of the document.

    If ``resolve_base_href`` is true, then any ``<base href>``
    tags in the document are used *and* removed from the document.
    If it is false then any such tag is ignored.

    If ``handle_failures`` is None (default), a failure to process
    a URL will abort the processing.  If set to 'ignore', errors
    are ignored.  If set to 'discard', failing URLs will be removed.
  summary: Make all links in the document absolute, given the
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: base_url
    default: None
    rest: false
  - kind: positional
    name: resolve_base_href
    default: 'True'
    rest: false
  - kind: positional
    name: handle_failures
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.makeelement
  kind: callable
  ns: scrapy.http
  description: |-
    makeelement(self, _tag, attrib=None, nsmap=None, **_extra)

    Creates a new element associated with the same document.
  summary: makeelement(self, _tag, attrib=None, nsmap=None, **_extra)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _tag
    default: null
    rest: false
  - kind: positional
    name: attrib
    default: None
    rest: false
  - kind: positional
    name: nsmap
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.name
  kind: property
  ns: scrapy.http
  description: Get/set the name of the element
  summary: Get/set the name of the element
  signatures: null
  inherits_from: null
- name: TextareaElement.nsmap
  kind: property
  ns: scrapy.http
  description: |-
    Namespace prefix->URI mapping known in the context of this
    Element.  This includes all namespace declarations of the
    parents.

    Note that changing the returned dict has no effect on the Element.
  summary: Namespace prefix->URI mapping known in the context of this
  signatures: null
  inherits_from: null
- name: TextareaElement.prefix
  kind: property
  ns: scrapy.http
  description: "Namespace prefix or None.\n        "
  summary: Namespace prefix or None
  signatures: null
  inherits_from: null
- name: TextareaElement.remove
  kind: callable
  ns: scrapy.http
  description: |-
    remove(self, element)

    Removes a matching subelement. Unlike the find methods, this
    method compares elements based on identity, not on tag value
    or contents.
  summary: remove(self, element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.replace
  kind: callable
  ns: scrapy.http
  description: |-
    replace(self, old_element, new_element)

    Replaces a subelement with the element passed as second argument.
  summary: replace(self, old_element, new_element)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: old_element
    default: null
    rest: false
  - kind: positional
    name: new_element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.resolve_base_href
  kind: method
  ns: scrapy.http
  description: |-
    Find any ``<base href>`` tag in the document, and apply its
    values to all links found in the document.  Also remove the
    tag once it has been applied.

    If ``handle_failures`` is None (default), a failure to process
    a URL will abort the processing.  If set to 'ignore', errors
    are ignored.  If set to 'discard', failing URLs will be removed.
  summary: Find any ``<base href>`` tag in the document, and apply its
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: handle_failures
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.rewrite_links
  kind: method
  ns: scrapy.http
  description: |-
    Rewrite all the links in the document.  For each link
    ``link_repl_func(link)`` will be called, and the return value
    will replace the old link.

    Note that links may not be absolute (unless you first called
    ``make_links_absolute()``), and may be internal (e.g.,
    ``'#anchor'``).  They can also be values like
    ``'mailto:email'`` or ``'javascript:expr'``.

    If you give ``base_href`` then all links passed to
    ``link_repl_func()`` will take that into account.

    If the ``link_repl_func`` returns None, the attribute or
    tag text will be removed completely.
  summary: Rewrite all the links in the document
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: link_repl_func
    default: null
    rest: false
  - kind: positional
    name: resolve_base_href
    default: 'True'
    rest: false
  - kind: positional
    name: base_href
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.set
  kind: method
  ns: scrapy.http
  description: |-
    set(self, key, value=None)

    Sets an element attribute.  If no value is provided, or if the value is None,
    creates a 'boolean' attribute without value, e.g. "<form novalidate></form>"
    for ``form.set('novalidate')``.
  summary: set(self, key, value=None)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.sourceline
  kind: property
  ns: scrapy.http
  description: "Original line number as found by the parser or None if unknown.\n        "
  summary: Original line number as found by the parser or None if unknown
  signatures: null
  inherits_from: null
- name: TextareaElement.tag
  kind: property
  ns: scrapy.http
  description: "Element tag\n        "
  summary: Element tag
  signatures: null
  inherits_from: null
- name: TextareaElement.tail
  kind: property
  ns: scrapy.http
  description: |-
    Text after this element's end tag, but before the next sibling
    element's start tag. This is either a string or the value None, if
    there was no text.
  summary: Text after this element's end tag, but before the next sibling
  signatures: null
  inherits_from: null
- name: TextareaElement.text
  kind: property
  ns: scrapy.http
  description: |-
    Text before the first subelement. This is either a string or
    the value None, if there was no text.
  summary: Text before the first subelement
  signatures: null
  inherits_from: null
- name: TextareaElement.text_content
  kind: method
  ns: scrapy.http
  description: Return the text content of the tag (and the text in any children).
  summary: Return the text content of the tag (and the text in any children)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.value
  kind: property
  ns: scrapy.http
  description: Get/set the value (which is the contents of this element)
  summary: Get/set the value (which is the contents of this element)
  signatures: null
  inherits_from: null
- name: TextareaElement.values
  kind: callable
  ns: scrapy.http
  description: |-
    values(self)

    Gets element attribute values as a sequence of strings.  The
    attributes are returned in an arbitrary order.
  summary: values(self)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TextareaElement.xpath
  kind: callable
  ns: scrapy.http
  description: |-
    xpath(self, _path, namespaces=None, extensions=None, smart_strings=True, **_variables)

    Evaluate an xpath expression using the element as context node.
  summary: xpath(self, _path, namespaces=None, extensions=None, smart_strings=True, **_variables)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _path
    default: null
    rest: false
  - name: namespaces
    default: None
    rest: false
    kind: kw-only
  - name: extensions
    default: None
    rest: false
    kind: kw-only
  - name: smart_strings
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.http
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.http
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.http
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.http
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: is_listlike
  kind: function
  ns: scrapy.http
  description: |-
    >>> is_listlike("foo")
    False
    >>> is_listlike(5)
    False
    >>> is_listlike(b"foo")
    False
    >>> is_listlike([b"foo"])
    True
    >>> is_listlike((b"foo",))
    True
    >>> is_listlike({})
    True
    >>> is_listlike(set())
    True
    >>> is_listlike((x for x in range(3)))
    True
    >>> is_listlike(range(5))
    True
  summary: '>>> is_listlike("foo")'
  signatures:
  - kind: positional
    name: x
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: strip_html5_whitespace
  kind: function
  ns: scrapy.http
  description: |-
    Strip all leading and trailing space characters (as defined in
    https://www.w3.org/TR/html5/infrastructure.html#space-character).

    Such stripping is useful e.g. for processing HTML element attributes which
    contain URLs, like ``href``, ``src`` or form ``action`` - HTML5 standard
    defines them as "valid URL potentially surrounded by spaces"
    or "valid non-empty URL potentially surrounded by spaces".

    >>> strip_html5_whitespace(' hello\n')
    'hello'
  summary: Strip all leading and trailing space characters (as defined in
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: to_bytes
  kind: function
  ns: scrapy.http
  description: |-
    Return the binary representation of ``text``. If ``text``
    is already a bytes object, return it as-is.
  summary: Return the binary representation of ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: urlencode
  kind: function
  ns: scrapy.http
  description: |-
    Encode a dict or sequence of two-element tuples into a URL query string.

    If any values in the query arg are sequences and doseq is true, each
    sequence element is converted to a separate parameter.

    If the query arg is a sequence of two-element tuples, the order of the
    parameters in the output will match the order of parameters in the
    input.

    The components of a query arg may each be either a string or a bytes type.

    The safe, encoding, and errors parameters are passed down to the function
    specified by quote_via (encoding and errors only if a component is a str).
  summary: Encode a dict or sequence of two-element tuples into a URL query string
  signatures:
  - kind: positional
    name: query
    default: null
    rest: false
  - kind: positional
    name: doseq
    default: 'False'
    rest: false
  - kind: positional
    name: safe
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: None
    rest: false
  - kind: positional
    name: quote_via
    default: <function quote_plus at 0x7fa9740732e0>
    rest: false
  - type: '?'
  inherits_from: null
- name: urljoin
  kind: function
  ns: scrapy.http
  description: |-
    Join a base URL and a possibly relative URL to form an absolute
    interpretation of the latter.
  summary: Join a base URL and a possibly relative URL to form an absolute
  signatures:
  - kind: positional
    name: base
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: allow_fragments
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: urlsplit
  kind: callable
  ns: scrapy.http
  description: |-
    Parse a URL into 5 components:
    <scheme>://<netloc>/<path>?<query>#<fragment>

    The result is a named 5-tuple with fields corresponding to the
    above. It is either a SplitResult or SplitResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
  summary: 'Parse a URL into 5 components:'
  signatures: null
  inherits_from: null
- name: urlunsplit
  kind: function
  ns: scrapy.http
  description: |-
    Combine the elements of a tuple as returned by urlsplit() into a
    complete URL as a string. The data argument can be any five-item iterable.
    This may result in a slightly different, but equivalent URL, if the URL that
    was parsed originally had unnecessary delimiters (for example, a ? with an
    empty query; the RFC states that these are equivalent).
  summary: Combine the elements of a tuple as returned by urlsplit() into a
  signatures:
  - kind: positional
    name: components
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: json_request
  kind: module
  ns: scrapy.http
  description: |-
    This module implements the JsonRequest class which is a more convenient class
    (than Request) to generate JSON Requests.

    See documentation in docs/topics/request-response.rst
  summary: This module implements the JsonRequest class which is a more convenient class
  signatures: null
  inherits_from: null
- name: JSONRequest
  kind: class
  ns: scrapy.http
  description: |-
    Represents an HTTP request, which is usually generated in a Spider and
    executed by the Downloader, thus generating a :class:`Response`.
  summary: Represents an HTTP request, which is usually generated in a Spider and
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - type: JSONRequest
  inherits_from:
  - <class 'scrapy.http.request.json_request.JsonRequest'>
  - <class 'scrapy.http.request.Request'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: JSONRequest.attributes
  kind: property
  ns: scrapy.http
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: JSONRequest.body
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: JSONRequest.cb_kwargs
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: JSONRequest.copy
  kind: method
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JSONRequest.dumps_kwargs
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: JSONRequest.encoding
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: JSONRequest.from_curl
  kind: function
  ns: scrapy.http
  description: |-
    Create a Request object from a string containing a `cURL
    <https://curl.haxx.se/>`_ command. It populates the HTTP method, the
    URL, the headers, the cookies and the body. It accepts the same
    arguments as the :class:`Request` class, taking preference and
    overriding the values of the same arguments contained in the cURL
    command.

    Unrecognized options are ignored by default. To raise an error when
    finding unknown options call this method by passing
    ``ignore_unknown_options=False``.

    .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`
                 subclasses, such as :class:`~scrapy.http.JSONRequest`, or
                 :class:`~scrapy.http.XmlRpcRequest`, as well as having
                 :ref:`downloader middlewares <topics-downloader-middleware>`
                 and
                 :ref:`spider middlewares <topics-spider-middleware>`
                 enabled, such as
                 :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,
                 :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,
                 or
                 :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,
                 may modify the :class:`~scrapy.http.Request` object.

    To translate a cURL command into a Scrapy request,
    you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.
  summary: Create a Request object from a string containing a `cURL
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: curl_command
    default: null
    rest: false
  - kind: positional
    name: ignore_unknown_options
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: JSONRequest.meta
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: JSONRequest.replace
  kind: method
  ns: scrapy.http
  description: Create a new Request with the same attributes except for those given new values
  summary: Create a new Request with the same attributes except for those given new values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: JSONRequest.to_dict
  kind: method
  ns: scrapy.http
  description: |-
    Return a dictionary containing the Request's data.

    Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.

    If a spider is given, this method will try to find out the name of the spider methods used as callback
    and errback and include them in the output dict, raising an exception if they cannot be found.
  summary: Return a dictionary containing the Request's data
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: spider
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: JSONRequest.url
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.http
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.http
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: create_deprecated_class
  kind: function
  ns: scrapy.http
  description: |-
    Return a "deprecated" class that causes its subclasses to issue a warning.
    Subclasses of ``new_class`` are considered subclasses of this class.
    It also warns when the deprecated class is instantiated, but do not when
    its subclasses are instantiated.

    It can be used to rename a base class in a library. For example, if we
    have

        class OldName(SomeClass):
            # ...

    and we want to rename it to NewName, we can do the following::

        class NewName(SomeClass):
            # ...

        OldName = create_deprecated_class('OldName', NewName)

    Then, if user class inherits from OldName, warning is issued. Also, if
    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``
    checks they'll still return True if sub is a subclass of NewName instead of
    OldName.
  summary: Return a "deprecated" class that causes its subclasses to issue a warning
  signatures:
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: new_class
    default: null
    rest: false
  - kind: positional
    name: clsdict
    default: None
    rest: false
  - kind: positional
    name: warn_category
    default: <class 'scrapy.exceptions.ScrapyDeprecationWarning'>
    rest: false
  - kind: positional
    name: warn_once
    default: 'True'
    rest: false
  - kind: positional
    name: old_class_path
    default: None
    rest: false
  - kind: positional
    name: new_class_path
    default: None
    rest: false
  - kind: positional
    name: subclass_warn_message
    default: '{cls} inherits from deprecated class {old}, please inherit from {new}.'
    rest: false
  - kind: positional
    name: instance_warn_message
    default: '{cls} is deprecated, instantiate {new} instead.'
    rest: false
  - type: '?'
  inherits_from: null
- name: object_ref
  kind: class
  ns: scrapy.http
  description: Inherit from this class to a keep a record of live instances
  summary: Inherit from this class to a keep a record of live instances
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - type: object_ref
  inherits_from: null
- name: obsolete_setter
  kind: function
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: setter
    default: null
    rest: false
  - kind: positional
    name: attrname
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: rpc
  kind: module
  ns: scrapy.http
  description: |-
    This module implements the XmlRpcRequest class which is a more convenient class
    (that Request) to generate xml-rpc requests.

    See documentation in docs/topics/request-response.rst
  summary: This module implements the XmlRpcRequest class which is a more convenient class
  signatures: null
  inherits_from: null
- name: DUMPS_ARGS
  kind: const
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.http
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: get_func_args
  kind: function
  ns: scrapy.http
  description: Return the argument name list of a callable object
  summary: Return the argument name list of a callable object
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - kind: positional
    name: stripself
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: safe_url_string
  kind: function
  ns: scrapy.http
  description: |-
    Return a URL equivalent to *url* that a wide range of web browsers and
    web servers consider valid.

    *url* is parsed according to the rules of the `URL living standard`_,
    and during serialization additional characters are percent-encoded to make
    the URL valid by additional URL standards.

    .. _URL living standard: https://url.spec.whatwg.org/

    The returned URL should be valid by *all* of the following URL standards
    known to be enforced by modern-day web browsers and web servers:

    -   `URL living standard`_

    -   `RFC 3986`_

    -   `RFC 2396`_ and `RFC 2732`_, as interpreted by `Java 8s java.net.URI
        class`_.

    .. _Java 8s java.net.URI class: https://docs.oracle.com/javase/8/docs/api/java/net/URI.html
    .. _RFC 2396: https://www.ietf.org/rfc/rfc2396.txt
    .. _RFC 2732: https://www.ietf.org/rfc/rfc2732.txt
    .. _RFC 3986: https://www.ietf.org/rfc/rfc3986.txt

    If a bytes URL is given, it is first converted to `str` using the given
    encoding (which defaults to 'utf-8'). If quote_path is True (default),
    path_encoding ('utf-8' by default) is used to encode URL path component
    which is then quoted. Otherwise, if quote_path is False, path component
    is not encoded or quoted. Given encoding is used for query string
    or form data.

    When passing an encoding, you should use the encoding of the
    original page (the page from which the URL was extracted from).

    Calling this function on an already "safe" URL will return the URL
    unmodified.
  summary: Return a URL equivalent to *url* that a wide range of web browsers and
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: utf8
    rest: false
  - kind: positional
    name: path_encoding
    default: utf8
    rest: false
  - kind: positional
    name: quote_path
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: to_bytes
  kind: function
  ns: scrapy.http
  description: |-
    Return the binary representation of ``text``. If ``text``
    is already a bytes object, return it as-is.
  summary: Return the binary representation of ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: response
  kind: module
  ns: scrapy.http
  description: |-
    This module implements the Response class which is used to represent HTTP
    responses in Scrapy.

    See documentation in docs/topics/request-response.rst
  summary: This module implements the Response class which is used to represent HTTP
  signatures: null
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.http
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Link
  kind: class
  ns: scrapy.http
  description: |-
    Link objects represent an extracted link by the LinkExtractor.

    Using the anchor tag sample below to illustrate the parameters::

            <a href="https://example.com/nofollow.html#foo" rel="nofollow">Dont follow this one</a>

    :param url: the absolute url being linked to in the anchor tag.
                From the sample, this is ``https://example.com/nofollow.html``.

    :param text: the text in the anchor tag. From the sample, this is ``Dont follow this one``.

    :param fragment: the part of the url after the hash symbol. From the sample, this is ``foo``.

    :param nofollow: an indication of the presence or absence of a nofollow value in the ``rel`` attribute
                    of the anchor tag.
  summary: Link objects represent an extracted link by the LinkExtractor
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: fragment
    default: null
    rest: false
  - kind: positional
    name: nofollow
    default: 'False'
    rest: false
  - type: Link
  inherits_from: null
- name: Link.fragment
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Link.nofollow
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Link.text
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Link.url
  kind: property
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.http
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: html
  kind: module
  ns: scrapy.http
  description: |-
    This module implements the HtmlResponse class which adds encoding
    discovering through HTML encoding declarations to the TextResponse class.

    See documentation in docs/topics/request-response.rst
  summary: This module implements the HtmlResponse class which adds encoding
  signatures: null
  inherits_from: null
- name: obsolete_setter
  kind: function
  ns: scrapy.http
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: setter
    default: null
    rest: false
  - kind: positional
    name: attrname
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: text
  kind: module
  ns: scrapy.http
  description: |-
    This module implements the TextResponse class which adds encoding handling and
    discovering (through HTTP headers) to base Response class.

    See documentation in docs/topics/request-response.rst
  summary: This module implements the TextResponse class which adds encoding handling and
  signatures: null
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.http
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.http
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.http
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: annotations
  kind: const
  ns: scrapy.http
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: get_base_url
  kind: function
  ns: scrapy.http
  description: Return the base url of the given response, joined with the response url
  summary: Return the base url of the given response, joined with the response url
  signatures:
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: html_body_declared_encoding
  kind: function
  ns: scrapy.http
  description: |-
    Return the encoding specified in meta tags in the html body,
    or ``None`` if no suitable encoding was found

    >>> import w3lib.encoding
    >>> w3lib.encoding.html_body_declared_encoding(
    ... """<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    ...      "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
    ... <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    ... <head>
    ...     <title>Some title</title>
    ...     <meta http-equiv="content-type" content="text/html;charset=utf-8" />
    ... </head>
    ... <body>
    ... ...
    ... </body>
    ... </html>""")
    'utf-8'
    >>>
  summary: Return the encoding specified in meta tags in the html body,
  signatures:
  - kind: positional
    name: html_body_str
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: html_to_unicode
  kind: function
  ns: scrapy.http
  description: |-
    Convert raw html bytes to unicode

    This attempts to make a reasonable guess at the content encoding of the
    html body, following a similar process to a web browser.

    It will try in order:

    * BOM (byte-order mark)
    * http content type header
    * meta or xml tag declarations
    * auto-detection, if the `auto_detect_fun` keyword argument is not ``None``
    * default encoding in keyword arg (which defaults to utf8)

    If an encoding other than the auto-detected or default encoding is used,
    overrides will be applied, converting some character encodings to more
    suitable alternatives.

    If a BOM is found matching the encoding, it will be stripped.

    The `auto_detect_fun` argument can be used to pass a function that will
    sniff the encoding of the text. This function must take the raw text as an
    argument and return the name of an encoding that python can process, or
    None.  To use chardet, for example, you can define the function as::

        auto_detect_fun=lambda x: chardet.detect(x).get('encoding')

    or to use UnicodeDammit (shipped with the BeautifulSoup library)::

        auto_detect_fun=lambda x: UnicodeDammit(x).originalEncoding

    If the locale of the website or user language preference is known, then a
    better default encoding can be supplied.

    If `content_type_header` is not present, ``None`` can be passed signifying
    that the header was not present.

    This method will not fail, if characters cannot be converted to unicode,
    ``\\ufffd`` (the unicode replacement character) will be inserted instead.

    Returns a tuple of ``(<encoding used>, <unicode_string>)``

    Examples:

    >>> import w3lib.encoding
    >>> w3lib.encoding.html_to_unicode(None,
    ... b"""<!DOCTYPE html>
    ... <head>
    ... <meta charset="UTF-8" />
    ... <meta name="viewport" content="width=device-width" />
    ... <title>Creative Commons France</title>
    ... <link rel='canonical' href='http://creativecommons.fr/' />
    ... <body>
    ... <p>Creative Commons est une organisation \xc3\xa0 but non lucratif
    ... qui a pour dessein de faciliter la diffusion et le partage des oeuvres
    ... tout en accompagnant les nouvelles pratiques de cr\xc3\xa9ation \xc3\xa0 l\xe2\x80\x99\xc3\xa8re numerique.</p>
    ... </body>
    ... </html>""")
    ('utf-8', '<!DOCTYPE html>\n<head>\n<meta charset="UTF-8" />\n<meta name="viewport" content="width=device-width" />\n<title>Creative Commons France</title>\n<link rel=\'canonical\' href=\'http://creativecommons.fr/\' />\n<body>\n<p>Creative Commons est une organisation \xe0 but non lucratif\nqui a pour dessein de faciliter la diffusion et le partage des oeuvres\ntout en accompagnant les nouvelles pratiques de cr\xe9ation \xe0 l\u2019\xe8re numerique.</p>\n</body>\n</html>')
    >>>
  summary: Convert raw html bytes to unicode
  signatures:
  - kind: positional
    name: content_type_header
    default: null
    rest: false
  - kind: positional
    name: html_body_str
    default: null
    rest: false
  - kind: positional
    name: default_encoding
    default: utf8
    rest: false
  - kind: positional
    name: auto_detect_fun
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: http_content_type_encoding
  kind: function
  ns: scrapy.http
  description: |-
    Extract the encoding in the content-type header

    >>> import w3lib.encoding
    >>> w3lib.encoding.http_content_type_encoding("Content-Type: text/html; charset=ISO-8859-4")
    'iso8859-4'
  summary: Extract the encoding in the content-type header
  signatures:
  - kind: positional
    name: content_type
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: memoizemethod_noargs
  kind: function
  ns: scrapy.http
  description: |-
    Decorator to cache the result of a method (without arguments) using a
    weak reference to its object
  summary: Decorator to cache the result of a method (without arguments) using a
  signatures:
  - kind: positional
    name: method
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: read_bom
  kind: function
  ns: scrapy.http
  description: |-
    Read the byte order mark in the text, if present, and
    return the encoding represented by the BOM and the BOM.

    If no BOM can be detected, ``(None, None)`` is returned.

    >>> import w3lib.encoding
    >>> w3lib.encoding.read_bom(b'\xfe\xff\x6c\x34')
    ('utf-16-be', '\xfe\xff')
    >>> w3lib.encoding.read_bom(b'\xff\xfe\x34\x6c')
    ('utf-16-le', '\xff\xfe')
    >>> w3lib.encoding.read_bom(b'\x00\x00\xfe\xff\x00\x00\x6c\x34')
    ('utf-32-be', '\x00\x00\xfe\xff')
    >>> w3lib.encoding.read_bom(b'\xff\xfe\x00\x00\x34\x6c\x00\x00')
    ('utf-32-le', '\xff\xfe\x00\x00')
    >>> w3lib.encoding.read_bom(b'\x01\x02\x03\x04')
    (None, None)
    >>>
  summary: Read the byte order mark in the text, if present, and
  signatures:
  - kind: positional
    name: data
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: resolve_encoding
  kind: function
  ns: scrapy.http
  description: |-
    Return the encoding that `encoding_alias` maps to, or ``None``
    if the encoding cannot be interpreted

    >>> import w3lib.encoding
    >>> w3lib.encoding.resolve_encoding('latin1')
    'cp1252'
    >>> w3lib.encoding.resolve_encoding('gb_2312-80')
    'gb18030'
    >>>
  summary: Return the encoding that `encoding_alias` maps to, or ``None``
  signatures:
  - kind: positional
    name: encoding_alias
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: strip_html5_whitespace
  kind: function
  ns: scrapy.http
  description: |-
    Strip all leading and trailing space characters (as defined in
    https://www.w3.org/TR/html5/infrastructure.html#space-character).

    Such stripping is useful e.g. for processing HTML element attributes which
    contain URLs, like ``href``, ``src`` or form ``action`` - HTML5 standard
    defines them as "valid URL potentially surrounded by spaces"
    or "valid non-empty URL potentially surrounded by spaces".

    >>> strip_html5_whitespace(' hello\n')
    'hello'
  summary: Strip all leading and trailing space characters (as defined in
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: suppress
  kind: class
  ns: scrapy.http
  description: |-
    Context manager to suppress specified exceptions

    After the exception is suppressed, execution proceeds with the next
    statement following the with statement.

         with suppress(FileNotFoundError):
             os.remove(somefile)
         # Execution still resumes here if the file was already removed
  summary: Context manager to suppress specified exceptions
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: suppress
  inherits_from:
  - <class 'contextlib.AbstractContextManager'>
  - <class 'abc.ABC'>
- name: to_unicode
  kind: function
  ns: scrapy.http
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: urljoin
  kind: function
  ns: scrapy.http
  description: |-
    Join a base URL and a possibly relative URL to form an absolute
    interpretation of the latter.
  summary: Join a base URL and a possibly relative URL to form an absolute
  signatures:
  - kind: positional
    name: base
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: allow_fragments
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: urljoin
  kind: function
  ns: scrapy.http
  description: |-
    Join a base URL and a possibly relative URL to form an absolute
    interpretation of the latter.
  summary: Join a base URL and a possibly relative URL to form an absolute
  signatures:
  - kind: positional
    name: base
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: allow_fragments
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: xml
  kind: module
  ns: scrapy.http
  description: |-
    This module implements the XmlResponse class which adds encoding
    discovering through XML encoding declarations to the TextResponse class.

    See documentation in docs/topics/request-response.rst
  summary: This module implements the XmlResponse class which adds encoding
  signatures: null
  inherits_from: null
- name: scrapy.http.cookies
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CookieJar
  kind: class
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: policy
    default: None
    rest: false
  - kind: positional
    name: check_expired_frequency
    default: '10000'
    rest: false
  - type: CookieJar
  inherits_from: null
- name: CookieJar.add_cookie_header
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CookieJar.clear
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: domain
    default: None
    rest: false
  - kind: positional
    name: path
    default: None
    rest: false
  - kind: positional
    name: name
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CookieJar.clear_session_cookies
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CookieJar.extract_cookies
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CookieJar.make_cookies
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CookieJar.set_cookie
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CookieJar.set_cookie_if_ok
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CookieJar.set_policy
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: pol
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy
  kind: class
  ns: scrapy.http.cookies
  description: Implements the standard rules for accepting and returning cookies.
  summary: Implements the standard rules for accepting and returning cookies
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: blocked_domains
    default: None
    rest: false
  - kind: positional
    name: allowed_domains
    default: None
    rest: false
  - kind: positional
    name: netscape
    default: 'True'
    rest: false
  - kind: positional
    name: rfc2965
    default: 'False'
    rest: false
  - kind: positional
    name: rfc2109_as_netscape
    default: None
    rest: false
  - kind: positional
    name: hide_cookie2
    default: 'False'
    rest: false
  - kind: positional
    name: strict_domain
    default: 'False'
    rest: false
  - kind: positional
    name: strict_rfc2965_unverifiable
    default: 'True'
    rest: false
  - kind: positional
    name: strict_ns_unverifiable
    default: 'False'
    rest: false
  - kind: positional
    name: strict_ns_domain
    default: '0'
    rest: false
  - kind: positional
    name: strict_ns_set_initial_dollar
    default: 'False'
    rest: false
  - kind: positional
    name: strict_ns_set_path
    default: 'False'
    rest: false
  - kind: positional
    name: secure_protocols
    default: ('https', 'wss')
    rest: false
  - type: DefaultCookiePolicy
  inherits_from:
  - <class 'http.cookiejar.CookiePolicy'>
- name: DefaultCookiePolicy.DomainLiberal
  kind: property
  ns: scrapy.http.cookies
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: DefaultCookiePolicy.DomainRFC2965Match
  kind: property
  ns: scrapy.http.cookies
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: DefaultCookiePolicy.DomainStrict
  kind: property
  ns: scrapy.http.cookies
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: DefaultCookiePolicy.DomainStrictNoDots
  kind: property
  ns: scrapy.http.cookies
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: DefaultCookiePolicy.DomainStrictNonDomain
  kind: property
  ns: scrapy.http.cookies
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: DefaultCookiePolicy.allowed_domains
  kind: method
  ns: scrapy.http.cookies
  description: Return None, or the sequence of allowed domains (as a tuple).
  summary: Return None, or the sequence of allowed domains (as a tuple)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.blocked_domains
  kind: method
  ns: scrapy.http.cookies
  description: Return the sequence of blocked domains (as a tuple).
  summary: Return the sequence of blocked domains (as a tuple)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.domain_return_ok
  kind: method
  ns: scrapy.http.cookies
  description: "Return false if cookies should not be returned, given cookie domain.\n        "
  summary: Return false if cookies should not be returned, given cookie domain
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: domain
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.is_blocked
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: domain
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.is_not_allowed
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: domain
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.path_return_ok
  kind: method
  ns: scrapy.http.cookies
  description: "Return false if cookies should not be returned, given cookie path.\n        "
  summary: Return false if cookies should not be returned, given cookie path
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.return_ok
  kind: method
  ns: scrapy.http.cookies
  description: |-
    If you override .return_ok(), be sure to call this method.  If it
    returns false, so should your subclass (assuming your subclass wants to
    be more strict about which cookies to return).
  summary: 'If you override '
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.return_ok_domain
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.return_ok_expires
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.return_ok_port
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.return_ok_secure
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.return_ok_verifiability
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.return_ok_version
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.set_allowed_domains
  kind: method
  ns: scrapy.http.cookies
  description: Set the sequence of allowed domains, or None.
  summary: Set the sequence of allowed domains, or None
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: allowed_domains
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.set_blocked_domains
  kind: method
  ns: scrapy.http.cookies
  description: Set the sequence of blocked domains.
  summary: Set the sequence of blocked domains
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: blocked_domains
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.set_ok
  kind: method
  ns: scrapy.http.cookies
  description: |-
    If you override .set_ok(), be sure to call this method.  If it returns
    false, so should your subclass (assuming your subclass wants to be more
    strict about which cookies to accept).
  summary: 'If you override '
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.set_ok_domain
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.set_ok_name
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.set_ok_path
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.set_ok_port
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.set_ok_verifiability
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultCookiePolicy.set_ok_version
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: cookie
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IPV4_RE
  kind: const
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: WrappedRequest
  kind: class
  ns: scrapy.http.cookies
  description: |-
    Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class

    see http://docs.python.org/library/urllib2.html#urllib2.Request
  summary: Wraps a scrapy Request class with methods defined by urllib2
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: WrappedRequest
  inherits_from: null
- name: WrappedRequest.add_unredirected_header
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WrappedRequest.full_url
  kind: property
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: WrappedRequest.get_full_url
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WrappedRequest.get_header
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: WrappedRequest.get_host
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WrappedRequest.get_type
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WrappedRequest.has_header
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WrappedRequest.header_items
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WrappedRequest.host
  kind: property
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: WrappedRequest.is_unverifiable
  kind: method
  ns: scrapy.http.cookies
  description: |-
    Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.

    It defaults to False. An unverifiable request is one whose URL the user did not have the
    option to approve. For example, if the request is for an image in an
    HTML document, and the user had no option to approve the automatic
    fetching of the image, this should be true.
  summary: Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WrappedRequest.origin_req_host
  kind: property
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: WrappedRequest.type
  kind: property
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: WrappedRequest.unverifiable
  kind: property
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: WrappedResponse
  kind: class
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: WrappedResponse
  inherits_from: null
- name: WrappedResponse.get_all
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: WrappedResponse.info
  kind: method
  ns: scrapy.http.cookies
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: potential_domain_matches
  kind: function
  ns: scrapy.http.cookies
  description: |-
    Potential domain matches for a cookie

    >>> potential_domain_matches('www.example.com')
    ['www.example.com', 'example.com', '.www.example.com', '.example.com']
  summary: Potential domain matches for a cookie
  signatures:
  - kind: positional
    name: domain
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: to_unicode
  kind: function
  ns: scrapy.http.cookies
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: urlparse_cached
  kind: function
  ns: scrapy.http.cookies
  description: |-
    Return urlparse.urlparse caching the result, where the argument can be a
    Request or Response object
  summary: Return urlparse
  signatures:
  - kind: positional
    name: request_or_response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.interfaces
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ISpiderLoader
  kind: callable
  ns: scrapy.interfaces
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Interface
  kind: callable
  ns: scrapy.interfaces
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.item
  kind: module
  ns: null
  description: |-
    Scrapy Item

    See documentation in docs/topics/item.rst
  summary: Scrapy Item
  signatures: null
  inherits_from: null
- name: ABCMeta
  kind: class
  ns: scrapy.item
  description: |-
    Metaclass for defining Abstract Base Classes (ABCs).

    Use this metaclass to create an ABC.  An ABC can be subclassed
    directly, and then acts as a mix-in class.  You can also register
    unrelated concrete classes (even built-in classes) and unrelated
    ABCs as 'virtual subclasses' -- these and their descendants will
    be considered subclasses of the registering ABC by the built-in
    issubclass() function, but the registering ABC won't show up in
    their MRO (Method Resolution Order) nor will method
    implementations defined by the registering ABC be callable (not
    even via super()).
  summary: Metaclass for defining Abstract Base Classes (ABCs)
  signatures:
  - kind: positional
    name: mcls
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: bases
    default: null
    rest: false
  - kind: positional
    name: namespace
    default: null
    rest: false
  - type: ABCMeta
  inherits_from:
  - <class 'type'>
- name: ABCMeta.mro
  kind: callable
  ns: scrapy.item
  description: Return a type's method resolution order.
  summary: Return a type's method resolution order
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ABCMeta.register
  kind: method
  ns: scrapy.item
  description: |-
    Register a virtual subclass of an ABC.

    Returns the subclass, to allow usage as a class decorator.
  summary: Register a virtual subclass of an ABC
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: subclass
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.item
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemMeta
  kind: class
  ns: scrapy.item
  description: |-
    Metaclass_ of :class:`Item` that handles field definitions.

    .. _metaclass: https://realpython.com/python-metaclasses
  summary: Metaclass_ of :class:`Item` that handles field definitions
  signatures:
  - kind: positional
    name: mcs
    default: null
    rest: false
  - kind: positional
    name: class_name
    default: null
    rest: false
  - kind: positional
    name: bases
    default: null
    rest: false
  - kind: positional
    name: attrs
    default: null
    rest: false
  - type: ItemMeta
  inherits_from:
  - <class 'abc.ABCMeta'>
  - <class 'type'>
- name: ItemMeta.mro
  kind: callable
  ns: scrapy.item
  description: Return a type's method resolution order.
  summary: Return a type's method resolution order
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemMeta.register
  kind: method
  ns: scrapy.item
  description: |-
    Register a virtual subclass of an ABC.

    Returns the subclass, to allow usage as a class decorator.
  summary: Register a virtual subclass of an ABC
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: subclass
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableMapping
  kind: class
  ns: scrapy.item
  description: |-
    A MutableMapping is a generic container for associating
    key/value pairs.

    This class provides concrete generic implementations of all
    methods except for __getitem__, __setitem__, __delitem__,
    __iter__, and __len__.
  summary: A MutableMapping is a generic container for associating
  signatures:
  - type: MutableMapping
  inherits_from:
  - <class 'collections.abc.Mapping'>
  - <class 'collections.abc.Collection'>
  - <class 'collections.abc.Sized'>
  - <class 'collections.abc.Iterable'>
  - <class 'collections.abc.Container'>
- name: MutableMapping.clear
  kind: method
  ns: scrapy.item
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableMapping.get
  kind: method
  ns: scrapy.item
  description: D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableMapping.items
  kind: method
  ns: scrapy.item
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableMapping.keys
  kind: method
  ns: scrapy.item
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableMapping.pop
  kind: method
  ns: scrapy.item
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
    If key is not found, d is returned if given, otherwise KeyError is raised.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: <object object at 0x7fa9743b0160>
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableMapping.popitem
  kind: method
  ns: scrapy.item
  description: |-
    D.popitem() -> (k, v), remove and return some (key, value) pair
    as a 2-tuple; but raise KeyError if D is empty.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableMapping.setdefault
  kind: method
  ns: scrapy.item
  description: D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableMapping.update
  kind: method
  ns: scrapy.item
  description: |-
    D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.
    If E present and has a .keys() method, does:     for k in E: D[k] = E[k]
    If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v
    In either case, this is followed by: for k, v in F.items(): D[k] = v
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: ()
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableMapping.values
  kind: method
  ns: scrapy.item
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: deepcopy
  kind: function
  ns: scrapy.item
  description: |-
    Deep copy operation on arbitrary Python objects.

    See the module's __doc__ string for more info.
  summary: Deep copy operation on arbitrary Python objects
  signatures:
  - kind: positional
    name: x
    default: null
    rest: false
  - kind: positional
    name: memo
    default: None
    rest: false
  - kind: positional
    name: _nil
    default: '[]'
    rest: false
  - type: '?'
  inherits_from: null
- name: pformat
  kind: function
  ns: scrapy.item
  description: Format a Python object into a pretty-printed representation.
  summary: Format a Python object into a pretty-printed representation
  signatures:
  - kind: positional
    name: object
    default: null
    rest: false
  - kind: positional
    name: indent
    default: '1'
    rest: false
  - kind: positional
    name: width
    default: '80'
    rest: false
  - kind: positional
    name: depth
    default: None
    rest: false
  - name: compact
    default: 'False'
    rest: false
    kind: kw-only
  - name: sort_dicts
    default: 'True'
    rest: false
    kind: kw-only
  - name: underscore_numbers
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: scrapy.link
  kind: module
  ns: null
  description: |-
    This module defines the Link object used in Link extractors.

    For actual link extractors implementation see scrapy.linkextractors, or
    its documentation in: docs/topics/link-extractors.rst
  summary: This module defines the Link object used in Link extractors
  signatures: null
  inherits_from: null
- name: scrapy.linkextractors
  kind: module
  ns: null
  description: |-
    scrapy.linkextractors

    This package contains a collection of Link Extractors.

    For more info see docs/topics/link-extractors.rst
  summary: scrapy
  signatures: null
  inherits_from: null
- name: IGNORED_EXTENSIONS
  kind: const
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LinkExtractor
  kind: class
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: allow
    default: ()
    rest: false
  - kind: positional
    name: deny
    default: ()
    rest: false
  - kind: positional
    name: allow_domains
    default: ()
    rest: false
  - kind: positional
    name: deny_domains
    default: ()
    rest: false
  - kind: positional
    name: restrict_xpaths
    default: ()
    rest: false
  - kind: positional
    name: tags
    default: ('a', 'area')
    rest: false
  - kind: positional
    name: attrs
    default: ('href',)
    rest: false
  - kind: positional
    name: canonicalize
    default: 'False'
    rest: false
  - kind: positional
    name: unique
    default: 'True'
    rest: false
  - kind: positional
    name: process_value
    default: None
    rest: false
  - kind: positional
    name: deny_extensions
    default: None
    rest: false
  - kind: positional
    name: restrict_css
    default: ()
    rest: false
  - kind: positional
    name: strip
    default: 'True'
    rest: false
  - kind: positional
    name: restrict_text
    default: None
    rest: false
  - type: LxmlLinkExtractor
  inherits_from: null
- name: LinkExtractor.extract_links
  kind: method
  ns: scrapy.linkextractors
  description: |-
    Returns a list of :class:`~scrapy.link.Link` objects from the
    specified :class:`response <scrapy.http.Response>`.

    Only links that match the settings passed to the ``__init__`` method of
    the link extractor are returned.

    Duplicate links are omitted if the ``unique`` attribute is set to ``True``,
    otherwise they are returned.
  summary: Returns a list of :class:`~scrapy
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LinkExtractor.matches
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: lxmlhtml
  kind: module
  ns: scrapy.linkextractors
  description: Link extractor based on lxml.html
  summary: Link extractor based on lxml
  signatures: null
  inherits_from: null
- name: HTMLTranslator
  kind: class
  ns: scrapy.linkextractors
  description: |-
    This mixin adds support to CSS pseudo elements via dynamic dispatch.

    Currently supported pseudo-elements are ``::text`` and ``::attr(ATTR_NAME)``.
  summary: This mixin adds support to CSS pseudo elements via dynamic dispatch
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xhtml
    default: 'False'
    rest: false
  - type: HTMLTranslator
  inherits_from:
  - <class 'parsel.csstranslator.TranslatorMixin'>
  - <class 'cssselect.xpath.HTMLTranslator'>
  - <class 'cssselect.xpath.GenericTranslator'>
- name: HTMLTranslator.attribute_operator_mapping
  kind: property
  ns: scrapy.linkextractors
  description: |-
    dict() -> new empty dictionary
    dict(mapping) -> new dictionary initialized from a mapping object's
        (key, value) pairs
    dict(iterable) -> new dictionary initialized as if via:
        d = {}
        for k, v in iterable:
            d[k] = v
    dict(**kwargs) -> new dictionary initialized with the name=value pairs
        in the keyword argument list.  For example:  dict(one=1, two=2)
  summary: dict() -> new empty dictionary
  signatures: null
  inherits_from: null
- name: HTMLTranslator.combinator_mapping
  kind: property
  ns: scrapy.linkextractors
  description: |-
    dict() -> new empty dictionary
    dict(mapping) -> new dictionary initialized from a mapping object's
        (key, value) pairs
    dict(iterable) -> new dictionary initialized as if via:
        d = {}
        for k, v in iterable:
            d[k] = v
    dict(**kwargs) -> new dictionary initialized with the name=value pairs
        in the keyword argument list.  For example:  dict(one=1, two=2)
  summary: dict() -> new empty dictionary
  signatures: null
  inherits_from: null
- name: HTMLTranslator.css_to_xpath
  kind: callable
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTMLTranslator.id_attribute
  kind: property
  ns: scrapy.linkextractors
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: HTMLTranslator.lang_attribute
  kind: property
  ns: scrapy.linkextractors
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: HTMLTranslator.lower_case_attribute_names
  kind: property
  ns: scrapy.linkextractors
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: HTMLTranslator.lower_case_attribute_values
  kind: property
  ns: scrapy.linkextractors
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: HTMLTranslator.lower_case_element_names
  kind: property
  ns: scrapy.linkextractors
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: HTMLTranslator.pseudo_never_matches
  kind: method
  ns: scrapy.linkextractors
  description: Common implementation for pseudo-classes that never match.
  summary: Common implementation for pseudo-classes that never match
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.selector_to_xpath
  kind: method
  ns: scrapy.linkextractors
  description: |-
    Translate a parsed selector to XPath.


    :param selector:
        A parsed :class:`Selector` object.
    :param prefix:
        This string is prepended to the resulting XPath expression.
        The default makes selectors scoped to the context nodes subtree.
    :param translate_pseudo_elements:
        Unless this is set to ``True`` (as :meth:`css_to_xpath` does),
        the :attr:`~Selector.pseudo_element` attribute of the selector
        is ignored.
        It is the caller's responsibility to reject selectors
        with pseudo-elements, or to account for them somehow.
    :raises:
        :class:`ExpressionError` on unknown/unsupported selectors.
    :returns:
        The equivalent XPath 1.0 expression as a string.
  summary: Translate a parsed selector to XPath
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: selector
    default: null
    rest: false
  - kind: positional
    name: prefix
    default: 'descendant-or-self::'
    rest: false
  - kind: positional
    name: translate_pseudo_elements
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath
  kind: method
  ns: scrapy.linkextractors
  description: Translate any parsed selector object.
  summary: Translate any parsed selector object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: parsed_selector
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_active_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: Common implementation for pseudo-classes that never match.
  summary: Common implementation for pseudo-classes that never match
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_attr_functional_pseudo_element
  kind: method
  ns: scrapy.linkextractors
  description: Support selecting attribute values using ::attr() pseudo-element
  summary: Support selecting attribute values using ::attr() pseudo-element
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: function
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_attrib
  kind: method
  ns: scrapy.linkextractors
  description: Translate an attribute selector.
  summary: Translate an attribute selector
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: selector
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_attrib_dashmatch
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_attrib_different
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_attrib_equals
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_attrib_exists
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_attrib_includes
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_attrib_prefixmatch
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_attrib_substringmatch
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_attrib_suffixmatch
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_checked_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: Common implementation for pseudo-classes that never match.
  summary: Common implementation for pseudo-classes that never match
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_child_combinator
  kind: method
  ns: scrapy.linkextractors
  description: right is an immediate child of left
  summary: right is an immediate child of left
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: left
    default: null
    rest: false
  - kind: positional
    name: right
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_class
  kind: method
  ns: scrapy.linkextractors
  description: Translate a class selector.
  summary: Translate a class selector
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: class_selector
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_combinedselector
  kind: method
  ns: scrapy.linkextractors
  description: Translate a combined selector.
  summary: Translate a combined selector
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: combined
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_contains_function
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: function
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_descendant_combinator
  kind: method
  ns: scrapy.linkextractors
  description: right is a child, grand-child or further descendant of left
  summary: right is a child, grand-child or further descendant of left
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: left
    default: null
    rest: false
  - kind: positional
    name: right
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_direct_adjacent_combinator
  kind: method
  ns: scrapy.linkextractors
  description: right is a sibling immediately after left
  summary: right is a sibling immediately after left
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: left
    default: null
    rest: false
  - kind: positional
    name: right
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_disabled_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: Common implementation for pseudo-classes that never match.
  summary: Common implementation for pseudo-classes that never match
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_element
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: selector
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_empty_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_enabled_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: Common implementation for pseudo-classes that never match.
  summary: Common implementation for pseudo-classes that never match
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_first_child_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_first_of_type_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_focus_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: Common implementation for pseudo-classes that never match.
  summary: Common implementation for pseudo-classes that never match
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_function
  kind: method
  ns: scrapy.linkextractors
  description: Translate a functional pseudo-class.
  summary: Translate a functional pseudo-class
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: function
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_hash
  kind: method
  ns: scrapy.linkextractors
  description: Translate an ID selector.
  summary: Translate an ID selector
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: id_selector
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_hover_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: Common implementation for pseudo-classes that never match.
  summary: Common implementation for pseudo-classes that never match
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_indirect_adjacent_combinator
  kind: method
  ns: scrapy.linkextractors
  description: right is a sibling after left, immediately or not
  summary: right is a sibling after left, immediately or not
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: left
    default: null
    rest: false
  - kind: positional
    name: right
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_lang_function
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: function
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_last_child_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_last_of_type_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_link_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: Common implementation for pseudo-classes that never match.
  summary: Common implementation for pseudo-classes that never match
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_literal
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: s
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_matching
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: matching
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_negation
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: negation
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_nth_child_function
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: function
    default: null
    rest: false
  - kind: positional
    name: last
    default: 'False'
    rest: false
  - kind: positional
    name: add_name_test
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_nth_last_child_function
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: function
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_nth_last_of_type_function
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: function
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_nth_of_type_function
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: function
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_only_child_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_only_of_type_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: Translate a pseudo-class.
  summary: Translate a pseudo-class
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: pseudo
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_pseudo_element
  kind: method
  ns: scrapy.linkextractors
  description: Dispatch method that transforms XPath to support pseudo-element
  summary: Dispatch method that transforms XPath to support pseudo-element
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - kind: positional
    name: pseudo_element
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_relation
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: relation
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_relation_child_combinator
  kind: method
  ns: scrapy.linkextractors
  description: right is an immediate child of left; select left
  summary: right is an immediate child of left; select left
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: left
    default: null
    rest: false
  - kind: positional
    name: right
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_relation_descendant_combinator
  kind: method
  ns: scrapy.linkextractors
  description: right is a child, grand-child or further descendant of left; select left
  summary: right is a child, grand-child or further descendant of left; select left
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: left
    default: null
    rest: false
  - kind: positional
    name: right
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_relation_direct_adjacent_combinator
  kind: method
  ns: scrapy.linkextractors
  description: right is a sibling immediately after left; select left
  summary: right is a sibling immediately after left; select left
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: left
    default: null
    rest: false
  - kind: positional
    name: right
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_relation_indirect_adjacent_combinator
  kind: method
  ns: scrapy.linkextractors
  description: right is a sibling after left, immediately or not; select left
  summary: right is a sibling after left, immediately or not; select left
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: left
    default: null
    rest: false
  - kind: positional
    name: right
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_root_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_scope_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_specificityadjustment
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: matching
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_target_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: Common implementation for pseudo-classes that never match.
  summary: Common implementation for pseudo-classes that never match
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_text_simple_pseudo_element
  kind: method
  ns: scrapy.linkextractors
  description: Support selecting text nodes using ::text pseudo-element
  summary: Support selecting text nodes using ::text pseudo-element
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpath_visited_pseudo
  kind: method
  ns: scrapy.linkextractors
  description: Common implementation for pseudo-classes that never match.
  summary: Common implementation for pseudo-classes that never match
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpathexpr_cls
  kind: class
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: element
    default: '*'
    rest: false
  - kind: positional
    name: condition
    default: null
    rest: false
  - kind: positional
    name: star_prefix
    default: 'False'
    rest: false
  - type: XPathExpr
  inherits_from: null
- name: HTMLTranslator.xpathexpr_cls.add_condition
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: condition
    default: null
    rest: false
  - kind: positional
    name: conjuction
    default: and
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpathexpr_cls.add_name_test
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpathexpr_cls.add_star_prefix
  kind: method
  ns: scrapy.linkextractors
  description: |-
    Append '*/' to the path to keep the context constrained
    to a single parent.
  summary: Append '*/' to the path to keep the context constrained
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HTMLTranslator.xpathexpr_cls.join
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: combiner
    default: null
    rest: false
  - kind: positional
    name: other
    default: null
    rest: false
  - kind: positional
    name: closing_combiner
    default: None
    rest: false
  - kind: positional
    name: has_inner_condition
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: IGNORED_EXTENSIONS
  kind: const
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LxmlParserLinkExtractor
  kind: class
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: tag
    default: a
    rest: false
  - kind: positional
    name: attr
    default: href
    rest: false
  - kind: positional
    name: process
    default: None
    rest: false
  - kind: positional
    name: unique
    default: 'False'
    rest: false
  - kind: positional
    name: strip
    default: 'True'
    rest: false
  - kind: positional
    name: canonicalized
    default: 'False'
    rest: false
  - type: LxmlParserLinkExtractor
  inherits_from: null
- name: LxmlParserLinkExtractor.extract_links
  kind: method
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XHTML_NAMESPACE
  kind: const
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: arg_to_iter
  kind: function
  ns: scrapy.linkextractors
  description: |-
    Convert an argument to an iterable. The argument can be a None, single
    value, or an iterable.

    Exception: if arg is a dict, [arg] will be returned
  summary: Convert an argument to an iterable
  signatures:
  - kind: positional
    name: arg
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: canonicalize_url
  kind: function
  ns: scrapy.linkextractors
  description: |-
    Canonicalize the given url by applying the following procedures:

    - make the URL safe
    - sort query arguments, first by key, then by value
    - normalize all spaces (in query arguments) '+' (plus symbol)
    - normalize percent encodings case (%2f -> %2F)
    - remove query arguments with blank values (unless `keep_blank_values` is True)
    - remove fragments (unless `keep_fragments` is True)

    The url passed can be bytes or unicode, while the url returned is
    always a native str (bytes in Python 2, unicode in Python 3).

    >>> import w3lib.url
    >>>
    >>> # sorting query arguments
    >>> w3lib.url.canonicalize_url('http://www.example.com/do?c=3&b=5&b=2&a=50')
    'http://www.example.com/do?a=50&b=2&b=5&c=3'
    >>>
    >>> # UTF-8 conversion + percent-encoding of non-ASCII characters
    >>> w3lib.url.canonicalize_url('http://www.example.com/r\u00e9sum\u00e9')
    'http://www.example.com/r%C3%A9sum%C3%A9'
    >>>

    For more examples, see the tests in `tests/test_url.py`.
  summary: 'Canonicalize the given url by applying the following procedures:'
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: keep_blank_values
    default: 'True'
    rest: false
  - kind: positional
    name: keep_fragments
    default: 'False'
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: get_base_url
  kind: function
  ns: scrapy.linkextractors
  description: Return the base url of the given response, joined with the response url
  summary: Return the base url of the given response, joined with the response url
  signatures:
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.linkextractors
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: partial
  kind: class
  ns: scrapy.linkextractors
  description: |-
    partial(func, *args, **keywords) - new function with partial application
    of the given arguments and keywords.
  summary: partial(func, *args, **keywords) - new function with partial application
  signatures: null
  inherits_from: null
- name: partial.args
  kind: property
  ns: scrapy.linkextractors
  description: tuple of arguments to future partial calls
  summary: tuple of arguments to future partial calls
  signatures: null
  inherits_from: null
- name: partial.func
  kind: property
  ns: scrapy.linkextractors
  description: function object to use in future partial calls
  summary: function object to use in future partial calls
  signatures: null
  inherits_from: null
- name: partial.keywords
  kind: property
  ns: scrapy.linkextractors
  description: dictionary of keyword arguments to future partial calls
  summary: dictionary of keyword arguments to future partial calls
  signatures: null
  inherits_from: null
- name: rel_has_nofollow
  kind: function
  ns: scrapy.linkextractors
  description: Return True if link rel attribute has nofollow type
  summary: Return True if link rel attribute has nofollow type
  signatures:
  - kind: positional
    name: rel
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: safe_url_string
  kind: function
  ns: scrapy.linkextractors
  description: |-
    Return a URL equivalent to *url* that a wide range of web browsers and
    web servers consider valid.

    *url* is parsed according to the rules of the `URL living standard`_,
    and during serialization additional characters are percent-encoded to make
    the URL valid by additional URL standards.

    .. _URL living standard: https://url.spec.whatwg.org/

    The returned URL should be valid by *all* of the following URL standards
    known to be enforced by modern-day web browsers and web servers:

    -   `URL living standard`_

    -   `RFC 3986`_

    -   `RFC 2396`_ and `RFC 2732`_, as interpreted by `Java 8s java.net.URI
        class`_.

    .. _Java 8s java.net.URI class: https://docs.oracle.com/javase/8/docs/api/java/net/URI.html
    .. _RFC 2396: https://www.ietf.org/rfc/rfc2396.txt
    .. _RFC 2732: https://www.ietf.org/rfc/rfc2732.txt
    .. _RFC 3986: https://www.ietf.org/rfc/rfc3986.txt

    If a bytes URL is given, it is first converted to `str` using the given
    encoding (which defaults to 'utf-8'). If quote_path is True (default),
    path_encoding ('utf-8' by default) is used to encode URL path component
    which is then quoted. Otherwise, if quote_path is False, path component
    is not encoded or quoted. Given encoding is used for query string
    or form data.

    When passing an encoding, you should use the encoding of the
    original page (the page from which the URL was extracted from).

    Calling this function on an already "safe" URL will return the URL
    unmodified.
  summary: Return a URL equivalent to *url* that a wide range of web browsers and
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: utf8
    rest: false
  - kind: positional
    name: path_encoding
    default: utf8
    rest: false
  - kind: positional
    name: quote_path
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: strip_html5_whitespace
  kind: function
  ns: scrapy.linkextractors
  description: |-
    Strip all leading and trailing space characters (as defined in
    https://www.w3.org/TR/html5/infrastructure.html#space-character).

    Such stripping is useful e.g. for processing HTML element attributes which
    contain URLs, like ``href``, ``src`` or form ``action`` - HTML5 standard
    defines them as "valid URL potentially surrounded by spaces"
    or "valid non-empty URL potentially surrounded by spaces".

    >>> strip_html5_whitespace(' hello\n')
    'hello'
  summary: Strip all leading and trailing space characters (as defined in
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: unique_list
  kind: function
  ns: scrapy.linkextractors
  description: efficient function to uniquify a list preserving item order
  summary: efficient function to uniquify a list preserving item order
  signatures:
  - kind: positional
    name: list_
    default: null
    rest: false
  - kind: positional
    name: key
    default: <function <lambda> at 0x7fa973d09da0>
    rest: false
  - type: '?'
  inherits_from: null
- name: url_has_any_extension
  kind: function
  ns: scrapy.linkextractors
  description: Return True if the url ends with one of the extensions provided
  summary: Return True if the url ends with one of the extensions provided
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: extensions
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: url_is_from_any_domain
  kind: function
  ns: scrapy.linkextractors
  description: Return True if the url belongs to any of the given domains
  summary: Return True if the url belongs to any of the given domains
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: domains
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: urljoin
  kind: function
  ns: scrapy.linkextractors
  description: |-
    Join a base URL and a possibly relative URL to form an absolute
    interpretation of the latter.
  summary: Join a base URL and a possibly relative URL to form an absolute
  signatures:
  - kind: positional
    name: base
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: allow_fragments
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: urlparse
  kind: function
  ns: scrapy.linkextractors
  description: |-
    Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
  summary: 'Parse a URL into 6 components:'
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: scheme
    default: null
    rest: false
  - kind: positional
    name: allow_fragments
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.loader
  kind: module
  ns: null
  description: |-
    Item Loader

    See documentation in docs/topics/loaders.rst
  summary: Item Loader
  signatures: null
  inherits_from: null
- name: ItemLoader
  kind: class
  ns: scrapy.loader
  description: |-
    A user-friendly abstraction to populate an :ref:`item <topics-items>` with data
    by applying :ref:`field processors <topics-loaders-processors>` to scraped data.
    When instantiated with a ``selector`` or a ``response`` it supports
    data extraction from web pages using :ref:`selectors <topics-selectors>`.

    :param item: The item instance to populate using subsequent calls to
        :meth:`~ItemLoader.add_xpath`, :meth:`~ItemLoader.add_css`,
        or :meth:`~ItemLoader.add_value`.
    :type item: scrapy.item.Item

    :param selector: The selector to extract data from, when using the
        :meth:`add_xpath`, :meth:`add_css`, :meth:`replace_xpath`, or
        :meth:`replace_css` method.
    :type selector: :class:`~scrapy.selector.Selector` object

    :param response: The response used to construct the selector using the
        :attr:`default_selector_class`, unless the selector argument is given,
        in which case this argument is ignored.
    :type response: :class:`~scrapy.http.Response` object

    If no item is given, one is instantiated automatically using the class in
    :attr:`default_item_class`.

    The item, selector, response and remaining keyword arguments are
    assigned to the Loader context (accessible through the :attr:`context` attribute).

    .. attribute:: item

        The item object being parsed by this Item Loader.
        This is mostly used as a property so, when attempting to override this
        value, you may want to check out :attr:`default_item_class` first.

    .. attribute:: context

        The currently active :ref:`Context <loaders-context>` of this Item Loader.

    .. attribute:: default_item_class

        An :ref:`item <topics-items>` class (or factory), used to instantiate
        items when not given in the ``__init__`` method.

    .. attribute:: default_input_processor

        The default input processor to use for those fields which don't specify
        one.

    .. attribute:: default_output_processor

        The default output processor to use for those fields which don't specify
        one.

    .. attribute:: default_selector_class

        The class used to construct the :attr:`selector` of this
        :class:`ItemLoader`, if only a response is given in the ``__init__`` method.
        If a selector is given in the ``__init__`` method this attribute is ignored.
        This attribute is sometimes overridden in subclasses.

    .. attribute:: selector

        The :class:`~scrapy.selector.Selector` object to extract data from.
        It's either the selector given in the ``__init__`` method or one created from
        the response given in the ``__init__`` method using the
        :attr:`default_selector_class`. This attribute is meant to be
        read-only.
  summary: A user-friendly abstraction to populate an :ref:`item <topics-items>` with data
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: None
    rest: false
  - kind: positional
    name: selector
    default: None
    rest: false
  - kind: positional
    name: response
    default: None
    rest: false
  - kind: positional
    name: parent
    default: None
    rest: false
  - type: ItemLoader
  inherits_from:
  - <class 'itemloaders.ItemLoader'>
- name: ItemLoader.add_css
  kind: method
  ns: scrapy.loader
  description: |-
    Similar to :meth:`ItemLoader.add_value` but receives a CSS selector
    instead of a value, which is used to extract a list of unicode strings
    from the selector associated with this :class:`ItemLoader`.

    See :meth:`get_css` for ``kwargs``.

    :param css: the CSS selector to extract data from
    :type css: str

    Examples::

        # HTML snippet: <p class="product-name">Color TV</p>
        loader.add_css('name', 'p.product-name')
        # HTML snippet: <p id="price">the price is $1200</p>
        loader.add_css('price', 'p#price', re='the price is (.*)')
  summary: Similar to :meth:`ItemLoader
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - kind: positional
    name: css
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ItemLoader.add_jmes
  kind: method
  ns: scrapy.loader
  description: |-
    Similar to :meth:`ItemLoader.add_value` but receives a JMESPath selector
    instead of a value, which is used to extract a list of unicode strings
    from the selector associated with this :class:`ItemLoader`.

    See :meth:`get_jmes` for ``kwargs``.

    :param jmes: the JMESPath selector to extract data from
    :type jmes: str

    Examples::

        # HTML snippet: {"name": "Color TV"}
        loader.add_jmes('name')
        # HTML snippet: {"price": the price is $1200"}
        loader.add_jmes('price', TakeFirst(), re='the price is (.*)')
  summary: Similar to :meth:`ItemLoader
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - kind: positional
    name: jmes
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ItemLoader.add_value
  kind: method
  ns: scrapy.loader
  description: |-
    Process and then add the given ``value`` for the given field.

    The value is first passed through :meth:`get_value` by giving the
    ``processors`` and ``kwargs``, and then passed through the
    :ref:`field input processor <processors>` and its result
    appended to the data collected for that field. If the field already
    contains collected data, the new data is added.

    The given ``field_name`` can be ``None``, in which case values for
    multiple fields may be added. And the processed value should be a dict
    with field_name mapped to values.

    Examples::

        loader.add_value('name', 'Color TV')
        loader.add_value('colours', ['white', 'blue'])
        loader.add_value('length', '100')
        loader.add_value('name', 'name: foo', TakeFirst(), re='name: (.+)')
        loader.add_value(None, {'name': 'foo', 'sex': 'male'})
  summary: Process and then add the given ``value`` for the given field
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ItemLoader.add_xpath
  kind: method
  ns: scrapy.loader
  description: |-
    Similar to :meth:`ItemLoader.add_value` but receives an XPath instead of a
    value, which is used to extract a list of strings from the
    selector associated with this :class:`ItemLoader`.

    See :meth:`get_xpath` for ``kwargs``.

    :param xpath: the XPath to extract data from
    :type xpath: str

    Examples::

        # HTML snippet: <p class="product-name">Color TV</p>
        loader.add_xpath('name', '//p[@class="product-name"]')
        # HTML snippet: <p id="price">the price is $1200</p>
        loader.add_xpath('price', '//p[@id="price"]', re='the price is (.*)')
  summary: Similar to :meth:`ItemLoader
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ItemLoader.default_input_processor
  kind: callable
  ns: scrapy.loader
  description: |-
    The simplest processor, which doesn't do anything. It returns the original
    values unchanged. It doesn't receive any ``__init__`` method arguments, nor does it
    accept Loader contexts.

    Example:

    >>> from itemloaders.processors import Identity
    >>> proc = Identity()
    >>> proc(['one', 'two', 'three'])
    ['one', 'two', 'three']
  summary: The simplest processor, which doesn't do anything
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: values
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemLoader.default_output_processor
  kind: callable
  ns: scrapy.loader
  description: |-
    The simplest processor, which doesn't do anything. It returns the original
    values unchanged. It doesn't receive any ``__init__`` method arguments, nor does it
    accept Loader contexts.

    Example:

    >>> from itemloaders.processors import Identity
    >>> proc = Identity()
    >>> proc(['one', 'two', 'three'])
    ['one', 'two', 'three']
  summary: The simplest processor, which doesn't do anything
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: values
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemLoader.get_collected_values
  kind: method
  ns: scrapy.loader
  description: Return the collected values for the given field.
  summary: Return the collected values for the given field
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemLoader.get_css
  kind: method
  ns: scrapy.loader
  description: |-
    Similar to :meth:`ItemLoader.get_value` but receives a CSS selector
    instead of a value, which is used to extract a list of unicode strings
    from the selector associated with this :class:`ItemLoader`.

    :param css: the CSS selector to extract data from
    :type css: str

    :param re: a regular expression to use for extracting data from the
        selected CSS region
    :type re: str or typing.Pattern

    Examples::

        # HTML snippet: <p class="product-name">Color TV</p>
        loader.get_css('p.product-name')
        # HTML snippet: <p id="price">the price is $1200</p>
        loader.get_css('p#price', TakeFirst(), re='the price is (.*)')
  summary: Similar to :meth:`ItemLoader
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: css
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ItemLoader.get_input_processor
  kind: method
  ns: scrapy.loader
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemLoader.get_jmes
  kind: method
  ns: scrapy.loader
  description: |-
    Similar to :meth:`ItemLoader.get_value` but receives a JMESPath selector
    instead of a value, which is used to extract a list of unicode strings
    from the selector associated with this :class:`ItemLoader`.

    :param jmes: the JMESPath selector to extract data from
    :type jmes: str

    :param re: a regular expression to use for extracting data from the
        selected JMESPath
    :type re: str or typing.Pattern

    Examples::

        # HTML snippet: {"name": "Color TV"}
        loader.get_jmes('name')
        # HTML snippet: {"price": the price is $1200"}
        loader.get_jmes('price', TakeFirst(), re='the price is (.*)')
  summary: Similar to :meth:`ItemLoader
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: jmes
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ItemLoader.get_output_processor
  kind: method
  ns: scrapy.loader
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemLoader.get_output_value
  kind: method
  ns: scrapy.loader
  description: |-
    Return the collected values parsed using the output processor, for the
    given field. This method doesn't populate or modify the item at all.
  summary: Return the collected values parsed using the output processor, for the
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemLoader.get_value
  kind: method
  ns: scrapy.loader
  description: |-
    Process the given ``value`` by the given ``processors`` and keyword
    arguments.

    Available keyword arguments:

    :param re: a regular expression to use for extracting data from the
        given value using :func:`~parsel.utils.extract_regex` method,
        applied before processors
    :type re: str or typing.Pattern

    Examples:

    >>> from itemloaders import ItemLoader
    >>> from itemloaders.processors import TakeFirst
    >>> loader = ItemLoader()
    >>> loader.get_value('name: foo', TakeFirst(), str.upper, re='name: (.+)')
    'FOO'
  summary: Process the given ``value`` by the given ``processors`` and keyword
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ItemLoader.get_xpath
  kind: method
  ns: scrapy.loader
  description: |-
    Similar to :meth:`ItemLoader.get_value` but receives an XPath instead of a
    value, which is used to extract a list of unicode strings from the
    selector associated with this :class:`ItemLoader`.

    :param xpath: the XPath to extract data from
    :type xpath: str

    :param re: a regular expression to use for extracting data from the
        selected XPath region
    :type re: str or typing.Pattern

    Examples::

        # HTML snippet: <p class="product-name">Color TV</p>
        loader.get_xpath('//p[@class="product-name"]')
        # HTML snippet: <p id="price">the price is $1200</p>
        loader.get_xpath('//p[@id="price"]', TakeFirst(), re='the price is (.*)')
  summary: Similar to :meth:`ItemLoader
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ItemLoader.item
  kind: property
  ns: scrapy.loader
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ItemLoader.load_item
  kind: method
  ns: scrapy.loader
  description: |-
    Populate the item with the data collected so far, and return it. The
    data collected is first passed through the :ref:`output processors
    <processors>` to get the final value to assign to each item field.
  summary: Populate the item with the data collected so far, and return it
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemLoader.nested_css
  kind: method
  ns: scrapy.loader
  description: |-
    Create a nested loader with a css selector.
    The supplied selector is applied relative to selector associated
    with this :class:`ItemLoader`. The nested loader shares the item
    with the parent :class:`ItemLoader` so calls to :meth:`add_xpath`,
    :meth:`add_value`, :meth:`replace_value`, etc. will behave as expected.
  summary: Create a nested loader with a css selector
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: css
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemLoader.nested_xpath
  kind: method
  ns: scrapy.loader
  description: |-
    Create a nested loader with an xpath selector.
    The supplied selector is applied relative to selector associated
    with this :class:`ItemLoader`. The nested loader shares the item
    with the parent :class:`ItemLoader` so calls to :meth:`add_xpath`,
    :meth:`add_value`, :meth:`replace_value`, etc. will behave as expected.
  summary: Create a nested loader with an xpath selector
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemLoader.replace_css
  kind: method
  ns: scrapy.loader
  description: Similar to :meth:`add_css` but replaces collected data instead of adding it.
  summary: Similar to :meth:`add_css` but replaces collected data instead of adding it
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - kind: positional
    name: css
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ItemLoader.replace_jmes
  kind: method
  ns: scrapy.loader
  description: Similar to :meth:`add_jmes` but replaces collected data instead of adding it.
  summary: Similar to :meth:`add_jmes` but replaces collected data instead of adding it
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - kind: positional
    name: jmes
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ItemLoader.replace_value
  kind: method
  ns: scrapy.loader
  description: |-
    Similar to :meth:`add_value` but replaces the collected data with the
    new value instead of adding it.
  summary: Similar to :meth:`add_value` but replaces the collected data with the
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ItemLoader.replace_xpath
  kind: method
  ns: scrapy.loader
  description: Similar to :meth:`add_xpath` but replaces collected data instead of adding it.
  summary: Similar to :meth:`add_xpath` but replaces collected data instead of adding it
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - kind: positional
    name: xpath
    default: null
    rest: false
  - name: re
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: scrapy.logformatter
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CRAWLEDMSG
  kind: const
  ns: scrapy.logformatter
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOADERRORMSG_LONG
  kind: const
  ns: scrapy.logformatter
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOADERRORMSG_SHORT
  kind: const
  ns: scrapy.logformatter
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DROPPEDMSG
  kind: const
  ns: scrapy.logformatter
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.logformatter
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ITEMERRORMSG
  kind: const
  ns: scrapy.logformatter
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.logformatter
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SCRAPEDMSG
  kind: const
  ns: scrapy.logformatter
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SPIDERERRORMSG
  kind: const
  ns: scrapy.logformatter
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.logformatter
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.logformatter
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: annotations
  kind: const
  ns: scrapy.logformatter
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: referer_str
  kind: function
  ns: scrapy.logformatter
  description: Return Referer HTTP header suitable for logging.
  summary: Return Referer HTTP header suitable for logging
  signatures:
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.mail
  kind: module
  ns: null
  description: |-
    Mail sending helpers

    See documentation in docs/topics/email.rst
  summary: Mail sending helpers
  signatures: null
  inherits_from: null
- name: BytesIO
  kind: class
  ns: scrapy.mail
  description: Buffered I/O implementation using an in-memory bytes buffer.
  summary: Buffered I/O implementation using an in-memory bytes buffer
  signatures:
  - kind: positional
    name: initial_bytes
    default: b''
    rest: false
  - type: BytesIO
  inherits_from:
  - <class '_io._BufferedIOBase'>
  - <class '_io._IOBase'>
- name: BytesIO.close
  kind: callable
  ns: scrapy.mail
  description: Disable all I/O operations.
  summary: Disable all I/O operations
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.closed
  kind: property
  ns: scrapy.mail
  description: True if the file is closed.
  summary: True if the file is closed
  signatures: null
  inherits_from: null
- name: BytesIO.detach
  kind: callable
  ns: scrapy.mail
  description: |-
    Disconnect this buffer from its underlying raw stream and return it.

    After the raw stream has been detached, the buffer is in an unusable
    state.
  summary: Disconnect this buffer from its underlying raw stream and return it
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.fileno
  kind: callable
  ns: scrapy.mail
  description: |-
    Returns underlying file descriptor if one exists.

    OSError is raised if the IO object does not use a file descriptor.
  summary: Returns underlying file descriptor if one exists
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.flush
  kind: callable
  ns: scrapy.mail
  description: Does nothing.
  summary: Does nothing
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.getbuffer
  kind: callable
  ns: scrapy.mail
  description: Get a read-write view over the contents of the BytesIO object.
  summary: Get a read-write view over the contents of the BytesIO object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.getvalue
  kind: callable
  ns: scrapy.mail
  description: Retrieve the entire contents of the BytesIO object.
  summary: Retrieve the entire contents of the BytesIO object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.isatty
  kind: callable
  ns: scrapy.mail
  description: |-
    Always returns False.

    BytesIO objects are not connected to a TTY-like device.
  summary: Always returns False
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.read
  kind: callable
  ns: scrapy.mail
  description: |-
    Read at most size bytes, returned as a bytes object.

    If the size argument is negative, read until EOF is reached.
    Return an empty bytes object at EOF.
  summary: Read at most size bytes, returned as a bytes object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: size
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.read1
  kind: callable
  ns: scrapy.mail
  description: |-
    Read at most size bytes, returned as a bytes object.

    If the size argument is negative or omitted, read until EOF is reached.
    Return an empty bytes object at EOF.
  summary: Read at most size bytes, returned as a bytes object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: size
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.readable
  kind: callable
  ns: scrapy.mail
  description: Returns True if the IO object can be read.
  summary: Returns True if the IO object can be read
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.readinto
  kind: callable
  ns: scrapy.mail
  description: |-
    Read bytes into buffer.

    Returns number of bytes read (0 for EOF), or None if the object
    is set not to block and has no data to read.
  summary: Read bytes into buffer
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: buffer
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.readinto1
  kind: callable
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: buffer
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.readline
  kind: callable
  ns: scrapy.mail
  description: |-
    Next line from the file, as a bytes object.

    Retain newline.  A non-negative size argument limits the maximum
    number of bytes to return (an incomplete line may be returned then).
    Return an empty bytes object at EOF.
  summary: Next line from the file, as a bytes object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: size
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.readlines
  kind: callable
  ns: scrapy.mail
  description: |-
    List of bytes objects, each a line from the file.

    Call readline() repeatedly and return a list of the lines so read.
    The optional size argument, if given, is an approximate bound on the
    total number of bytes in the lines returned.
  summary: List of bytes objects, each a line from the file
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: size
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.seek
  kind: callable
  ns: scrapy.mail
  description: |-
    Change stream position.

    Seek to byte offset pos relative to position indicated by whence:
         0  Start of stream (the default).  pos should be >= 0;
         1  Current position - pos may be negative;
         2  End of stream - pos usually negative.
    Returns the new absolute position.
  summary: Change stream position
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: pos
    default: null
    rest: false
  - kind: positional
    name: whence
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.seekable
  kind: callable
  ns: scrapy.mail
  description: Returns True if the IO object can be seeked.
  summary: Returns True if the IO object can be seeked
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.tell
  kind: callable
  ns: scrapy.mail
  description: Current file position, an integer.
  summary: Current file position, an integer
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.truncate
  kind: callable
  ns: scrapy.mail
  description: |-
    Truncate the file to at most size bytes.

    Size defaults to the current file position, as returned by tell().
    The current file position is unchanged.  Returns the new size.
  summary: Truncate the file to at most size bytes
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: size
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.writable
  kind: callable
  ns: scrapy.mail
  description: Returns True if the IO object can be written.
  summary: Returns True if the IO object can be written
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.write
  kind: callable
  ns: scrapy.mail
  description: |-
    Write bytes to file.

    Return the number of bytes written.
  summary: Write bytes to file
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: b
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: BytesIO.writelines
  kind: callable
  ns: scrapy.mail
  description: |-
    Write lines to the file.

    Note that newlines are not added.  lines can be any iterable object
    producing bytes-like objects. This is equivalent to calling write() for
    each element.
  summary: Write lines to the file
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: lines
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: COMMASPACE
  kind: const
  ns: scrapy.mail
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MIMEBase
  kind: class
  ns: scrapy.mail
  description: Base class for MIME specializations.
  summary: Base class for MIME specializations
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _maintype
    default: null
    rest: false
  - kind: positional
    name: _subtype
    default: null
    rest: false
  - name: policy
    default: None
    rest: false
    kind: kw-only
  - type: MIMEBase
  inherits_from:
  - <class 'email.message.Message'>
- name: MIMEBase.add_header
  kind: method
  ns: scrapy.mail
  description: |-
    Extended header setting.

    name is the header field to add.  keyword arguments can be used to set
    additional parameters for the header field, with underscores converted
    to dashes.  Normally the parameter will be added as key="value" unless
    value is None, in which case only the key will be added.  If a
    parameter value contains non-ASCII characters it can be specified as a
    three-tuple of (charset, language, value), in which case it will be
    encoded according to RFC2231 rules.  Otherwise it will be encoded using
    the utf-8 charset and a language of ''.

    Examples:

    msg.add_header('content-disposition', 'attachment', filename='bud.gif')
    msg.add_header('content-disposition', 'attachment',
                   filename=('utf-8', '', Fuballer.ppt'))
    msg.add_header('content-disposition', 'attachment',
                   filename='Fuballer.ppt'))
  summary: Extended header setting
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _name
    default: null
    rest: false
  - kind: positional
    name: _value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.as_bytes
  kind: method
  ns: scrapy.mail
  description: |-
    Return the entire formatted message as a bytes object.

    Optional 'unixfrom', when true, means include the Unix From_ envelope
    header.  'policy' is passed to the BytesGenerator instance used to
    serialize the message; if not specified the policy associated with
    the message instance is used.
  summary: Return the entire formatted message as a bytes object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: 'False'
    rest: false
  - kind: positional
    name: policy
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.as_string
  kind: method
  ns: scrapy.mail
  description: |-
    Return the entire formatted message as a string.

    Optional 'unixfrom', when true, means include the Unix From_ envelope
    header.  For backward compatibility reasons, if maxheaderlen is
    not specified it defaults to 0, so you must override it explicitly
    if you want a different maxheaderlen.  'policy' is passed to the
    Generator instance used to serialize the message; if it is not
    specified the policy associated with the message instance is used.

    If the message object contains binary data that is not encoded
    according to RFC standards, the non-compliant data will be replaced by
    unicode "unknown character" code points.
  summary: Return the entire formatted message as a string
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: 'False'
    rest: false
  - kind: positional
    name: maxheaderlen
    default: '0'
    rest: false
  - kind: positional
    name: policy
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.attach
  kind: method
  ns: scrapy.mail
  description: |-
    Add the given payload to the current payload.

    The current payload will always be a list of objects after this method
    is called.  If you want to set the payload to a scalar object, use
    set_payload() instead.
  summary: Add the given payload to the current payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: payload
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.del_param
  kind: method
  ns: scrapy.mail
  description: |-
    Remove the given parameter completely from the Content-Type header.

    The header will be re-written in place without the parameter or its
    value. All values will be quoted as necessary unless requote is
    False.  Optional header specifies an alternative to the Content-Type
    header.
  summary: Remove the given parameter completely from the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get
  kind: method
  ns: scrapy.mail
  description: |-
    Get a header value.

    Like __getitem__() but return failobj instead of None when the field
    is missing.
  summary: Get a header value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_all
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the values for the named field.

    These will be sorted in the order they appeared in the original
    message, and may contain duplicates.  Any fields deleted and
    re-inserted are always appended to the header list.

    If no such fields exist, failobj is returned (defaults to None).
  summary: Return a list of all the values for the named field
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_boundary
  kind: method
  ns: scrapy.mail
  description: |-
    Return the boundary associated with the payload if present.

    The boundary is extracted from the Content-Type header's `boundary'
    parameter, and it is unquoted.
  summary: Return the boundary associated with the payload if present
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_charset
  kind: method
  ns: scrapy.mail
  description: "Return the Charset instance associated with the message's payload.\n        "
  summary: Return the Charset instance associated with the message's payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_charsets
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list containing the charset(s) used in this message.

    The returned list of items describes the Content-Type headers'
    charset parameter for this message and all the subparts in its
    payload.

    Each item will either be a string (the value of the charset parameter
    in the Content-Type header of that part) or the value of the
    'failobj' parameter (defaults to None), if the part does not have a
    main MIME type of "text", or the charset is not defined.

    The list will contain one string for each part of the message, plus
    one for the container message (i.e. self), so that a non-multipart
    message will still return a list of length 1.
  summary: Return a list containing the charset(s) used in this message
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_content_charset
  kind: method
  ns: scrapy.mail
  description: |-
    Return the charset parameter of the Content-Type header.

    The returned string is always coerced to lower case.  If there is no
    Content-Type header, or if that header has no charset parameter,
    failobj is returned.
  summary: Return the charset parameter of the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_content_disposition
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's content-disposition if it exists, or None.

    The return values can be either 'inline', 'attachment' or None
    according to the rfc2183.
  summary: Return the message's content-disposition if it exists, or None
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_content_maintype
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's main content type.

    This is the `maintype' part of the string returned by
    get_content_type().
  summary: Return the message's main content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_content_subtype
  kind: method
  ns: scrapy.mail
  description: |-
    Returns the message's sub-content type.

    This is the `subtype' part of the string returned by
    get_content_type().
  summary: Returns the message's sub-content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_content_type
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's content type.

    The returned string is coerced to lower case of the form
    `maintype/subtype'.  If there was no Content-Type header in the
    message, the default type as given by get_default_type() will be
    returned.  Since according to RFC 2045, messages always have a default
    type this will always return a value.

    RFC 2045 defines a message's default type to be text/plain unless it
    appears inside a multipart/digest container, in which case it would be
    message/rfc822.
  summary: Return the message's content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_default_type
  kind: method
  ns: scrapy.mail
  description: |-
    Return the `default' content type.

    Most messages have a default content type of text/plain, except for
    messages that are subparts of multipart/digest containers.  Such
    subparts have a default content type of message/rfc822.
  summary: Return the `default' content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_filename
  kind: method
  ns: scrapy.mail
  description: |-
    Return the filename associated with the payload if present.

    The filename is extracted from the Content-Disposition header's
    `filename' parameter, and it is unquoted.  If that header is missing
    the `filename' parameter, this method falls back to looking for the
    `name' parameter.
  summary: Return the filename associated with the payload if present
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_param
  kind: method
  ns: scrapy.mail
  description: |-
    Return the parameter value if found in the Content-Type header.

    Optional failobj is the object to return if there is no Content-Type
    header, or the Content-Type header has no such parameter.  Optional
    header is the header to search instead of Content-Type.

    Parameter keys are always compared case insensitively.  The return
    value can either be a string, or a 3-tuple if the parameter was RFC
    2231 encoded.  When it's a 3-tuple, the elements of the value are of
    the form (CHARSET, LANGUAGE, VALUE).  Note that both CHARSET and
    LANGUAGE can be None, in which case you should consider VALUE to be
    encoded in the us-ascii charset.  You can usually ignore LANGUAGE.
    The parameter value (either the returned string, or the VALUE item in
    the 3-tuple) is always unquoted, unless unquote is set to False.

    If your application doesn't care whether the parameter was RFC 2231
    encoded, it can turn the return value into a string as follows:

        rawparam = msg.get_param('foo')
        param = email.utils.collapse_rfc2231_value(rawparam)
  summary: Return the parameter value if found in the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: unquote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_params
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's Content-Type parameters, as a list.

    The elements of the returned list are 2-tuples of key/value pairs, as
    split on the `=' sign.  The left hand side of the `=' is the key,
    while the right hand side is the value.  If there is no `=' sign in
    the parameter the value is the empty string.  The value is as
    described in the get_param() method.

    Optional failobj is the object to return if there is no Content-Type
    header.  Optional header is the header to search instead of
    Content-Type.  If unquote is True, the value is unquoted.
  summary: Return the message's Content-Type parameters, as a list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: unquote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_payload
  kind: method
  ns: scrapy.mail
  description: |-
    Return a reference to the payload.

    The payload will either be a list object or a string.  If you mutate
    the list object, you modify the message's payload in place.  Optional
    i returns that index into the payload.

    Optional decode is a flag indicating whether the payload should be
    decoded or not, according to the Content-Transfer-Encoding header
    (default is False).

    When True and the message is not a multipart, the payload will be
    decoded if this header's value is `quoted-printable' or `base64'.  If
    some other encoding is used, or the header is missing, or if the
    payload has bogus data (i.e. bogus base64 or uuencoded data), the
    payload is returned as-is.

    If the message is a multipart and the decode flag is True, then None
    is returned.
  summary: Return a reference to the payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: i
    default: None
    rest: false
  - kind: positional
    name: decode
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.get_unixfrom
  kind: method
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.is_multipart
  kind: method
  ns: scrapy.mail
  description: Return True if the message consists of multiple parts.
  summary: Return True if the message consists of multiple parts
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.items
  kind: method
  ns: scrapy.mail
  description: |-
    Get all the message's header fields and values.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Get all the message's header fields and values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.keys
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the message's header field names.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Return a list of all the message's header field names
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.raw_items
  kind: method
  ns: scrapy.mail
  description: |-
    Return the (name, value) header pairs without modification.

    This is an "internal" API, intended only for use by a generator.
  summary: Return the (name, value) header pairs without modification
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.replace_header
  kind: method
  ns: scrapy.mail
  description: |-
    Replace a header.

    Replace the first matching header found in the message, retaining
    header order and case.  If no matching header was found, a KeyError is
    raised.
  summary: Replace a header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _name
    default: null
    rest: false
  - kind: positional
    name: _value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.set_boundary
  kind: method
  ns: scrapy.mail
  description: |-
    Set the boundary parameter in Content-Type to 'boundary'.

    This is subtly different than deleting the Content-Type header and
    adding a new one with a new boundary parameter via add_header().  The
    main difference is that using the set_boundary() method preserves the
    order of the Content-Type header in the original message.

    HeaderParseError is raised if the message has no Content-Type header.
  summary: Set the boundary parameter in Content-Type to 'boundary'
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: boundary
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.set_charset
  kind: method
  ns: scrapy.mail
  description: |-
    Set the charset of the payload to a given character set.

    charset can be a Charset instance, a string naming a character set, or
    None.  If it is a string it will be converted to a Charset instance.
    If charset is None, the charset parameter will be removed from the
    Content-Type field.  Anything else will generate a TypeError.

    The message will be assumed to be of type text/* encoded with
    charset.input_charset.  It will be converted to charset.output_charset
    and encoded properly, if needed, when generating the plain text
    representation of the message.  MIME headers (MIME-Version,
    Content-Type, Content-Transfer-Encoding) will be added as needed.
  summary: Set the charset of the payload to a given character set
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: charset
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.set_default_type
  kind: method
  ns: scrapy.mail
  description: |-
    Set the `default' content type.

    ctype should be either "text/plain" or "message/rfc822", although this
    is not enforced.  The default content type is not stored in the
    Content-Type header.
  summary: Set the `default' content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: ctype
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.set_param
  kind: method
  ns: scrapy.mail
  description: |-
    Set a parameter in the Content-Type header.

    If the parameter already exists in the header, its value will be
    replaced with the new value.

    If header is Content-Type and has not yet been defined for this
    message, it will be set to "text/plain" and the new parameter and
    value will be appended as per RFC 2045.

    An alternate header can be specified in the header argument, and all
    parameters will be quoted as necessary unless requote is False.

    If charset is specified, the parameter will be encoded according to RFC
    2231.  Optional language specifies the RFC 2231 language, defaulting
    to the empty string.  Both charset and language should be strings.
  summary: Set a parameter in the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: header
    default: Content-Type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - kind: positional
    name: charset
    default: None
    rest: false
  - kind: positional
    name: language
    default: null
    rest: false
  - kind: positional
    name: replace
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.set_payload
  kind: method
  ns: scrapy.mail
  description: |-
    Set the payload to the given value.

    Optional charset sets the message's default character set.  See
    set_charset() for details.
  summary: Set the payload to the given value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: payload
    default: null
    rest: false
  - kind: positional
    name: charset
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.set_raw
  kind: method
  ns: scrapy.mail
  description: |-
    Store name and value in the model without modification.

    This is an "internal" API, intended only for use by a parser.
  summary: Store name and value in the model without modification
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.set_type
  kind: method
  ns: scrapy.mail
  description: |-
    Set the main type and subtype for the Content-Type header.

    type must be a string in the form "maintype/subtype", otherwise a
    ValueError is raised.

    This method replaces the Content-Type header, keeping all the
    parameters in place.  If requote is False, this leaves the existing
    header's quoting as is.  Otherwise, the parameters will be quoted (the
    default).

    An alternative header can be specified in the header argument.  When
    the Content-Type header is set, we'll always also add a MIME-Version
    header.
  summary: Set the main type and subtype for the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: type
    default: null
    rest: false
  - kind: positional
    name: header
    default: Content-Type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.set_unixfrom
  kind: method
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.values
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the message's header values.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Return a list of all the message's header values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEBase.walk
  kind: method
  ns: scrapy.mail
  description: |-
    Walk over the message tree, yielding each subpart.

    The walk is performed in depth-first order.  This method is a
    generator.
  summary: Walk over the message tree, yielding each subpart
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart
  kind: class
  ns: scrapy.mail
  description: Base class for MIME multipart/* type messages.
  summary: Base class for MIME multipart/* type messages
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _subtype
    default: mixed
    rest: false
  - kind: positional
    name: boundary
    default: None
    rest: false
  - kind: positional
    name: _subparts
    default: None
    rest: false
  - name: policy
    default: None
    rest: false
    kind: kw-only
  - type: MIMEMultipart
  inherits_from:
  - <class 'email.mime.base.MIMEBase'>
  - <class 'email.message.Message'>
- name: MIMEMultipart.add_header
  kind: method
  ns: scrapy.mail
  description: |-
    Extended header setting.

    name is the header field to add.  keyword arguments can be used to set
    additional parameters for the header field, with underscores converted
    to dashes.  Normally the parameter will be added as key="value" unless
    value is None, in which case only the key will be added.  If a
    parameter value contains non-ASCII characters it can be specified as a
    three-tuple of (charset, language, value), in which case it will be
    encoded according to RFC2231 rules.  Otherwise it will be encoded using
    the utf-8 charset and a language of ''.

    Examples:

    msg.add_header('content-disposition', 'attachment', filename='bud.gif')
    msg.add_header('content-disposition', 'attachment',
                   filename=('utf-8', '', Fuballer.ppt'))
    msg.add_header('content-disposition', 'attachment',
                   filename='Fuballer.ppt'))
  summary: Extended header setting
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _name
    default: null
    rest: false
  - kind: positional
    name: _value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.as_bytes
  kind: method
  ns: scrapy.mail
  description: |-
    Return the entire formatted message as a bytes object.

    Optional 'unixfrom', when true, means include the Unix From_ envelope
    header.  'policy' is passed to the BytesGenerator instance used to
    serialize the message; if not specified the policy associated with
    the message instance is used.
  summary: Return the entire formatted message as a bytes object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: 'False'
    rest: false
  - kind: positional
    name: policy
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.as_string
  kind: method
  ns: scrapy.mail
  description: |-
    Return the entire formatted message as a string.

    Optional 'unixfrom', when true, means include the Unix From_ envelope
    header.  For backward compatibility reasons, if maxheaderlen is
    not specified it defaults to 0, so you must override it explicitly
    if you want a different maxheaderlen.  'policy' is passed to the
    Generator instance used to serialize the message; if it is not
    specified the policy associated with the message instance is used.

    If the message object contains binary data that is not encoded
    according to RFC standards, the non-compliant data will be replaced by
    unicode "unknown character" code points.
  summary: Return the entire formatted message as a string
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: 'False'
    rest: false
  - kind: positional
    name: maxheaderlen
    default: '0'
    rest: false
  - kind: positional
    name: policy
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.attach
  kind: method
  ns: scrapy.mail
  description: |-
    Add the given payload to the current payload.

    The current payload will always be a list of objects after this method
    is called.  If you want to set the payload to a scalar object, use
    set_payload() instead.
  summary: Add the given payload to the current payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: payload
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.del_param
  kind: method
  ns: scrapy.mail
  description: |-
    Remove the given parameter completely from the Content-Type header.

    The header will be re-written in place without the parameter or its
    value. All values will be quoted as necessary unless requote is
    False.  Optional header specifies an alternative to the Content-Type
    header.
  summary: Remove the given parameter completely from the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get
  kind: method
  ns: scrapy.mail
  description: |-
    Get a header value.

    Like __getitem__() but return failobj instead of None when the field
    is missing.
  summary: Get a header value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_all
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the values for the named field.

    These will be sorted in the order they appeared in the original
    message, and may contain duplicates.  Any fields deleted and
    re-inserted are always appended to the header list.

    If no such fields exist, failobj is returned (defaults to None).
  summary: Return a list of all the values for the named field
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_boundary
  kind: method
  ns: scrapy.mail
  description: |-
    Return the boundary associated with the payload if present.

    The boundary is extracted from the Content-Type header's `boundary'
    parameter, and it is unquoted.
  summary: Return the boundary associated with the payload if present
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_charset
  kind: method
  ns: scrapy.mail
  description: "Return the Charset instance associated with the message's payload.\n        "
  summary: Return the Charset instance associated with the message's payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_charsets
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list containing the charset(s) used in this message.

    The returned list of items describes the Content-Type headers'
    charset parameter for this message and all the subparts in its
    payload.

    Each item will either be a string (the value of the charset parameter
    in the Content-Type header of that part) or the value of the
    'failobj' parameter (defaults to None), if the part does not have a
    main MIME type of "text", or the charset is not defined.

    The list will contain one string for each part of the message, plus
    one for the container message (i.e. self), so that a non-multipart
    message will still return a list of length 1.
  summary: Return a list containing the charset(s) used in this message
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_content_charset
  kind: method
  ns: scrapy.mail
  description: |-
    Return the charset parameter of the Content-Type header.

    The returned string is always coerced to lower case.  If there is no
    Content-Type header, or if that header has no charset parameter,
    failobj is returned.
  summary: Return the charset parameter of the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_content_disposition
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's content-disposition if it exists, or None.

    The return values can be either 'inline', 'attachment' or None
    according to the rfc2183.
  summary: Return the message's content-disposition if it exists, or None
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_content_maintype
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's main content type.

    This is the `maintype' part of the string returned by
    get_content_type().
  summary: Return the message's main content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_content_subtype
  kind: method
  ns: scrapy.mail
  description: |-
    Returns the message's sub-content type.

    This is the `subtype' part of the string returned by
    get_content_type().
  summary: Returns the message's sub-content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_content_type
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's content type.

    The returned string is coerced to lower case of the form
    `maintype/subtype'.  If there was no Content-Type header in the
    message, the default type as given by get_default_type() will be
    returned.  Since according to RFC 2045, messages always have a default
    type this will always return a value.

    RFC 2045 defines a message's default type to be text/plain unless it
    appears inside a multipart/digest container, in which case it would be
    message/rfc822.
  summary: Return the message's content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_default_type
  kind: method
  ns: scrapy.mail
  description: |-
    Return the `default' content type.

    Most messages have a default content type of text/plain, except for
    messages that are subparts of multipart/digest containers.  Such
    subparts have a default content type of message/rfc822.
  summary: Return the `default' content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_filename
  kind: method
  ns: scrapy.mail
  description: |-
    Return the filename associated with the payload if present.

    The filename is extracted from the Content-Disposition header's
    `filename' parameter, and it is unquoted.  If that header is missing
    the `filename' parameter, this method falls back to looking for the
    `name' parameter.
  summary: Return the filename associated with the payload if present
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_param
  kind: method
  ns: scrapy.mail
  description: |-
    Return the parameter value if found in the Content-Type header.

    Optional failobj is the object to return if there is no Content-Type
    header, or the Content-Type header has no such parameter.  Optional
    header is the header to search instead of Content-Type.

    Parameter keys are always compared case insensitively.  The return
    value can either be a string, or a 3-tuple if the parameter was RFC
    2231 encoded.  When it's a 3-tuple, the elements of the value are of
    the form (CHARSET, LANGUAGE, VALUE).  Note that both CHARSET and
    LANGUAGE can be None, in which case you should consider VALUE to be
    encoded in the us-ascii charset.  You can usually ignore LANGUAGE.
    The parameter value (either the returned string, or the VALUE item in
    the 3-tuple) is always unquoted, unless unquote is set to False.

    If your application doesn't care whether the parameter was RFC 2231
    encoded, it can turn the return value into a string as follows:

        rawparam = msg.get_param('foo')
        param = email.utils.collapse_rfc2231_value(rawparam)
  summary: Return the parameter value if found in the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: unquote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_params
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's Content-Type parameters, as a list.

    The elements of the returned list are 2-tuples of key/value pairs, as
    split on the `=' sign.  The left hand side of the `=' is the key,
    while the right hand side is the value.  If there is no `=' sign in
    the parameter the value is the empty string.  The value is as
    described in the get_param() method.

    Optional failobj is the object to return if there is no Content-Type
    header.  Optional header is the header to search instead of
    Content-Type.  If unquote is True, the value is unquoted.
  summary: Return the message's Content-Type parameters, as a list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: unquote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_payload
  kind: method
  ns: scrapy.mail
  description: |-
    Return a reference to the payload.

    The payload will either be a list object or a string.  If you mutate
    the list object, you modify the message's payload in place.  Optional
    i returns that index into the payload.

    Optional decode is a flag indicating whether the payload should be
    decoded or not, according to the Content-Transfer-Encoding header
    (default is False).

    When True and the message is not a multipart, the payload will be
    decoded if this header's value is `quoted-printable' or `base64'.  If
    some other encoding is used, or the header is missing, or if the
    payload has bogus data (i.e. bogus base64 or uuencoded data), the
    payload is returned as-is.

    If the message is a multipart and the decode flag is True, then None
    is returned.
  summary: Return a reference to the payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: i
    default: None
    rest: false
  - kind: positional
    name: decode
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.get_unixfrom
  kind: method
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.is_multipart
  kind: method
  ns: scrapy.mail
  description: Return True if the message consists of multiple parts.
  summary: Return True if the message consists of multiple parts
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.items
  kind: method
  ns: scrapy.mail
  description: |-
    Get all the message's header fields and values.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Get all the message's header fields and values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.keys
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the message's header field names.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Return a list of all the message's header field names
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.raw_items
  kind: method
  ns: scrapy.mail
  description: |-
    Return the (name, value) header pairs without modification.

    This is an "internal" API, intended only for use by a generator.
  summary: Return the (name, value) header pairs without modification
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.replace_header
  kind: method
  ns: scrapy.mail
  description: |-
    Replace a header.

    Replace the first matching header found in the message, retaining
    header order and case.  If no matching header was found, a KeyError is
    raised.
  summary: Replace a header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _name
    default: null
    rest: false
  - kind: positional
    name: _value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.set_boundary
  kind: method
  ns: scrapy.mail
  description: |-
    Set the boundary parameter in Content-Type to 'boundary'.

    This is subtly different than deleting the Content-Type header and
    adding a new one with a new boundary parameter via add_header().  The
    main difference is that using the set_boundary() method preserves the
    order of the Content-Type header in the original message.

    HeaderParseError is raised if the message has no Content-Type header.
  summary: Set the boundary parameter in Content-Type to 'boundary'
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: boundary
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.set_charset
  kind: method
  ns: scrapy.mail
  description: |-
    Set the charset of the payload to a given character set.

    charset can be a Charset instance, a string naming a character set, or
    None.  If it is a string it will be converted to a Charset instance.
    If charset is None, the charset parameter will be removed from the
    Content-Type field.  Anything else will generate a TypeError.

    The message will be assumed to be of type text/* encoded with
    charset.input_charset.  It will be converted to charset.output_charset
    and encoded properly, if needed, when generating the plain text
    representation of the message.  MIME headers (MIME-Version,
    Content-Type, Content-Transfer-Encoding) will be added as needed.
  summary: Set the charset of the payload to a given character set
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: charset
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.set_default_type
  kind: method
  ns: scrapy.mail
  description: |-
    Set the `default' content type.

    ctype should be either "text/plain" or "message/rfc822", although this
    is not enforced.  The default content type is not stored in the
    Content-Type header.
  summary: Set the `default' content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: ctype
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.set_param
  kind: method
  ns: scrapy.mail
  description: |-
    Set a parameter in the Content-Type header.

    If the parameter already exists in the header, its value will be
    replaced with the new value.

    If header is Content-Type and has not yet been defined for this
    message, it will be set to "text/plain" and the new parameter and
    value will be appended as per RFC 2045.

    An alternate header can be specified in the header argument, and all
    parameters will be quoted as necessary unless requote is False.

    If charset is specified, the parameter will be encoded according to RFC
    2231.  Optional language specifies the RFC 2231 language, defaulting
    to the empty string.  Both charset and language should be strings.
  summary: Set a parameter in the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: header
    default: Content-Type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - kind: positional
    name: charset
    default: None
    rest: false
  - kind: positional
    name: language
    default: null
    rest: false
  - kind: positional
    name: replace
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.set_payload
  kind: method
  ns: scrapy.mail
  description: |-
    Set the payload to the given value.

    Optional charset sets the message's default character set.  See
    set_charset() for details.
  summary: Set the payload to the given value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: payload
    default: null
    rest: false
  - kind: positional
    name: charset
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.set_raw
  kind: method
  ns: scrapy.mail
  description: |-
    Store name and value in the model without modification.

    This is an "internal" API, intended only for use by a parser.
  summary: Store name and value in the model without modification
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.set_type
  kind: method
  ns: scrapy.mail
  description: |-
    Set the main type and subtype for the Content-Type header.

    type must be a string in the form "maintype/subtype", otherwise a
    ValueError is raised.

    This method replaces the Content-Type header, keeping all the
    parameters in place.  If requote is False, this leaves the existing
    header's quoting as is.  Otherwise, the parameters will be quoted (the
    default).

    An alternative header can be specified in the header argument.  When
    the Content-Type header is set, we'll always also add a MIME-Version
    header.
  summary: Set the main type and subtype for the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: type
    default: null
    rest: false
  - kind: positional
    name: header
    default: Content-Type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.set_unixfrom
  kind: method
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.values
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the message's header values.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Return a list of all the message's header values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEMultipart.walk
  kind: method
  ns: scrapy.mail
  description: |-
    Walk over the message tree, yielding each subpart.

    The walk is performed in depth-first order.  This method is a
    generator.
  summary: Walk over the message tree, yielding each subpart
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart
  kind: class
  ns: scrapy.mail
  description: Base class for MIME non-multipart type messages.
  summary: Base class for MIME non-multipart type messages
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _maintype
    default: null
    rest: false
  - kind: positional
    name: _subtype
    default: null
    rest: false
  - name: policy
    default: None
    rest: false
    kind: kw-only
  - type: MIMENonMultipart
  inherits_from:
  - <class 'email.mime.base.MIMEBase'>
  - <class 'email.message.Message'>
- name: MIMENonMultipart.add_header
  kind: method
  ns: scrapy.mail
  description: |-
    Extended header setting.

    name is the header field to add.  keyword arguments can be used to set
    additional parameters for the header field, with underscores converted
    to dashes.  Normally the parameter will be added as key="value" unless
    value is None, in which case only the key will be added.  If a
    parameter value contains non-ASCII characters it can be specified as a
    three-tuple of (charset, language, value), in which case it will be
    encoded according to RFC2231 rules.  Otherwise it will be encoded using
    the utf-8 charset and a language of ''.

    Examples:

    msg.add_header('content-disposition', 'attachment', filename='bud.gif')
    msg.add_header('content-disposition', 'attachment',
                   filename=('utf-8', '', Fuballer.ppt'))
    msg.add_header('content-disposition', 'attachment',
                   filename='Fuballer.ppt'))
  summary: Extended header setting
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _name
    default: null
    rest: false
  - kind: positional
    name: _value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.as_bytes
  kind: method
  ns: scrapy.mail
  description: |-
    Return the entire formatted message as a bytes object.

    Optional 'unixfrom', when true, means include the Unix From_ envelope
    header.  'policy' is passed to the BytesGenerator instance used to
    serialize the message; if not specified the policy associated with
    the message instance is used.
  summary: Return the entire formatted message as a bytes object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: 'False'
    rest: false
  - kind: positional
    name: policy
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.as_string
  kind: method
  ns: scrapy.mail
  description: |-
    Return the entire formatted message as a string.

    Optional 'unixfrom', when true, means include the Unix From_ envelope
    header.  For backward compatibility reasons, if maxheaderlen is
    not specified it defaults to 0, so you must override it explicitly
    if you want a different maxheaderlen.  'policy' is passed to the
    Generator instance used to serialize the message; if it is not
    specified the policy associated with the message instance is used.

    If the message object contains binary data that is not encoded
    according to RFC standards, the non-compliant data will be replaced by
    unicode "unknown character" code points.
  summary: Return the entire formatted message as a string
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: 'False'
    rest: false
  - kind: positional
    name: maxheaderlen
    default: '0'
    rest: false
  - kind: positional
    name: policy
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.attach
  kind: method
  ns: scrapy.mail
  description: |-
    Add the given payload to the current payload.

    The current payload will always be a list of objects after this method
    is called.  If you want to set the payload to a scalar object, use
    set_payload() instead.
  summary: Add the given payload to the current payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: payload
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.del_param
  kind: method
  ns: scrapy.mail
  description: |-
    Remove the given parameter completely from the Content-Type header.

    The header will be re-written in place without the parameter or its
    value. All values will be quoted as necessary unless requote is
    False.  Optional header specifies an alternative to the Content-Type
    header.
  summary: Remove the given parameter completely from the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get
  kind: method
  ns: scrapy.mail
  description: |-
    Get a header value.

    Like __getitem__() but return failobj instead of None when the field
    is missing.
  summary: Get a header value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_all
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the values for the named field.

    These will be sorted in the order they appeared in the original
    message, and may contain duplicates.  Any fields deleted and
    re-inserted are always appended to the header list.

    If no such fields exist, failobj is returned (defaults to None).
  summary: Return a list of all the values for the named field
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_boundary
  kind: method
  ns: scrapy.mail
  description: |-
    Return the boundary associated with the payload if present.

    The boundary is extracted from the Content-Type header's `boundary'
    parameter, and it is unquoted.
  summary: Return the boundary associated with the payload if present
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_charset
  kind: method
  ns: scrapy.mail
  description: "Return the Charset instance associated with the message's payload.\n        "
  summary: Return the Charset instance associated with the message's payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_charsets
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list containing the charset(s) used in this message.

    The returned list of items describes the Content-Type headers'
    charset parameter for this message and all the subparts in its
    payload.

    Each item will either be a string (the value of the charset parameter
    in the Content-Type header of that part) or the value of the
    'failobj' parameter (defaults to None), if the part does not have a
    main MIME type of "text", or the charset is not defined.

    The list will contain one string for each part of the message, plus
    one for the container message (i.e. self), so that a non-multipart
    message will still return a list of length 1.
  summary: Return a list containing the charset(s) used in this message
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_content_charset
  kind: method
  ns: scrapy.mail
  description: |-
    Return the charset parameter of the Content-Type header.

    The returned string is always coerced to lower case.  If there is no
    Content-Type header, or if that header has no charset parameter,
    failobj is returned.
  summary: Return the charset parameter of the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_content_disposition
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's content-disposition if it exists, or None.

    The return values can be either 'inline', 'attachment' or None
    according to the rfc2183.
  summary: Return the message's content-disposition if it exists, or None
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_content_maintype
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's main content type.

    This is the `maintype' part of the string returned by
    get_content_type().
  summary: Return the message's main content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_content_subtype
  kind: method
  ns: scrapy.mail
  description: |-
    Returns the message's sub-content type.

    This is the `subtype' part of the string returned by
    get_content_type().
  summary: Returns the message's sub-content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_content_type
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's content type.

    The returned string is coerced to lower case of the form
    `maintype/subtype'.  If there was no Content-Type header in the
    message, the default type as given by get_default_type() will be
    returned.  Since according to RFC 2045, messages always have a default
    type this will always return a value.

    RFC 2045 defines a message's default type to be text/plain unless it
    appears inside a multipart/digest container, in which case it would be
    message/rfc822.
  summary: Return the message's content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_default_type
  kind: method
  ns: scrapy.mail
  description: |-
    Return the `default' content type.

    Most messages have a default content type of text/plain, except for
    messages that are subparts of multipart/digest containers.  Such
    subparts have a default content type of message/rfc822.
  summary: Return the `default' content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_filename
  kind: method
  ns: scrapy.mail
  description: |-
    Return the filename associated with the payload if present.

    The filename is extracted from the Content-Disposition header's
    `filename' parameter, and it is unquoted.  If that header is missing
    the `filename' parameter, this method falls back to looking for the
    `name' parameter.
  summary: Return the filename associated with the payload if present
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_param
  kind: method
  ns: scrapy.mail
  description: |-
    Return the parameter value if found in the Content-Type header.

    Optional failobj is the object to return if there is no Content-Type
    header, or the Content-Type header has no such parameter.  Optional
    header is the header to search instead of Content-Type.

    Parameter keys are always compared case insensitively.  The return
    value can either be a string, or a 3-tuple if the parameter was RFC
    2231 encoded.  When it's a 3-tuple, the elements of the value are of
    the form (CHARSET, LANGUAGE, VALUE).  Note that both CHARSET and
    LANGUAGE can be None, in which case you should consider VALUE to be
    encoded in the us-ascii charset.  You can usually ignore LANGUAGE.
    The parameter value (either the returned string, or the VALUE item in
    the 3-tuple) is always unquoted, unless unquote is set to False.

    If your application doesn't care whether the parameter was RFC 2231
    encoded, it can turn the return value into a string as follows:

        rawparam = msg.get_param('foo')
        param = email.utils.collapse_rfc2231_value(rawparam)
  summary: Return the parameter value if found in the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: unquote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_params
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's Content-Type parameters, as a list.

    The elements of the returned list are 2-tuples of key/value pairs, as
    split on the `=' sign.  The left hand side of the `=' is the key,
    while the right hand side is the value.  If there is no `=' sign in
    the parameter the value is the empty string.  The value is as
    described in the get_param() method.

    Optional failobj is the object to return if there is no Content-Type
    header.  Optional header is the header to search instead of
    Content-Type.  If unquote is True, the value is unquoted.
  summary: Return the message's Content-Type parameters, as a list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: unquote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_payload
  kind: method
  ns: scrapy.mail
  description: |-
    Return a reference to the payload.

    The payload will either be a list object or a string.  If you mutate
    the list object, you modify the message's payload in place.  Optional
    i returns that index into the payload.

    Optional decode is a flag indicating whether the payload should be
    decoded or not, according to the Content-Transfer-Encoding header
    (default is False).

    When True and the message is not a multipart, the payload will be
    decoded if this header's value is `quoted-printable' or `base64'.  If
    some other encoding is used, or the header is missing, or if the
    payload has bogus data (i.e. bogus base64 or uuencoded data), the
    payload is returned as-is.

    If the message is a multipart and the decode flag is True, then None
    is returned.
  summary: Return a reference to the payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: i
    default: None
    rest: false
  - kind: positional
    name: decode
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.get_unixfrom
  kind: method
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.is_multipart
  kind: method
  ns: scrapy.mail
  description: Return True if the message consists of multiple parts.
  summary: Return True if the message consists of multiple parts
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.items
  kind: method
  ns: scrapy.mail
  description: |-
    Get all the message's header fields and values.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Get all the message's header fields and values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.keys
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the message's header field names.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Return a list of all the message's header field names
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.raw_items
  kind: method
  ns: scrapy.mail
  description: |-
    Return the (name, value) header pairs without modification.

    This is an "internal" API, intended only for use by a generator.
  summary: Return the (name, value) header pairs without modification
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.replace_header
  kind: method
  ns: scrapy.mail
  description: |-
    Replace a header.

    Replace the first matching header found in the message, retaining
    header order and case.  If no matching header was found, a KeyError is
    raised.
  summary: Replace a header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _name
    default: null
    rest: false
  - kind: positional
    name: _value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.set_boundary
  kind: method
  ns: scrapy.mail
  description: |-
    Set the boundary parameter in Content-Type to 'boundary'.

    This is subtly different than deleting the Content-Type header and
    adding a new one with a new boundary parameter via add_header().  The
    main difference is that using the set_boundary() method preserves the
    order of the Content-Type header in the original message.

    HeaderParseError is raised if the message has no Content-Type header.
  summary: Set the boundary parameter in Content-Type to 'boundary'
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: boundary
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.set_charset
  kind: method
  ns: scrapy.mail
  description: |-
    Set the charset of the payload to a given character set.

    charset can be a Charset instance, a string naming a character set, or
    None.  If it is a string it will be converted to a Charset instance.
    If charset is None, the charset parameter will be removed from the
    Content-Type field.  Anything else will generate a TypeError.

    The message will be assumed to be of type text/* encoded with
    charset.input_charset.  It will be converted to charset.output_charset
    and encoded properly, if needed, when generating the plain text
    representation of the message.  MIME headers (MIME-Version,
    Content-Type, Content-Transfer-Encoding) will be added as needed.
  summary: Set the charset of the payload to a given character set
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: charset
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.set_default_type
  kind: method
  ns: scrapy.mail
  description: |-
    Set the `default' content type.

    ctype should be either "text/plain" or "message/rfc822", although this
    is not enforced.  The default content type is not stored in the
    Content-Type header.
  summary: Set the `default' content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: ctype
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.set_param
  kind: method
  ns: scrapy.mail
  description: |-
    Set a parameter in the Content-Type header.

    If the parameter already exists in the header, its value will be
    replaced with the new value.

    If header is Content-Type and has not yet been defined for this
    message, it will be set to "text/plain" and the new parameter and
    value will be appended as per RFC 2045.

    An alternate header can be specified in the header argument, and all
    parameters will be quoted as necessary unless requote is False.

    If charset is specified, the parameter will be encoded according to RFC
    2231.  Optional language specifies the RFC 2231 language, defaulting
    to the empty string.  Both charset and language should be strings.
  summary: Set a parameter in the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: header
    default: Content-Type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - kind: positional
    name: charset
    default: None
    rest: false
  - kind: positional
    name: language
    default: null
    rest: false
  - kind: positional
    name: replace
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.set_payload
  kind: method
  ns: scrapy.mail
  description: |-
    Set the payload to the given value.

    Optional charset sets the message's default character set.  See
    set_charset() for details.
  summary: Set the payload to the given value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: payload
    default: null
    rest: false
  - kind: positional
    name: charset
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.set_raw
  kind: method
  ns: scrapy.mail
  description: |-
    Store name and value in the model without modification.

    This is an "internal" API, intended only for use by a parser.
  summary: Store name and value in the model without modification
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.set_type
  kind: method
  ns: scrapy.mail
  description: |-
    Set the main type and subtype for the Content-Type header.

    type must be a string in the form "maintype/subtype", otherwise a
    ValueError is raised.

    This method replaces the Content-Type header, keeping all the
    parameters in place.  If requote is False, this leaves the existing
    header's quoting as is.  Otherwise, the parameters will be quoted (the
    default).

    An alternative header can be specified in the header argument.  When
    the Content-Type header is set, we'll always also add a MIME-Version
    header.
  summary: Set the main type and subtype for the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: type
    default: null
    rest: false
  - kind: positional
    name: header
    default: Content-Type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.set_unixfrom
  kind: method
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.values
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the message's header values.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Return a list of all the message's header values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMENonMultipart.walk
  kind: method
  ns: scrapy.mail
  description: |-
    Walk over the message tree, yielding each subpart.

    The walk is performed in depth-first order.  This method is a
    generator.
  summary: Walk over the message tree, yielding each subpart
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText
  kind: class
  ns: scrapy.mail
  description: Class for generating text/* type MIME documents.
  summary: Class for generating text/* type MIME documents
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _text
    default: null
    rest: false
  - kind: positional
    name: _subtype
    default: plain
    rest: false
  - kind: positional
    name: _charset
    default: None
    rest: false
  - name: policy
    default: None
    rest: false
    kind: kw-only
  - type: MIMEText
  inherits_from:
  - <class 'email.mime.nonmultipart.MIMENonMultipart'>
  - <class 'email.mime.base.MIMEBase'>
  - <class 'email.message.Message'>
- name: MIMEText.add_header
  kind: method
  ns: scrapy.mail
  description: |-
    Extended header setting.

    name is the header field to add.  keyword arguments can be used to set
    additional parameters for the header field, with underscores converted
    to dashes.  Normally the parameter will be added as key="value" unless
    value is None, in which case only the key will be added.  If a
    parameter value contains non-ASCII characters it can be specified as a
    three-tuple of (charset, language, value), in which case it will be
    encoded according to RFC2231 rules.  Otherwise it will be encoded using
    the utf-8 charset and a language of ''.

    Examples:

    msg.add_header('content-disposition', 'attachment', filename='bud.gif')
    msg.add_header('content-disposition', 'attachment',
                   filename=('utf-8', '', Fuballer.ppt'))
    msg.add_header('content-disposition', 'attachment',
                   filename='Fuballer.ppt'))
  summary: Extended header setting
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _name
    default: null
    rest: false
  - kind: positional
    name: _value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.as_bytes
  kind: method
  ns: scrapy.mail
  description: |-
    Return the entire formatted message as a bytes object.

    Optional 'unixfrom', when true, means include the Unix From_ envelope
    header.  'policy' is passed to the BytesGenerator instance used to
    serialize the message; if not specified the policy associated with
    the message instance is used.
  summary: Return the entire formatted message as a bytes object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: 'False'
    rest: false
  - kind: positional
    name: policy
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.as_string
  kind: method
  ns: scrapy.mail
  description: |-
    Return the entire formatted message as a string.

    Optional 'unixfrom', when true, means include the Unix From_ envelope
    header.  For backward compatibility reasons, if maxheaderlen is
    not specified it defaults to 0, so you must override it explicitly
    if you want a different maxheaderlen.  'policy' is passed to the
    Generator instance used to serialize the message; if it is not
    specified the policy associated with the message instance is used.

    If the message object contains binary data that is not encoded
    according to RFC standards, the non-compliant data will be replaced by
    unicode "unknown character" code points.
  summary: Return the entire formatted message as a string
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: 'False'
    rest: false
  - kind: positional
    name: maxheaderlen
    default: '0'
    rest: false
  - kind: positional
    name: policy
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.attach
  kind: method
  ns: scrapy.mail
  description: |-
    Add the given payload to the current payload.

    The current payload will always be a list of objects after this method
    is called.  If you want to set the payload to a scalar object, use
    set_payload() instead.
  summary: Add the given payload to the current payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: payload
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.del_param
  kind: method
  ns: scrapy.mail
  description: |-
    Remove the given parameter completely from the Content-Type header.

    The header will be re-written in place without the parameter or its
    value. All values will be quoted as necessary unless requote is
    False.  Optional header specifies an alternative to the Content-Type
    header.
  summary: Remove the given parameter completely from the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get
  kind: method
  ns: scrapy.mail
  description: |-
    Get a header value.

    Like __getitem__() but return failobj instead of None when the field
    is missing.
  summary: Get a header value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_all
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the values for the named field.

    These will be sorted in the order they appeared in the original
    message, and may contain duplicates.  Any fields deleted and
    re-inserted are always appended to the header list.

    If no such fields exist, failobj is returned (defaults to None).
  summary: Return a list of all the values for the named field
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_boundary
  kind: method
  ns: scrapy.mail
  description: |-
    Return the boundary associated with the payload if present.

    The boundary is extracted from the Content-Type header's `boundary'
    parameter, and it is unquoted.
  summary: Return the boundary associated with the payload if present
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_charset
  kind: method
  ns: scrapy.mail
  description: "Return the Charset instance associated with the message's payload.\n        "
  summary: Return the Charset instance associated with the message's payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_charsets
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list containing the charset(s) used in this message.

    The returned list of items describes the Content-Type headers'
    charset parameter for this message and all the subparts in its
    payload.

    Each item will either be a string (the value of the charset parameter
    in the Content-Type header of that part) or the value of the
    'failobj' parameter (defaults to None), if the part does not have a
    main MIME type of "text", or the charset is not defined.

    The list will contain one string for each part of the message, plus
    one for the container message (i.e. self), so that a non-multipart
    message will still return a list of length 1.
  summary: Return a list containing the charset(s) used in this message
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_content_charset
  kind: method
  ns: scrapy.mail
  description: |-
    Return the charset parameter of the Content-Type header.

    The returned string is always coerced to lower case.  If there is no
    Content-Type header, or if that header has no charset parameter,
    failobj is returned.
  summary: Return the charset parameter of the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_content_disposition
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's content-disposition if it exists, or None.

    The return values can be either 'inline', 'attachment' or None
    according to the rfc2183.
  summary: Return the message's content-disposition if it exists, or None
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_content_maintype
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's main content type.

    This is the `maintype' part of the string returned by
    get_content_type().
  summary: Return the message's main content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_content_subtype
  kind: method
  ns: scrapy.mail
  description: |-
    Returns the message's sub-content type.

    This is the `subtype' part of the string returned by
    get_content_type().
  summary: Returns the message's sub-content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_content_type
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's content type.

    The returned string is coerced to lower case of the form
    `maintype/subtype'.  If there was no Content-Type header in the
    message, the default type as given by get_default_type() will be
    returned.  Since according to RFC 2045, messages always have a default
    type this will always return a value.

    RFC 2045 defines a message's default type to be text/plain unless it
    appears inside a multipart/digest container, in which case it would be
    message/rfc822.
  summary: Return the message's content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_default_type
  kind: method
  ns: scrapy.mail
  description: |-
    Return the `default' content type.

    Most messages have a default content type of text/plain, except for
    messages that are subparts of multipart/digest containers.  Such
    subparts have a default content type of message/rfc822.
  summary: Return the `default' content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_filename
  kind: method
  ns: scrapy.mail
  description: |-
    Return the filename associated with the payload if present.

    The filename is extracted from the Content-Disposition header's
    `filename' parameter, and it is unquoted.  If that header is missing
    the `filename' parameter, this method falls back to looking for the
    `name' parameter.
  summary: Return the filename associated with the payload if present
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_param
  kind: method
  ns: scrapy.mail
  description: |-
    Return the parameter value if found in the Content-Type header.

    Optional failobj is the object to return if there is no Content-Type
    header, or the Content-Type header has no such parameter.  Optional
    header is the header to search instead of Content-Type.

    Parameter keys are always compared case insensitively.  The return
    value can either be a string, or a 3-tuple if the parameter was RFC
    2231 encoded.  When it's a 3-tuple, the elements of the value are of
    the form (CHARSET, LANGUAGE, VALUE).  Note that both CHARSET and
    LANGUAGE can be None, in which case you should consider VALUE to be
    encoded in the us-ascii charset.  You can usually ignore LANGUAGE.
    The parameter value (either the returned string, or the VALUE item in
    the 3-tuple) is always unquoted, unless unquote is set to False.

    If your application doesn't care whether the parameter was RFC 2231
    encoded, it can turn the return value into a string as follows:

        rawparam = msg.get_param('foo')
        param = email.utils.collapse_rfc2231_value(rawparam)
  summary: Return the parameter value if found in the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: unquote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_params
  kind: method
  ns: scrapy.mail
  description: |-
    Return the message's Content-Type parameters, as a list.

    The elements of the returned list are 2-tuples of key/value pairs, as
    split on the `=' sign.  The left hand side of the `=' is the key,
    while the right hand side is the value.  If there is no `=' sign in
    the parameter the value is the empty string.  The value is as
    described in the get_param() method.

    Optional failobj is the object to return if there is no Content-Type
    header.  Optional header is the header to search instead of
    Content-Type.  If unquote is True, the value is unquoted.
  summary: Return the message's Content-Type parameters, as a list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: failobj
    default: None
    rest: false
  - kind: positional
    name: header
    default: content-type
    rest: false
  - kind: positional
    name: unquote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_payload
  kind: method
  ns: scrapy.mail
  description: |-
    Return a reference to the payload.

    The payload will either be a list object or a string.  If you mutate
    the list object, you modify the message's payload in place.  Optional
    i returns that index into the payload.

    Optional decode is a flag indicating whether the payload should be
    decoded or not, according to the Content-Transfer-Encoding header
    (default is False).

    When True and the message is not a multipart, the payload will be
    decoded if this header's value is `quoted-printable' or `base64'.  If
    some other encoding is used, or the header is missing, or if the
    payload has bogus data (i.e. bogus base64 or uuencoded data), the
    payload is returned as-is.

    If the message is a multipart and the decode flag is True, then None
    is returned.
  summary: Return a reference to the payload
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: i
    default: None
    rest: false
  - kind: positional
    name: decode
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.get_unixfrom
  kind: method
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.is_multipart
  kind: method
  ns: scrapy.mail
  description: Return True if the message consists of multiple parts.
  summary: Return True if the message consists of multiple parts
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.items
  kind: method
  ns: scrapy.mail
  description: |-
    Get all the message's header fields and values.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Get all the message's header fields and values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.keys
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the message's header field names.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Return a list of all the message's header field names
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.raw_items
  kind: method
  ns: scrapy.mail
  description: |-
    Return the (name, value) header pairs without modification.

    This is an "internal" API, intended only for use by a generator.
  summary: Return the (name, value) header pairs without modification
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.replace_header
  kind: method
  ns: scrapy.mail
  description: |-
    Replace a header.

    Replace the first matching header found in the message, retaining
    header order and case.  If no matching header was found, a KeyError is
    raised.
  summary: Replace a header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: _name
    default: null
    rest: false
  - kind: positional
    name: _value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.set_boundary
  kind: method
  ns: scrapy.mail
  description: |-
    Set the boundary parameter in Content-Type to 'boundary'.

    This is subtly different than deleting the Content-Type header and
    adding a new one with a new boundary parameter via add_header().  The
    main difference is that using the set_boundary() method preserves the
    order of the Content-Type header in the original message.

    HeaderParseError is raised if the message has no Content-Type header.
  summary: Set the boundary parameter in Content-Type to 'boundary'
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: boundary
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.set_charset
  kind: method
  ns: scrapy.mail
  description: |-
    Set the charset of the payload to a given character set.

    charset can be a Charset instance, a string naming a character set, or
    None.  If it is a string it will be converted to a Charset instance.
    If charset is None, the charset parameter will be removed from the
    Content-Type field.  Anything else will generate a TypeError.

    The message will be assumed to be of type text/* encoded with
    charset.input_charset.  It will be converted to charset.output_charset
    and encoded properly, if needed, when generating the plain text
    representation of the message.  MIME headers (MIME-Version,
    Content-Type, Content-Transfer-Encoding) will be added as needed.
  summary: Set the charset of the payload to a given character set
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: charset
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.set_default_type
  kind: method
  ns: scrapy.mail
  description: |-
    Set the `default' content type.

    ctype should be either "text/plain" or "message/rfc822", although this
    is not enforced.  The default content type is not stored in the
    Content-Type header.
  summary: Set the `default' content type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: ctype
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.set_param
  kind: method
  ns: scrapy.mail
  description: |-
    Set a parameter in the Content-Type header.

    If the parameter already exists in the header, its value will be
    replaced with the new value.

    If header is Content-Type and has not yet been defined for this
    message, it will be set to "text/plain" and the new parameter and
    value will be appended as per RFC 2045.

    An alternate header can be specified in the header argument, and all
    parameters will be quoted as necessary unless requote is False.

    If charset is specified, the parameter will be encoded according to RFC
    2231.  Optional language specifies the RFC 2231 language, defaulting
    to the empty string.  Both charset and language should be strings.
  summary: Set a parameter in the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: param
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: header
    default: Content-Type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - kind: positional
    name: charset
    default: None
    rest: false
  - kind: positional
    name: language
    default: null
    rest: false
  - kind: positional
    name: replace
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.set_payload
  kind: method
  ns: scrapy.mail
  description: |-
    Set the payload to the given value.

    Optional charset sets the message's default character set.  See
    set_charset() for details.
  summary: Set the payload to the given value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: payload
    default: null
    rest: false
  - kind: positional
    name: charset
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.set_raw
  kind: method
  ns: scrapy.mail
  description: |-
    Store name and value in the model without modification.

    This is an "internal" API, intended only for use by a parser.
  summary: Store name and value in the model without modification
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.set_type
  kind: method
  ns: scrapy.mail
  description: |-
    Set the main type and subtype for the Content-Type header.

    type must be a string in the form "maintype/subtype", otherwise a
    ValueError is raised.

    This method replaces the Content-Type header, keeping all the
    parameters in place.  If requote is False, this leaves the existing
    header's quoting as is.  Otherwise, the parameters will be quoted (the
    default).

    An alternative header can be specified in the header argument.  When
    the Content-Type header is set, we'll always also add a MIME-Version
    header.
  summary: Set the main type and subtype for the Content-Type header
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: type
    default: null
    rest: false
  - kind: positional
    name: header
    default: Content-Type
    rest: false
  - kind: positional
    name: requote
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.set_unixfrom
  kind: method
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: unixfrom
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.values
  kind: method
  ns: scrapy.mail
  description: |-
    Return a list of all the message's header values.

    These will be sorted in the order they appeared in the original
    message, or were added to the message, and may contain duplicates.
    Any fields deleted and re-inserted are always appended to the header
    list.
  summary: Return a list of all the message's header values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MIMEText.walk
  kind: method
  ns: scrapy.mail
  description: |-
    Walk over the message tree, yielding each subpart.

    The walk is performed in depth-first order.  This method is a
    generator.
  summary: Walk over the message tree, yielding each subpart
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MailSender
  kind: class
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: smtphost
    default: localhost
    rest: false
  - kind: positional
    name: mailfrom
    default: scrapy@localhost
    rest: false
  - kind: positional
    name: smtpuser
    default: None
    rest: false
  - kind: positional
    name: smtppass
    default: None
    rest: false
  - kind: positional
    name: smtpport
    default: '25'
    rest: false
  - kind: positional
    name: smtptls
    default: 'False'
    rest: false
  - kind: positional
    name: smtpssl
    default: 'False'
    rest: false
  - kind: positional
    name: debug
    default: 'False'
    rest: false
  - type: MailSender
  inherits_from: null
- name: MailSender.from_settings
  kind: function
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MailSender.send
  kind: method
  ns: scrapy.mail
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: to
    default: null
    rest: false
  - kind: positional
    name: subject
    default: null
    rest: false
  - kind: positional
    name: body
    default: null
    rest: false
  - kind: positional
    name: cc
    default: None
    rest: false
  - kind: positional
    name: attachs
    default: ()
    rest: false
  - kind: positional
    name: mimetype
    default: text/plain
    rest: false
  - kind: positional
    name: charset
    default: None
    rest: false
  - kind: positional
    name: _callback
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Version
  kind: class
  ns: scrapy.mail
  description: |-
    An encapsulation of a version for a project, with support for outputting
    PEP-440 compatible version strings.

    This class supports the standard major.minor.micro[rcN] scheme of
    versioning.
  summary: An encapsulation of a version for a project, with support for outputting
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: package
    default: null
    rest: false
  - kind: positional
    name: major
    default: null
    rest: false
  - kind: positional
    name: minor
    default: null
    rest: false
  - kind: positional
    name: micro
    default: null
    rest: false
  - kind: positional
    name: release_candidate
    default: None
    rest: false
  - kind: positional
    name: prerelease
    default: None
    rest: false
  - kind: positional
    name: post
    default: None
    rest: false
  - kind: positional
    name: dev
    default: None
    rest: false
  - type: Version
  inherits_from: null
- name: Version.base
  kind: method
  ns: scrapy.mail
  description: |-
    Return a PEP440-compatible "public" representation of this L{Version}.

    Examples:

      - 14.4.0
      - 1.2.3rc1
      - 14.2.1rc1dev9
      - 16.04.0dev0
  summary: Return a PEP440-compatible "public" representation of this L{Version}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Version.local
  kind: method
  ns: scrapy.mail
  description: |-
    Return a PEP440-compatible "public" representation of this L{Version}.

    Examples:

      - 14.4.0
      - 1.2.3rc1
      - 14.2.1rc1dev9
      - 16.04.0dev0
  summary: Return a PEP440-compatible "public" representation of this L{Version}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Version.prerelease
  kind: property
  ns: scrapy.mail
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Version.public
  kind: method
  ns: scrapy.mail
  description: |-
    Return a PEP440-compatible "public" representation of this L{Version}.

    Examples:

      - 14.4.0
      - 1.2.3rc1
      - 14.2.1rc1dev9
      - 16.04.0dev0
  summary: Return a PEP440-compatible "public" representation of this L{Version}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Version.short
  kind: method
  ns: scrapy.mail
  description: |-
    Return a PEP440-compatible "public" representation of this L{Version}.

    Examples:

      - 14.4.0
      - 1.2.3rc1
      - 14.2.1rc1dev9
      - 16.04.0dev0
  summary: Return a PEP440-compatible "public" representation of this L{Version}
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: arg_to_iter
  kind: function
  ns: scrapy.mail
  description: |-
    Convert an argument to an iterable. The argument can be a None, single
    value, or an iterable.

    Exception: if arg is a dict, [arg] will be returned
  summary: Convert an argument to an iterable
  signatures:
  - kind: positional
    name: arg
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: formatdate
  kind: function
  ns: scrapy.mail
  description: |-
    Returns a date string as specified by RFC 2822, e.g.:

    Fri, 09 Nov 2001 01:08:47 -0000

    Optional timeval if given is a floating point time value as accepted by
    gmtime() and localtime(), otherwise the current time is used.

    Optional localtime is a flag that when True, interprets timeval, and
    returns a date relative to the local timezone instead of UTC, properly
    taking daylight savings time into account.

    Optional argument usegmt means that the timezone is written out as
    an ascii string, not numeric one (so "GMT" instead of "+0000"). This
    is needed for HTTP, and is only used when localtime==False.
  summary: Returns a date string as specified by RFC 2822, e
  signatures:
  - kind: positional
    name: timeval
    default: None
    rest: false
  - kind: positional
    name: localtime
    default: 'False'
    rest: false
  - kind: positional
    name: usegmt
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.mail
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: to_bytes
  kind: function
  ns: scrapy.mail
  description: |-
    Return the binary representation of ``text``. If ``text``
    is already a bytes object, return it as-is.
  summary: Return the binary representation of ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: twisted_version
  kind: const
  ns: scrapy.mail
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: scrapy.middleware
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.middleware
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deque
  kind: callable
  ns: scrapy.middleware
  description: A generic version of collections.deque.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.middleware
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.middleware
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.middleware
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.middleware
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.middleware
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.middleware
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.middleware
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: annotations
  kind: const
  ns: scrapy.middleware
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.middleware
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: create_instance
  kind: function
  ns: scrapy.middleware
  description: |-
    Construct a class instance using its ``from_crawler`` or
    ``from_settings`` constructors, if available.

    At least one of ``settings`` and ``crawler`` needs to be different from
    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.
    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be
    tried.

    ``*args`` and ``**kwargs`` are forwarded to the constructors.

    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.

    .. versionchanged:: 2.2
       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an
       extension has not been implemented correctly).
  summary: Construct a class instance using its ``from_crawler`` or
  signatures:
  - kind: positional
    name: objcls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: defaultdict
  kind: class
  ns: scrapy.middleware
  description: |-
    defaultdict(default_factory=None, /, [...]) --> dict with default factory

    The default factory is called without arguments to produce
    a new value when a key is not present, in __getitem__ only.
    A defaultdict compares equal to a dict with the same items.
    All remaining arguments are treated the same as if they were
    passed to the dict constructor, including keyword arguments.
  summary: defaultdict(default_factory=None, /, [
  signatures: null
  inherits_from:
  - <class 'dict'>
- name: defaultdict.clear
  kind: callable
  ns: scrapy.middleware
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures: null
  inherits_from: null
- name: defaultdict.copy
  kind: callable
  ns: scrapy.middleware
  description: D.copy() -> a shallow copy of D.
  summary: D
  signatures: null
  inherits_from: null
- name: defaultdict.default_factory
  kind: property
  ns: scrapy.middleware
  description: Factory for default value called by __missing__().
  summary: Factory for default value called by __missing__()
  signatures: null
  inherits_from: null
- name: defaultdict.get
  kind: callable
  ns: scrapy.middleware
  description: Return the value for key if key is in the dictionary, else default.
  summary: Return the value for key if key is in the dictionary, else default
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: defaultdict.items
  kind: callable
  ns: scrapy.middleware
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures: null
  inherits_from: null
- name: defaultdict.keys
  kind: callable
  ns: scrapy.middleware
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures: null
  inherits_from: null
- name: defaultdict.pop
  kind: callable
  ns: scrapy.middleware
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.

    If the key is not found, return the default if given; otherwise,
    raise a KeyError.
  summary: D
  signatures: null
  inherits_from: null
- name: defaultdict.popitem
  kind: callable
  ns: scrapy.middleware
  description: |-
    Remove and return a (key, value) pair as a 2-tuple.

    Pairs are returned in LIFO (last-in, first-out) order.
    Raises KeyError if the dict is empty.
  summary: Remove and return a (key, value) pair as a 2-tuple
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: defaultdict.setdefault
  kind: callable
  ns: scrapy.middleware
  description: |-
    Insert key with a value of default if key is not in the dictionary.

    Return the value for key if key is in the dictionary, else default.
  summary: Insert key with a value of default if key is not in the dictionary
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: defaultdict.update
  kind: callable
  ns: scrapy.middleware
  description: |-
    D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.
    If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
    If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
    In either case, this is followed by: for k in F:  D[k] = F[k]
  summary: D
  signatures: null
  inherits_from: null
- name: defaultdict.values
  kind: callable
  ns: scrapy.middleware
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures: null
  inherits_from: null
- name: load_object
  kind: function
  ns: scrapy.middleware
  description: |-
    Load an object given its absolute object path, and return it.

    The object can be the import path of a class, function, variable or an
    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.

    If ``path`` is not a string, but is a callable object, such as a class or
    a function, then return it as is.
  summary: Load an object given its absolute object path, and return it
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.middleware
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: process_chain
  kind: function
  ns: scrapy.middleware
  description: Return a Deferred built by chaining the given callbacks
  summary: Return a Deferred built by chaining the given callbacks
  signatures:
  - kind: positional
    name: callbacks
    default: null
    rest: false
  - kind: positional
    name: input
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: process_parallel
  kind: function
  ns: scrapy.middleware
  description: |-
    Return a Deferred with the output of all successful calls to the given
    callbacks
  summary: Return a Deferred with the output of all successful calls to the given
  signatures:
  - kind: positional
    name: callbacks
    default: null
    rest: false
  - kind: positional
    name: input
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.pipelines
  kind: module
  ns: null
  description: |-
    Item pipeline

    See documentation in docs/item-pipeline.rst
  summary: Item pipeline
  signatures: null
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.pipelines
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: build_component_list
  kind: function
  ns: scrapy.pipelines
  description: 'Compose a component list from a { class: order } dictionary.'
  summary: 'Compose a component list from a { class: order } dictionary'
  signatures:
  - kind: positional
    name: compdict
    default: null
    rest: false
  - kind: positional
    name: custom
    default: None
    rest: false
  - kind: positional
    name: convert
    default: <function update_classpath at 0x7fa9729e1a80>
    rest: false
  - type: '?'
  inherits_from: null
- name: deferred_f_from_coro_f
  kind: function
  ns: scrapy.pipelines
  description: |-
    Converts a coroutine function into a function that returns a Deferred.

    The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.
    This is useful for callback chains, as callback functions are called with the previous callback result.
  summary: Converts a coroutine function into a function that returns a Deferred
  signatures:
  - kind: positional
    name: coro_f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.pqueues
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DownloaderAwarePriorityQueue
  kind: class
  ns: scrapy.pqueues
  description: |-
    PriorityQueue which takes Downloader activity into account:
    domains (slots) with the least amount of active downloads are dequeued
    first.
  summary: 'PriorityQueue which takes Downloader activity into account:'
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: downstream_queue_cls
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: slot_startprios
    default: ()
    rest: false
  - type: DownloaderAwarePriorityQueue
  inherits_from: null
- name: DownloaderAwarePriorityQueue.close
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderAwarePriorityQueue.from_crawler
  kind: function
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: downstream_queue_cls
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: startprios
    default: ()
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderAwarePriorityQueue.peek
  kind: method
  ns: scrapy.pqueues
  description: |-
    Returns the next object to be returned by :meth:`pop`,
    but without removing it from the queue.

    Raises :exc:`NotImplementedError` if the underlying queue class does
    not implement a ``peek`` method, which is optional for queues.
  summary: Returns the next object to be returned by :meth:`pop`,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderAwarePriorityQueue.pop
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderAwarePriorityQueue.pqfactory
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: slot
    default: null
    rest: false
  - kind: positional
    name: startprios
    default: ()
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderAwarePriorityQueue.push
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderInterface
  kind: class
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: DownloaderInterface
  inherits_from: null
- name: DownloaderInterface.get_slot_key
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DownloaderInterface.stats
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: possible_slots
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyPriorityQueue
  kind: class
  ns: scrapy.pqueues
  description: |-
    A priority queue implemented using multiple internal queues (typically,
    FIFO queues). It uses one internal queue for each priority value. The internal
    queue must implement the following methods:

        * push(obj)
        * pop()
        * close()
        * __len__()

    Optionally, the queue could provide a ``peek`` method, that should return the
    next object to be returned by ``pop``, but without removing it from the queue.

    ``__init__`` method of ScrapyPriorityQueue receives a downstream_queue_cls
    argument, which is a class used to instantiate a new (internal) queue when
    a new priority is allocated.

    Only integer priorities should be used. Lower numbers are higher
    priorities.

    startprios is a sequence of priorities to start with. If the queue was
    previously closed leaving some priority buckets non-empty, those priorities
    should be passed in startprios.
  summary: A priority queue implemented using multiple internal queues (typically,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: downstream_queue_cls
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: startprios
    default: ()
    rest: false
  - type: ScrapyPriorityQueue
  inherits_from: null
- name: ScrapyPriorityQueue.close
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyPriorityQueue.from_crawler
  kind: function
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: downstream_queue_cls
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: startprios
    default: ()
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyPriorityQueue.init_prios
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: startprios
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyPriorityQueue.peek
  kind: method
  ns: scrapy.pqueues
  description: |-
    Returns the next object to be returned by :meth:`pop`,
    but without removing it from the queue.

    Raises :exc:`NotImplementedError` if the underlying queue class does
    not implement a ``peek`` method, which is optional for queues.
  summary: Returns the next object to be returned by :meth:`pop`,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyPriorityQueue.pop
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyPriorityQueue.priority
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyPriorityQueue.push
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyPriorityQueue.qfactory
  kind: method
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: create_instance
  kind: function
  ns: scrapy.pqueues
  description: |-
    Construct a class instance using its ``from_crawler`` or
    ``from_settings`` constructors, if available.

    At least one of ``settings`` and ``crawler`` needs to be different from
    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.
    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be
    tried.

    ``*args`` and ``**kwargs`` are forwarded to the constructors.

    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.

    .. versionchanged:: 2.2
       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an
       extension has not been implemented correctly).
  summary: Construct a class instance using its ``from_crawler`` or
  signatures:
  - kind: positional
    name: objcls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.pqueues
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: scrapy.resolver
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CachingHostnameResolver
  kind: class
  ns: scrapy.resolver
  description: |-
    Experimental caching resolver. Resolves IPv4 and IPv6 addresses,
    does not support setting a timeout value for DNS requests.
  summary: Experimental caching resolver
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: reactor
    default: null
    rest: false
  - kind: positional
    name: cache_size
    default: null
    rest: false
  - type: CachingHostnameResolver
  inherits_from: null
- name: CachingHostnameResolver.from_crawler
  kind: function
  ns: scrapy.resolver
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: reactor
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CachingHostnameResolver.install_on_reactor
  kind: method
  ns: scrapy.resolver
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CachingHostnameResolver.resolveHostName
  kind: method
  ns: scrapy.resolver
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: resolutionReceiver
    default: null
    rest: false
  - kind: positional
    name: hostName
    default: null
    rest: false
  - kind: positional
    name: portNumber
    default: '0'
    rest: false
  - kind: positional
    name: addressTypes
    default: None
    rest: false
  - kind: positional
    name: transportSemantics
    default: TCP
    rest: false
  - type: '?'
  inherits_from: null
- name: CachingThreadedResolver
  kind: class
  ns: scrapy.resolver
  description: Default caching resolver. IPv4 only, supports setting a timeout value for DNS requests.
  summary: Default caching resolver
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: reactor
    default: null
    rest: false
  - kind: positional
    name: cache_size
    default: null
    rest: false
  - kind: positional
    name: timeout
    default: null
    rest: false
  - type: CachingThreadedResolver
  inherits_from:
  - <class 'twisted.internet.base.ThreadedResolver'>
- name: CachingThreadedResolver.from_crawler
  kind: function
  ns: scrapy.resolver
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: reactor
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CachingThreadedResolver.getHostByName
  kind: method
  ns: scrapy.resolver
  description: |-
    See L{twisted.internet.interfaces.IResolverSimple.getHostByName}.

    Note that the elements of C{timeout} are summed and the result is used
    as a timeout for the lookup.  Any intermediate timeout or retry logic
    is left up to the platform via L{socket.gethostbyname}.
  summary: See L{twisted
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: timeout
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CachingThreadedResolver.install_on_reactor
  kind: method
  ns: scrapy.resolver
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: HostResolution
  kind: class
  ns: scrapy.resolver
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - type: HostResolution
  inherits_from: null
- name: HostResolution.cancel
  kind: method
  ns: scrapy.resolver
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IHostResolution
  kind: callable
  ns: scrapy.resolver
  description: |-
    An L{IHostResolution} represents represents an in-progress recursive query
    for a DNS name.

    @since: Twisted 17.1.0
  summary: An L{IHostResolution} represents represents an in-progress recursive query
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IHostnameResolver
  kind: callable
  ns: scrapy.resolver
  description: |-
    An L{IHostnameResolver} can resolve a host name and port number into a
    series of L{IAddress} objects.

    @since: Twisted 17.1.0
  summary: An L{IHostnameResolver} can resolve a host name and port number into a
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IResolutionReceiver
  kind: callable
  ns: scrapy.resolver
  description: |-
    An L{IResolutionReceiver} receives the results of a hostname resolution in
    progress, initiated by an L{IHostnameResolver}.

    @since: Twisted 17.1.0
  summary: An L{IResolutionReceiver} receives the results of a hostname resolution in
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IResolverSimple
  kind: callable
  ns: scrapy.resolver
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalCache
  kind: class
  ns: scrapy.resolver
  description: |-
    Dictionary with a finite number of keys.

    Older items expires first.
  summary: Dictionary with a finite number of keys
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: limit
    default: None
    rest: false
  - type: LocalCache
  inherits_from:
  - <class 'collections.OrderedDict'>
  - <class 'dict'>
  - <class 'typing.Generic'>
- name: LocalCache.clear
  kind: callable
  ns: scrapy.resolver
  description: od.clear() -> None.  Remove all items from od.
  summary: od
  signatures: null
  inherits_from: null
- name: LocalCache.copy
  kind: callable
  ns: scrapy.resolver
  description: od.copy() -> a shallow copy of od
  summary: od
  signatures: null
  inherits_from: null
- name: LocalCache.get
  kind: callable
  ns: scrapy.resolver
  description: Return the value for key if key is in the dictionary, else default.
  summary: Return the value for key if key is in the dictionary, else default
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalCache.items
  kind: callable
  ns: scrapy.resolver
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures: null
  inherits_from: null
- name: LocalCache.keys
  kind: callable
  ns: scrapy.resolver
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures: null
  inherits_from: null
- name: LocalCache.move_to_end
  kind: callable
  ns: scrapy.resolver
  description: |-
    Move an existing element to the end (or beginning if last is false).

    Raise KeyError if the element does not exist.
  summary: Move an existing element to the end (or beginning if last is false)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: last
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalCache.pop
  kind: callable
  ns: scrapy.resolver
  description: |-
    od.pop(key[,default]) -> v, remove specified key and return the corresponding value.

    If the key is not found, return the default if given; otherwise,
    raise a KeyError.
  summary: od
  signatures: null
  inherits_from: null
- name: LocalCache.popitem
  kind: callable
  ns: scrapy.resolver
  description: |-
    Remove and return a (key, value) pair from the dictionary.

    Pairs are returned in LIFO order if last is true or FIFO order if false.
  summary: Remove and return a (key, value) pair from the dictionary
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: last
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalCache.setdefault
  kind: callable
  ns: scrapy.resolver
  description: |-
    Insert key with a value of default if key is not in the dictionary.

    Return the value for key if key is in the dictionary, else default.
  summary: Insert key with a value of default if key is not in the dictionary
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalCache.update
  kind: callable
  ns: scrapy.resolver
  description: |-
    D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.
    If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
    If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
    In either case, this is followed by: for k in F:  D[k] = F[k]
  summary: D
  signatures: null
  inherits_from: null
- name: LocalCache.values
  kind: callable
  ns: scrapy.resolver
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures: null
  inherits_from: null
- name: ThreadedResolver
  kind: class
  ns: scrapy.resolver
  description: |-
    L{ThreadedResolver} uses a reactor, a threadpool, and
    L{socket.gethostbyname} to perform name lookups without blocking the
    reactor thread.  It also supports timeouts indepedently from whatever
    timeout logic L{socket.gethostbyname} might have.

    @ivar reactor: The reactor the threadpool of which will be used to call
        L{socket.gethostbyname} and the I/O thread of which the result will be
        delivered.
  summary: L{ThreadedResolver} uses a reactor, a threadpool, and
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: reactor
    default: null
    rest: false
  - type: ThreadedResolver
  inherits_from: null
- name: ThreadedResolver.getHostByName
  kind: method
  ns: scrapy.resolver
  description: |-
    See L{twisted.internet.interfaces.IResolverSimple.getHostByName}.

    Note that the elements of C{timeout} are summed and the result is used
    as a timeout for the lookup.  Any intermediate timeout or retry logic
    is left up to the platform via L{socket.gethostbyname}.
  summary: See L{twisted
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: timeout
    default: (1, 3, 11, 45)
    rest: false
  - type: '?'
  inherits_from: null
- name: dnscache
  kind: const
  ns: scrapy.resolver
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: implementer
  kind: class
  ns: scrapy.resolver
  description: |-
    Declare the interfaces implemented by instances of a class.

    This function is called as a class decorator.

    The arguments are one or more interfaces or interface
    specifications (`~zope.interface.interfaces.IDeclaration`
    objects).

    The interfaces given (including the interfaces in the
    specifications) are added to any interfaces previously declared,
    unless the interface is already implemented.

    Previous declarations include declarations for base classes unless
    implementsOnly was used.

    This function is provided for convenience. It provides a more
    convenient way to call `classImplements`. For example::

        @implementer(I1)
        class C(object):
            pass

    is equivalent to calling::

        classImplements(C, I1)

    after the class has been created.

    .. seealso:: `classImplements`
       The change history provided there applies to this function too.
  summary: Declare the interfaces implemented by instances of a class
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: implementer
  inherits_from: null
- name: implementer.interfaces
  kind: property
  ns: scrapy.resolver
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: provider
  kind: class
  ns: scrapy.resolver
  description: Class decorator version of classProvides
  summary: Class decorator version of classProvides
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: provider
  inherits_from: null
- name: scrapy.responsetypes
  kind: module
  ns: null
  description: |-
    This module implements a class which returns the appropriate Response class
    based on different criteria.
  summary: This module implements a class which returns the appropriate Response class
  signatures: null
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.responsetypes
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Mapping
  kind: callable
  ns: scrapy.responsetypes
  description: A generic version of collections.abc.Mapping.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MimeTypes
  kind: class
  ns: scrapy.responsetypes
  description: |-
    MIME-types datastore.

    This datastore can handle information from mime.types-style files
    and supports basic determination of MIME type from a filename or
    URL, and can guess a reasonable extension given a MIME type.
  summary: MIME-types datastore
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: filenames
    default: ()
    rest: false
  - kind: positional
    name: strict
    default: 'True'
    rest: false
  - type: MimeTypes
  inherits_from: null
- name: MimeTypes.add_type
  kind: method
  ns: scrapy.responsetypes
  description: |-
    Add a mapping between a type and an extension.

    When the extension is already known, the new
    type will replace the old one. When the type
    is already known the extension will be added
    to the list of known extensions.

    If strict is true, information will be added to
    list of standard types, else to the list of non-standard
    types.
  summary: Add a mapping between a type and an extension
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: type
    default: null
    rest: false
  - kind: positional
    name: ext
    default: null
    rest: false
  - kind: positional
    name: strict
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MimeTypes.guess_all_extensions
  kind: method
  ns: scrapy.responsetypes
  description: |-
    Guess the extensions for a file based on its MIME type.

    Return value is a list of strings giving the possible filename
    extensions, including the leading dot ('.').  The extension is not
    guaranteed to have been associated with any particular data stream,
    but would be mapped to the MIME type `type' by guess_type().

    Optional `strict' argument when false adds a bunch of commonly found,
    but non-standard types.
  summary: Guess the extensions for a file based on its MIME type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: type
    default: null
    rest: false
  - kind: positional
    name: strict
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MimeTypes.guess_extension
  kind: method
  ns: scrapy.responsetypes
  description: |-
    Guess the extension for a file based on its MIME type.

    Return value is a string giving a filename extension,
    including the leading dot ('.').  The extension is not
    guaranteed to have been associated with any particular data
    stream, but would be mapped to the MIME type `type' by
    guess_type().  If no extension can be guessed for `type', None
    is returned.

    Optional `strict' argument when false adds a bunch of commonly found,
    but non-standard types.
  summary: Guess the extension for a file based on its MIME type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: type
    default: null
    rest: false
  - kind: positional
    name: strict
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MimeTypes.guess_type
  kind: method
  ns: scrapy.responsetypes
  description: |-
    Guess the type of a file which is either a URL or a path-like object.

    Return value is a tuple (type, encoding) where type is None if
    the type can't be guessed (no or unknown suffix) or a string
    of the form type/subtype, usable for a MIME Content-type
    header; and encoding is None for no encoding or the name of
    the program used to encode (e.g. compress or gzip).  The
    mappings are table driven.  Encoding suffixes are case
    sensitive; type suffixes are first tried case sensitive, then
    case insensitive.

    The suffixes .tgz, .taz and .tz (case sensitive!) are all
    mapped to '.tar.gz'.  (This is table-driven too, using the
    dictionary suffix_map.)

    Optional `strict' argument when False adds a bunch of commonly found,
    but non-standard types.
  summary: Guess the type of a file which is either a URL or a path-like object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: strict
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MimeTypes.read
  kind: method
  ns: scrapy.responsetypes
  description: |-
    Read a single mime.types-format file, specified by pathname.

    If strict is true, information will be added to
    list of standard types, else to the list of non-standard
    types.
  summary: Read a single mime
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: filename
    default: null
    rest: false
  - kind: positional
    name: strict
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MimeTypes.read_windows_registry
  kind: method
  ns: scrapy.responsetypes
  description: |-
    Load the MIME types database from Windows registry.

    If strict is true, information will be added to
    list of standard types, else to the list of non-standard
    types.
  summary: Load the MIME types database from Windows registry
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: strict
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: MimeTypes.readfp
  kind: method
  ns: scrapy.responsetypes
  description: |-
    Read a single mime.types-format file.

    If strict is true, information will be added to
    list of standard types, else to the list of non-standard
    types.
  summary: Read a single mime
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: fp
    default: null
    rest: false
  - kind: positional
    name: strict
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.responsetypes
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ResponseTypes
  kind: class
  ns: scrapy.responsetypes
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: ResponseTypes
  inherits_from: null
- name: ResponseTypes.CLASSES
  kind: property
  ns: scrapy.responsetypes
  description: |-
    dict() -> new empty dictionary
    dict(mapping) -> new dictionary initialized from a mapping object's
        (key, value) pairs
    dict(iterable) -> new dictionary initialized as if via:
        d = {}
        for k, v in iterable:
            d[k] = v
    dict(**kwargs) -> new dictionary initialized with the name=value pairs
        in the keyword argument list.  For example:  dict(one=1, two=2)
  summary: dict() -> new empty dictionary
  signatures: null
  inherits_from: null
- name: ResponseTypes.from_args
  kind: method
  ns: scrapy.responsetypes
  description: |-
    Guess the most appropriate Response class based on
    the given arguments.
  summary: Guess the most appropriate Response class based on
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: url
    default: None
    rest: false
  - kind: positional
    name: filename
    default: None
    rest: false
  - kind: positional
    name: body
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ResponseTypes.from_body
  kind: method
  ns: scrapy.responsetypes
  description: |-
    Try to guess the appropriate response based on the body content.
    This method is a bit magic and could be improved in the future, but
    it's not meant to be used except for special cases where response types
    cannot be guess using more straightforward methods.
  summary: Try to guess the appropriate response based on the body content
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: body
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ResponseTypes.from_content_disposition
  kind: method
  ns: scrapy.responsetypes
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: content_disposition
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ResponseTypes.from_content_type
  kind: method
  ns: scrapy.responsetypes
  description: |-
    Return the most appropriate Response class from an HTTP Content-Type
    header
  summary: Return the most appropriate Response class from an HTTP Content-Type
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: content_type
    default: null
    rest: false
  - kind: positional
    name: content_encoding
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ResponseTypes.from_filename
  kind: method
  ns: scrapy.responsetypes
  description: Return the most appropriate Response class from a file name
  summary: Return the most appropriate Response class from a file name
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: filename
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ResponseTypes.from_headers
  kind: method
  ns: scrapy.responsetypes
  description: |-
    Return the most appropriate Response class by looking at the HTTP
    headers
  summary: Return the most appropriate Response class by looking at the HTTP
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: headers
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ResponseTypes.from_mimetype
  kind: method
  ns: scrapy.responsetypes
  description: Return the most appropriate Response class for the given mimetype
  summary: Return the most appropriate Response class for the given mimetype
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: mimetype
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO
  kind: class
  ns: scrapy.responsetypes
  description: |-
    Text I/O implementation using an in-memory buffer.

    The initial_value argument sets the value of object.  The newline
    argument is like the one of TextIOWrapper's constructor.
  summary: Text I/O implementation using an in-memory buffer
  signatures:
  - kind: positional
    name: initial_value
    default: null
    rest: false
  - kind: positional
    name: newline
    default: |2+

    rest: false
  - type: StringIO
  inherits_from:
  - <class '_io._TextIOBase'>
  - <class '_io._IOBase'>
- name: StringIO.close
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Close the IO object.

    Attempting any further operation after the object is closed
    will raise a ValueError.

    This method has no effect if the file is already closed.
  summary: Close the IO object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.closed
  kind: property
  ns: scrapy.responsetypes
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: StringIO.detach
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Separate the underlying buffer from the TextIOBase and return it.

    After the underlying buffer has been detached, the TextIO is in an
    unusable state.
  summary: Separate the underlying buffer from the TextIOBase and return it
  signatures: null
  inherits_from: null
- name: StringIO.encoding
  kind: property
  ns: scrapy.responsetypes
  description: |-
    Encoding of the text stream.

    Subclasses should override.
  summary: Encoding of the text stream
  signatures: null
  inherits_from: null
- name: StringIO.errors
  kind: property
  ns: scrapy.responsetypes
  description: |-
    The error setting of the decoder or encoder.

    Subclasses should override.
  summary: The error setting of the decoder or encoder
  signatures: null
  inherits_from: null
- name: StringIO.fileno
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Returns underlying file descriptor if one exists.

    OSError is raised if the IO object does not use a file descriptor.
  summary: Returns underlying file descriptor if one exists
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.flush
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Flush write buffers, if applicable.

    This is not implemented for read-only and non-blocking streams.
  summary: Flush write buffers, if applicable
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.getvalue
  kind: callable
  ns: scrapy.responsetypes
  description: Retrieve the entire contents of the object.
  summary: Retrieve the entire contents of the object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.isatty
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Return whether this is an 'interactive' stream.

    Return False if it can't be determined.
  summary: Return whether this is an 'interactive' stream
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.line_buffering
  kind: property
  ns: scrapy.responsetypes
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: StringIO.newlines
  kind: property
  ns: scrapy.responsetypes
  description: |-
    Line endings translated so far.

    Only line endings translated during reading are considered.

    Subclasses should override.
  summary: Line endings translated so far
  signatures: null
  inherits_from: null
- name: StringIO.read
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Read at most size characters, returned as a string.

    If the argument is negative or omitted, read until EOF
    is reached. Return an empty string at EOF.
  summary: Read at most size characters, returned as a string
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: size
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.readable
  kind: callable
  ns: scrapy.responsetypes
  description: Returns True if the IO object can be read.
  summary: Returns True if the IO object can be read
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.readline
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Read until newline or EOF.

    Returns an empty string if EOF is hit immediately.
  summary: Read until newline or EOF
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: size
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.readlines
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Return a list of lines from the stream.

    hint can be specified to control the number of lines read: no more
    lines will be read if the total size (in bytes/characters) of all
    lines so far exceeds hint.
  summary: Return a list of lines from the stream
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: hint
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.seek
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Change stream position.

    Seek to character offset pos relative to position indicated by whence:
        0  Start of stream (the default).  pos should be >= 0;
        1  Current position - pos must be 0;
        2  End of stream - pos must be 0.
    Returns the new absolute position.
  summary: Change stream position
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: pos
    default: null
    rest: false
  - kind: positional
    name: whence
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.seekable
  kind: callable
  ns: scrapy.responsetypes
  description: Returns True if the IO object can be seeked.
  summary: Returns True if the IO object can be seeked
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.tell
  kind: callable
  ns: scrapy.responsetypes
  description: Tell the current file position.
  summary: Tell the current file position
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.truncate
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Truncate size to pos.

    The pos argument defaults to the current file position, as
    returned by tell().  The current file position is unchanged.
    Returns the new absolute position.
  summary: Truncate size to pos
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: pos
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.writable
  kind: callable
  ns: scrapy.responsetypes
  description: Returns True if the IO object can be written.
  summary: Returns True if the IO object can be written
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.write
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Write string to file.

    Returns the number of characters written, which is always equal to
    the length of the string.
  summary: Write string to file
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: s
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StringIO.writelines
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Write a list of lines to stream.

    Line separators are not added, so it is usual for each of the
    lines provided to have a line separator at the end.
  summary: Write a list of lines to stream
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: lines
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.responsetypes
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: binary_is_text
  kind: function
  ns: scrapy.responsetypes
  description: |-
    Returns ``True`` if the given ``data`` argument (a ``bytes`` object)
    does not contain unprintable control characters.
  summary: Returns ``True`` if the given ``data`` argument (a ``bytes`` object)
  signatures:
  - kind: positional
    name: data
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: get_data
  kind: function
  ns: scrapy.responsetypes
  description: |-
    Get a resource from a package.

    This is a wrapper round the PEP 302 loader get_data API. The package
    argument should be the name of a package, in standard module format
    (foo.bar). The resource argument should be in the form of a relative
    filename, using '/' as the path separator. The parent directory name '..'
    is not allowed, and nor is a rooted name (starting with a '/').

    The function returns a binary string, which is the contents of the
    specified resource.

    For packages located in the filesystem, which have already been imported,
    this is the rough equivalent of

        d = os.path.dirname(sys.modules[package].__file__)
        data = open(os.path.join(d, resource), 'rb').read()

    If the package cannot be located or loaded, or it uses a PEP 302 loader
    which does not support get_data(), then None is returned.
  summary: Get a resource from a package
  signatures:
  - kind: positional
    name: package
    default: null
    rest: false
  - kind: positional
    name: resource
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: load_object
  kind: function
  ns: scrapy.responsetypes
  description: |-
    Load an object given its absolute object path, and return it.

    The object can be the import path of a class, function, variable or an
    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.

    If ``path`` is not a string, but is a callable object, such as a class or
    a function, then return it as is.
  summary: Load an object given its absolute object path, and return it
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: responsetypes
  kind: const
  ns: scrapy.responsetypes
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: to_bytes
  kind: function
  ns: scrapy.responsetypes
  description: |-
    Return the binary representation of ``text``. If ``text``
    is already a bytes object, return it as-is.
  summary: Return the binary representation of ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: to_unicode
  kind: function
  ns: scrapy.responsetypes
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.robotstxt
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ProtegoRobotParser
  kind: class
  ns: scrapy.robotstxt
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: robotstxt_body
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: ProtegoRobotParser
  inherits_from:
  - <class 'scrapy.robotstxt.RobotParser'>
- name: ProtegoRobotParser.allowed
  kind: method
  ns: scrapy.robotstxt
  description: |-
    Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.

    :param url: Absolute URL
    :type url: str

    :param user_agent: User agent
    :type user_agent: str
  summary: Return ``True`` if ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: user_agent
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ProtegoRobotParser.from_crawler
  kind: function
  ns: scrapy.robotstxt
  description: |-
    Parse the content of a robots.txt_ file as bytes. This must be a class method.
    It must return a new instance of the parser backend.

    :param crawler: crawler which made the request
    :type crawler: :class:`~scrapy.crawler.Crawler` instance

    :param robotstxt_body: content of a robots.txt_ file.
    :type robotstxt_body: bytes
  summary: Parse the content of a robots
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: robotstxt_body
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PythonRobotParser
  kind: class
  ns: scrapy.robotstxt
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: robotstxt_body
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: PythonRobotParser
  inherits_from:
  - <class 'scrapy.robotstxt.RobotParser'>
- name: PythonRobotParser.allowed
  kind: method
  ns: scrapy.robotstxt
  description: |-
    Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.

    :param url: Absolute URL
    :type url: str

    :param user_agent: User agent
    :type user_agent: str
  summary: Return ``True`` if ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: user_agent
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PythonRobotParser.from_crawler
  kind: function
  ns: scrapy.robotstxt
  description: |-
    Parse the content of a robots.txt_ file as bytes. This must be a class method.
    It must return a new instance of the parser backend.

    :param crawler: crawler which made the request
    :type crawler: :class:`~scrapy.crawler.Crawler` instance

    :param robotstxt_body: content of a robots.txt_ file.
    :type robotstxt_body: bytes
  summary: Parse the content of a robots
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: robotstxt_body
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ReppyRobotParser
  kind: class
  ns: scrapy.robotstxt
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: robotstxt_body
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: ReppyRobotParser
  inherits_from:
  - <class 'scrapy.robotstxt.RobotParser'>
- name: ReppyRobotParser.allowed
  kind: method
  ns: scrapy.robotstxt
  description: |-
    Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.

    :param url: Absolute URL
    :type url: str

    :param user_agent: User agent
    :type user_agent: str
  summary: Return ``True`` if ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: user_agent
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ReppyRobotParser.from_crawler
  kind: function
  ns: scrapy.robotstxt
  description: |-
    Parse the content of a robots.txt_ file as bytes. This must be a class method.
    It must return a new instance of the parser backend.

    :param crawler: crawler which made the request
    :type crawler: :class:`~scrapy.crawler.Crawler` instance

    :param robotstxt_body: content of a robots.txt_ file.
    :type robotstxt_body: bytes
  summary: Parse the content of a robots
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: robotstxt_body
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RerpRobotParser
  kind: class
  ns: scrapy.robotstxt
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: robotstxt_body
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: RerpRobotParser
  inherits_from:
  - <class 'scrapy.robotstxt.RobotParser'>
- name: RerpRobotParser.allowed
  kind: method
  ns: scrapy.robotstxt
  description: |-
    Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.

    :param url: Absolute URL
    :type url: str

    :param user_agent: User agent
    :type user_agent: str
  summary: Return ``True`` if ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: user_agent
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RerpRobotParser.from_crawler
  kind: function
  ns: scrapy.robotstxt
  description: |-
    Parse the content of a robots.txt_ file as bytes. This must be a class method.
    It must return a new instance of the parser backend.

    :param crawler: crawler which made the request
    :type crawler: :class:`~scrapy.crawler.Crawler` instance

    :param robotstxt_body: content of a robots.txt_ file.
    :type robotstxt_body: bytes
  summary: Parse the content of a robots
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: robotstxt_body
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RobotParser
  kind: class
  ns: scrapy.robotstxt
  description: null
  summary: ''
  signatures:
  - type: RobotParser
  inherits_from: null
- name: RobotParser.allowed
  kind: method
  ns: scrapy.robotstxt
  description: |-
    Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.

    :param url: Absolute URL
    :type url: str

    :param user_agent: User agent
    :type user_agent: str
  summary: Return ``True`` if ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: user_agent
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: RobotParser.from_crawler
  kind: function
  ns: scrapy.robotstxt
  description: |-
    Parse the content of a robots.txt_ file as bytes. This must be a class method.
    It must return a new instance of the parser backend.

    :param crawler: crawler which made the request
    :type crawler: :class:`~scrapy.crawler.Crawler` instance

    :param robotstxt_body: content of a robots.txt_ file.
    :type robotstxt_body: bytes
  summary: Parse the content of a robots
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: robotstxt_body
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: abstractmethod
  kind: function
  ns: scrapy.robotstxt
  description: |-
    A decorator indicating abstract methods.

    Requires that the metaclass is ABCMeta or derived from it.  A
    class that has a metaclass derived from ABCMeta cannot be
    instantiated unless all of its abstract methods are overridden.
    The abstract methods can be called using any of the normal
    'super' call mechanisms.  abstractmethod() may be used to declare
    abstract methods for properties and descriptors.

    Usage:

        class C(metaclass=ABCMeta):
            @abstractmethod
            def my_abstract_method(self, arg1, arg2, argN):
                ...
  summary: A decorator indicating abstract methods
  signatures:
  - kind: positional
    name: funcobj
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: decode_robotstxt
  kind: function
  ns: scrapy.robotstxt
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: robotstxt_body
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: to_native_str_type
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.robotstxt
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: to_unicode
  kind: function
  ns: scrapy.robotstxt
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.selector
  kind: module
  ns: null
  description: Selectors
  summary: Selectors
  signatures: null
  inherits_from: null
- name: unified
  kind: module
  ns: scrapy.selector
  description: XPath selectors based on lxml
  summary: XPath selectors based on lxml
  signatures: null
  inherits_from: null
- name: scrapy.settings
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.settings
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.settings
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterator
  kind: callable
  ns: scrapy.settings
  description: A generic version of collections.abc.Iterator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.settings
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Mapping
  kind: callable
  ns: scrapy.settings
  description: A generic version of collections.abc.Mapping.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ModuleType
  kind: class
  ns: scrapy.settings
  description: |-
    Create a module object.

    The name must be a string; the optional doc argument can have any type.
  summary: Create a module object
  signatures:
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: doc
    default: None
    rest: false
  - type: module
  inherits_from: null
- name: MutableMapping
  kind: callable
  ns: scrapy.settings
  description: A generic version of collections.abc.MutableMapping.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.settings
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SETTINGS_PRIORITIES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SettingsAttribute
  kind: class
  ns: scrapy.settings
  description: |-
    Class for storing data related to settings attributes.

    This class is intended for internal usage, you should try Settings class
    for settings configuration, not this one.
  summary: Class for storing data related to settings attributes
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: priority
    default: null
    rest: false
  - type: SettingsAttribute
  inherits_from: null
- name: SettingsAttribute.set
  kind: method
  ns: scrapy.settings
  description: Sets value if priority is higher or equal than current priority.
  summary: Sets value if priority is higher or equal than current priority
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: priority
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.settings
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.settings
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: annotations
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.settings
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: default_settings
  kind: module
  ns: scrapy.settings
  description: |-
    This module contains the default values for all settings used by Scrapy.

    For more information about these settings you can read the settings
    documentation in docs/topics/settings.rst

    Scrapy developers, if you add a setting here remember to:

    * add it in alphabetical order
    * group similar settings without leaving blank lines
    * add its documentation to the available settings documentation
      (docs/topics/settings.rst)
  summary: This module contains the default values for all settings used by Scrapy
  signatures: null
  inherits_from: null
- name: ADDONS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: AJAXCRAWL_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ASYNCIO_EVENT_LOOP
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: AUTOTHROTTLE_DEBUG
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: AUTOTHROTTLE_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: AUTOTHROTTLE_MAX_DELAY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: AUTOTHROTTLE_START_DELAY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: AUTOTHROTTLE_TARGET_CONCURRENCY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: BOT_NAME
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CLOSESPIDER_ERRORCOUNT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CLOSESPIDER_ITEMCOUNT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CLOSESPIDER_PAGECOUNT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CLOSESPIDER_TIMEOUT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: COMMANDS_MODULE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: COMPRESSION_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CONCURRENT_ITEMS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CONCURRENT_REQUESTS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CONCURRENT_REQUESTS_PER_DOMAIN
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CONCURRENT_REQUESTS_PER_IP
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: COOKIES_DEBUG
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: COOKIES_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DEFAULT_ITEM_CLASS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DEFAULT_REQUEST_HEADERS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DEPTH_LIMIT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DEPTH_PRIORITY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DEPTH_STATS_VERBOSE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DNSCACHE_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DNSCACHE_SIZE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DNS_RESOLVER
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DNS_TIMEOUT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOADER
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOADER_CLIENTCONTEXTFACTORY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOADER_CLIENT_TLS_CIPHERS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOADER_CLIENT_TLS_METHOD
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOADER_HTTPCLIENTFACTORY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOADER_MIDDLEWARES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOADER_MIDDLEWARES_BASE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOADER_STATS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOAD_DELAY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOAD_FAIL_ON_DATALOSS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOAD_HANDLERS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOAD_HANDLERS_BASE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOAD_MAXSIZE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOAD_TIMEOUT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DOWNLOAD_WARNSIZE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DUPEFILTER_CLASS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: EDITOR
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: EXTENSIONS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: EXTENSIONS_BASE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEEDS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_EXPORTERS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_EXPORTERS_BASE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_EXPORT_BATCH_ITEM_COUNT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_EXPORT_ENCODING
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_EXPORT_FIELDS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_EXPORT_INDENT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_STORAGES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_STORAGES_BASE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_STORAGE_FTP_ACTIVE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_STORAGE_GCS_ACL
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_STORAGE_S3_ACL
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_STORE_EMPTY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_TEMPDIR
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FEED_URI_PARAMS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FILES_STORE_GCS_ACL
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FILES_STORE_S3_ACL
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FTP_PASSIVE_MODE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FTP_PASSWORD
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FTP_USER
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: GCS_PROJECT_ID
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_ALWAYS_STORE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_DBM_MODULE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_DIR
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_EXPIRATION_SECS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_GZIP
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_IGNORE_HTTP_CODES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_IGNORE_MISSING
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_IGNORE_SCHEMES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_POLICY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPCACHE_STORAGE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPPROXY_AUTH_ENCODING
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: HTTPPROXY_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: IMAGES_STORE_GCS_ACL
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: IMAGES_STORE_S3_ACL
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ITEM_PIPELINES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ITEM_PIPELINES_BASE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ITEM_PROCESSOR
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: JOBDIR
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LOGSTATS_INTERVAL
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LOG_DATEFORMAT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LOG_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LOG_ENCODING
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LOG_FILE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LOG_FILE_APPEND
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LOG_FORMAT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LOG_FORMATTER
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LOG_LEVEL
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LOG_SHORT_NAMES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LOG_STDOUT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MAIL_FROM
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MAIL_HOST
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MAIL_PASS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MAIL_PORT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MAIL_USER
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MEMDEBUG_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MEMDEBUG_NOTIFY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MEMUSAGE_CHECK_INTERVAL_SECONDS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MEMUSAGE_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MEMUSAGE_LIMIT_MB
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MEMUSAGE_NOTIFY_MAIL
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: MEMUSAGE_WARNING_MB
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: METAREFRESH_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: METAREFRESH_IGNORE_TAGS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: METAREFRESH_MAXDELAY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: NEWSPIDER_MODULE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: PERIODIC_LOG_DELTA
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: PERIODIC_LOG_STATS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: PERIODIC_LOG_TIMING_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: RANDOMIZE_DOWNLOAD_DELAY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: REACTOR_THREADPOOL_MAXSIZE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: REDIRECT_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: REDIRECT_MAX_TIMES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: REDIRECT_PRIORITY_ADJUST
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: REFERER_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: REFERRER_POLICY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: REQUEST_FINGERPRINTER_CLASS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: REQUEST_FINGERPRINTER_IMPLEMENTATION
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: RETRY_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: RETRY_EXCEPTIONS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: RETRY_HTTP_CODES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: RETRY_PRIORITY_ADJUST
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: RETRY_TIMES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ROBOTSTXT_OBEY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ROBOTSTXT_PARSER
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ROBOTSTXT_USER_AGENT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SCHEDULER
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SCHEDULER_DEBUG
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SCHEDULER_DISK_QUEUE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SCHEDULER_MEMORY_QUEUE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SCHEDULER_PRIORITY_QUEUE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SCRAPER_SLOT_MAX_ACTIVE_SIZE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SPIDER_CONTRACTS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SPIDER_CONTRACTS_BASE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SPIDER_LOADER_CLASS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SPIDER_LOADER_WARN_ONLY
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SPIDER_MIDDLEWARES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SPIDER_MIDDLEWARES_BASE
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SPIDER_MODULES
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: STATSMAILER_RCPTS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: STATS_CLASS
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: STATS_DUMP
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TELNETCONSOLE_ENABLED
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TELNETCONSOLE_HOST
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TELNETCONSOLE_PASSWORD
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TELNETCONSOLE_PORT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TELNETCONSOLE_USERNAME
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TEMPLATES_DIR
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TWISTED_REACTOR
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: URLLENGTH_LIMIT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: USER_AGENT
  kind: const
  ns: scrapy.settings
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: import_module
  kind: function
  ns: scrapy.settings
  description: |-
    Import a module.

    The 'package' argument is required when performing a relative import. It
    specifies the package to use as the anchor point from which to resolve the
    relative import to an absolute import.
  summary: Import a module
  signatures:
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: package
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: get_settings_priority
  kind: function
  ns: scrapy.settings
  description: |-
    Small helper function that looks up a given string priority in the
    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its
    numerical value, or directly returns a given numerical priority.
  summary: Small helper function that looks up a given string priority in the
  signatures:
  - kind: positional
    name: priority
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: import_module
  kind: function
  ns: scrapy.settings
  description: |-
    Import a module.

    The 'package' argument is required when performing a relative import. It
    specifies the package to use as the anchor point from which to resolve the
    relative import to an absolute import.
  summary: Import a module
  signatures:
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: package
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: iter_default_settings
  kind: function
  ns: scrapy.settings
  description: Return the default settings as an iterator of (name, value) tuples
  summary: Return the default settings as an iterator of (name, value) tuples
  signatures:
  - type: '?'
  inherits_from: null
- name: overridden_settings
  kind: function
  ns: scrapy.settings
  description: Return an iterable of the settings that have been overridden
  summary: Return an iterable of the settings that have been overridden
  signatures:
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: pformat
  kind: function
  ns: scrapy.settings
  description: Format a Python object into a pretty-printed representation.
  summary: Format a Python object into a pretty-printed representation
  signatures:
  - kind: positional
    name: object
    default: null
    rest: false
  - kind: positional
    name: indent
    default: '1'
    rest: false
  - kind: positional
    name: width
    default: '80'
    rest: false
  - kind: positional
    name: depth
    default: None
    rest: false
  - name: compact
    default: 'False'
    rest: false
    kind: kw-only
  - name: sort_dicts
    default: 'True'
    rest: false
    kind: kw-only
  - name: underscore_numbers
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: scrapy.shell
  kind: module
  ns: null
  description: |-
    Scrapy Shell

    See documentation in docs/topics/shell.rst
  summary: Scrapy Shell
  signatures: null
  inherits_from: null
- name: DEFAULT_PYTHON_SHELLS
  kind: const
  ns: scrapy.shell
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SequenceExclude
  kind: class
  ns: scrapy.shell
  description: Object to test if an item is NOT within some sequence.
  summary: Object to test if an item is NOT within some sequence
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: seq
    default: null
    rest: false
  - type: SequenceExclude
  inherits_from: null
- name: Shell
  kind: class
  ns: scrapy.shell
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: update_vars
    default: None
    rest: false
  - kind: positional
    name: code
    default: None
    rest: false
  - type: Shell
  inherits_from: null
- name: Shell.fetch
  kind: method
  ns: scrapy.shell
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request_or_url
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - kind: positional
    name: redirect
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: Shell.get_help
  kind: method
  ns: scrapy.shell
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Shell.populate_vars
  kind: method
  ns: scrapy.shell
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: None
    rest: false
  - kind: positional
    name: request
    default: None
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Shell.print_help
  kind: method
  ns: scrapy.shell
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Shell.relevant_classes
  kind: property
  ns: scrapy.shell
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: Shell.start
  kind: method
  ns: scrapy.shell
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: url
    default: None
    rest: false
  - kind: positional
    name: request
    default: None
    rest: false
  - kind: positional
    name: response
    default: None
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - kind: positional
    name: redirect
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: any_to_uri
  kind: function
  ns: scrapy.shell
  description: |-
    If given a path name, return its File URI, otherwise return it
    unmodified
  summary: If given a path name, return its File URI, otherwise return it
  signatures:
  - kind: positional
    name: uri_or_path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: get_config
  kind: function
  ns: scrapy.shell
  description: Get Scrapy config file as a ConfigParser
  summary: Get Scrapy config file as a ConfigParser
  signatures:
  - kind: positional
    name: use_closest
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: inspect_response
  kind: function
  ns: scrapy.shell
  description: Open a shell to inspect the given response
  summary: Open a shell to inspect the given response
  signatures:
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: is_asyncio_reactor_installed
  kind: function
  ns: scrapy.shell
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: is_item
  kind: function
  ns: scrapy.shell
  description: |-
    Return True if the given object belongs to one of the supported types, False otherwise.

    Alias for ItemAdapter.is_item
  summary: Return True if the given object belongs to one of the supported types, False otherwise
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: load_object
  kind: function
  ns: scrapy.shell
  description: |-
    Load an object given its absolute object path, and return it.

    The object can be the import path of a class, function, variable or an
    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.

    If ``path`` is not a string, but is a callable object, such as a class or
    a function, then return it as is.
  summary: Load an object given its absolute object path, and return it
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: open_in_browser
  kind: function
  ns: scrapy.shell
  description: |-
    Open *response* in a local web browser, adjusting the `base tag`_ for
    external links to work, e.g. so that images and styles are displayed.

    .. _base tag: https://www.w3schools.com/tags/tag_base.asp

    For example:

    .. code-block:: python

        from scrapy.utils.response import open_in_browser


        def parse_details(self, response):
            if "item name" not in response.body:
                open_in_browser(response)
  summary: Open *response* in a local web browser, adjusting the `base tag`_ for
  signatures:
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: _openfunc
    default: <function open at 0x7fa97305ff60>
    rest: false
  - type: '?'
  inherits_from: null
- name: set_asyncio_event_loop
  kind: function
  ns: scrapy.shell
  description: Sets and returns the event loop with specified import path.
  summary: Sets and returns the event loop with specified import path
  signatures:
  - kind: positional
    name: event_loop_path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: start_python_console
  kind: function
  ns: scrapy.shell
  description: |-
    Start Python console bound to the given namespace.
    Readline support and tab completion will be used on Unix, if available.
  summary: Start Python console bound to the given namespace
  signatures:
  - kind: positional
    name: namespace
    default: None
    rest: false
  - kind: positional
    name: banner
    default: null
    rest: false
  - kind: positional
    name: shells
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.signalmanager
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.signalmanager
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.signalmanager
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.signals
  kind: module
  ns: null
  description: |-
    Scrapy signals

    These signals are documented in docs/topics/signals.rst. Please don't add new
    signals here without documenting them there.
  summary: Scrapy signals
  signatures: null
  inherits_from: null
- name: bytes_received
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: engine_started
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: engine_stopped
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: feed_exporter_closed
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: feed_slot_closed
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: headers_received
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: item_dropped
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: item_error
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: item_passed
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: item_scraped
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: request_dropped
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: request_left_downloader
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: request_reached_downloader
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: request_received
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: request_scheduled
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: response_downloaded
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: response_received
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: spider_closed
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: spider_error
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: spider_idle
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: spider_opened
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: stats_spider_closed
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: stats_spider_closing
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: stats_spider_opened
  kind: const
  ns: scrapy.signals
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: scrapy.spiderloader
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DefaultDict
  kind: callable
  ns: scrapy.spiderloader
  description: A generic version of collections.defaultdict.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.spiderloader
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ISpiderLoader
  kind: callable
  ns: scrapy.spiderloader
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.spiderloader
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoader
  kind: class
  ns: scrapy.spiderloader
  description: |-
    SpiderLoader is a class which locates and loads spiders
    in a Scrapy project.
  summary: SpiderLoader is a class which locates and loads spiders
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: SpiderLoader
  inherits_from: null
- name: SpiderLoader.find_by_request
  kind: method
  ns: scrapy.spiderloader
  description: Return the list of spider names that can handle the given request.
  summary: Return the list of spider names that can handle the given request
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoader.from_settings
  kind: function
  ns: scrapy.spiderloader
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoader.list
  kind: method
  ns: scrapy.spiderloader
  description: Return a list with the names of all spiders available in the project.
  summary: Return a list with the names of all spiders available in the project
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoader.load
  kind: method
  ns: scrapy.spiderloader
  description: |-
    Return the Spider class for the given spider name. If the spider
    name is not found, raise a KeyError.
  summary: Return the Spider class for the given spider name
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.spiderloader
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.spiderloader
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.spiderloader
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: annotations
  kind: const
  ns: scrapy.spiderloader
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: iter_spider_classes
  kind: function
  ns: scrapy.spiderloader
  description: |-
    Return an iterator over all spider classes defined in the given module
    that can be instantiated (i.e. which have name)
  summary: Return an iterator over all spider classes defined in the given module
  signatures:
  - kind: positional
    name: module
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: walk_modules
  kind: function
  ns: scrapy.spiderloader
  description: |-
    Loads a module and all its submodules from the given module path and
    returns them. If *any* module throws an exception while importing, that
    exception is thrown back.

    For example: walk_modules('scrapy.utils')
  summary: Loads a module and all its submodules from the given module path and
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.spidermiddlewares
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: scrapy.spiders
  kind: module
  ns: null
  description: |-
    Base class for Scrapy spiders

    See documentation in docs/topics/spiders.rst
  summary: Base class for Scrapy spiders
  signatures: null
  inherits_from: null
- name: CSVFeedSpider
  kind: class
  ns: scrapy.spiders
  description: |-
    Spider for parsing CSV feeds.
    It receives a CSV file in a response; iterates through each of its rows,
    and calls parse_row with a dict containing each field's data.

    You can set some options regarding the CSV file, such as the delimiter, quotechar
    and the file's headers.
  summary: Spider for parsing CSV feeds
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: None
    rest: false
  - type: CSVFeedSpider
  inherits_from:
  - <class 'scrapy.spiders.Spider'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: CSVFeedSpider.adapt_response
  kind: method
  ns: scrapy.spiders
  description: This method has the same purpose as the one in XMLFeedSpider
  summary: This method has the same purpose as the one in XMLFeedSpider
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CSVFeedSpider.close
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CSVFeedSpider.custom_settings
  kind: property
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CSVFeedSpider.delimiter
  kind: property
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CSVFeedSpider.from_crawler
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CSVFeedSpider.handles_request
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CSVFeedSpider.headers
  kind: property
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CSVFeedSpider.log
  kind: method
  ns: scrapy.spiders
  description: |-
    Log the given message at the given log level

    This helper wraps a log call to the logger within the spider, but you
    can use it directly (e.g. Spider.logger.info('msg')) or use any other
    Python logger too.
  summary: Log the given message at the given log level
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: message
    default: null
    rest: false
  - kind: positional
    name: level
    default: '10'
    rest: false
  - type: '?'
  inherits_from: null
- name: CSVFeedSpider.logger
  kind: property
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CSVFeedSpider.parse
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CSVFeedSpider.parse_row
  kind: method
  ns: scrapy.spiders
  description: This method must be overridden with your custom spider functionality
  summary: This method must be overridden with your custom spider functionality
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: row
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CSVFeedSpider.parse_rows
  kind: method
  ns: scrapy.spiders
  description: |-
    Receives a response and a dict (representing each row) with a key for
    each provided (or detected) header of the CSV file.  This spider also
    gives the opportunity to override adapt_response and
    process_results methods for pre and post-processing purposes.
  summary: Receives a response and a dict (representing each row) with a key for
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CSVFeedSpider.process_results
  kind: method
  ns: scrapy.spiders
  description: This method has the same purpose as the one in XMLFeedSpider
  summary: This method has the same purpose as the one in XMLFeedSpider
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: results
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CSVFeedSpider.quotechar
  kind: property
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CSVFeedSpider.start_requests
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CSVFeedSpider.update_settings
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlSpider
  kind: class
  ns: scrapy.spiders
  description: |-
    Base class for scrapy spiders. All spiders must inherit from this
    class.
  summary: Base class for scrapy spiders
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: CrawlSpider
  inherits_from:
  - <class 'scrapy.spiders.Spider'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: CrawlSpider.close
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlSpider.custom_settings
  kind: property
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CrawlSpider.from_crawler
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlSpider.handles_request
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlSpider.log
  kind: method
  ns: scrapy.spiders
  description: |-
    Log the given message at the given log level

    This helper wraps a log call to the logger within the spider, but you
    can use it directly (e.g. Spider.logger.info('msg')) or use any other
    Python logger too.
  summary: Log the given message at the given log level
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: message
    default: null
    rest: false
  - kind: positional
    name: level
    default: '10'
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlSpider.logger
  kind: property
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CrawlSpider.parse
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlSpider.parse_start_url
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlSpider.process_results
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: results
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlSpider.rules
  kind: property
  ns: scrapy.spiders
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: CrawlSpider.start_requests
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CrawlSpider.update_settings
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.spiders
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.spiders
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.spiders
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Rule
  kind: class
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: link_extractor
    default: None
    rest: false
  - kind: positional
    name: callback
    default: None
    rest: false
  - kind: positional
    name: cb_kwargs
    default: None
    rest: false
  - kind: positional
    name: follow
    default: None
    rest: false
  - kind: positional
    name: process_links
    default: None
    rest: false
  - kind: positional
    name: process_request
    default: None
    rest: false
  - kind: positional
    name: errback
    default: None
    rest: false
  - type: Rule
  inherits_from: null
- name: SitemapSpider
  kind: class
  ns: scrapy.spiders
  description: |-
    Base class for scrapy spiders. All spiders must inherit from this
    class.
  summary: Base class for scrapy spiders
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: SitemapSpider
  inherits_from:
  - <class 'scrapy.spiders.Spider'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: SitemapSpider.close
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SitemapSpider.custom_settings
  kind: property
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SitemapSpider.from_crawler
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SitemapSpider.handles_request
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SitemapSpider.log
  kind: method
  ns: scrapy.spiders
  description: |-
    Log the given message at the given log level

    This helper wraps a log call to the logger within the spider, but you
    can use it directly (e.g. Spider.logger.info('msg')) or use any other
    Python logger too.
  summary: Log the given message at the given log level
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: message
    default: null
    rest: false
  - kind: positional
    name: level
    default: '10'
    rest: false
  - type: '?'
  inherits_from: null
- name: SitemapSpider.logger
  kind: property
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SitemapSpider.parse
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SitemapSpider.sitemap_alternate_links
  kind: property
  ns: scrapy.spiders
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: SitemapSpider.sitemap_filter
  kind: method
  ns: scrapy.spiders
  description: |-
    This method can be used to filter sitemap entries by their
    attributes, for example, you can filter locs with lastmod greater
    than a given date (see docs).
  summary: This method can be used to filter sitemap entries by their
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: entries
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SitemapSpider.sitemap_follow
  kind: property
  ns: scrapy.spiders
  description: |-
    Built-in mutable sequence.

    If no argument is given, the constructor creates a new empty list.
    The argument must be an iterable if specified.
  summary: Built-in mutable sequence
  signatures: null
  inherits_from: null
- name: SitemapSpider.sitemap_rules
  kind: property
  ns: scrapy.spiders
  description: |-
    Built-in mutable sequence.

    If no argument is given, the constructor creates a new empty list.
    The argument must be an iterable if specified.
  summary: Built-in mutable sequence
  signatures: null
  inherits_from: null
- name: SitemapSpider.sitemap_urls
  kind: property
  ns: scrapy.spiders
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: SitemapSpider.start_requests
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SitemapSpider.update_settings
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.spiders
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XMLFeedSpider
  kind: class
  ns: scrapy.spiders
  description: |-
    This class intends to be the base class for spiders that scrape
    from XML feeds.

    You can choose whether to parse the file using the 'iternodes' iterator, an
    'xml' selector, or an 'html' selector.  In most cases, it's convenient to
    use iternodes, since it's a faster and cleaner.
  summary: This class intends to be the base class for spiders that scrape
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: None
    rest: false
  - type: XMLFeedSpider
  inherits_from:
  - <class 'scrapy.spiders.Spider'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: XMLFeedSpider.adapt_response
  kind: method
  ns: scrapy.spiders
  description: |-
    You can override this function in order to make any changes you want
    to into the feed before parsing it. This function must return a
    response.
  summary: You can override this function in order to make any changes you want
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XMLFeedSpider.close
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XMLFeedSpider.custom_settings
  kind: property
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XMLFeedSpider.from_crawler
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XMLFeedSpider.handles_request
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XMLFeedSpider.iterator
  kind: property
  ns: scrapy.spiders
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: XMLFeedSpider.itertag
  kind: property
  ns: scrapy.spiders
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: XMLFeedSpider.log
  kind: method
  ns: scrapy.spiders
  description: |-
    Log the given message at the given log level

    This helper wraps a log call to the logger within the spider, but you
    can use it directly (e.g. Spider.logger.info('msg')) or use any other
    Python logger too.
  summary: Log the given message at the given log level
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: message
    default: null
    rest: false
  - kind: positional
    name: level
    default: '10'
    rest: false
  - type: '?'
  inherits_from: null
- name: XMLFeedSpider.logger
  kind: property
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: XMLFeedSpider.namespaces
  kind: property
  ns: scrapy.spiders
  description: |-
    Built-in immutable sequence.

    If no argument is given, the constructor returns an empty tuple.
    If iterable is specified the tuple is initialized from iterable's items.

    If the argument is a tuple, the return value is the same object.
  summary: Built-in immutable sequence
  signatures: null
  inherits_from: null
- name: XMLFeedSpider.parse
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XMLFeedSpider.parse_node
  kind: method
  ns: scrapy.spiders
  description: This method must be overridden with your custom spider functionality
  summary: This method must be overridden with your custom spider functionality
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: selector
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XMLFeedSpider.parse_nodes
  kind: method
  ns: scrapy.spiders
  description: |-
    This method is called for the nodes matching the provided tag name
    (itertag). Receives the response and an Selector for each node.
    Overriding this method is mandatory. Otherwise, you spider won't work.
    This method must return either an item, a request, or a list
    containing any of them.
  summary: This method is called for the nodes matching the provided tag name
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: nodes
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XMLFeedSpider.process_results
  kind: method
  ns: scrapy.spiders
  description: |-
    This overridable method is called for each result (item or request)
    returned by the spider, and it's intended to perform any last time
    processing required before returning the results to the framework core,
    for example setting the item GUIDs. It receives a list of results and
    the response which originated that results. It must return a list of
    results (items or requests).
  summary: This overridable method is called for each result (item or request)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: results
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XMLFeedSpider.start_requests
  kind: method
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: XMLFeedSpider.update_settings
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: annotations
  kind: const
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.spiders
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: crawl
  kind: module
  ns: scrapy.spiders
  description: |-
    This modules implements the CrawlSpider which is the recommended spider to use
    for scraping typical web sites that requires crawling pages.

    See documentation in docs/topics/spiders.rst
  summary: This modules implements the CrawlSpider which is the recommended spider to use
  signatures: null
  inherits_from: null
- name: AsyncIterable
  kind: callable
  ns: scrapy.spiders
  description: A generic version of collections.abc.AsyncIterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Awaitable
  kind: callable
  ns: scrapy.spiders
  description: A generic version of collections.abc.Awaitable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Sequence
  kind: callable
  ns: scrapy.spiders
  description: A generic version of collections.abc.Sequence.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: collect_asyncgen
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: iterate_spider_output
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: feed
  kind: module
  ns: scrapy.spiders
  description: |-
    This module implements the XMLFeedSpider which is the recommended spider to use
    for scraping from an XML feed.

    See documentation in docs/topics/spiders.rst
  summary: This module implements the XMLFeedSpider which is the recommended spider to use
  signatures: null
  inherits_from: null
- name: csviter
  kind: function
  ns: scrapy.spiders
  description: |-
    Returns an iterator of dictionaries from the given csv object

    obj can be:
    - a Response object
    - a unicode string
    - a string encoded as utf-8

    delimiter is the character used to separate fields on the given obj.

    headers is an iterable that when provided offers the keys
    for the returned dictionaries, if not the first row is used.

    quotechar is the character used to enclosure fields on the given obj.
  summary: Returns an iterator of dictionaries from the given csv object
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - kind: positional
    name: delimiter
    default: None
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: quotechar
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: iterate_spider_output
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: xmliter_lxml
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - kind: positional
    name: nodename
    default: null
    rest: false
  - kind: positional
    name: namespace
    default: None
    rest: false
  - kind: positional
    name: prefix
    default: x
    rest: false
  - type: '?'
  inherits_from: null
- name: sitemap
  kind: module
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Sitemap
  kind: class
  ns: scrapy.spiders
  description: |-
    Class to parse Sitemap (type=urlset) and Sitemap Index
    (type=sitemapindex) files
  summary: Class to parse Sitemap (type=urlset) and Sitemap Index
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: xmltext
    default: null
    rest: false
  - type: Sitemap
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: gunzip
  kind: function
  ns: scrapy.spiders
  description: |-
    Gunzip the given data and return as much data as possible.

    This is resilient to CRC checksum errors.
  summary: Gunzip the given data and return as much data as possible
  signatures:
  - kind: positional
    name: data
    default: null
    rest: false
  - name: max_size
    default: '0'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: gzip_magic_number
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: iterloc
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: it
    default: null
    rest: false
  - kind: positional
    name: alt
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: regex
  kind: function
  ns: scrapy.spiders
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: x
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: sitemap_urls_from_robots
  kind: function
  ns: scrapy.spiders
  description: |-
    Return an iterator over all sitemap urls contained in the given
    robots.txt file
  summary: Return an iterator over all sitemap urls contained in the given
  signatures:
  - kind: positional
    name: robots_text
    default: null
    rest: false
  - kind: positional
    name: base_url
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: url_is_from_spider
  kind: function
  ns: scrapy.spiders
  description: Return True if the url belongs to the given spider
  summary: Return True if the url belongs to the given spider
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: scrapy.squeues
  kind: module
  ns: null
  description: Scheduler queues
  summary: Scheduler queues
  signatures: null
  inherits_from: null
- name: FifoMemoryQueue
  kind: class
  ns: scrapy.squeues
  description: In-memory FIFO queue, API compliant with FifoDiskQueue.
  summary: In-memory FIFO queue, API compliant with FifoDiskQueue
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: ScrapyRequestQueue
  inherits_from:
  - <class 'queuelib.queue.FifoMemoryQueue'>
- name: FifoMemoryQueue.close
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FifoMemoryQueue.from_crawler
  kind: function
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FifoMemoryQueue.peek
  kind: method
  ns: scrapy.squeues
  description: |-
    Returns the next object to be returned by :meth:`pop`,
    but without removing it from the queue.

    Raises :exc:`NotImplementedError` if the underlying queue class does
    not implement a ``peek`` method, which is optional for queues.
  summary: Returns the next object to be returned by :meth:`pop`,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FifoMemoryQueue.pop
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FifoMemoryQueue.push
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: obj
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LifoMemoryQueue
  kind: class
  ns: scrapy.squeues
  description: In-memory LIFO queue, API compliant with LifoDiskQueue.
  summary: In-memory LIFO queue, API compliant with LifoDiskQueue
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: ScrapyRequestQueue
  inherits_from:
  - <class 'queuelib.queue.LifoMemoryQueue'>
  - <class 'queuelib.queue.FifoMemoryQueue'>
- name: LifoMemoryQueue.close
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LifoMemoryQueue.from_crawler
  kind: function
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LifoMemoryQueue.peek
  kind: method
  ns: scrapy.squeues
  description: |-
    Returns the next object to be returned by :meth:`pop`,
    but without removing it from the queue.

    Raises :exc:`NotImplementedError` if the underlying queue class does
    not implement a ``peek`` method, which is optional for queues.
  summary: Returns the next object to be returned by :meth:`pop`,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LifoMemoryQueue.pop
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LifoMemoryQueue.push
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: obj
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalFifoDiskQueue
  kind: class
  ns: scrapy.squeues
  description: Persistent FIFO queue.
  summary: Persistent FIFO queue
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: ScrapyRequestQueue
  inherits_from:
  - <class 'scrapy.squeues._serializable_queue.<locals>.SerializableQueue'>
  - <class 'scrapy.squeues._with_mkdir.<locals>.DirectoriesCreated'>
  - <class 'queuelib.queue.FifoDiskQueue'>
- name: MarshalFifoDiskQueue.close
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalFifoDiskQueue.from_crawler
  kind: function
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalFifoDiskQueue.peek
  kind: method
  ns: scrapy.squeues
  description: |-
    Returns the next object to be returned by :meth:`pop`,
    but without removing it from the queue.

    Raises :exc:`NotImplementedError` if the underlying queue class does
    not implement a ``peek`` method, which is optional for queues.
  summary: Returns the next object to be returned by :meth:`pop`,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalFifoDiskQueue.pop
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalFifoDiskQueue.push
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalFifoDiskQueue.szhdr_format
  kind: property
  ns: scrapy.squeues
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: MarshalFifoDiskQueue.szhdr_size
  kind: property
  ns: scrapy.squeues
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: MarshalLifoDiskQueue
  kind: class
  ns: scrapy.squeues
  description: Persistent LIFO queue.
  summary: Persistent LIFO queue
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: ScrapyRequestQueue
  inherits_from:
  - <class 'scrapy.squeues._serializable_queue.<locals>.SerializableQueue'>
  - <class 'scrapy.squeues._with_mkdir.<locals>.DirectoriesCreated'>
  - <class 'queuelib.queue.LifoDiskQueue'>
- name: MarshalLifoDiskQueue.SIZE_FORMAT
  kind: property
  ns: scrapy.squeues
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: MarshalLifoDiskQueue.SIZE_SIZE
  kind: property
  ns: scrapy.squeues
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: MarshalLifoDiskQueue.close
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalLifoDiskQueue.from_crawler
  kind: function
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalLifoDiskQueue.peek
  kind: method
  ns: scrapy.squeues
  description: |-
    Returns the next object to be returned by :meth:`pop`,
    but without removing it from the queue.

    Raises :exc:`NotImplementedError` if the underlying queue class does
    not implement a ``peek`` method, which is optional for queues.
  summary: Returns the next object to be returned by :meth:`pop`,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalLifoDiskQueue.pop
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MarshalLifoDiskQueue.push
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PathLike
  kind: class
  ns: scrapy.squeues
  description: Abstract base class for implementing the file system path protocol.
  summary: Abstract base class for implementing the file system path protocol
  signatures:
  - type: PathLike
  inherits_from:
  - <class 'abc.ABC'>
- name: PickleFifoDiskQueue
  kind: class
  ns: scrapy.squeues
  description: Persistent FIFO queue.
  summary: Persistent FIFO queue
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: ScrapyRequestQueue
  inherits_from:
  - <class 'scrapy.squeues._serializable_queue.<locals>.SerializableQueue'>
  - <class 'scrapy.squeues._with_mkdir.<locals>.DirectoriesCreated'>
  - <class 'queuelib.queue.FifoDiskQueue'>
- name: PickleFifoDiskQueue.close
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleFifoDiskQueue.from_crawler
  kind: function
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleFifoDiskQueue.peek
  kind: method
  ns: scrapy.squeues
  description: |-
    Returns the next object to be returned by :meth:`pop`,
    but without removing it from the queue.

    Raises :exc:`NotImplementedError` if the underlying queue class does
    not implement a ``peek`` method, which is optional for queues.
  summary: Returns the next object to be returned by :meth:`pop`,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleFifoDiskQueue.pop
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleFifoDiskQueue.push
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleFifoDiskQueue.szhdr_format
  kind: property
  ns: scrapy.squeues
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: PickleFifoDiskQueue.szhdr_size
  kind: property
  ns: scrapy.squeues
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: PickleLifoDiskQueue
  kind: class
  ns: scrapy.squeues
  description: Persistent LIFO queue.
  summary: Persistent LIFO queue
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: ScrapyRequestQueue
  inherits_from:
  - <class 'scrapy.squeues._serializable_queue.<locals>.SerializableQueue'>
  - <class 'scrapy.squeues._with_mkdir.<locals>.DirectoriesCreated'>
  - <class 'queuelib.queue.LifoDiskQueue'>
- name: PickleLifoDiskQueue.SIZE_FORMAT
  kind: property
  ns: scrapy.squeues
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: PickleLifoDiskQueue.SIZE_SIZE
  kind: property
  ns: scrapy.squeues
  description: |-
    int([x]) -> integer
    int(x, base=10) -> integer

    Convert a number or string to an integer, or return 0 if no arguments
    are given.  If x is a number, return x.__int__().  For floating point
    numbers, this truncates towards zero.

    If x is not a number or if base is given, then x must be a string,
    bytes, or bytearray instance representing an integer literal in the
    given base.  The literal can be preceded by '+' or '-' and be surrounded
    by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
    Base 0 means to interpret the base from the string as an integer literal.
    >>> int('0b100', base=0)
    4
  summary: int([x]) -> integer
  signatures: null
  inherits_from: null
- name: PickleLifoDiskQueue.close
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleLifoDiskQueue.from_crawler
  kind: function
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleLifoDiskQueue.peek
  kind: method
  ns: scrapy.squeues
  description: |-
    Returns the next object to be returned by :meth:`pop`,
    but without removing it from the queue.

    Raises :exc:`NotImplementedError` if the underlying queue class does
    not implement a ``peek`` method, which is optional for queues.
  summary: Returns the next object to be returned by :meth:`pop`,
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleLifoDiskQueue.pop
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: PickleLifoDiskQueue.push
  kind: method
  ns: scrapy.squeues
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.squeues
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: request_from_dict
  kind: function
  ns: scrapy.squeues
  description: |-
    Create a :class:`~scrapy.Request` object from a dict.

    If a spider is given, it will try to resolve the callbacks looking at the
    spider for methods with the same name.
  summary: Create a :class:`~scrapy
  signatures:
  - kind: positional
    name: d
    default: null
    rest: false
  - name: spider
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: scrapy.statscollectors
  kind: module
  ns: null
  description: Scrapy extension for collecting scraping stats
  summary: Scrapy extension for collecting scraping stats
  signatures: null
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.statscollectors
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DummyStatsCollector
  kind: class
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: DummyStatsCollector
  inherits_from:
  - <class 'scrapy.statscollectors.StatsCollector'>
- name: DummyStatsCollector.clear_stats
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: DummyStatsCollector.close_spider
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DummyStatsCollector.get_stats
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: DummyStatsCollector.get_value
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: DummyStatsCollector.inc_value
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: count
    default: '1'
    rest: false
  - kind: positional
    name: start
    default: '0'
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: DummyStatsCollector.max_value
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: DummyStatsCollector.min_value
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: DummyStatsCollector.open_spider
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DummyStatsCollector.set_stats
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: stats
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: DummyStatsCollector.set_value
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MemoryStatsCollector
  kind: class
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: MemoryStatsCollector
  inherits_from:
  - <class 'scrapy.statscollectors.StatsCollector'>
- name: MemoryStatsCollector.clear_stats
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MemoryStatsCollector.close_spider
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MemoryStatsCollector.get_stats
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MemoryStatsCollector.get_value
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MemoryStatsCollector.inc_value
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: count
    default: '1'
    rest: false
  - kind: positional
    name: start
    default: '0'
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MemoryStatsCollector.max_value
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MemoryStatsCollector.min_value
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MemoryStatsCollector.open_spider
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MemoryStatsCollector.set_stats
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: stats
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: MemoryStatsCollector.set_value
  kind: method
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: spider
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.statscollectors
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StatsT
  kind: callable
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.statscollectors
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: scrapy.utils
  kind: module
  ns: null
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: asyncgen
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: AsyncGenerator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.AsyncGenerator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AsyncIterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.AsyncIterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: as_async_generator
  kind: function
  ns: scrapy.utils
  description: Wraps an iterable (sync or async) into an async generator.
  summary: Wraps an iterable (sync or async) into an async generator
  signatures:
  - kind: positional
    name: it
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: collect_asyncgen
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: conf
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Collection
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Collection.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser
  kind: class
  ns: scrapy.utils
  description: ConfigParser implementing interpolation.
  summary: ConfigParser implementing interpolation
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: defaults
    default: None
    rest: false
  - kind: positional
    name: dict_type
    default: <class 'dict'>
    rest: false
  - kind: positional
    name: allow_no_value
    default: 'False'
    rest: false
  - name: delimiters
    default: ('=', ':')
    rest: false
    kind: kw-only
  - name: comment_prefixes
    default: ('#', ';')
    rest: false
    kind: kw-only
  - name: inline_comment_prefixes
    default: None
    rest: false
    kind: kw-only
  - name: strict
    default: 'True'
    rest: false
    kind: kw-only
  - name: empty_lines_in_values
    default: 'True'
    rest: false
    kind: kw-only
  - name: default_section
    default: DEFAULT
    rest: false
    kind: kw-only
  - name: interpolation
    default: <object object at 0x7fa9743b1d10>
    rest: false
    kind: kw-only
  - name: converters
    default: <object object at 0x7fa9743b1d10>
    rest: false
    kind: kw-only
  - type: ConfigParser
  inherits_from:
  - <class 'configparser.RawConfigParser'>
  - <class 'collections.abc.MutableMapping'>
  - <class 'collections.abc.Mapping'>
  - <class 'collections.abc.Collection'>
  - <class 'collections.abc.Sized'>
  - <class 'collections.abc.Iterable'>
  - <class 'collections.abc.Container'>
- name: ConfigParser.BOOLEAN_STATES
  kind: property
  ns: scrapy.utils
  description: |-
    dict() -> new empty dictionary
    dict(mapping) -> new dictionary initialized from a mapping object's
        (key, value) pairs
    dict(iterable) -> new dictionary initialized as if via:
        d = {}
        for k, v in iterable:
            d[k] = v
    dict(**kwargs) -> new dictionary initialized with the name=value pairs
        in the keyword argument list.  For example:  dict(one=1, two=2)
  summary: dict() -> new empty dictionary
  signatures: null
  inherits_from: null
- name: ConfigParser.NONSPACECRE
  kind: property
  ns: scrapy.utils
  description: Compiled regular expression object.
  summary: Compiled regular expression object
  signatures: null
  inherits_from: null
- name: ConfigParser.OPTCRE
  kind: property
  ns: scrapy.utils
  description: Compiled regular expression object.
  summary: Compiled regular expression object
  signatures: null
  inherits_from: null
- name: ConfigParser.OPTCRE_NV
  kind: property
  ns: scrapy.utils
  description: Compiled regular expression object.
  summary: Compiled regular expression object
  signatures: null
  inherits_from: null
- name: ConfigParser.SECTCRE
  kind: property
  ns: scrapy.utils
  description: Compiled regular expression object.
  summary: Compiled regular expression object
  signatures: null
  inherits_from: null
- name: ConfigParser.add_section
  kind: method
  ns: scrapy.utils
  description: |-
    Create a new section in the configuration.  Extends
    RawConfigParser.add_section by validating if the section name is
    a string.
  summary: Create a new section in the configuration
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.clear
  kind: method
  ns: scrapy.utils
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.converters
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ConfigParser.defaults
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.get
  kind: method
  ns: scrapy.utils
  description: |-
    Get an option value for a given section.

    If `vars` is provided, it must be a dictionary. The option is looked up
    in `vars` (if provided), `section`, and in `DEFAULTSECT` in that order.
    If the key is not found and `fallback` is provided, it is used as
    a fallback value. `None` can be provided as a `fallback` value.

    If interpolation is enabled and the optional argument `raw` is False,
    all interpolations are expanded in the return values.

    Arguments `raw`, `vars`, and `fallback` are keyword only.

    The section DEFAULT is special.
  summary: Get an option value for a given section
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: null
    rest: false
  - kind: positional
    name: option
    default: null
    rest: false
  - name: raw
    default: 'False'
    rest: false
    kind: kw-only
  - name: vars
    default: None
    rest: false
    kind: kw-only
  - name: fallback
    default: <object object at 0x7fa9743b1d10>
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ConfigParser.getboolean
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: null
    rest: false
  - kind: positional
    name: option
    default: null
    rest: false
  - name: raw
    default: 'False'
    rest: false
    kind: kw-only
  - name: vars
    default: None
    rest: false
    kind: kw-only
  - name: fallback
    default: <object object at 0x7fa9743b1d10>
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ConfigParser.getfloat
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: null
    rest: false
  - kind: positional
    name: option
    default: null
    rest: false
  - name: raw
    default: 'False'
    rest: false
    kind: kw-only
  - name: vars
    default: None
    rest: false
    kind: kw-only
  - name: fallback
    default: <object object at 0x7fa9743b1d10>
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ConfigParser.getint
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: null
    rest: false
  - kind: positional
    name: option
    default: null
    rest: false
  - name: raw
    default: 'False'
    rest: false
    kind: kw-only
  - name: vars
    default: None
    rest: false
    kind: kw-only
  - name: fallback
    default: <object object at 0x7fa9743b1d10>
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: ConfigParser.has_option
  kind: method
  ns: scrapy.utils
  description: |-
    Check for the existence of a given option in a given section.
    If the specified `section` is None or an empty string, DEFAULT is
    assumed. If the specified `section` does not exist, returns False.
  summary: Check for the existence of a given option in a given section
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: null
    rest: false
  - kind: positional
    name: option
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.has_section
  kind: method
  ns: scrapy.utils
  description: |-
    Indicate whether the named section is present in the configuration.

    The DEFAULT section is not acknowledged.
  summary: Indicate whether the named section is present in the configuration
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.items
  kind: method
  ns: scrapy.utils
  description: |-
    Return a list of (name, value) tuples for each option in a section.

    All % interpolations are expanded in the return values, based on the
    defaults passed into the constructor, unless the optional argument
    `raw` is true.  Additional substitutions may be provided using the
    `vars` argument, which must be a dictionary whose contents overrides
    any pre-existing defaults.

    The section DEFAULT is special.
  summary: Return a list of (name, value) tuples for each option in a section
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: <object object at 0x7fa9743b1d10>
    rest: false
  - kind: positional
    name: raw
    default: 'False'
    rest: false
  - kind: positional
    name: vars
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.keys
  kind: method
  ns: scrapy.utils
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.options
  kind: method
  ns: scrapy.utils
  description: Return a list of option names for the given section name.
  summary: Return a list of option names for the given section name
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.optionxform
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: optionstr
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.pop
  kind: method
  ns: scrapy.utils
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
    If key is not found, d is returned if given, otherwise KeyError is raised.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: <object object at 0x7fa9743b0160>
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.popitem
  kind: method
  ns: scrapy.utils
  description: |-
    Remove a section from the parser and return it as
    a (section_name, section_proxy) tuple. If no section is present, raise
    KeyError.

    The section DEFAULT is never returned because it cannot be removed.
  summary: Remove a section from the parser and return it as
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.read
  kind: method
  ns: scrapy.utils
  description: |-
    Read and parse a filename or an iterable of filenames.

    Files that cannot be opened are silently ignored; this is
    designed so that you can specify an iterable of potential
    configuration file locations (e.g. current directory, user's
    home directory, systemwide directory), and all existing
    configuration files in the iterable will be read.  A single
    filename may also be given.

    Return list of successfully read files.
  summary: Read and parse a filename or an iterable of filenames
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: filenames
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.read_dict
  kind: method
  ns: scrapy.utils
  description: |-
    Read configuration from a dictionary.

    Keys are section names, values are dictionaries with keys and values
    that should be present in the section. If the used dictionary type
    preserves order, sections and their keys will be added in order.

    All types held in the dictionary are converted to strings during
    reading, including section names, option names and keys.

    Optional second argument is the `source` specifying the name of the
    dictionary being read.
  summary: Read configuration from a dictionary
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: dictionary
    default: null
    rest: false
  - kind: positional
    name: source
    default: <dict>
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.read_file
  kind: method
  ns: scrapy.utils
  description: |-
    Like read() but the argument must be a file-like object.

    The `f` argument must be iterable, returning one line at a time.
    Optional second argument is the `source` specifying the name of the
    file being read. If not given, it is taken from f.name. If `f` has no
    `name` attribute, `<???>` is used.
  summary: Like read() but the argument must be a file-like object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: f
    default: null
    rest: false
  - kind: positional
    name: source
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.read_string
  kind: method
  ns: scrapy.utils
  description: Read configuration from a given string.
  summary: Read configuration from a given string
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: string
    default: null
    rest: false
  - kind: positional
    name: source
    default: <string>
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.readfp
  kind: method
  ns: scrapy.utils
  description: Deprecated, use read_file instead.
  summary: Deprecated, use read_file instead
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: fp
    default: null
    rest: false
  - kind: positional
    name: filename
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.remove_option
  kind: method
  ns: scrapy.utils
  description: Remove an option.
  summary: Remove an option
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: null
    rest: false
  - kind: positional
    name: option
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.remove_section
  kind: method
  ns: scrapy.utils
  description: Remove a file section.
  summary: Remove a file section
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.sections
  kind: method
  ns: scrapy.utils
  description: Return a list of section names, excluding [DEFAULT]
  summary: Return a list of section names, excluding [DEFAULT]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.set
  kind: method
  ns: scrapy.utils
  description: |-
    Set an option.  Extends RawConfigParser.set by validating type and
    interpolation syntax on the value.
  summary: Set an option
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: section
    default: null
    rest: false
  - kind: positional
    name: option
    default: null
    rest: false
  - kind: positional
    name: value
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.setdefault
  kind: method
  ns: scrapy.utils
  description: D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.update
  kind: method
  ns: scrapy.utils
  description: |-
    D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.
    If E present and has a .keys() method, does:     for k in E: D[k] = E[k]
    If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v
    In either case, this is followed by: for k, v in F.items(): D[k] = v
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: ()
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.values
  kind: method
  ns: scrapy.utils
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ConfigParser.write
  kind: method
  ns: scrapy.utils
  description: |-
    Write an .ini-format representation of the configuration state.

    If `space_around_delimiters` is True (the default), delimiters
    between keys and values are surrounded by spaces.

    Please note that comments in the original configuration file are not
    preserved when writing the configuration back.
  summary: 'Write an '
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: fp
    default: null
    rest: false
  - kind: positional
    name: space_around_delimiters
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.utils
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.utils
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Mapping
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Mapping.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableMapping
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.MutableMapping.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: arglist_to_dict
  kind: function
  ns: scrapy.utils
  description: |-
    Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a
    dict
  summary: 'Convert a list of arguments like [''arg1=val1'', ''arg2=val2'', '
  signatures:
  - kind: positional
    name: arglist
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: build_component_list
  kind: function
  ns: scrapy.utils
  description: 'Compose a component list from a { class: order } dictionary.'
  summary: 'Compose a component list from a { class: order } dictionary'
  signatures:
  - kind: positional
    name: compdict
    default: null
    rest: false
  - kind: positional
    name: custom
    default: None
    rest: false
  - kind: positional
    name: convert
    default: <function update_classpath at 0x7fa9729e1a80>
    rest: false
  - type: '?'
  inherits_from: null
- name: closest_scrapy_cfg
  kind: function
  ns: scrapy.utils
  description: |-
    Return the path to the closest scrapy.cfg file by traversing the current
    directory and its parents
  summary: Return the path to the closest scrapy
  signatures:
  - kind: positional
    name: path
    default: .
    rest: false
  - kind: positional
    name: prevpath
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: feed_complete_default_values_from_settings
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: feed
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: feed_process_params_from_cli
  kind: function
  ns: scrapy.utils
  description: |-
    Receives feed export params (from the 'crawl' or 'runspider' commands),
    checks for inconsistencies in their quantities and returns a dictionary
    suitable to be used as the FEEDS setting.
  summary: Receives feed export params (from the 'crawl' or 'runspider' commands),
  signatures:
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: output
    default: null
    rest: false
  - kind: positional
    name: output_format
    default: None
    rest: false
  - kind: positional
    name: overwrite_output
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: get_config
  kind: function
  ns: scrapy.utils
  description: Get Scrapy config file as a ConfigParser
  summary: Get Scrapy config file as a ConfigParser
  signatures:
  - kind: positional
    name: use_closest
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: get_sources
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: use_closest
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: init_env
  kind: function
  ns: scrapy.utils
  description: |-
    Initialize environment to use command-line tool from inside a project
    dir. This sets the Scrapy settings module and modifies the Python path to
    be able to locate the project module.
  summary: Initialize environment to use command-line tool from inside a project
  signatures:
  - kind: positional
    name: project
    default: default
    rest: false
  - kind: positional
    name: set_syspath
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: itemgetter
  kind: class
  ns: scrapy.utils
  description: |-
    itemgetter(item, ...) --> itemgetter object

    Return a callable object that fetches the given item(s) from its operand.
    After f = itemgetter(2), the call f(r) returns r[2].
    After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])
  summary: 'itemgetter(item, '
  signatures: null
  inherits_from: null
- name: update_classpath
  kind: function
  ns: scrapy.utils
  description: Update a deprecated path from an object with its new location
  summary: Update a deprecated path from an object with its new location
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: without_none_values
  kind: function
  ns: scrapy.utils
  description: |-
    Return a copy of ``iterable`` with all ``None`` entries removed.

    If ``iterable`` is a mapping, return a dictionary where all pairs that have
    value ``None`` have been removed.
  summary: Return a copy of ``iterable`` with all ``None`` entries removed
  signatures:
  - kind: positional
    name: iterable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: console
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DEFAULT_PYTHON_SHELLS
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: get_shell_embed_func
  kind: function
  ns: scrapy.utils
  description: |-
    Return the first acceptable shell-embed function
    from a given list of shell names.
  summary: Return the first acceptable shell-embed function
  signatures:
  - kind: positional
    name: shells
    default: None
    rest: false
  - kind: positional
    name: known_shells
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: start_python_console
  kind: function
  ns: scrapy.utils
  description: |-
    Start Python console bound to the given namespace.
    Readline support and tab completion will be used on Unix, if available.
  summary: Start Python console bound to the given namespace
  signatures:
  - kind: positional
    name: namespace
    default: None
    rest: false
  - kind: positional
    name: banner
    default: null
    rest: false
  - kind: positional
    name: shells
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: wraps
  kind: function
  ns: scrapy.utils
  description: |-
    Decorator factory to apply update_wrapper() to a wrapper function

    Returns a decorator that invokes update_wrapper() with the decorated
    function as the wrapper argument and the arguments to wraps() as the
    remaining arguments. Default arguments are as for update_wrapper().
    This is a convenience function to simplify applying partial() to
    update_wrapper().
  summary: Decorator factory to apply update_wrapper() to a wrapper function
  signatures:
  - kind: positional
    name: wrapped
    default: null
    rest: false
  - kind: positional
    name: assigned
    default: ('__module__', '__name__', '__qualname__', '__doc__', '__annotations__')
    rest: false
  - kind: positional
    name: updated
    default: ('__dict__',)
    rest: false
  - type: '?'
  inherits_from: null
- name: curl
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: CurlParser
  kind: class
  ns: scrapy.utils
  description: |-
    Object for parsing command line strings into Python objects.

    Keyword Arguments:
        - prog -- The name of the program (default:
            ``os.path.basename(sys.argv[0])``)
        - usage -- A usage message (default: auto-generated from arguments)
        - description -- A description of what the program does
        - epilog -- Text following the argument descriptions
        - parents -- Parsers whose arguments should be copied into this one
        - formatter_class -- HelpFormatter class for printing help messages
        - prefix_chars -- Characters that prefix optional arguments
        - fromfile_prefix_chars -- Characters that prefix files containing
            additional arguments
        - argument_default -- The default value for all arguments
        - conflict_handler -- String indicating how to handle conflicts
        - add_help -- Add a -h/-help option
        - allow_abbrev -- Allow long options to be abbreviated unambiguously
        - exit_on_error -- Determines whether or not ArgumentParser exits with
            error info when an error occurs
  summary: Object for parsing command line strings into Python objects
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: prog
    default: None
    rest: false
  - kind: positional
    name: usage
    default: None
    rest: false
  - kind: positional
    name: description
    default: None
    rest: false
  - kind: positional
    name: epilog
    default: None
    rest: false
  - kind: positional
    name: parents
    default: '[]'
    rest: false
  - kind: positional
    name: formatter_class
    default: <class 'argparse.HelpFormatter'>
    rest: false
  - kind: positional
    name: prefix_chars
    default: '-'
    rest: false
  - kind: positional
    name: fromfile_prefix_chars
    default: None
    rest: false
  - kind: positional
    name: argument_default
    default: None
    rest: false
  - kind: positional
    name: conflict_handler
    default: error
    rest: false
  - kind: positional
    name: add_help
    default: 'True'
    rest: false
  - kind: positional
    name: allow_abbrev
    default: 'True'
    rest: false
  - kind: positional
    name: exit_on_error
    default: 'True'
    rest: false
  - type: CurlParser
  inherits_from:
  - <class 'argparse.ArgumentParser'>
  - <class 'argparse._AttributeHolder'>
  - <class 'argparse._ActionsContainer'>
- name: CurlParser.add_argument
  kind: method
  ns: scrapy.utils
  description: |-
    add_argument(dest, ..., name=value, ...)
    add_argument(option_string, option_string, ..., name=value, ...)
  summary: 'add_argument(dest, '
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.add_argument_group
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.add_mutually_exclusive_group
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.add_subparsers
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.convert_arg_line_to_args
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: arg_line
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.error
  kind: method
  ns: scrapy.utils
  description: |-
    error(message: string)

    Prints a usage message incorporating the message to stderr and
    exits.

    If you override this in a subclass, it should not return -- it
    should either exit or raise an exception.
  summary: 'error(message: string)'
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: message
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.exit
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: status
    default: '0'
    rest: false
  - kind: positional
    name: message
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.format_help
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.format_usage
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.get_default
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: dest
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.parse_args
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: None
    rest: false
  - kind: positional
    name: namespace
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.parse_intermixed_args
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: None
    rest: false
  - kind: positional
    name: namespace
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.parse_known_args
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: None
    rest: false
  - kind: positional
    name: namespace
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.parse_known_intermixed_args
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: args
    default: None
    rest: false
  - kind: positional
    name: namespace
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.print_help
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.print_usage
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: file
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.register
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: registry_name
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: object
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: CurlParser.set_defaults
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DataAction
  kind: class
  ns: scrapy.utils
  description: |-
    Information about how to convert command line strings to Python objects.

    Action objects are used by an ArgumentParser to represent the information
    needed to parse a single argument from one or more strings from the
    command line. The keyword arguments to the Action constructor are also
    all attributes of Action instances.

    Keyword Arguments:

        - option_strings -- A list of command-line option strings which
            should be associated with this action.

        - dest -- The name of the attribute to hold the created object(s)

        - nargs -- The number of command-line arguments that should be
            consumed. By default, one argument will be consumed and a single
            value will be produced.  Other values include:
                - N (an integer) consumes N arguments (and produces a list)
                - '?' consumes zero or one arguments
                - '*' consumes zero or more arguments (and produces a list)
                - '+' consumes one or more arguments (and produces a list)
            Note that the difference between the default and nargs=1 is that
            with the default, a single value will be produced, while with
            nargs=1, a list containing a single value will be produced.

        - const -- The value to be produced if the option is specified and the
            option uses an action that takes no values.

        - default -- The value to be produced if the option is not specified.

        - type -- A callable that accepts a single string argument, and
            returns the converted value.  The standard Python types str, int,
            float, and complex are useful examples of such callables.  If None,
            str is used.

        - choices -- A container of values that should be allowed. If not None,
            after a command-line argument has been converted to the appropriate
            type, an exception will be raised if it is not a member of this
            collection.

        - required -- True if the action must always be specified at the
            command line. This is only meaningful for optional command-line
            arguments.

        - help -- The help string describing the argument.

        - metavar -- The name to be used for the option's argument with the
            help string. If None, the 'dest' value will be used as the name.
  summary: Information about how to convert command line strings to Python objects
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: option_strings
    default: null
    rest: false
  - kind: positional
    name: dest
    default: null
    rest: false
  - kind: positional
    name: nargs
    default: None
    rest: false
  - kind: positional
    name: const
    default: None
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: type
    default: None
    rest: false
  - kind: positional
    name: choices
    default: None
    rest: false
  - kind: positional
    name: required
    default: 'False'
    rest: false
  - kind: positional
    name: help
    default: None
    rest: false
  - kind: positional
    name: metavar
    default: None
    rest: false
  - type: DataAction
  inherits_from:
  - <class 'argparse.Action'>
  - <class 'argparse._AttributeHolder'>
- name: DataAction.format_usage
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SimpleCookie
  kind: class
  ns: scrapy.utils
  description: |-
    SimpleCookie supports strings as cookie values.  When setting
    the value using the dictionary assignment notation, SimpleCookie
    calls the builtin str() to convert the value to a string.  Values
    received from HTTP are kept as strings.
  summary: SimpleCookie supports strings as cookie values
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: input
    default: None
    rest: false
  - type: SimpleCookie
  inherits_from:
  - <class 'http.cookies.BaseCookie'>
  - <class 'dict'>
- name: SimpleCookie.clear
  kind: callable
  ns: scrapy.utils
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures: null
  inherits_from: null
- name: SimpleCookie.copy
  kind: callable
  ns: scrapy.utils
  description: D.copy() -> a shallow copy of D
  summary: D
  signatures: null
  inherits_from: null
- name: SimpleCookie.get
  kind: callable
  ns: scrapy.utils
  description: Return the value for key if key is in the dictionary, else default.
  summary: Return the value for key if key is in the dictionary, else default
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SimpleCookie.items
  kind: callable
  ns: scrapy.utils
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures: null
  inherits_from: null
- name: SimpleCookie.js_output
  kind: method
  ns: scrapy.utils
  description: Return a string suitable for JavaScript.
  summary: Return a string suitable for JavaScript
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: attrs
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SimpleCookie.keys
  kind: callable
  ns: scrapy.utils
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures: null
  inherits_from: null
- name: SimpleCookie.load
  kind: method
  ns: scrapy.utils
  description: |-
    Load cookies from a string (presumably HTTP_COOKIE) or
    from a dictionary.  Loading cookies from a dictionary 'd'
    is equivalent to calling:
        map(Cookie.__setitem__, d.keys(), d.values())
  summary: Load cookies from a string (presumably HTTP_COOKIE) or
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: rawdata
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SimpleCookie.output
  kind: method
  ns: scrapy.utils
  description: Return a string suitable for HTTP.
  summary: Return a string suitable for HTTP
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: attrs
    default: None
    rest: false
  - kind: positional
    name: header
    default: 'Set-Cookie:'
    rest: false
  - kind: positional
    name: sep
    default: "\r\n"
    rest: false
  - type: '?'
  inherits_from: null
- name: SimpleCookie.pop
  kind: callable
  ns: scrapy.utils
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.

    If the key is not found, return the default if given; otherwise,
    raise a KeyError.
  summary: D
  signatures: null
  inherits_from: null
- name: SimpleCookie.popitem
  kind: callable
  ns: scrapy.utils
  description: |-
    Remove and return a (key, value) pair as a 2-tuple.

    Pairs are returned in LIFO (last-in, first-out) order.
    Raises KeyError if the dict is empty.
  summary: Remove and return a (key, value) pair as a 2-tuple
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SimpleCookie.setdefault
  kind: callable
  ns: scrapy.utils
  description: |-
    Insert key with a value of default if key is not in the dictionary.

    Return the value for key if key is in the dictionary, else default.
  summary: Insert key with a value of default if key is not in the dictionary
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: SimpleCookie.update
  kind: callable
  ns: scrapy.utils
  description: |-
    D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.
    If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
    If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
    In either case, this is followed by: for k in F:  D[k] = F[k]
  summary: D
  signatures: null
  inherits_from: null
- name: SimpleCookie.value_decode
  kind: method
  ns: scrapy.utils
  description: |-
    real_value, coded_value = value_decode(STRING)
    Called prior to setting a cookie's value from the network
    representation.  The VALUE is the value read from HTTP
    header.
    Override this function to modify the behavior of cookies.
  summary: real_value, coded_value = value_decode(STRING)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SimpleCookie.value_encode
  kind: method
  ns: scrapy.utils
  description: |-
    real_value, coded_value = value_encode(VALUE)
    Called prior to setting a cookie's value from the dictionary
    representation.  The VALUE is the value being assigned.
    Override this function to modify the behavior of cookies.
  summary: real_value, coded_value = value_encode(VALUE)
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SimpleCookie.values
  kind: callable
  ns: scrapy.utils
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures: null
  inherits_from: null
- name: argument
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: basic_auth_header
  kind: function
  ns: scrapy.utils
  description: |-
    Return an `Authorization` header field value for `HTTP Basic Access Authentication (RFC 2617)`_

    >>> import w3lib.http
    >>> w3lib.http.basic_auth_header('someuser', 'somepass')
    'Basic c29tZXVzZXI6c29tZXBhc3M='

    .. _HTTP Basic Access Authentication (RFC 2617): http://www.ietf.org/rfc/rfc2617.txt
  summary: Return an `Authorization` header field value for `HTTP Basic Access Authentication (RFC 2617)`_
  signatures:
  - kind: positional
    name: username
    default: null
    rest: false
  - kind: positional
    name: password
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: ISO-8859-1
    rest: false
  - type: '?'
  inherits_from: null
- name: curl_parser
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: curl_to_request_kwargs
  kind: function
  ns: scrapy.utils
  description: |-
    Convert a cURL command syntax to Request kwargs.

    :param str curl_command: string containing the curl command
    :param bool ignore_unknown_options: If true, only a warning is emitted when
                                        cURL options are unknown. Otherwise
                                        raises an error. (default: True)
    :return: dictionary of Request kwargs
  summary: Convert a cURL command syntax to Request kwargs
  signatures:
  - kind: positional
    name: curl_command
    default: null
    rest: false
  - kind: positional
    name: ignore_unknown_options
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: safe_to_ignore_arguments
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: split
  kind: function
  ns: scrapy.utils
  description: Split the string *s* using shell-like syntax.
  summary: Split the string *s* using shell-like syntax
  signatures:
  - kind: positional
    name: s
    default: null
    rest: false
  - kind: positional
    name: comments
    default: 'False'
    rest: false
  - kind: positional
    name: posix
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: urlparse
  kind: function
  ns: scrapy.utils
  description: |-
    Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
  summary: 'Parse a URL into 6 components:'
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: scheme
    default: null
    rest: false
  - kind: positional
    name: allow_fragments
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: datatypes
  kind: module
  ns: scrapy.utils
  description: |-
    This module contains data types used by Scrapy which are not included in the
    Python Standard Library.

    This module must not depend on any module outside the Standard Library.
  summary: This module contains data types used by Scrapy which are not included in the
  signatures: null
  inherits_from: null
- name: AnyStr
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: LocalWeakReferencedCache
  kind: class
  ns: scrapy.utils
  description: |-
    A weakref.WeakKeyDictionary implementation that uses LocalCache as its
    underlying data structure, making it ordered and capable of being size-limited.

    Useful for memoization, while avoiding keeping received
    arguments in memory only because of the cached references.

    Note: like LocalCache and unlike weakref.WeakKeyDictionary,
    it cannot be instantiated with an initial dictionary.
  summary: A weakref
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: limit
    default: None
    rest: false
  - type: LocalWeakReferencedCache
  inherits_from:
  - <class 'weakref.WeakKeyDictionary'>
  - <class 'collections.abc.MutableMapping'>
  - <class 'collections.abc.Mapping'>
  - <class 'collections.abc.Collection'>
  - <class 'collections.abc.Sized'>
  - <class 'collections.abc.Iterable'>
  - <class 'collections.abc.Container'>
- name: LocalWeakReferencedCache.clear
  kind: method
  ns: scrapy.utils
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalWeakReferencedCache.copy
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalWeakReferencedCache.get
  kind: method
  ns: scrapy.utils
  description: D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalWeakReferencedCache.items
  kind: method
  ns: scrapy.utils
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalWeakReferencedCache.keyrefs
  kind: method
  ns: scrapy.utils
  description: |-
    Return a list of weak references to the keys.

    The references are not guaranteed to be 'live' at the time
    they are used, so the result of calling the references needs
    to be checked before being used.  This can be used to avoid
    creating references that will cause the garbage collector to
    keep the keys around longer than needed.
  summary: Return a list of weak references to the keys
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalWeakReferencedCache.keys
  kind: method
  ns: scrapy.utils
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalWeakReferencedCache.pop
  kind: method
  ns: scrapy.utils
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
    If key is not found, d is returned if given, otherwise KeyError is raised.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalWeakReferencedCache.popitem
  kind: method
  ns: scrapy.utils
  description: |-
    D.popitem() -> (k, v), remove and return some (key, value) pair
    as a 2-tuple; but raise KeyError if D is empty.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalWeakReferencedCache.setdefault
  kind: method
  ns: scrapy.utils
  description: D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalWeakReferencedCache.update
  kind: method
  ns: scrapy.utils
  description: |-
    D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.
    If E present and has a .keys() method, does:     for k in E: D[k] = E[k]
    If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v
    In either case, this is followed by: for k, v in F.items(): D[k] = v
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: dict
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: LocalWeakReferencedCache.values
  kind: method
  ns: scrapy.utils
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: OrderedDict
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.OrderedDict.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Sequence
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Sequence.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: decorators
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: defers
  kind: function
  ns: scrapy.utils
  description: Decorator to make sure a function always returns a deferred
  summary: Decorator to make sure a function always returns a deferred
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: deprecated
  kind: function
  ns: scrapy.utils
  description: |-
    This is a decorator which can be used to mark functions
    as deprecated. It will result in a warning being emitted
    when the function is used.
  summary: This is a decorator which can be used to mark functions
  signatures:
  - kind: positional
    name: use_instead
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: inthread
  kind: function
  ns: scrapy.utils
  description: |-
    Decorator to call a function in a thread and return a deferred with the
    result
  summary: Decorator to call a function in a thread and return a deferred with the
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: wraps
  kind: function
  ns: scrapy.utils
  description: |-
    Decorator factory to apply update_wrapper() to a wrapper function

    Returns a decorator that invokes update_wrapper() with the decorated
    function as the wrapper argument and the arguments to wraps() as the
    remaining arguments. Default arguments are as for update_wrapper().
    This is a convenience function to simplify applying partial() to
    update_wrapper().
  summary: Decorator factory to apply update_wrapper() to a wrapper function
  signatures:
  - kind: positional
    name: wrapped
    default: null
    rest: false
  - kind: positional
    name: assigned
    default: ('__module__', '__name__', '__qualname__', '__doc__', '__annotations__')
    rest: false
  - kind: positional
    name: updated
    default: ('__dict__',)
    rest: false
  - type: '?'
  inherits_from: null
- name: defer
  kind: module
  ns: scrapy.utils
  description: Helper functions for dealing with Twisted deferreds
  summary: Helper functions for dealing with Twisted deferreds
  signatures: null
  inherits_from: null
- name: AsyncGenerator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.AsyncGenerator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AsyncIterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.AsyncIterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AsyncIterator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.AsyncIterator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Awaitable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Awaitable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Cooperator
  kind: class
  ns: scrapy.utils
  description: |-
    Cooperative task scheduler.

    A cooperative task is an iterator where each iteration represents an
    atomic unit of work.  When the iterator yields, it allows the
    L{Cooperator} to decide which of its tasks to execute next.  If the
    iterator yields a L{Deferred} then work will pause until the
    L{Deferred} fires and completes its callback chain.

    When a L{Cooperator} has more than one task, it distributes work between
    all tasks.

    There are two ways to add tasks to a L{Cooperator}, L{cooperate} and
    L{coiterate}.  L{cooperate} is the more useful of the two, as it returns a
    L{CooperativeTask}, which can be L{paused<CooperativeTask.pause>},
    L{resumed<CooperativeTask.resume>} and L{waited
    on<CooperativeTask.whenDone>}.  L{coiterate} has the same effect, but
    returns only a L{Deferred} that fires when the task is done.

    L{Cooperator} can be used for many things, including but not limited to:

      - running one or more computationally intensive tasks without blocking
      - limiting parallelism by running a subset of the total tasks
        simultaneously
      - doing one thing, waiting for a L{Deferred} to fire,
        doing the next thing, repeat (i.e. serializing a sequence of
        asynchronous tasks)

    Multiple L{Cooperator}s do not cooperate with each other, so for most
    cases you should use the L{global cooperator<task.cooperate>}.
  summary: Cooperative task scheduler
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: terminationPredicateFactory
    default: <class 'twisted.internet.task._Timer'>
    rest: false
  - kind: positional
    name: scheduler
    default: <function _defaultScheduler at 0x7fa972abb560>
    rest: false
  - kind: positional
    name: started
    default: 'True'
    rest: false
  - type: Cooperator
  inherits_from: null
- name: Cooperator.coiterate
  kind: method
  ns: scrapy.utils
  description: |-
    Add an iterator to the list of iterators this L{Cooperator} is
    currently running.

    Equivalent to L{cooperate}, but returns a L{Deferred} that will
    be fired when the task is done.

    @param doneDeferred: If specified, this will be the Deferred used as
        the completion deferred.  It is suggested that you use the default,
        which creates a new Deferred for you.

    @return: a Deferred that will fire when the iterator finishes.
  summary: Add an iterator to the list of iterators this L{Cooperator} is
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: iterator
    default: null
    rest: false
  - kind: positional
    name: doneDeferred
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Cooperator.cooperate
  kind: method
  ns: scrapy.utils
  description: |-
    Start running the given iterator as a long-running cooperative task, by
    calling next() on it as a periodic timed event.

    @param iterator: the iterator to invoke.

    @return: a L{CooperativeTask} object representing this task.
  summary: Start running the given iterator as a long-running cooperative task, by
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: iterator
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Cooperator.running
  kind: property
  ns: scrapy.utils
  description: |-
    Is this L{Cooperator} is currently running?

    @return: C{True} if the L{Cooperator} is running, C{False} otherwise.
    @rtype: C{bool}
  summary: Is this L{Cooperator} is currently running?
  signatures: null
  inherits_from: null
- name: Cooperator.start
  kind: method
  ns: scrapy.utils
  description: Begin scheduling steps.
  summary: Begin scheduling steps
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Cooperator.stop
  kind: method
  ns: scrapy.utils
  description: |-
    Stop scheduling steps.  Errback the completion Deferreds of all
    iterators which have been added and forget about them.
  summary: Stop scheduling steps
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Coroutine
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Coroutine.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.utils
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Future
  kind: class
  ns: scrapy.utils
  description: |-
    This class is *almost* compatible with concurrent.futures.Future.

    Differences:

    - result() and exception() do not take a timeout argument and
      raise an exception when the future isn't done yet.

    - Callbacks registered with add_done_callback() are always called
      via the event loop's call_soon_threadsafe().

    - This class is not compatible with the wait() and as_completed()
      methods in the concurrent.futures package.
  summary: This class is *almost* compatible with concurrent
  signatures:
  - name: loop
    default: None
    rest: false
    kind: kw-only
  - type: Future
  inherits_from: null
- name: Future.add_done_callback
  kind: callable
  ns: scrapy.utils
  description: |-
    Add a callback to be run when the future becomes done.

    The callback is called with a single argument - the future object. If
    the future is already done when this is called, the callback is
    scheduled with call_soon.
  summary: Add a callback to be run when the future becomes done
  signatures: null
  inherits_from: null
- name: Future.cancel
  kind: callable
  ns: scrapy.utils
  description: |-
    Cancel the future and schedule callbacks.

    If the future is already done or cancelled, return False.  Otherwise,
    change the future's state to cancelled, schedule the callbacks and
    return True.
  summary: Cancel the future and schedule callbacks
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: msg
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: Future.cancelled
  kind: callable
  ns: scrapy.utils
  description: Return True if the future was cancelled.
  summary: Return True if the future was cancelled
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Future.done
  kind: callable
  ns: scrapy.utils
  description: |-
    Return True if the future is done.

    Done means either that a result / exception are available, or that the
    future was cancelled.
  summary: Return True if the future is done
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Future.exception
  kind: callable
  ns: scrapy.utils
  description: |-
    Return the exception that was set on this future.

    The exception (or None if no exception was set) is returned only if
    the future is done.  If the future has been cancelled, raises
    CancelledError.  If the future isn't done yet, raises
    InvalidStateError.
  summary: Return the exception that was set on this future
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Future.get_loop
  kind: callable
  ns: scrapy.utils
  description: Return the event loop the Future is bound to.
  summary: Return the event loop the Future is bound to
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Future.remove_done_callback
  kind: callable
  ns: scrapy.utils
  description: |-
    Remove all instances of a callback from the "call when done" list.

    Returns the number of callbacks removed.
  summary: Remove all instances of a callback from the "call when done" list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: fn
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Future.result
  kind: callable
  ns: scrapy.utils
  description: |-
    Return the result this future represents.

    If the future has been cancelled, raises CancelledError.  If the
    future's result isn't yet available, raises InvalidStateError.  If
    the future is done and has an exception set, this exception is raised.
  summary: Return the result this future represents
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Future.set_exception
  kind: callable
  ns: scrapy.utils
  description: |-
    Mark the future done and set an exception.

    If the future is already done when this method is called, raises
    InvalidStateError.
  summary: Mark the future done and set an exception
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: exception
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Future.set_result
  kind: callable
  ns: scrapy.utils
  description: |-
    Mark the future done and set its result.

    If the future is already done when this method is called, raises
    InvalidStateError.
  summary: Mark the future done and set its result
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.utils
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: aiter_errback
  kind: function
  ns: scrapy.utils
  description: |-
    Wraps an async iterable calling an errback if an error is caught while
    iterating it. Similar to scrapy.utils.defer.iter_errback()
  summary: Wraps an async iterable calling an errback if an error is caught while
  signatures:
  - kind: positional
    name: aiterable
    default: null
    rest: false
  - kind: positional
    name: errback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.utils
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: defer_fail
  kind: function
  ns: scrapy.utils
  description: |-
    Same as twisted.internet.defer.fail but delay calling errback until
    next reactor loop

    It delays by 100ms so reactor has a chance to go through readers and writers
    before attending pending delayed calls, so do not set delay to zero.
  summary: Same as twisted
  signatures:
  - kind: positional
    name: _failure
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: defer_result
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: defer_succeed
  kind: function
  ns: scrapy.utils
  description: |-
    Same as twisted.internet.defer.succeed but delay calling callback until
    next reactor loop

    It delays by 100ms so reactor has a chance to go through readers and writers
    before attending pending delayed calls, so do not set delay to zero.
  summary: Same as twisted
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: deferred_f_from_coro_f
  kind: function
  ns: scrapy.utils
  description: |-
    Converts a coroutine function into a function that returns a Deferred.

    The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.
    This is useful for callback chains, as callback functions are called with the previous callback result.
  summary: Converts a coroutine function into a function that returns a Deferred
  signatures:
  - kind: positional
    name: coro_f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: deferred_from_coro
  kind: function
  ns: scrapy.utils
  description: Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine
  summary: Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine
  signatures:
  - kind: positional
    name: o
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: deferred_to_future
  kind: function
  ns: scrapy.utils
  description: |-
    .. versionadded:: 2.6.0

    Return an :class:`asyncio.Future` object that wraps *d*.

    When :ref:`using the asyncio reactor <install-asyncio>`, you cannot await
    on :class:`~twisted.internet.defer.Deferred` objects from :ref:`Scrapy
    callables defined as coroutines <coroutine-support>`, you can only await on
    ``Future`` objects. Wrapping ``Deferred`` objects into ``Future`` objects
    allows you to wait on them::

        class MySpider(Spider):
            ...
            async def parse(self, response):
                additional_request = scrapy.Request('https://example.org/price')
                deferred = self.crawler.engine.download(additional_request)
                additional_response = await deferred_to_future(deferred)
  summary: ''
  signatures:
  - kind: positional
    name: d
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ensureDeferred
  kind: function
  ns: scrapy.utils
  description: |-
    Schedule the execution of a coroutine that awaits/yields from L{Deferred}s,
    wrapping it in a L{Deferred} that will fire on success/failure of the
    coroutine. If a Deferred is passed to this function, it will be returned
    directly (mimicking the L{asyncio.ensure_future} function).

    See L{Deferred.fromCoroutine} for examples of coroutines.

    @param coro: The coroutine object to schedule, or a L{Deferred}.
  summary: Schedule the execution of a coroutine that awaits/yields from L{Deferred}s,
  signatures:
  - kind: positional
    name: coro
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: is_asyncio_reactor_installed
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: iter_errback
  kind: function
  ns: scrapy.utils
  description: |-
    Wraps an iterable calling an errback if an error is caught while
    iterating it.
  summary: Wraps an iterable calling an errback if an error is caught while
  signatures:
  - kind: positional
    name: iterable
    default: null
    rest: false
  - kind: positional
    name: errback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: maybeDeferred_coro
  kind: function
  ns: scrapy.utils
  description: Copy of defer.maybeDeferred that also converts coroutines to Deferreds.
  summary: Copy of defer
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: maybe_deferred_to_future
  kind: function
  ns: scrapy.utils
  description: |-
    .. versionadded:: 2.6.0

    Return *d* as an object that can be awaited from a :ref:`Scrapy callable
    defined as a coroutine <coroutine-support>`.

    What you can await in Scrapy callables defined as coroutines depends on the
    value of :setting:`TWISTED_REACTOR`:

    -   When not using the asyncio reactor, you can only await on
        :class:`~twisted.internet.defer.Deferred` objects.

    -   When :ref:`using the asyncio reactor <install-asyncio>`, you can only
        await on :class:`asyncio.Future` objects.

    If you want to write code that uses ``Deferred`` objects but works with any
    reactor, use this function on all ``Deferred`` objects::

        class MySpider(Spider):
            ...
            async def parse(self, response):
                additional_request = scrapy.Request('https://example.org/price')
                deferred = self.crawler.engine.download(additional_request)
                additional_response = await maybe_deferred_to_future(deferred)
  summary: ''
  signatures:
  - kind: positional
    name: d
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: mustbe_deferred
  kind: function
  ns: scrapy.utils
  description: |-
    Same as twisted.internet.defer.maybeDeferred, but delay calling
    callback/errback to next reactor loop
  summary: Same as twisted
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: overload
  kind: function
  ns: scrapy.utils
  description: |-
    Decorator for overloaded functions/methods.

    In a stub file, place two or more stub definitions for the same
    function in a row, each decorated with @overload.

    For example::

        @overload
        def utf8(value: None) -> None: ...
        @overload
        def utf8(value: bytes) -> bytes: ...
        @overload
        def utf8(value: str) -> bytes: ...

    In a non-stub file (i.e. a regular .py file), do the same but
    follow it with an implementation.  The implementation should *not*
    be decorated with @overload::

        @overload
        def utf8(value: None) -> None: ...
        @overload
        def utf8(value: bytes) -> bytes: ...
        @overload
        def utf8(value: str) -> bytes: ...
        def utf8(value):
            ...  # implementation goes here

    The overloads for a function can be retrieved at runtime using the
    get_overloads() function.
  summary: Decorator for overloaded functions/methods
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: parallel
  kind: function
  ns: scrapy.utils
  description: |-
    Execute a callable over the objects in the given iterable, in parallel,
    using no more than ``count`` concurrent calls.

    Taken from: https://jcalderone.livejournal.com/24285.html
  summary: Execute a callable over the objects in the given iterable, in parallel,
  signatures:
  - kind: positional
    name: iterable
    default: null
    rest: false
  - kind: positional
    name: count
    default: null
    rest: false
  - kind: positional
    name: callable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: parallel_async
  kind: function
  ns: scrapy.utils
  description: Like parallel but for async iterators
  summary: Like parallel but for async iterators
  signatures:
  - kind: positional
    name: async_iterable
    default: null
    rest: false
  - kind: positional
    name: count
    default: null
    rest: false
  - kind: positional
    name: callable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: process_chain
  kind: function
  ns: scrapy.utils
  description: Return a Deferred built by chaining the given callbacks
  summary: Return a Deferred built by chaining the given callbacks
  signatures:
  - kind: positional
    name: callbacks
    default: null
    rest: false
  - kind: positional
    name: input
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: process_chain_both
  kind: function
  ns: scrapy.utils
  description: Return a Deferred built by chaining the given callbacks and errbacks
  summary: Return a Deferred built by chaining the given callbacks and errbacks
  signatures:
  - kind: positional
    name: callbacks
    default: null
    rest: false
  - kind: positional
    name: errbacks
    default: null
    rest: false
  - kind: positional
    name: input
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: process_parallel
  kind: function
  ns: scrapy.utils
  description: |-
    Return a Deferred with the output of all successful calls to the given
    callbacks
  summary: Return a Deferred with the output of all successful calls to the given
  signatures:
  - kind: positional
    name: callbacks
    default: null
    rest: false
  - kind: positional
    name: input
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: wraps
  kind: function
  ns: scrapy.utils
  description: |-
    Decorator factory to apply update_wrapper() to a wrapper function

    Returns a decorator that invokes update_wrapper() with the decorated
    function as the wrapper argument and the arguments to wraps() as the
    remaining arguments. Default arguments are as for update_wrapper().
    This is a convenience function to simplify applying partial() to
    update_wrapper().
  summary: Decorator factory to apply update_wrapper() to a wrapper function
  signatures:
  - kind: positional
    name: wrapped
    default: null
    rest: false
  - kind: positional
    name: assigned
    default: ('__module__', '__name__', '__qualname__', '__doc__', '__annotations__')
    rest: false
  - kind: positional
    name: updated
    default: ('__dict__',)
    rest: false
  - type: '?'
  inherits_from: null
- name: deprecate
  kind: module
  ns: scrapy.utils
  description: Some helpers for deprecation messages
  summary: Some helpers for deprecation messages
  signatures: null
  inherits_from: null
- name: DEPRECATION_RULES
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.utils
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.utils
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: attribute
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - kind: positional
    name: oldattr
    default: null
    rest: false
  - kind: positional
    name: newattr
    default: null
    rest: false
  - kind: positional
    name: version
    default: '0.12'
    rest: false
  - type: '?'
  inherits_from: null
- name: create_deprecated_class
  kind: function
  ns: scrapy.utils
  description: |-
    Return a "deprecated" class that causes its subclasses to issue a warning.
    Subclasses of ``new_class`` are considered subclasses of this class.
    It also warns when the deprecated class is instantiated, but do not when
    its subclasses are instantiated.

    It can be used to rename a base class in a library. For example, if we
    have

        class OldName(SomeClass):
            # ...

    and we want to rename it to NewName, we can do the following::

        class NewName(SomeClass):
            # ...

        OldName = create_deprecated_class('OldName', NewName)

    Then, if user class inherits from OldName, warning is issued. Also, if
    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``
    checks they'll still return True if sub is a subclass of NewName instead of
    OldName.
  summary: Return a "deprecated" class that causes its subclasses to issue a warning
  signatures:
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: new_class
    default: null
    rest: false
  - kind: positional
    name: clsdict
    default: None
    rest: false
  - kind: positional
    name: warn_category
    default: <class 'scrapy.exceptions.ScrapyDeprecationWarning'>
    rest: false
  - kind: positional
    name: warn_once
    default: 'True'
    rest: false
  - kind: positional
    name: old_class_path
    default: None
    rest: false
  - kind: positional
    name: new_class_path
    default: None
    rest: false
  - kind: positional
    name: subclass_warn_message
    default: '{cls} inherits from deprecated class {old}, please inherit from {new}.'
    rest: false
  - kind: positional
    name: instance_warn_message
    default: '{cls} is deprecated, instantiate {new} instead.'
    rest: false
  - type: '?'
  inherits_from: null
- name: method_is_overridden
  kind: function
  ns: scrapy.utils
  description: |-
    Return True if a method named ``method_name`` of a ``base_class``
    is overridden in a ``subclass``.

    >>> class Base:
    ...     def foo(self):
    ...         pass
    >>> class Sub1(Base):
    ...     pass
    >>> class Sub2(Base):
    ...     def foo(self):
    ...         pass
    >>> class Sub3(Sub1):
    ...     def foo(self):
    ...         pass
    >>> class Sub4(Sub2):
    ...     pass
    >>> method_is_overridden(Sub1, Base, 'foo')
    False
    >>> method_is_overridden(Sub2, Base, 'foo')
    True
    >>> method_is_overridden(Sub3, Base, 'foo')
    True
    >>> method_is_overridden(Sub4, Base, 'foo')
    True
  summary: Return True if a method named ``method_name`` of a ``base_class``
  signatures:
  - kind: positional
    name: subclass
    default: null
    rest: false
  - kind: positional
    name: base_class
    default: null
    rest: false
  - kind: positional
    name: method_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: overload
  kind: function
  ns: scrapy.utils
  description: |-
    Decorator for overloaded functions/methods.

    In a stub file, place two or more stub definitions for the same
    function in a row, each decorated with @overload.

    For example::

        @overload
        def utf8(value: None) -> None: ...
        @overload
        def utf8(value: bytes) -> bytes: ...
        @overload
        def utf8(value: str) -> bytes: ...

    In a non-stub file (i.e. a regular .py file), do the same but
    follow it with an implementation.  The implementation should *not*
    be decorated with @overload::

        @overload
        def utf8(value: None) -> None: ...
        @overload
        def utf8(value: bytes) -> bytes: ...
        @overload
        def utf8(value: str) -> bytes: ...
        def utf8(value):
            ...  # implementation goes here

    The overloads for a function can be retrieved at runtime using the
    get_overloads() function.
  summary: Decorator for overloaded functions/methods
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: update_classpath
  kind: function
  ns: scrapy.utils
  description: Update a deprecated path from an object with its new location
  summary: Update a deprecated path from an object with its new location
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: gz
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: GzipFile
  kind: class
  ns: scrapy.utils
  description: |-
    The GzipFile class simulates most of the methods of a file object with
    the exception of the truncate() method.

    This class only supports opening files in binary mode. If you need to open a
    compressed file in text mode, use the gzip.open() function.
  summary: The GzipFile class simulates most of the methods of a file object with
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: filename
    default: None
    rest: false
  - kind: positional
    name: mode
    default: None
    rest: false
  - kind: positional
    name: compresslevel
    default: '9'
    rest: false
  - kind: positional
    name: fileobj
    default: None
    rest: false
  - kind: positional
    name: mtime
    default: None
    rest: false
  - type: GzipFile
  inherits_from:
  - <class '_compression.BaseStream'>
  - <class 'io.BufferedIOBase'>
  - <class '_io._BufferedIOBase'>
  - <class 'io.IOBase'>
  - <class '_io._IOBase'>
- name: GzipFile.close
  kind: method
  ns: scrapy.utils
  description: |-
    Flush and close the IO object.

    This method has no effect if the file is already closed.
  summary: Flush and close the IO object
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.closed
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: GzipFile.detach
  kind: callable
  ns: scrapy.utils
  description: |-
    Disconnect this buffer from its underlying raw stream and return it.

    After the raw stream has been detached, the buffer is in an unusable
    state.
  summary: Disconnect this buffer from its underlying raw stream and return it
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.filename
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: GzipFile.fileno
  kind: method
  ns: scrapy.utils
  description: |-
    Invoke the underlying file object's fileno() method.

    This will raise AttributeError if the underlying file object
    doesn't support fileno().
  summary: Invoke the underlying file object's fileno() method
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.flush
  kind: method
  ns: scrapy.utils
  description: |-
    Flush write buffers, if applicable.

    This is not implemented for read-only and non-blocking streams.
  summary: Flush write buffers, if applicable
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: zlib_mode
    default: '2'
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.isatty
  kind: callable
  ns: scrapy.utils
  description: |-
    Return whether this is an 'interactive' stream.

    Return False if it can't be determined.
  summary: Return whether this is an 'interactive' stream
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.mtime
  kind: property
  ns: scrapy.utils
  description: Last modification time read from stream, or None
  summary: Last modification time read from stream, or None
  signatures: null
  inherits_from: null
- name: GzipFile.myfileobj
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: GzipFile.peek
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: n
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.read
  kind: method
  ns: scrapy.utils
  description: |-
    Read and return up to n bytes.

    If the argument is omitted, None, or negative, reads and
    returns all data until EOF.

    If the argument is positive, and the underlying raw stream is
    not 'interactive', multiple raw reads may be issued to satisfy
    the byte count (unless EOF is reached first).  But for
    interactive raw streams (as well as sockets and pipes), at most
    one raw read will be issued, and a short result does not imply
    that EOF is imminent.

    Returns an empty bytes object on EOF.

    Returns None if the underlying raw stream was open in non-blocking
    mode and no data is available at the moment.
  summary: Read and return up to n bytes
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: size
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.read1
  kind: method
  ns: scrapy.utils
  description: |-
    Implements BufferedIOBase.read1()

    Reads up to a buffer's worth of data if size is negative.
  summary: Implements BufferedIOBase
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: size
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.readable
  kind: method
  ns: scrapy.utils
  description: |-
    Return whether object was opened for reading.

    If False, read() will raise OSError.
  summary: Return whether object was opened for reading
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.readinto
  kind: callable
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: buffer
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.readinto1
  kind: callable
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: buffer
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.readline
  kind: method
  ns: scrapy.utils
  description: |-
    Read and return a line from the stream.

    If size is specified, at most size bytes will be read.

    The line terminator is always b'\n' for binary files; for text
    files, the newlines argument to open can be used to select the line
    terminator(s) recognized.
  summary: Read and return a line from the stream
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: size
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.readlines
  kind: callable
  ns: scrapy.utils
  description: |-
    Return a list of lines from the stream.

    hint can be specified to control the number of lines read: no more
    lines will be read if the total size (in bytes/characters) of all
    lines so far exceeds hint.
  summary: Return a list of lines from the stream
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: hint
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.rewind
  kind: method
  ns: scrapy.utils
  description: |-
    Return the uncompressed stream file position indicator to the
    beginning of the file
  summary: Return the uncompressed stream file position indicator to the
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.seek
  kind: method
  ns: scrapy.utils
  description: |-
    Change the stream position to the given byte offset.

      offset
        The stream position, relative to 'whence'.
      whence
        The relative position to seek from.

    The offset is interpreted relative to the position indicated by whence.
    Values for whence are:

    * os.SEEK_SET or 0 -- start of stream (the default); offset should be zero or positive
    * os.SEEK_CUR or 1 -- current stream position; offset may be negative
    * os.SEEK_END or 2 -- end of stream; offset is usually negative

    Return the new absolute position.
  summary: Change the stream position to the given byte offset
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: offset
    default: null
    rest: false
  - kind: positional
    name: whence
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.seekable
  kind: method
  ns: scrapy.utils
  description: |-
    Return whether object supports random access.

    If False, seek(), tell() and truncate() will raise OSError.
    This method may need to do a test seek().
  summary: Return whether object supports random access
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.tell
  kind: callable
  ns: scrapy.utils
  description: Return current stream position.
  summary: Return current stream position
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.truncate
  kind: callable
  ns: scrapy.utils
  description: |-
    Truncate file to size bytes.

    File pointer is left unchanged.  Size defaults to the current IO
    position as reported by tell().  Returns the new size.
  summary: Truncate file to size bytes
  signatures: null
  inherits_from: null
- name: GzipFile.writable
  kind: method
  ns: scrapy.utils
  description: |-
    Return whether object was opened for writing.

    If False, write() will raise OSError.
  summary: Return whether object was opened for writing
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.write
  kind: method
  ns: scrapy.utils
  description: |-
    Write the given buffer to the IO stream.

    Returns the number of bytes written, which is always the length of b
    in bytes.

    Raises BlockingIOError if the buffer is full and the
    underlying raw stream cannot accept more data at the moment.
  summary: Write the given buffer to the IO stream
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: data
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: GzipFile.writelines
  kind: callable
  ns: scrapy.utils
  description: |-
    Write a list of lines to stream.

    Line separators are not added, so it is usual for each of the
    lines provided to have a line separator at the end.
  summary: Write a list of lines to stream
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: lines
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: gunzip
  kind: function
  ns: scrapy.utils
  description: |-
    Gunzip the given data and return as much data as possible.

    This is resilient to CRC checksum errors.
  summary: Gunzip the given data and return as much data as possible
  signatures:
  - kind: positional
    name: data
    default: null
    rest: false
  - name: max_size
    default: '0'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: gzip_magic_number
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: httpobj
  kind: module
  ns: scrapy.utils
  description: Helper functions for scrapy.http objects (Request, Response)
  summary: Helper functions for scrapy
  signatures: null
  inherits_from: null
- name: ParseResult
  kind: class
  ns: scrapy.utils
  description: |-
    ParseResult(scheme, netloc, path, params, query, fragment)

    A 6-tuple that contains components of a parsed URL.
  summary: ParseResult(scheme, netloc, path, params, query, fragment)
  signatures:
  - kind: positional
    name: _cls
    default: null
    rest: false
  - kind: positional
    name: scheme
    default: null
    rest: false
  - kind: positional
    name: netloc
    default: null
    rest: false
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: params
    default: null
    rest: false
  - kind: positional
    name: query
    default: null
    rest: false
  - kind: positional
    name: fragment
    default: null
    rest: false
  - type: ParseResult
  inherits_from:
  - <class 'urllib.parse.ParseResult'>
  - <class 'tuple'>
  - <class 'urllib.parse._NetlocResultMixinStr'>
  - <class 'urllib.parse._NetlocResultMixinBase'>
  - <class 'urllib.parse._ResultMixinStr'>
- name: ParseResult.count
  kind: callable
  ns: scrapy.utils
  description: Return number of occurrences of value.
  summary: Return number of occurrences of value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ParseResult.encode
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: ascii
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: ParseResult.fragment
  kind: property
  ns: scrapy.utils
  description: |-
    Fragment identifier, that allows indirect identification of a secondary resource
    by reference to a primary resource and additional identifying information.
  summary: Fragment identifier, that allows indirect identification of a secondary resource
  signatures: null
  inherits_from: null
- name: ParseResult.geturl
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ParseResult.hostname
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ParseResult.index
  kind: callable
  ns: scrapy.utils
  description: |-
    Return first index of value.

    Raises ValueError if the value is not present.
  summary: Return first index of value
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: value
    default: null
    rest: false
  - kind: positional
    name: start
    default: '0'
    rest: false
  - kind: positional
    name: stop
    default: '9223372036854775807'
    rest: false
  - type: '?'
  inherits_from: null
- name: ParseResult.netloc
  kind: property
  ns: scrapy.utils
  description: Network location where the request is made to.
  summary: Network location where the request is made to
  signatures: null
  inherits_from: null
- name: ParseResult.params
  kind: property
  ns: scrapy.utils
  description: |-
    Parameters for last path element used to dereference the URI in order to provide
    access to perform some operation on the resource.
  summary: Parameters for last path element used to dereference the URI in order to provide
  signatures: null
  inherits_from: null
- name: ParseResult.password
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ParseResult.path
  kind: property
  ns: scrapy.utils
  description: The hierarchical path, such as the path to a file to download.
  summary: The hierarchical path, such as the path to a file to download
  signatures: null
  inherits_from: null
- name: ParseResult.port
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ParseResult.query
  kind: property
  ns: scrapy.utils
  description: |-
    The query component, that contains non-hierarchical data, that along with data
    in path component, identifies a resource in the scope of URI's scheme and
    network location.
  summary: The query component, that contains non-hierarchical data, that along with data
  signatures: null
  inherits_from: null
- name: ParseResult.scheme
  kind: property
  ns: scrapy.utils
  description: Specifies URL scheme for the request.
  summary: Specifies URL scheme for the request
  signatures: null
  inherits_from: null
- name: ParseResult.username
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WeakKeyDictionary
  kind: class
  ns: scrapy.utils
  description: |-
    Mapping class that references keys weakly.

    Entries in the dictionary will be discarded when there is no
    longer a strong reference to the key. This can be used to
    associate additional data with an object owned by other parts of
    an application without adding attributes to those objects. This
    can be especially useful with objects that override attribute
    accesses.
  summary: Mapping class that references keys weakly
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: dict
    default: None
    rest: false
  - type: WeakKeyDictionary
  inherits_from:
  - <class 'collections.abc.MutableMapping'>
  - <class 'collections.abc.Mapping'>
  - <class 'collections.abc.Collection'>
  - <class 'collections.abc.Sized'>
  - <class 'collections.abc.Iterable'>
  - <class 'collections.abc.Container'>
- name: WeakKeyDictionary.clear
  kind: method
  ns: scrapy.utils
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WeakKeyDictionary.copy
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WeakKeyDictionary.get
  kind: method
  ns: scrapy.utils
  description: D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: WeakKeyDictionary.items
  kind: method
  ns: scrapy.utils
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WeakKeyDictionary.keyrefs
  kind: method
  ns: scrapy.utils
  description: |-
    Return a list of weak references to the keys.

    The references are not guaranteed to be 'live' at the time
    they are used, so the result of calling the references needs
    to be checked before being used.  This can be used to avoid
    creating references that will cause the garbage collector to
    keep the keys around longer than needed.
  summary: Return a list of weak references to the keys
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WeakKeyDictionary.keys
  kind: method
  ns: scrapy.utils
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WeakKeyDictionary.pop
  kind: method
  ns: scrapy.utils
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
    If key is not found, d is returned if given, otherwise KeyError is raised.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WeakKeyDictionary.popitem
  kind: method
  ns: scrapy.utils
  description: |-
    D.popitem() -> (k, v), remove and return some (key, value) pair
    as a 2-tuple; but raise KeyError if D is empty.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: WeakKeyDictionary.setdefault
  kind: method
  ns: scrapy.utils
  description: D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: WeakKeyDictionary.update
  kind: method
  ns: scrapy.utils
  description: |-
    D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.
    If E present and has a .keys() method, does:     for k in E: D[k] = E[k]
    If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v
    In either case, this is followed by: for k, v in F.items(): D[k] = v
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: dict
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: WeakKeyDictionary.values
  kind: method
  ns: scrapy.utils
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: urlparse
  kind: function
  ns: scrapy.utils
  description: |-
    Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
  summary: 'Parse a URL into 6 components:'
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: scheme
    default: null
    rest: false
  - kind: positional
    name: allow_fragments
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: urlparse_cached
  kind: function
  ns: scrapy.utils
  description: |-
    Return urlparse.urlparse caching the result, where the argument can be a
    Request or Response object
  summary: Return urlparse
  signatures:
  - kind: positional
    name: request_or_response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: iterators
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.utils
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.utils
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Literal
  kind: callable
  ns: scrapy.utils
  description: |-
    Special typing form to define literal types (a.k.a. value types).

    This form can be used to indicate to type checkers that the corresponding
    variable or function parameter has a value equivalent to the provided
    literal (or one of several literals)::

        def validate_simple(data: Any) -> Literal[True]:  # always returns True
            ...

        MODE = Literal['r', 'rb', 'w', 'wb']
        def open_helper(file: str, mode: MODE) -> str:
            ...

        open_helper('/some/path', 'r')  # Passes type check
        open_helper('/other/path', 'typo')  # Error in type checker

    Literal[...] cannot be subclassed. At runtime, an arbitrary value
    is allowed as type argument to Literal[...], but type checkers may
    impose restrictions.
  summary: Special typing form to define literal types (a
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.utils
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: csviter
  kind: function
  ns: scrapy.utils
  description: |-
    Returns an iterator of dictionaries from the given csv object

    obj can be:
    - a Response object
    - a unicode string
    - a string encoded as utf-8

    delimiter is the character used to separate fields on the given obj.

    headers is an iterable that when provided offers the keys
    for the returned dictionaries, if not the first row is used.

    quotechar is the character used to enclosure fields on the given obj.
  summary: Returns an iterator of dictionaries from the given csv object
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - kind: positional
    name: delimiter
    default: None
    rest: false
  - kind: positional
    name: headers
    default: None
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: quotechar
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: overload
  kind: function
  ns: scrapy.utils
  description: |-
    Decorator for overloaded functions/methods.

    In a stub file, place two or more stub definitions for the same
    function in a row, each decorated with @overload.

    For example::

        @overload
        def utf8(value: None) -> None: ...
        @overload
        def utf8(value: bytes) -> bytes: ...
        @overload
        def utf8(value: str) -> bytes: ...

    In a non-stub file (i.e. a regular .py file), do the same but
    follow it with an implementation.  The implementation should *not*
    be decorated with @overload::

        @overload
        def utf8(value: None) -> None: ...
        @overload
        def utf8(value: bytes) -> bytes: ...
        @overload
        def utf8(value: str) -> bytes: ...
        def utf8(value):
            ...  # implementation goes here

    The overloads for a function can be retrieved at runtime using the
    get_overloads() function.
  summary: Decorator for overloaded functions/methods
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: re_rsearch
  kind: function
  ns: scrapy.utils
  description: |-
    This function does a reverse search in a text using a regular expression
    given in the attribute 'pattern'.
    Since the re module does not provide this functionality, we have to find for
    the expression into chunks of text extracted from the end (for the sake of efficiency).
    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for
    the pattern. If the pattern is not found, another chunk is extracted, and another
    search is performed.
    This process continues until a match is found, or until the whole file is read.
    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing
    the start position of the match, and the ending (regarding the entire text).
  summary: This function does a reverse search in a text using a regular expression
  signatures:
  - kind: positional
    name: pattern
    default: null
    rest: false
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: chunk_size
    default: '1024'
    rest: false
  - type: '?'
  inherits_from: null
- name: to_unicode
  kind: function
  ns: scrapy.utils
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: xmliter
  kind: function
  ns: scrapy.utils
  description: |-
    Return a iterator of Selector's over all nodes of a XML document,
       given the name of the node to iterate. Useful for parsing XML feeds.

    obj can be:
    - a Response object
    - a unicode string
    - a string encoded as utf-8
  summary: Return a iterator of Selector's over all nodes of a XML document,
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - kind: positional
    name: nodename
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: xmliter_lxml
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - kind: positional
    name: nodename
    default: null
    rest: false
  - kind: positional
    name: namespace
    default: None
    rest: false
  - kind: positional
    name: prefix
    default: x
    rest: false
  - type: '?'
  inherits_from: null
- name: job
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: job_dir
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: log
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DEFAULT_LOGGING
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.utils
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: MutableMapping
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.MutableMapping.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter
  kind: class
  ns: scrapy.utils
  description: |-
    An adapter for loggers which makes it easier to specify contextual
    information in logging output.
  summary: An adapter for loggers which makes it easier to specify contextual
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: logger
    default: null
    rest: false
  - kind: positional
    name: extra
    default: None
    rest: false
  - type: SpiderLoggerAdapter
  inherits_from:
  - <class 'logging.LoggerAdapter'>
- name: SpiderLoggerAdapter.critical
  kind: method
  ns: scrapy.utils
  description: Delegate a critical call to the underlying logger.
  summary: Delegate a critical call to the underlying logger
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: msg
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.debug
  kind: method
  ns: scrapy.utils
  description: Delegate a debug call to the underlying logger.
  summary: Delegate a debug call to the underlying logger
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: msg
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.error
  kind: method
  ns: scrapy.utils
  description: Delegate an error call to the underlying logger.
  summary: Delegate an error call to the underlying logger
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: msg
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.exception
  kind: method
  ns: scrapy.utils
  description: Delegate an exception call to the underlying logger.
  summary: Delegate an exception call to the underlying logger
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: msg
    default: null
    rest: false
  - name: exc_info
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.getEffectiveLevel
  kind: method
  ns: scrapy.utils
  description: Get the effective level for the underlying logger.
  summary: Get the effective level for the underlying logger
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.hasHandlers
  kind: method
  ns: scrapy.utils
  description: See if the underlying logger has any handlers.
  summary: See if the underlying logger has any handlers
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.info
  kind: method
  ns: scrapy.utils
  description: Delegate an info call to the underlying logger.
  summary: Delegate an info call to the underlying logger
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: msg
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.isEnabledFor
  kind: method
  ns: scrapy.utils
  description: Is this logger enabled for level 'level'?
  summary: Is this logger enabled for level 'level'?
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: level
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.log
  kind: method
  ns: scrapy.utils
  description: |-
    Delegate a log call to the underlying logger, after adding
    contextual information from this adapter instance.
  summary: Delegate a log call to the underlying logger, after adding
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: level
    default: null
    rest: false
  - kind: positional
    name: msg
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.manager
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SpiderLoggerAdapter.name
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: SpiderLoggerAdapter.process
  kind: method
  ns: scrapy.utils
  description: Method that augments logging with additional 'extra' data
  summary: Method that augments logging with additional 'extra' data
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: msg
    default: null
    rest: false
  - kind: positional
    name: kwargs
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.setLevel
  kind: method
  ns: scrapy.utils
  description: Set the specified level on the underlying logger.
  summary: Set the specified level on the underlying logger
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: level
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.warn
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: msg
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SpiderLoggerAdapter.warning
  kind: method
  ns: scrapy.utils
  description: Delegate a warning call to the underlying logger.
  summary: Delegate a warning call to the underlying logger
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: msg
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StreamLogger
  kind: class
  ns: scrapy.utils
  description: |-
    Fake file-like stream object that redirects writes to a logger instance

    Taken from:
        https://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/
  summary: Fake file-like stream object that redirects writes to a logger instance
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: logger
    default: null
    rest: false
  - kind: positional
    name: log_level
    default: '20'
    rest: false
  - type: StreamLogger
  inherits_from: null
- name: StreamLogger.flush
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: StreamLogger.write
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: buf
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TopLevelFormatter
  kind: class
  ns: scrapy.utils
  description: |-
    Keep only top level loggers's name (direct children from root) from
    records.

    This filter will replace Scrapy loggers' names with 'scrapy'. This mimics
    the old Scrapy log behaviour and helps shortening long names.

    Since it can't be set for just one logger (it won't propagate for its
    children), it's going to be set in the root handler, with a parametrized
    ``loggers`` list where it should act.
  summary: Keep only top level loggers's name (direct children from root) from
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: loggers
    default: None
    rest: false
  - type: TopLevelFormatter
  inherits_from:
  - <class 'logging.Filter'>
- name: TopLevelFormatter.filter
  kind: method
  ns: scrapy.utils
  description: |-
    Determine if the specified record is to be logged.

    Returns True if the record should be logged, or False otherwise.
    If deemed appropriate, the record may be modified in-place.
  summary: Determine if the specified record is to be logged
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: record
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TracebackType
  kind: class
  ns: scrapy.utils
  description: |-
    TracebackType(tb_next, tb_frame, tb_lasti, tb_lineno)
    --

    Create a new traceback object.
  summary: TracebackType(tb_next, tb_frame, tb_lasti, tb_lineno)
  signatures: null
  inherits_from: null
- name: TracebackType.tb_frame
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TracebackType.tb_lasti
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TracebackType.tb_lineno
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TracebackType.tb_next
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: annotations
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.utils
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: configure_logging
  kind: function
  ns: scrapy.utils
  description: |-
    Initialize logging defaults for Scrapy.

    :param settings: settings used to create and configure a handler for the
        root logger (default: None).
    :type settings: dict, :class:`~scrapy.settings.Settings` object or ``None``

    :param install_root_handler: whether to install root logging handler
        (default: True)
    :type install_root_handler: bool

    This function does:

    - Route warnings and twisted logging through Python standard logging
    - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively
    - Route stdout to log if LOG_STDOUT setting is True

    When ``install_root_handler`` is True (default), this function also
    creates a handler for the root logger according to given settings
    (see :ref:`topics-logging-settings`). You can override default options
    using ``settings`` argument. When ``settings`` is empty or None, defaults
    are used.
  summary: Initialize logging defaults for Scrapy
  signatures:
  - kind: positional
    name: settings
    default: None
    rest: false
  - kind: positional
    name: install_root_handler
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: dictConfig
  kind: function
  ns: scrapy.utils
  description: Configure logging using a dictionary.
  summary: Configure logging using a dictionary
  signatures:
  - kind: positional
    name: config
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: failure_to_exc_info
  kind: function
  ns: scrapy.utils
  description: Extract exc_info from Failure instances
  summary: Extract exc_info from Failure instances
  signatures:
  - kind: positional
    name: failure
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: get_scrapy_root_handler
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: install_scrapy_root_handler
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: log_reactor_info
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: log_scrapy_info
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logformatter_adapter
  kind: function
  ns: scrapy.utils
  description: |-
    Helper that takes the dictionary output from the methods in LogFormatter
    and adapts it into a tuple of positional arguments for logger.log calls,
    handling backward compatibility as well.
  summary: Helper that takes the dictionary output from the methods in LogFormatter
  signatures:
  - kind: positional
    name: logkws
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: scrapy_components_versions
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: misc
  kind: module
  ns: scrapy.utils
  description: Helper functions which don't fit anywhere else
  summary: Helper functions which don't fit anywhere else
  signatures: null
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Deque
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.deque.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IO
  kind: class
  ns: scrapy.utils
  description: |-
    Generic base class for TextIO and BinaryIO.

    This is an abstract, generic version of the return of open().

    NOTE: This does not distinguish between the different possible
    classes (text vs. binary, read vs. write vs. read/write,
    append-only, unbuffered).  The TextIO and BinaryIO subclasses
    below capture the distinctions between text vs. binary, which is
    pervasive in the interface; however we currently do not offer a
    way to track the other distinctions in the type system.
  summary: Generic base class for TextIO and BinaryIO
  signatures:
  - type: IO
  inherits_from:
  - <class 'typing.Generic'>
- name: IO.close
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.closed
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: IO.fileno
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.flush
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.isatty
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.mode
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: IO.name
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: IO.read
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: n
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.readable
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.readline
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: limit
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.readlines
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: hint
    default: '-1'
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.seek
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: offset
    default: null
    rest: false
  - kind: positional
    name: whence
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.seekable
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.tell
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.truncate
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: size
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.writable
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.write
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: s
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: IO.writelines
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: lines
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.utils
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Pattern
  kind: callable
  ns: scrapy.utils
  description: A generic version of re.Pattern.
  summary: A generic version of re
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: arg_to_iter
  kind: function
  ns: scrapy.utils
  description: |-
    Convert an argument to an iterable. The argument can be a None, single
    value, or an iterable.

    Exception: if arg is a dict, [arg] will be returned
  summary: Convert an argument to an iterable
  signatures:
  - kind: positional
    name: arg
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.utils
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: contextmanager
  kind: function
  ns: scrapy.utils
  description: |-
    @contextmanager decorator.

    Typical usage:

        @contextmanager
        def some_generator(<arguments>):
            <setup>
            try:
                yield <value>
            finally:
                <cleanup>

    This makes this:

        with some_generator(<arguments>) as <variable>:
            <body>

    equivalent to this:

        <setup>
        try:
            <variable> = <value>
            <body>
        finally:
            <cleanup>
  summary: '@contextmanager decorator'
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: create_instance
  kind: function
  ns: scrapy.utils
  description: |-
    Construct a class instance using its ``from_crawler`` or
    ``from_settings`` constructors, if available.

    At least one of ``settings`` and ``crawler`` needs to be different from
    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.
    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be
    tried.

    ``*args`` and ``**kwargs`` are forwarded to the constructors.

    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.

    .. versionchanged:: 2.2
       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an
       extension has not been implemented correctly).
  summary: Construct a class instance using its ``from_crawler`` or
  signatures:
  - kind: positional
    name: objcls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: extract_regex
  kind: function
  ns: scrapy.utils
  description: |-
    Extract a list of unicode strings from the given text/encoding using the following policies:

    * if the regex contains a named group called "extract" that will be returned
    * if the regex contains multiple numbered groups, all those will be returned (flattened)
    * if the regex doesn't contain any group the entire regex matching is returned
  summary: 'Extract a list of unicode strings from the given text/encoding using the following policies:'
  signatures:
  - kind: positional
    name: regex
    default: null
    rest: false
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: utf-8
    rest: false
  - type: '?'
  inherits_from: null
- name: flatten
  kind: function
  ns: scrapy.utils
  description: |-
    flatten(sequence) -> list

    Returns a single, flat list which contains all elements retrieved
    from the sequence and all recursively contained sub-sequences
    (iterables).

    Examples:
    >>> [1, 2, [3,4], (5,6)]
    [1, 2, [3, 4], (5, 6)]
    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])
    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]
    >>> flatten(["foo", "bar"])
    ['foo', 'bar']
    >>> flatten(["foo", ["baz", 42], "bar"])
    ['foo', 'baz', 42, 'bar']
  summary: flatten(sequence) -> list
  signatures:
  - kind: positional
    name: x
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: import_module
  kind: function
  ns: scrapy.utils
  description: |-
    Import a module.

    The 'package' argument is required when performing a relative import. It
    specifies the package to use as the anchor point from which to resolve the
    relative import to an absolute import.
  summary: Import a module
  signatures:
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: package
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: is_generator_with_return_value
  kind: function
  ns: scrapy.utils
  description: |-
    Returns True if a callable is a generator function which includes a
    'return' statement with a value different than None, False otherwise
  summary: Returns True if a callable is a generator function which includes a
  signatures:
  - kind: positional
    name: callable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: iter_modules
  kind: function
  ns: scrapy.utils
  description: |-
    Yields ModuleInfo for all submodules on path,
    or, if path is None, all top-level modules on sys.path.

    'path' should be either None or a list of paths to look for
    modules in.

    'prefix' is a string to output on the front of every module name
    on output.
  summary: Yields ModuleInfo for all submodules on path,
  signatures:
  - kind: positional
    name: path
    default: None
    rest: false
  - kind: positional
    name: prefix
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: load_object
  kind: function
  ns: scrapy.utils
  description: |-
    Load an object given its absolute object path, and return it.

    The object can be the import path of a class, function, variable or an
    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.

    If ``path`` is not a string, but is a callable object, such as a class or
    a function, then return it as is.
  summary: Load an object given its absolute object path, and return it
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: md5sum
  kind: function
  ns: scrapy.utils
  description: |-
    Calculate the md5 checksum of a file-like object without reading its
    whole content in memory.

    >>> from io import BytesIO
    >>> md5sum(BytesIO(b'file content to hash'))
    '784406af91dd5a54fbb9c84c2236595a'
  summary: Calculate the md5 checksum of a file-like object without reading its
  signatures:
  - kind: positional
    name: file
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: rel_has_nofollow
  kind: function
  ns: scrapy.utils
  description: Return True if link rel attribute has nofollow type
  summary: Return True if link rel attribute has nofollow type
  signatures:
  - kind: positional
    name: rel
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: replace_entities
  kind: function
  ns: scrapy.utils
  description: |-
    Remove entities from the given `text` by converting them to their
    corresponding unicode character.

    `text` can be a unicode string or a byte string encoded in the given
    `encoding` (which defaults to 'utf-8').

    If `keep` is passed (with a list of entity names) those entities will
    be kept (they won't be removed).

    It supports both numeric entities (``&#nnnn;`` and ``&#hhhh;``)
    and named entities (such as ``&nbsp;`` or ``&gt;``).

    If `remove_illegal` is ``True``, entities that can't be converted are removed.
    If `remove_illegal` is ``False``, entities that can't be converted are kept "as
    is". For more information see the tests.

    Always returns a unicode string (with the entities removed).

    >>> import w3lib.html
    >>> w3lib.html.replace_entities(b'Price: &pound;100')
    'Price: \xa3100'
    >>> print(w3lib.html.replace_entities(b'Price: &pound;100'))
    Price: 100
    >>>
  summary: Remove entities from the given `text` by converting them to their
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: keep
    default: ()
    rest: false
  - kind: positional
    name: remove_illegal
    default: 'True'
    rest: false
  - kind: positional
    name: encoding
    default: utf-8
    rest: false
  - type: '?'
  inherits_from: null
- name: set_environ
  kind: function
  ns: scrapy.utils
  description: |-
    Temporarily set environment variables inside the context manager and
    fully restore previous environment afterwards
  summary: Temporarily set environment variables inside the context manager and
  signatures:
  - type: '?'
  inherits_from: null
- name: to_unicode
  kind: function
  ns: scrapy.utils
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: walk_callable
  kind: function
  ns: scrapy.utils
  description: |-
    Similar to ``ast.walk``, but walks only function body and skips nested
    functions defined within the node.
  summary: Similar to ``ast
  signatures:
  - kind: positional
    name: node
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: walk_modules
  kind: function
  ns: scrapy.utils
  description: |-
    Loads a module and all its submodules from the given module path and
    returns them. If *any* module throws an exception while importing, that
    exception is thrown back.

    For example: walk_modules('scrapy.utils')
  summary: Loads a module and all its submodules from the given module path and
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: warn_on_generator_with_return_value
  kind: function
  ns: scrapy.utils
  description: |-
    Logs a warning if a callable is a generator function and includes
    a 'return' statement with a value different than None
  summary: Logs a warning if a callable is a generator function and includes
  signatures:
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: callable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ossignal
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.utils
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: FrameType
  kind: class
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: frame
  inherits_from: null
- name: FrameType.clear
  kind: callable
  ns: scrapy.utils
  description: 'F.clear(): clear most references held by the frame'
  summary: F
  signatures: null
  inherits_from: null
- name: FrameType.f_back
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FrameType.f_builtins
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FrameType.f_code
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FrameType.f_globals
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FrameType.f_lasti
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FrameType.f_lineno
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FrameType.f_locals
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FrameType.f_trace
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FrameType.f_trace_lines
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: FrameType.f_trace_opcodes
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: SignalHandlerT
  kind: callable
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: install_shutdown_handlers
  kind: function
  ns: scrapy.utils
  description: |-
    Install the given function as a signal handler for all common shutdown
    signals (such as SIGINT, SIGTERM, etc). If ``override_sigint`` is ``False`` the
    SIGINT handler won't be installed if there is already a handler in place
    (e.g. Pdb)
  summary: Install the given function as a signal handler for all common shutdown
  signatures:
  - kind: positional
    name: function
    default: null
    rest: false
  - kind: positional
    name: override_sigint
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: signal_names
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: signame
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: signum
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: project
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DATADIR_CFG_SECTION
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ENVVAR
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: closest_scrapy_cfg
  kind: function
  ns: scrapy.utils
  description: |-
    Return the path to the closest scrapy.cfg file by traversing the current
    directory and its parents
  summary: Return the path to the closest scrapy
  signatures:
  - kind: positional
    name: path
    default: .
    rest: false
  - kind: positional
    name: prevpath
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: data_path
  kind: function
  ns: scrapy.utils
  description: |-
    Return the given path joined with the .scrapy data directory.
    If given an absolute path, return it unmodified.
  summary: 'Return the given path joined with the '
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - kind: positional
    name: createdir
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: get_config
  kind: function
  ns: scrapy.utils
  description: Get Scrapy config file as a ConfigParser
  summary: Get Scrapy config file as a ConfigParser
  signatures:
  - kind: positional
    name: use_closest
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: get_project_settings
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: import_module
  kind: function
  ns: scrapy.utils
  description: |-
    Import a module.

    The 'package' argument is required when performing a relative import. It
    specifies the package to use as the anchor point from which to resolve the
    relative import to an absolute import.
  summary: Import a module
  signatures:
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: package
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: init_env
  kind: function
  ns: scrapy.utils
  description: |-
    Initialize environment to use command-line tool from inside a project
    dir. This sets the Scrapy settings module and modifies the Python path to
    be able to locate the project module.
  summary: Initialize environment to use command-line tool from inside a project
  signatures:
  - kind: positional
    name: project
    default: default
    rest: false
  - kind: positional
    name: set_syspath
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: inside_project
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: project_data_dir
  kind: function
  ns: scrapy.utils
  description: Return the current project data dir, creating it if it doesn't exist
  summary: Return the current project data dir, creating it if it doesn't exist
  signatures:
  - kind: positional
    name: project
    default: default
    rest: false
  - type: '?'
  inherits_from: null
- name: python
  kind: module
  ns: scrapy.utils
  description: This module contains essential stuff that should've come with Python itself ;)
  summary: This module contains essential stuff that should've come with Python itself ;)
  signatures: null
  inherits_from: null
- name: AsyncGenerator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.AsyncGenerator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AsyncIterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.AsyncIterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AsyncIterator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.AsyncIterator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.utils
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.utils
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Mapping
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Mapping.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Pattern
  kind: callable
  ns: scrapy.utils
  description: A generic version of re.Pattern.
  summary: A generic version of re
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: as_async_generator
  kind: function
  ns: scrapy.utils
  description: Wraps an iterable (sync or async) into an async generator.
  summary: Wraps an iterable (sync or async) into an async generator
  signatures:
  - kind: positional
    name: it
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: binary_is_text
  kind: function
  ns: scrapy.utils
  description: |-
    Returns ``True`` if the given ``data`` argument (a ``bytes`` object)
    does not contain unprintable control characters.
  summary: Returns ``True`` if the given ``data`` argument (a ``bytes`` object)
  signatures:
  - kind: positional
    name: data
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: chain
  kind: class
  ns: scrapy.utils
  description: |-
    chain(*iterables) --> chain object

    Return a chain object whose .__next__() method returns elements from the
    first iterable until it is exhausted, then elements from the next
    iterable, until all of the iterables are exhausted.
  summary: chain(*iterables) --> chain object
  signatures: null
  inherits_from: null
- name: equal_attributes
  kind: function
  ns: scrapy.utils
  description: Compare two objects attributes
  summary: Compare two objects attributes
  signatures:
  - kind: positional
    name: obj1
    default: null
    rest: false
  - kind: positional
    name: obj2
    default: null
    rest: false
  - kind: positional
    name: attributes
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: flatten
  kind: function
  ns: scrapy.utils
  description: |-
    flatten(sequence) -> list

    Returns a single, flat list which contains all elements retrieved
    from the sequence and all recursively contained sub-sequences
    (iterables).

    Examples:
    >>> [1, 2, [3,4], (5,6)]
    [1, 2, [3, 4], (5, 6)]
    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])
    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]
    >>> flatten(["foo", "bar"])
    ['foo', 'bar']
    >>> flatten(["foo", ["baz", 42], "bar"])
    ['foo', 'baz', 42, 'bar']
  summary: flatten(sequence) -> list
  signatures:
  - kind: positional
    name: x
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: garbage_collect
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: get_func_args
  kind: function
  ns: scrapy.utils
  description: Return the argument name list of a callable object
  summary: Return the argument name list of a callable object
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - kind: positional
    name: stripself
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: get_spec
  kind: function
  ns: scrapy.utils
  description: |-
    Returns (args, kwargs) tuple for a function
    >>> import re
    >>> get_spec(re.match)
    (['pattern', 'string'], {'flags': 0})

    >>> class Test:
    ...     def __call__(self, val):
    ...         pass
    ...     def method(self, val, flags=0):
    ...         pass

    >>> get_spec(Test)
    (['self', 'val'], {})

    >>> get_spec(Test.method)
    (['self', 'val'], {'flags': 0})

    >>> get_spec(Test().method)
    (['self', 'val'], {'flags': 0})
  summary: Returns (args, kwargs) tuple for a function
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: global_object_name
  kind: function
  ns: scrapy.utils
  description: |-
    Return full name of a global object.

    >>> from scrapy import Request
    >>> global_object_name(Request)
    'scrapy.http.request.Request'
  summary: Return full name of a global object
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: iflatten
  kind: function
  ns: scrapy.utils
  description: |-
    iflatten(sequence) -> iterator

    Similar to ``.flatten()``, but returns iterator instead
  summary: iflatten(sequence) -> iterator
  signatures:
  - kind: positional
    name: x
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: is_listlike
  kind: function
  ns: scrapy.utils
  description: |-
    >>> is_listlike("foo")
    False
    >>> is_listlike(5)
    False
    >>> is_listlike(b"foo")
    False
    >>> is_listlike([b"foo"])
    True
    >>> is_listlike((b"foo",))
    True
    >>> is_listlike({})
    True
    >>> is_listlike(set())
    True
    >>> is_listlike((x for x in range(3)))
    True
    >>> is_listlike(range(5))
    True
  summary: '>>> is_listlike("foo")'
  signatures:
  - kind: positional
    name: x
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: memoizemethod_noargs
  kind: function
  ns: scrapy.utils
  description: |-
    Decorator to cache the result of a method (without arguments) using a
    weak reference to its object
  summary: Decorator to cache the result of a method (without arguments) using a
  signatures:
  - kind: positional
    name: method
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: overload
  kind: function
  ns: scrapy.utils
  description: |-
    Decorator for overloaded functions/methods.

    In a stub file, place two or more stub definitions for the same
    function in a row, each decorated with @overload.

    For example::

        @overload
        def utf8(value: None) -> None: ...
        @overload
        def utf8(value: bytes) -> bytes: ...
        @overload
        def utf8(value: str) -> bytes: ...

    In a non-stub file (i.e. a regular .py file), do the same but
    follow it with an implementation.  The implementation should *not*
    be decorated with @overload::

        @overload
        def utf8(value: None) -> None: ...
        @overload
        def utf8(value: bytes) -> bytes: ...
        @overload
        def utf8(value: str) -> bytes: ...
        def utf8(value):
            ...  # implementation goes here

    The overloads for a function can be retrieved at runtime using the
    get_overloads() function.
  summary: Decorator for overloaded functions/methods
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: re_rsearch
  kind: function
  ns: scrapy.utils
  description: |-
    This function does a reverse search in a text using a regular expression
    given in the attribute 'pattern'.
    Since the re module does not provide this functionality, we have to find for
    the expression into chunks of text extracted from the end (for the sake of efficiency).
    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for
    the pattern. If the pattern is not found, another chunk is extracted, and another
    search is performed.
    This process continues until a match is found, or until the whole file is read.
    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing
    the start position of the match, and the ending (regarding the entire text).
  summary: This function does a reverse search in a text using a regular expression
  signatures:
  - kind: positional
    name: pattern
    default: null
    rest: false
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: chunk_size
    default: '1024'
    rest: false
  - type: '?'
  inherits_from: null
- name: to_bytes
  kind: function
  ns: scrapy.utils
  description: |-
    Return the binary representation of ``text``. If ``text``
    is already a bytes object, return it as-is.
  summary: Return the binary representation of ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: to_unicode
  kind: function
  ns: scrapy.utils
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: unique
  kind: function
  ns: scrapy.utils
  description: efficient function to uniquify a list preserving item order
  summary: efficient function to uniquify a list preserving item order
  signatures:
  - kind: positional
    name: list_
    default: null
    rest: false
  - kind: positional
    name: key
    default: <function <lambda> at 0x7fa973d09da0>
    rest: false
  - type: '?'
  inherits_from: null
- name: without_none_values
  kind: function
  ns: scrapy.utils
  description: |-
    Return a copy of ``iterable`` with all ``None`` entries removed.

    If ``iterable`` is a mapping, return a dictionary where all pairs that have
    value ``None`` have been removed.
  summary: Return a copy of ``iterable`` with all ``None`` entries removed
  signatures:
  - kind: positional
    name: iterable
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: wraps
  kind: function
  ns: scrapy.utils
  description: |-
    Decorator factory to apply update_wrapper() to a wrapper function

    Returns a decorator that invokes update_wrapper() with the decorated
    function as the wrapper argument and the arguments to wraps() as the
    remaining arguments. Default arguments are as for update_wrapper().
    This is a convenience function to simplify applying partial() to
    update_wrapper().
  summary: Decorator factory to apply update_wrapper() to a wrapper function
  signatures:
  - kind: positional
    name: wrapped
    default: null
    rest: false
  - kind: positional
    name: assigned
    default: ('__module__', '__name__', '__qualname__', '__doc__', '__annotations__')
    rest: false
  - kind: positional
    name: updated
    default: ('__dict__',)
    rest: false
  - type: '?'
  inherits_from: null
- name: reactor
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: AbstractEventLoop
  kind: class
  ns: scrapy.utils
  description: Abstract event loop.
  summary: Abstract event loop
  signatures:
  - type: AbstractEventLoop
  inherits_from: null
- name: AbstractEventLoop.add_reader
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: fd
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.add_signal_handler
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sig
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.add_writer
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: fd
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.call_at
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: when
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - name: context
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.call_exception_handler
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: context
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.call_later
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: delay
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - name: context
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.call_soon
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - name: context
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.call_soon_threadsafe
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: callback
    default: null
    rest: false
  - name: context
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.close
  kind: method
  ns: scrapy.utils
  description: |-
    Close the loop.

    The loop should not be running.

    This is idempotent and irreversible.

    No other methods should be called after this one.
  summary: Close the loop
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.connect_accepted_socket
  kind: method
  ns: scrapy.utils
  description: |-
    Handle an accepted connection.

    This is used by servers that accept connections outside of
    asyncio, but use asyncio to handle connections.

    This method is a coroutine.  When completed, the coroutine
    returns a (transport, protocol) pair.
  summary: Handle an accepted connection
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: protocol_factory
    default: null
    rest: false
  - kind: positional
    name: sock
    default: null
    rest: false
  - name: ssl
    default: None
    rest: false
    kind: kw-only
  - name: ssl_handshake_timeout
    default: None
    rest: false
    kind: kw-only
  - name: ssl_shutdown_timeout
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.connect_read_pipe
  kind: method
  ns: scrapy.utils
  description: |-
    Register read pipe in event loop. Set the pipe to non-blocking mode.

    protocol_factory should instantiate object with Protocol interface.
    pipe is a file-like object.
    Return pair (transport, protocol), where transport supports the
    ReadTransport interface.
  summary: Register read pipe in event loop
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: protocol_factory
    default: null
    rest: false
  - kind: positional
    name: pipe
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.connect_write_pipe
  kind: method
  ns: scrapy.utils
  description: |-
    Register write pipe in event loop.

    protocol_factory should instantiate object with BaseProtocol interface.
    Pipe is file-like object already switched to nonblocking.
    Return pair (transport, protocol), where transport support
    WriteTransport interface.
  summary: Register write pipe in event loop
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: protocol_factory
    default: null
    rest: false
  - kind: positional
    name: pipe
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.create_connection
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: protocol_factory
    default: null
    rest: false
  - kind: positional
    name: host
    default: None
    rest: false
  - kind: positional
    name: port
    default: None
    rest: false
  - name: ssl
    default: None
    rest: false
    kind: kw-only
  - name: family
    default: '0'
    rest: false
    kind: kw-only
  - name: proto
    default: '0'
    rest: false
    kind: kw-only
  - name: flags
    default: '0'
    rest: false
    kind: kw-only
  - name: sock
    default: None
    rest: false
    kind: kw-only
  - name: local_addr
    default: None
    rest: false
    kind: kw-only
  - name: server_hostname
    default: None
    rest: false
    kind: kw-only
  - name: ssl_handshake_timeout
    default: None
    rest: false
    kind: kw-only
  - name: ssl_shutdown_timeout
    default: None
    rest: false
    kind: kw-only
  - name: happy_eyeballs_delay
    default: None
    rest: false
    kind: kw-only
  - name: interleave
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.create_datagram_endpoint
  kind: method
  ns: scrapy.utils
  description: |-
    A coroutine which creates a datagram endpoint.

    This method will try to establish the endpoint in the background.
    When successful, the coroutine returns a (transport, protocol) pair.

    protocol_factory must be a callable returning a protocol instance.

    socket family AF_INET, socket.AF_INET6 or socket.AF_UNIX depending on
    host (or family if specified), socket type SOCK_DGRAM.

    reuse_address tells the kernel to reuse a local socket in
    TIME_WAIT state, without waiting for its natural timeout to
    expire. If not specified it will automatically be set to True on
    UNIX.

    reuse_port tells the kernel to allow this endpoint to be bound to
    the same port as other existing endpoints are bound to, so long as
    they all set this flag when being created. This option is not
    supported on Windows and some UNIX's. If the
    :py:data:`~socket.SO_REUSEPORT` constant is not defined then this
    capability is unsupported.

    allow_broadcast tells the kernel to allow this endpoint to send
    messages to the broadcast address.

    sock can optionally be specified in order to use a preexisting
    socket object.
  summary: A coroutine which creates a datagram endpoint
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: protocol_factory
    default: null
    rest: false
  - kind: positional
    name: local_addr
    default: None
    rest: false
  - kind: positional
    name: remote_addr
    default: None
    rest: false
  - name: family
    default: '0'
    rest: false
    kind: kw-only
  - name: proto
    default: '0'
    rest: false
    kind: kw-only
  - name: flags
    default: '0'
    rest: false
    kind: kw-only
  - name: reuse_address
    default: None
    rest: false
    kind: kw-only
  - name: reuse_port
    default: None
    rest: false
    kind: kw-only
  - name: allow_broadcast
    default: None
    rest: false
    kind: kw-only
  - name: sock
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.create_future
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.create_server
  kind: method
  ns: scrapy.utils
  description: |-
    A coroutine which creates a TCP server bound to host and port.

    The return value is a Server object which can be used to stop
    the service.

    If host is an empty string or None all interfaces are assumed
    and a list of multiple sockets will be returned (most likely
    one for IPv4 and another one for IPv6). The host parameter can also be
    a sequence (e.g. list) of hosts to bind to.

    family can be set to either AF_INET or AF_INET6 to force the
    socket to use IPv4 or IPv6. If not set it will be determined
    from host (defaults to AF_UNSPEC).

    flags is a bitmask for getaddrinfo().

    sock can optionally be specified in order to use a preexisting
    socket object.

    backlog is the maximum number of queued connections passed to
    listen() (defaults to 100).

    ssl can be set to an SSLContext to enable SSL over the
    accepted connections.

    reuse_address tells the kernel to reuse a local socket in
    TIME_WAIT state, without waiting for its natural timeout to
    expire. If not specified will automatically be set to True on
    UNIX.

    reuse_port tells the kernel to allow this endpoint to be bound to
    the same port as other existing endpoints are bound to, so long as
    they all set this flag when being created. This option is not
    supported on Windows.

    ssl_handshake_timeout is the time in seconds that an SSL server
    will wait for completion of the SSL handshake before aborting the
    connection. Default is 60s.

    ssl_shutdown_timeout is the time in seconds that an SSL server
    will wait for completion of the SSL shutdown procedure
    before aborting the connection. Default is 30s.

    start_serving set to True (default) causes the created server
    to start accepting connections immediately.  When set to False,
    the user should await Server.start_serving() or Server.serve_forever()
    to make the server to start accepting connections.
  summary: A coroutine which creates a TCP server bound to host and port
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: protocol_factory
    default: null
    rest: false
  - kind: positional
    name: host
    default: None
    rest: false
  - kind: positional
    name: port
    default: None
    rest: false
  - name: family
    default: '0'
    rest: false
    kind: kw-only
  - name: flags
    default: '1'
    rest: false
    kind: kw-only
  - name: sock
    default: None
    rest: false
    kind: kw-only
  - name: backlog
    default: '100'
    rest: false
    kind: kw-only
  - name: ssl
    default: None
    rest: false
    kind: kw-only
  - name: reuse_address
    default: None
    rest: false
    kind: kw-only
  - name: reuse_port
    default: None
    rest: false
    kind: kw-only
  - name: ssl_handshake_timeout
    default: None
    rest: false
    kind: kw-only
  - name: ssl_shutdown_timeout
    default: None
    rest: false
    kind: kw-only
  - name: start_serving
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.create_task
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: coro
    default: null
    rest: false
  - name: name
    default: None
    rest: false
    kind: kw-only
  - name: context
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.create_unix_connection
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: protocol_factory
    default: null
    rest: false
  - kind: positional
    name: path
    default: None
    rest: false
  - name: ssl
    default: None
    rest: false
    kind: kw-only
  - name: sock
    default: None
    rest: false
    kind: kw-only
  - name: server_hostname
    default: None
    rest: false
    kind: kw-only
  - name: ssl_handshake_timeout
    default: None
    rest: false
    kind: kw-only
  - name: ssl_shutdown_timeout
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.create_unix_server
  kind: method
  ns: scrapy.utils
  description: |-
    A coroutine which creates a UNIX Domain Socket server.

    The return value is a Server object, which can be used to stop
    the service.

    path is a str, representing a file system path to bind the
    server socket to.

    sock can optionally be specified in order to use a preexisting
    socket object.

    backlog is the maximum number of queued connections passed to
    listen() (defaults to 100).

    ssl can be set to an SSLContext to enable SSL over the
    accepted connections.

    ssl_handshake_timeout is the time in seconds that an SSL server
    will wait for the SSL handshake to complete (defaults to 60s).

    ssl_shutdown_timeout is the time in seconds that an SSL server
    will wait for the SSL shutdown to finish (defaults to 30s).

    start_serving set to True (default) causes the created server
    to start accepting connections immediately.  When set to False,
    the user should await Server.start_serving() or Server.serve_forever()
    to make the server to start accepting connections.
  summary: A coroutine which creates a UNIX Domain Socket server
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: protocol_factory
    default: null
    rest: false
  - kind: positional
    name: path
    default: None
    rest: false
  - name: sock
    default: None
    rest: false
    kind: kw-only
  - name: backlog
    default: '100'
    rest: false
    kind: kw-only
  - name: ssl
    default: None
    rest: false
    kind: kw-only
  - name: ssl_handshake_timeout
    default: None
    rest: false
    kind: kw-only
  - name: ssl_shutdown_timeout
    default: None
    rest: false
    kind: kw-only
  - name: start_serving
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.default_exception_handler
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: context
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.get_debug
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.get_exception_handler
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.get_task_factory
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.getaddrinfo
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: host
    default: null
    rest: false
  - kind: positional
    name: port
    default: null
    rest: false
  - name: family
    default: '0'
    rest: false
    kind: kw-only
  - name: type
    default: '0'
    rest: false
    kind: kw-only
  - name: proto
    default: '0'
    rest: false
    kind: kw-only
  - name: flags
    default: '0'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.getnameinfo
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sockaddr
    default: null
    rest: false
  - kind: positional
    name: flags
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.is_closed
  kind: method
  ns: scrapy.utils
  description: Returns True if the event loop was closed.
  summary: Returns True if the event loop was closed
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.is_running
  kind: method
  ns: scrapy.utils
  description: Return whether the event loop is currently running.
  summary: Return whether the event loop is currently running
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.remove_reader
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: fd
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.remove_signal_handler
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sig
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.remove_writer
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: fd
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.run_forever
  kind: method
  ns: scrapy.utils
  description: Run the event loop until stop() is called.
  summary: Run the event loop until stop() is called
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.run_in_executor
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: executor
    default: null
    rest: false
  - kind: positional
    name: func
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.run_until_complete
  kind: method
  ns: scrapy.utils
  description: |-
    Run the event loop until a Future is done.

    Return the Future's result, or raise its exception.
  summary: Run the event loop until a Future is done
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: future
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.sendfile
  kind: method
  ns: scrapy.utils
  description: |-
    Send a file through a transport.

    Return an amount of sent bytes.
  summary: Send a file through a transport
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: transport
    default: null
    rest: false
  - kind: positional
    name: file
    default: null
    rest: false
  - kind: positional
    name: offset
    default: '0'
    rest: false
  - kind: positional
    name: count
    default: None
    rest: false
  - name: fallback
    default: 'True'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.set_debug
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: enabled
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.set_default_executor
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: executor
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.set_exception_handler
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: handler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.set_task_factory
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: factory
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.shutdown_asyncgens
  kind: method
  ns: scrapy.utils
  description: Shutdown all active asynchronous generators.
  summary: Shutdown all active asynchronous generators
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.shutdown_default_executor
  kind: method
  ns: scrapy.utils
  description: Schedule the shutdown of the default executor.
  summary: Schedule the shutdown of the default executor
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.sock_accept
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sock
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.sock_connect
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sock
    default: null
    rest: false
  - kind: positional
    name: address
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.sock_recv
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sock
    default: null
    rest: false
  - kind: positional
    name: nbytes
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.sock_recv_into
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sock
    default: null
    rest: false
  - kind: positional
    name: buf
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.sock_recvfrom
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sock
    default: null
    rest: false
  - kind: positional
    name: bufsize
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.sock_recvfrom_into
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sock
    default: null
    rest: false
  - kind: positional
    name: buf
    default: null
    rest: false
  - kind: positional
    name: nbytes
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.sock_sendall
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sock
    default: null
    rest: false
  - kind: positional
    name: data
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.sock_sendfile
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sock
    default: null
    rest: false
  - kind: positional
    name: file
    default: null
    rest: false
  - kind: positional
    name: offset
    default: '0'
    rest: false
  - kind: positional
    name: count
    default: None
    rest: false
  - name: fallback
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.sock_sendto
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: sock
    default: null
    rest: false
  - kind: positional
    name: data
    default: null
    rest: false
  - kind: positional
    name: address
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.start_tls
  kind: method
  ns: scrapy.utils
  description: |-
    Upgrade a transport to TLS.

    Return a new transport that *protocol* should start using
    immediately.
  summary: Upgrade a transport to TLS
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: transport
    default: null
    rest: false
  - kind: positional
    name: protocol
    default: null
    rest: false
  - kind: positional
    name: sslcontext
    default: null
    rest: false
  - name: server_side
    default: 'False'
    rest: false
    kind: kw-only
  - name: server_hostname
    default: None
    rest: false
    kind: kw-only
  - name: ssl_handshake_timeout
    default: None
    rest: false
    kind: kw-only
  - name: ssl_shutdown_timeout
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.stop
  kind: method
  ns: scrapy.utils
  description: |-
    Stop the event loop as soon as reasonable.

    Exactly how soon that is may depend on the implementation, but
    no more I/O callbacks should be scheduled.
  summary: Stop the event loop as soon as reasonable
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.subprocess_exec
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: protocol_factory
    default: null
    rest: false
  - name: stdin
    default: '-1'
    rest: false
    kind: kw-only
  - name: stdout
    default: '-1'
    rest: false
    kind: kw-only
  - name: stderr
    default: '-1'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.subprocess_shell
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: protocol_factory
    default: null
    rest: false
  - kind: positional
    name: cmd
    default: null
    rest: false
  - name: stdin
    default: '-1'
    rest: false
    kind: kw-only
  - name: stdout
    default: '-1'
    rest: false
    kind: kw-only
  - name: stderr
    default: '-1'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: AbstractEventLoop.time
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoopPolicy
  kind: class
  ns: scrapy.utils
  description: Abstract policy for accessing the event loop.
  summary: Abstract policy for accessing the event loop
  signatures:
  - type: AbstractEventLoopPolicy
  inherits_from: null
- name: AbstractEventLoopPolicy.get_child_watcher
  kind: method
  ns: scrapy.utils
  description: Get the watcher for child processes.
  summary: Get the watcher for child processes
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoopPolicy.get_event_loop
  kind: method
  ns: scrapy.utils
  description: |-
    Get the event loop for the current context.

    Returns an event loop object implementing the AbstractEventLoop interface,
    or raises an exception in case no event loop has been set for the
    current context and the current policy does not specify to create one.

    It should never return None.
  summary: Get the event loop for the current context
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoopPolicy.new_event_loop
  kind: method
  ns: scrapy.utils
  description: |-
    Create and return a new event loop object according to this
    policy's rules. If there's need to set this loop as the event loop for
    the current context, set_event_loop must be called explicitly.
  summary: Create and return a new event loop object according to this
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoopPolicy.set_child_watcher
  kind: method
  ns: scrapy.utils
  description: Set the watcher for child processes.
  summary: Set the watcher for child processes
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: watcher
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: AbstractEventLoopPolicy.set_event_loop
  kind: method
  ns: scrapy.utils
  description: Set the event loop for the current context to loop.
  summary: Set the event loop for the current context to loop
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: loop
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DelayedCall
  kind: class
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: time
    default: null
    rest: false
  - kind: positional
    name: func
    default: null
    rest: false
  - kind: positional
    name: args
    default: null
    rest: false
  - kind: positional
    name: kw
    default: null
    rest: false
  - kind: positional
    name: cancel
    default: null
    rest: false
  - kind: positional
    name: reset
    default: null
    rest: false
  - kind: positional
    name: seconds
    default: <built-in function time>
    rest: false
  - type: DelayedCall
  inherits_from: null
- name: DelayedCall.activate_delay
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DelayedCall.active
  kind: method
  ns: scrapy.utils
  description: |-
    Determine whether this call is still pending

    @return: True if this call has not yet been made or cancelled,
        False otherwise.
  summary: Determine whether this call is still pending
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DelayedCall.cancel
  kind: method
  ns: scrapy.utils
  description: |-
    Unschedule this call

    @raise AlreadyCancelled: Raised if this call has already been
        unscheduled.

    @raise AlreadyCalled: Raised if this call has already been made.
  summary: Unschedule this call
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DelayedCall.creator
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DelayedCall.debug
  kind: property
  ns: scrapy.utils
  description: |-
    bool(x) -> bool

    Returns True when the argument x is true, False otherwise.
    The builtins True and False are the only two instances of the class bool.
    The class bool is a subclass of the class int, and cannot be subclassed.
  summary: bool(x) -> bool
  signatures: null
  inherits_from: null
- name: DelayedCall.delay
  kind: method
  ns: scrapy.utils
  description: |-
    Reschedule this call for a later time

    @param secondsLater: The number of seconds after the originally
        scheduled time for which to reschedule this call.

    @raise AlreadyCancelled: Raised if this call has been cancelled.
    @raise AlreadyCalled: Raised if this call has already been made.
  summary: Reschedule this call for a later time
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: secondsLater
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DelayedCall.getTime
  kind: method
  ns: scrapy.utils
  description: |-
    Return the time at which this call will fire

    @return: The number of seconds after the epoch at which this call is
        scheduled to be made.
  summary: Return the time at which this call will fire
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DelayedCall.reset
  kind: method
  ns: scrapy.utils
  description: |-
    Reschedule this call for a different time

    @param secondsFromNow: The number of seconds from the time of the
        C{reset} call at which this call will be scheduled.

    @raise AlreadyCancelled: Raised if this call has been cancelled.
    @raise AlreadyCalled: Raised if this call has already been made.
  summary: Reschedule this call for a different time
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: secondsFromNow
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.utils
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Sequence
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Sequence.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: catch_warnings
  kind: class
  ns: scrapy.utils
  description: |-
    A context manager that copies and restores the warnings filter upon
    exiting the context.

    The 'record' argument specifies whether warnings should be captured by a
    custom implementation of warnings.showwarning() and be appended to a list
    returned by the context manager. Otherwise None is returned by the context
    manager. The objects appended to the list are arguments whose attributes
    mirror the arguments to showwarning().

    The 'module' argument is to specify an alternative module to the module
    named 'warnings' and imported under that name. This argument is only useful
    when testing the warnings module itself.

    If the 'action' argument is not None, the remaining arguments are passed
    to warnings.simplefilter() as if it were called immediately on entering the
    context.
  summary: A context manager that copies and restores the warnings filter upon
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: record
    default: 'False'
    rest: false
    kind: kw-only
  - name: module
    default: None
    rest: false
    kind: kw-only
  - name: action
    default: None
    rest: false
    kind: kw-only
  - name: category
    default: <class 'Warning'>
    rest: false
    kind: kw-only
  - name: lineno
    default: '0'
    rest: false
    kind: kw-only
  - name: append
    default: 'False'
    rest: false
    kind: kw-only
  - type: catch_warnings
  inherits_from: null
- name: filterwarnings
  kind: function
  ns: scrapy.utils
  description: |-
    Insert an entry into the list of warnings filters (at the front).

    'action' -- one of "error", "ignore", "always", "default", "module",
                or "once"
    'message' -- a regex that the warning message must match
    'category' -- a class that the warning must be a subclass of
    'module' -- a regex that the module name must match
    'lineno' -- an integer line number, 0 matches all warnings
    'append' -- if true, append to the list of filters
  summary: Insert an entry into the list of warnings filters (at the front)
  signatures:
  - kind: positional
    name: action
    default: null
    rest: false
  - kind: positional
    name: message
    default: null
    rest: false
  - kind: positional
    name: category
    default: <class 'Warning'>
    rest: false
  - kind: positional
    name: module
    default: null
    rest: false
  - kind: positional
    name: lineno
    default: '0'
    rest: false
  - kind: positional
    name: append
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: get_asyncio_event_loop_policy
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: install_reactor
  kind: function
  ns: scrapy.utils
  description: |-
    Installs the :mod:`~twisted.internet.reactor` with the specified
    import path. Also installs the asyncio event loop with the specified import
    path if the asyncio reactor is enabled
  summary: Installs the :mod:`~twisted
  signatures:
  - kind: positional
    name: reactor_path
    default: null
    rest: false
  - kind: positional
    name: event_loop_path
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: is_asyncio_reactor_installed
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: listen_tcp
  kind: function
  ns: scrapy.utils
  description: Like reactor.listenTCP but tries different ports in a range.
  summary: Like reactor
  signatures:
  - kind: positional
    name: portrange
    default: null
    rest: false
  - kind: positional
    name: host
    default: null
    rest: false
  - kind: positional
    name: factory
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: load_object
  kind: function
  ns: scrapy.utils
  description: |-
    Load an object given its absolute object path, and return it.

    The object can be the import path of a class, function, variable or an
    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.

    If ``path`` is not a string, but is a callable object, such as a class or
    a function, then return it as is.
  summary: Load an object given its absolute object path, and return it
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: set_asyncio_event_loop
  kind: function
  ns: scrapy.utils
  description: Sets and returns the event loop with specified import path.
  summary: Sets and returns the event loop with specified import path
  signatures:
  - kind: positional
    name: event_loop_path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: set_asyncio_event_loop_policy
  kind: function
  ns: scrapy.utils
  description: |-
    The policy functions from asyncio often behave unexpectedly,
    so we restrict their use to the absolutely essential case.
    This should only be used to install the reactor.
  summary: The policy functions from asyncio often behave unexpectedly,
  signatures:
  - type: '?'
  inherits_from: null
- name: verify_installed_asyncio_event_loop
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: loop_path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: verify_installed_reactor
  kind: function
  ns: scrapy.utils
  description: |-
    Raises :exc:`Exception` if the installed
    :mod:`~twisted.internet.reactor` does not match the specified import
    path.
  summary: Raises :exc:`Exception` if the installed
  signatures:
  - kind: positional
    name: reactor_path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: request
  kind: module
  ns: scrapy.utils
  description: |-
    This module provides some useful functions for working with
    scrapy.http.Request objects
  summary: This module provides some useful functions for working with
  signatures: null
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.utils
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.utils
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Protocol
  kind: class
  ns: scrapy.utils
  description: |-
    Base class for protocol classes.

    Protocol classes are defined as::

        class Proto(Protocol):
            def meth(self) -> int:
                ...

    Such classes are primarily used with static type checkers that recognize
    structural subtyping (static duck-typing).

    For example::

        class C:
            def meth(self) -> int:
                return 0

        def func(x: Proto) -> int:
            return x.meth()

        func(C())  # Passes static type check

    See PEP 544 for details. Protocol classes decorated with
    @typing.runtime_checkable act as simple-minded runtime protocols that check
    only the presence of given attributes, ignoring their type signatures.
    Protocol classes can be generic, they are defined as::

        class GenProto(Protocol[T]):
            def meth(self) -> T:
                ...
  summary: Base class for protocol classes
  signatures:
  - type: Protocol
  inherits_from:
  - <class 'typing.Generic'>
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: basic_auth_header
  kind: function
  ns: scrapy.utils
  description: |-
    Return an `Authorization` header field value for `HTTP Basic Access Authentication (RFC 2617)`_

    >>> import w3lib.http
    >>> w3lib.http.basic_auth_header('someuser', 'somepass')
    'Basic c29tZXVzZXI6c29tZXBhc3M='

    .. _HTTP Basic Access Authentication (RFC 2617): http://www.ietf.org/rfc/rfc2617.txt
  summary: Return an `Authorization` header field value for `HTTP Basic Access Authentication (RFC 2617)`_
  signatures:
  - kind: positional
    name: username
    default: null
    rest: false
  - kind: positional
    name: password
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: ISO-8859-1
    rest: false
  - type: '?'
  inherits_from: null
- name: canonicalize_url
  kind: function
  ns: scrapy.utils
  description: |-
    Canonicalize the given url by applying the following procedures:

    - make the URL safe
    - sort query arguments, first by key, then by value
    - normalize all spaces (in query arguments) '+' (plus symbol)
    - normalize percent encodings case (%2f -> %2F)
    - remove query arguments with blank values (unless `keep_blank_values` is True)
    - remove fragments (unless `keep_fragments` is True)

    The url passed can be bytes or unicode, while the url returned is
    always a native str (bytes in Python 2, unicode in Python 3).

    >>> import w3lib.url
    >>>
    >>> # sorting query arguments
    >>> w3lib.url.canonicalize_url('http://www.example.com/do?c=3&b=5&b=2&a=50')
    'http://www.example.com/do?a=50&b=2&b=5&c=3'
    >>>
    >>> # UTF-8 conversion + percent-encoding of non-ASCII characters
    >>> w3lib.url.canonicalize_url('http://www.example.com/r\u00e9sum\u00e9')
    'http://www.example.com/r%C3%A9sum%C3%A9'
    >>>

    For more examples, see the tests in `tests/test_url.py`.
  summary: 'Canonicalize the given url by applying the following procedures:'
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: keep_blank_values
    default: 'True'
    rest: false
  - kind: positional
    name: keep_fragments
    default: 'False'
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: fingerprint
  kind: function
  ns: scrapy.utils
  description: |-
    Return the request fingerprint.

    The request fingerprint is a hash that uniquely identifies the resource the
    request points to. For example, take the following two urls:

    http://www.example.com/query?id=111&cat=222
    http://www.example.com/query?cat=222&id=111

    Even though those are two different URLs both point to the same resource
    and are equivalent (i.e. they should return the same response).

    Another example are cookies used to store session ids. Suppose the
    following page is only accessible to authenticated users:

    http://www.example.com/members/offers.html

    Lots of sites use a cookie to store the session id, which adds a random
    component to the HTTP Request and thus should be ignored when calculating
    the fingerprint.

    For this reason, request headers are ignored by default when calculating
    the fingerprint. If you want to include specific headers use the
    include_headers argument, which is a list of Request headers to include.

    Also, servers usually ignore fragments in urls when handling requests,
    so they are also ignored by default when calculating the fingerprint.
    If you want to include them, set the keep_fragments argument to True
    (for instance when handling requests with a headless browser).
  summary: Return the request fingerprint
  signatures:
  - kind: positional
    name: request
    default: null
    rest: false
  - name: include_headers
    default: None
    rest: false
    kind: kw-only
  - name: keep_fragments
    default: 'False'
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: load_object
  kind: function
  ns: scrapy.utils
  description: |-
    Load an object given its absolute object path, and return it.

    The object can be the import path of a class, function, variable or an
    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.

    If ``path`` is not a string, but is a callable object, such as a class or
    a function, then return it as is.
  summary: Load an object given its absolute object path, and return it
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: referer_str
  kind: function
  ns: scrapy.utils
  description: Return Referer HTTP header suitable for logging.
  summary: Return Referer HTTP header suitable for logging
  signatures:
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: request_authenticate
  kind: function
  ns: scrapy.utils
  description: |-
    Authenticate the given request (in place) using the HTTP basic access
    authentication mechanism (RFC 2617) and the given username and password
  summary: Authenticate the given request (in place) using the HTTP basic access
  signatures:
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: username
    default: null
    rest: false
  - kind: positional
    name: password
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: request_fingerprint
  kind: function
  ns: scrapy.utils
  description: |-
    Return the request fingerprint as an hexadecimal string.

    The request fingerprint is a hash that uniquely identifies the resource the
    request points to. For example, take the following two urls:

    http://www.example.com/query?id=111&cat=222
    http://www.example.com/query?cat=222&id=111

    Even though those are two different URLs both point to the same resource
    and are equivalent (i.e. they should return the same response).

    Another example are cookies used to store session ids. Suppose the
    following page is only accessible to authenticated users:

    http://www.example.com/members/offers.html

    Lots of sites use a cookie to store the session id, which adds a random
    component to the HTTP Request and thus should be ignored when calculating
    the fingerprint.

    For this reason, request headers are ignored by default when calculating
    the fingerprint. If you want to include specific headers use the
    include_headers argument, which is a list of Request headers to include.

    Also, servers usually ignore fragments in urls when handling requests,
    so they are also ignored by default when calculating the fingerprint.
    If you want to include them, set the keep_fragments argument to True
    (for instance when handling requests with a headless browser).
  summary: Return the request fingerprint as an hexadecimal string
  signatures:
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: include_headers
    default: None
    rest: false
  - kind: positional
    name: keep_fragments
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: request_from_dict
  kind: function
  ns: scrapy.utils
  description: |-
    Create a :class:`~scrapy.Request` object from a dict.

    If a spider is given, it will try to resolve the callbacks looking at the
    spider for methods with the same name.
  summary: Create a :class:`~scrapy
  signatures:
  - kind: positional
    name: d
    default: null
    rest: false
  - name: spider
    default: None
    rest: false
    kind: kw-only
  - type: '?'
  inherits_from: null
- name: request_httprepr
  kind: function
  ns: scrapy.utils
  description: |-
    Return the raw HTTP representation (as bytes) of the given request.
    This is provided only for reference since it's not the actual stream of
    bytes that will be send when performing the request (that's controlled
    by Twisted).
  summary: Return the raw HTTP representation (as bytes) of the given request
  signatures:
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: request_to_curl
  kind: function
  ns: scrapy.utils
  description: |-
    Converts a :class:`~scrapy.Request` object to a curl command.

    :param :class:`~scrapy.Request`: Request object to be converted
    :return: string containing the curl command
  summary: Converts a :class:`~scrapy
  signatures:
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: to_bytes
  kind: function
  ns: scrapy.utils
  description: |-
    Return the binary representation of ``text``. If ``text``
    is already a bytes object, return it as-is.
  summary: Return the binary representation of ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: to_unicode
  kind: function
  ns: scrapy.utils
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: urlparse_cached
  kind: function
  ns: scrapy.utils
  description: |-
    Return urlparse.urlparse caching the result, where the argument can be a
    Request or Response object
  summary: Return urlparse
  signatures:
  - kind: positional
    name: request_or_response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: urlunparse
  kind: function
  ns: scrapy.utils
  description: |-
    Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).
  summary: Put a parsed URL back together again
  signatures:
  - kind: positional
    name: components
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: response
  kind: module
  ns: scrapy.utils
  description: |-
    This module provides some useful functions for working with
    scrapy.http.Response objects
  summary: This module provides some useful functions for working with
  signatures: null
  inherits_from: null
- name: Callable
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to collections.abc.Callable.

    Callable[[int], str] signifies a function that takes a single
    parameter of type int and returns a str.

    The subscription syntax must always be used with exactly two
    values: the argument list and the return type.
    The argument list must be a list of types, a ParamSpec,
    Concatenate or ellipsis. The return type must be a single type.

    There is no syntax to indicate optional or keyword arguments;
    such function types are rarely used as callback types.
  summary: Deprecated alias to collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: deprecated
  kind: function
  ns: scrapy.utils
  description: |-
    This is a decorator which can be used to mark functions
    as deprecated. It will result in a warning being emitted
    when the function is used.
  summary: This is a decorator which can be used to mark functions
  signatures:
  - kind: positional
    name: use_instead
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: get_base_url
  kind: function
  ns: scrapy.utils
  description: Return the base url of the given response, joined with the response url
  summary: Return the base url of the given response, joined with the response url
  signatures:
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: get_meta_refresh
  kind: function
  ns: scrapy.utils
  description: Parse the http-equiv refresh parameter from the given response
  summary: Parse the http-equiv refresh parameter from the given response
  signatures:
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: ignore_tags
    default: ('script', 'noscript')
    rest: false
  - type: '?'
  inherits_from: null
- name: open_in_browser
  kind: function
  ns: scrapy.utils
  description: |-
    Open *response* in a local web browser, adjusting the `base tag`_ for
    external links to work, e.g. so that images and styles are displayed.

    .. _base tag: https://www.w3schools.com/tags/tag_base.asp

    For example:

    .. code-block:: python

        from scrapy.utils.response import open_in_browser


        def parse_details(self, response):
            if "item name" not in response.body:
                open_in_browser(response)
  summary: Open *response* in a local web browser, adjusting the `base tag`_ for
  signatures:
  - kind: positional
    name: response
    default: null
    rest: false
  - kind: positional
    name: _openfunc
    default: <function open at 0x7fa97305ff60>
    rest: false
  - type: '?'
  inherits_from: null
- name: response_httprepr
  kind: function
  ns: scrapy.utils
  description: |-
    Return raw HTTP representation (as bytes) of the given response. This
    is provided only for reference, since it's not the exact stream of bytes
    that was received (that's not exposed by Twisted).
  summary: Return raw HTTP representation (as bytes) of the given response
  signatures:
  - type: '?'
  inherits_from: null
- name: response_status_message
  kind: function
  ns: scrapy.utils
  description: Return status code plus status text descriptive message
  summary: Return status code plus status text descriptive message
  signatures:
  - kind: positional
    name: status
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: to_bytes
  kind: function
  ns: scrapy.utils
  description: |-
    Return the binary representation of ``text``. If ``text``
    is already a bytes object, return it as-is.
  summary: Return the binary representation of ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: to_unicode
  kind: function
  ns: scrapy.utils
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: serialize
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ItemAdapter
  kind: class
  ns: scrapy.utils
  description: |-
    Wrapper class to interact with data container objects. It provides a common interface
    to extract and set data without having to take the object's type into account.
  summary: Wrapper class to interact with data container objects
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: ItemAdapter
  inherits_from:
  - <class 'collections.abc.MutableMapping'>
  - <class 'collections.abc.Mapping'>
  - <class 'collections.abc.Collection'>
  - <class 'collections.abc.Sized'>
  - <class 'collections.abc.Iterable'>
  - <class 'collections.abc.Container'>
- name: ItemAdapter.ADAPTER_CLASSES
  kind: property
  ns: scrapy.utils
  description: |-
    deque([iterable[, maxlen]]) --> deque object

    A list-like sequence optimized for data accesses near its endpoints.
  summary: deque([iterable[, maxlen]]) --> deque object
  signatures: null
  inherits_from: null
- name: ItemAdapter.asdict
  kind: method
  ns: scrapy.utils
  description: |-
    Return a dict object with the contents of the adapter. This works slightly different
    than calling `dict(adapter)`: it's applied recursively to nested items (if there are any).
  summary: Return a dict object with the contents of the adapter
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.clear
  kind: method
  ns: scrapy.utils
  description: D.clear() -> None.  Remove all items from D.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.field_names
  kind: method
  ns: scrapy.utils
  description: Return read-only key view with the names of all the defined fields for the item.
  summary: Return read-only key view with the names of all the defined fields for the item
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.get
  kind: method
  ns: scrapy.utils
  description: D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.get_field_meta
  kind: method
  ns: scrapy.utils
  description: Return metadata for the given field name.
  summary: Return metadata for the given field name
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.get_field_meta_from_class
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: item_class
    default: null
    rest: false
  - kind: positional
    name: field_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.get_field_names_from_class
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: item_class
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.is_item
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: item
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.is_item_class
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: item_class
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.item
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: ItemAdapter.items
  kind: method
  ns: scrapy.utils
  description: D.items() -> a set-like object providing a view on D's items
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.keys
  kind: method
  ns: scrapy.utils
  description: D.keys() -> a set-like object providing a view on D's keys
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.pop
  kind: method
  ns: scrapy.utils
  description: |-
    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
    If key is not found, d is returned if given, otherwise KeyError is raised.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: <object object at 0x7fa9743b0160>
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.popitem
  kind: method
  ns: scrapy.utils
  description: |-
    D.popitem() -> (k, v), remove and return some (key, value) pair
    as a 2-tuple; but raise KeyError if D is empty.
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.setdefault
  kind: method
  ns: scrapy.utils
  description: D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: key
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.update
  kind: method
  ns: scrapy.utils
  description: |-
    D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.
    If E present and has a .keys() method, does:     for k in E: D[k] = E[k]
    If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v
    In either case, this is followed by: for k, v in F.items(): D[k] = v
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: other
    default: ()
    rest: false
  - type: '?'
  inherits_from: null
- name: ItemAdapter.values
  kind: method
  ns: scrapy.utils
  description: D.values() -> an object providing a view on D's values
  summary: D
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyJSONDecoder
  kind: class
  ns: scrapy.utils
  description: |-
    Simple JSON <https://json.org> decoder

    Performs the following translations in decoding by default:

    +---------------+-------------------+
    | JSON          | Python            |
    +===============+===================+
    | object        | dict              |
    +---------------+-------------------+
    | array         | list              |
    +---------------+-------------------+
    | string        | str               |
    +---------------+-------------------+
    | number (int)  | int               |
    +---------------+-------------------+
    | number (real) | float             |
    +---------------+-------------------+
    | true          | True              |
    +---------------+-------------------+
    | false         | False             |
    +---------------+-------------------+
    | null          | None              |
    +---------------+-------------------+

    It also understands ``NaN``, ``Infinity``, and ``-Infinity`` as
    their corresponding ``float`` values, which is outside the JSON spec.
  summary: Simple JSON <https://json
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: object_hook
    default: None
    rest: false
    kind: kw-only
  - name: parse_float
    default: None
    rest: false
    kind: kw-only
  - name: parse_int
    default: None
    rest: false
    kind: kw-only
  - name: parse_constant
    default: None
    rest: false
    kind: kw-only
  - name: strict
    default: 'True'
    rest: false
    kind: kw-only
  - name: object_pairs_hook
    default: None
    rest: false
    kind: kw-only
  - type: ScrapyJSONDecoder
  inherits_from:
  - <class 'json.decoder.JSONDecoder'>
- name: ScrapyJSONDecoder.decode
  kind: method
  ns: scrapy.utils
  description: |-
    Return the Python representation of ``s`` (a ``str`` instance
    containing a JSON document).
  summary: Return the Python representation of ``s`` (a ``str`` instance
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: s
    default: null
    rest: false
  - kind: positional
    name: _w
    default: <built-in method match of re.Pattern object at 0x7fa97416c040>
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyJSONDecoder.raw_decode
  kind: method
  ns: scrapy.utils
  description: |-
    Decode a JSON document from ``s`` (a ``str`` beginning with
    a JSON document) and return a 2-tuple of the Python
    representation and the index in ``s`` where the document ended.

    This can be used to decode a JSON document from a string that may
    have extraneous data at the end.
  summary: Decode a JSON document from ``s`` (a ``str`` beginning with
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: s
    default: null
    rest: false
  - kind: positional
    name: idx
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyJSONEncoder
  kind: class
  ns: scrapy.utils
  description: |-
    Extensible JSON <https://json.org> encoder for Python data structures.

    Supports the following objects and types by default:

    +-------------------+---------------+
    | Python            | JSON          |
    +===================+===============+
    | dict              | object        |
    +-------------------+---------------+
    | list, tuple       | array         |
    +-------------------+---------------+
    | str               | string        |
    +-------------------+---------------+
    | int, float        | number        |
    +-------------------+---------------+
    | True              | true          |
    +-------------------+---------------+
    | False             | false         |
    +-------------------+---------------+
    | None              | null          |
    +-------------------+---------------+

    To extend this to recognize other objects, subclass and implement a
    ``.default()`` method with another method that returns a serializable
    object for ``o`` if possible, otherwise it should call the superclass
    implementation (to raise ``TypeError``).
  summary: Extensible JSON <https://json
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - name: skipkeys
    default: 'False'
    rest: false
    kind: kw-only
  - name: ensure_ascii
    default: 'True'
    rest: false
    kind: kw-only
  - name: check_circular
    default: 'True'
    rest: false
    kind: kw-only
  - name: allow_nan
    default: 'True'
    rest: false
    kind: kw-only
  - name: sort_keys
    default: 'False'
    rest: false
    kind: kw-only
  - name: indent
    default: None
    rest: false
    kind: kw-only
  - name: separators
    default: None
    rest: false
    kind: kw-only
  - name: default
    default: None
    rest: false
    kind: kw-only
  - type: ScrapyJSONEncoder
  inherits_from:
  - <class 'json.encoder.JSONEncoder'>
- name: ScrapyJSONEncoder.DATE_FORMAT
  kind: property
  ns: scrapy.utils
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: ScrapyJSONEncoder.TIME_FORMAT
  kind: property
  ns: scrapy.utils
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: ScrapyJSONEncoder.default
  kind: method
  ns: scrapy.utils
  description: |-
    Implement this method in a subclass such that it returns
    a serializable object for ``o``, or calls the base implementation
    (to raise a ``TypeError``).

    For example, to support arbitrary iterators, you could
    implement default like this::

        def default(self, o):
            try:
                iterable = iter(o)
            except TypeError:
                pass
            else:
                return list(iterable)
            # Let the base class default method raise the TypeError
            return super().default(o)
  summary: Implement this method in a subclass such that it returns
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: o
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyJSONEncoder.encode
  kind: method
  ns: scrapy.utils
  description: |-
    Return a JSON string representation of a Python data structure.

    >>> from json.encoder import JSONEncoder
    >>> JSONEncoder().encode({"foo": ["bar", "baz"]})
    '{"foo": ["bar", "baz"]}'
  summary: Return a JSON string representation of a Python data structure
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: o
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyJSONEncoder.item_separator
  kind: property
  ns: scrapy.utils
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: ScrapyJSONEncoder.iterencode
  kind: method
  ns: scrapy.utils
  description: |-
    Encode the given object and yield each string
    representation as available.

    For example::

        for chunk in JSONEncoder().iterencode(bigobject):
            mysocket.write(chunk)
  summary: Encode the given object and yield each string
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: o
    default: null
    rest: false
  - kind: positional
    name: _one_shot
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: ScrapyJSONEncoder.key_separator
  kind: property
  ns: scrapy.utils
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: is_item
  kind: function
  ns: scrapy.utils
  description: |-
    Return True if the given object belongs to one of the supported types, False otherwise.

    Alias for ItemAdapter.is_item
  summary: Return True if the given object belongs to one of the supported types, False otherwise
  signatures:
  - kind: positional
    name: obj
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: signal
  kind: module
  ns: scrapy.utils
  description: Helper functions for working with signals
  summary: Helper functions for working with signals
  signatures: null
  inherits_from: null
- name: Anonymous
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Any
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.utils
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: disconnect
  kind: function
  ns: scrapy.utils
  description: "Disconnect receiver from sender for signal\n\nreceiver -- the registered receiver to disconnect\nsignal -- the registered signal to disconnect\nsender -- the registered sender to disconnect\nweak -- the weakref state to disconnect\n\ndisconnect reverses the process of connect,\nthe semantics for the individual elements are\nlogically equivalent to a tuple of\n(receiver, signal, sender, weak) used as a key\nto be deleted from the internal routing tables.\n(The actual process is slightly more complex\nbut the semantics are basically the same).\n\nNote:\n    Using disconnect is not required to cleanup\n    routing when an object is deleted, the framework\n    will remove routes for deleted objects\n    automatically.  It's only necessary to disconnect\n    if you want to stop routing to a live object.\n    \nreturns None, may raise DispatcherTypeError or\n    DispatcherKeyError"
  summary: Disconnect receiver from sender for signal
  signatures:
  - kind: positional
    name: receiver
    default: null
    rest: false
  - kind: positional
    name: signal
    default: _Any
    rest: false
  - kind: positional
    name: sender
    default: _Any
    rest: false
  - kind: positional
    name: weak
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: disconnect_all
  kind: function
  ns: scrapy.utils
  description: |-
    Disconnect all signal handlers. Useful for cleaning up after running
    tests
  summary: Disconnect all signal handlers
  signatures:
  - kind: positional
    name: signal
    default: _Any
    rest: false
  - kind: positional
    name: sender
    default: _Any
    rest: false
  - type: '?'
  inherits_from: null
- name: failure_to_exc_info
  kind: function
  ns: scrapy.utils
  description: Extract exc_info from Failure instances
  summary: Extract exc_info from Failure instances
  signatures:
  - kind: positional
    name: failure
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: getAllReceivers
  kind: function
  ns: scrapy.utils
  description: |-
    Get list of all receivers from global tables

    This gets all receivers which should receive
    the given signal from sender, each receiver should
    be produced only once by the resulting generator
  summary: Get list of all receivers from global tables
  signatures:
  - kind: positional
    name: sender
    default: _Any
    rest: false
  - kind: positional
    name: signal
    default: _Any
    rest: false
  - type: '?'
  inherits_from: null
- name: liveReceivers
  kind: function
  ns: scrapy.utils
  description: |-
    Filter sequence of receivers to get resolved, live receivers

    This is a generator which will iterate over
    the passed sequence, checking for weak references
    and resolving them, then returning all live
    receivers.
  summary: Filter sequence of receivers to get resolved, live receivers
  signatures:
  - kind: positional
    name: receivers
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: maybeDeferred_coro
  kind: function
  ns: scrapy.utils
  description: Copy of defer.maybeDeferred that also converts coroutines to Deferreds.
  summary: Copy of defer
  signatures:
  - kind: positional
    name: f
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: robustApply
  kind: function
  ns: scrapy.utils
  description: "Call receiver with arguments and an appropriate subset of named\n    "
  summary: Call receiver with arguments and an appropriate subset of named
  signatures:
  - kind: positional
    name: receiver
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: send_catch_log
  kind: function
  ns: scrapy.utils
  description: |-
    Like pydispatcher.robust.sendRobust but it also logs errors and returns
    Failures instead of exceptions.
  summary: Like pydispatcher
  signatures:
  - kind: positional
    name: signal
    default: _Any
    rest: false
  - kind: positional
    name: sender
    default: _Anonymous
    rest: false
  - type: '?'
  inherits_from: null
- name: send_catch_log_deferred
  kind: function
  ns: scrapy.utils
  description: |-
    Like send_catch_log but supports returning deferreds on signal handlers.
    Returns a deferred that gets fired once all signal handlers deferreds were
    fired.
  summary: Like send_catch_log but supports returning deferreds on signal handlers
  signatures:
  - kind: positional
    name: signal
    default: _Any
    rest: false
  - kind: positional
    name: sender
    default: _Anonymous
    rest: false
  - type: '?'
  inherits_from: null
- name: sitemap
  kind: module
  ns: scrapy.utils
  description: |-
    Module for processing Sitemaps.

    Note: The main purpose of this module is to provide support for the
    SitemapSpider, its API is subject to change without notice.
  summary: Module for processing Sitemaps
  signatures: null
  inherits_from: null
- name: Dict
  kind: callable
  ns: scrapy.utils
  description: A generic version of dict.
  summary: A generic version of dict
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: sitemap_urls_from_robots
  kind: function
  ns: scrapy.utils
  description: |-
    Return an iterator over all sitemap urls contained in the given
    robots.txt file
  summary: Return an iterator over all sitemap urls contained in the given
  signatures:
  - kind: positional
    name: robots_text
    default: null
    rest: false
  - kind: positional
    name: base_url
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: urljoin
  kind: function
  ns: scrapy.utils
  description: |-
    Join a base URL and a possibly relative URL to form an absolute
    interpretation of the latter.
  summary: Join a base URL and a possibly relative URL to form an absolute
  signatures:
  - kind: positional
    name: base
    default: null
    rest: false
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: allow_fragments
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: spider
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: AsyncGenerator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.AsyncGenerator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultSpider
  kind: class
  ns: scrapy.utils
  description: |-
    Base class for scrapy spiders. All spiders must inherit from this
    class.
  summary: Base class for scrapy spiders
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: None
    rest: false
  - type: DefaultSpider
  inherits_from:
  - <class 'scrapy.spiders.Spider'>
  - <class 'scrapy.utils.trackref.object_ref'>
- name: DefaultSpider.close
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: spider
    default: null
    rest: false
  - kind: positional
    name: reason
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultSpider.custom_settings
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DefaultSpider.from_crawler
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: crawler
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultSpider.handles_request
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultSpider.log
  kind: method
  ns: scrapy.utils
  description: |-
    Log the given message at the given log level

    This helper wraps a log call to the logger within the spider, but you
    can use it directly (e.g. Spider.logger.info('msg')) or use any other
    Python logger too.
  summary: Log the given message at the given log level
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: message
    default: null
    rest: false
  - kind: positional
    name: level
    default: '10'
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultSpider.logger
  kind: property
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: DefaultSpider.name
  kind: property
  ns: scrapy.utils
  description: |-
    str(object='') -> str
    str(bytes_or_buffer[, encoding[, errors]]) -> str

    Create a new string object from the given object. If encoding or
    errors is specified, then the object must expose a data buffer
    that will be decoded using the given encoding and error handler.
    Otherwise, returns the result of object.__str__() (if defined)
    or repr(object).
    encoding defaults to sys.getdefaultencoding().
    errors defaults to 'strict'.
  summary: str(object='') -> str
  signatures: null
  inherits_from: null
- name: DefaultSpider.parse
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: response
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultSpider.start_requests
  kind: method
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: DefaultSpider.update_settings
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: cls
    default: null
    rest: false
  - kind: positional
    name: settings
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Generator
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Generator.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Literal
  kind: callable
  ns: scrapy.utils
  description: |-
    Special typing form to define literal types (a.k.a. value types).

    This form can be used to indicate to type checkers that the corresponding
    variable or function parameter has a value equivalent to the provided
    literal (or one of several literals)::

        def validate_simple(data: Any) -> Literal[True]:  # always returns True
            ...

        MODE = Literal['r', 'rb', 'w', 'wb']
        def open_helper(file: str, mode: MODE) -> str:
            ...

        open_helper('/some/path', 'r')  # Passes type check
        open_helper('/other/path', 'typo')  # Error in type checker

    Literal[...] cannot be subclassed. At runtime, an arbitrary value
    is allowed as type argument to Literal[...], but type checkers may
    impose restrictions.
  summary: Special typing form to define literal types (a
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: annotations
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: arg_to_iter
  kind: function
  ns: scrapy.utils
  description: |-
    Convert an argument to an iterable. The argument can be a None, single
    value, or an iterable.

    Exception: if arg is a dict, [arg] will be returned
  summary: Convert an argument to an iterable
  signatures:
  - kind: positional
    name: arg
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: deferred_from_coro
  kind: function
  ns: scrapy.utils
  description: Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine
  summary: Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine
  signatures:
  - kind: positional
    name: o
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: iter_spider_classes
  kind: function
  ns: scrapy.utils
  description: |-
    Return an iterator over all spider classes defined in the given module
    that can be instantiated (i.e. which have name)
  summary: Return an iterator over all spider classes defined in the given module
  signatures:
  - kind: positional
    name: module
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: iterate_spider_output
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: result
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: logger
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: overload
  kind: function
  ns: scrapy.utils
  description: |-
    Decorator for overloaded functions/methods.

    In a stub file, place two or more stub definitions for the same
    function in a row, each decorated with @overload.

    For example::

        @overload
        def utf8(value: None) -> None: ...
        @overload
        def utf8(value: bytes) -> bytes: ...
        @overload
        def utf8(value: str) -> bytes: ...

    In a non-stub file (i.e. a regular .py file), do the same but
    follow it with an implementation.  The implementation should *not*
    be decorated with @overload::

        @overload
        def utf8(value: None) -> None: ...
        @overload
        def utf8(value: bytes) -> bytes: ...
        @overload
        def utf8(value: str) -> bytes: ...
        def utf8(value):
            ...  # implementation goes here

    The overloads for a function can be retrieved at runtime using the
    get_overloads() function.
  summary: Decorator for overloaded functions/methods
  signatures:
  - kind: positional
    name: func
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: spidercls_for_request
  kind: function
  ns: scrapy.utils
  description: |-
    Return a spider class that handles the given Request.

    This will look for the spiders that can handle the given request (using
    the spider loader) and return a Spider class if (and only if) there is
    only one Spider able to handle the Request.

    If multiple spiders (or no spider) are found, it will return the
    default_spidercls passed. It can optionally log if multiple or no spiders
    are found.
  summary: Return a spider class that handles the given Request
  signatures:
  - kind: positional
    name: spider_loader
    default: null
    rest: false
  - kind: positional
    name: request
    default: null
    rest: false
  - kind: positional
    name: default_spidercls
    default: None
    rest: false
  - kind: positional
    name: log_none
    default: 'False'
    rest: false
  - kind: positional
    name: log_multiple
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: ssl
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: X509Name
  kind: class
  ns: scrapy.utils
  description: |-
    An X.509 Distinguished Name.

    :ivar countryName: The country of the entity.
    :ivar C: Alias for  :py:attr:`countryName`.

    :ivar stateOrProvinceName: The state or province of the entity.
    :ivar ST: Alias for :py:attr:`stateOrProvinceName`.

    :ivar localityName: The locality of the entity.
    :ivar L: Alias for :py:attr:`localityName`.

    :ivar organizationName: The organization name of the entity.
    :ivar O: Alias for :py:attr:`organizationName`.

    :ivar organizationalUnitName: The organizational unit of the entity.
    :ivar OU: Alias for :py:attr:`organizationalUnitName`

    :ivar commonName: The common name of the entity.
    :ivar CN: Alias for :py:attr:`commonName`.

    :ivar emailAddress: The e-mail address of the entity.
  summary: An X
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - type: X509Name
  inherits_from: null
- name: X509Name.der
  kind: method
  ns: scrapy.utils
  description: |-
    Return the DER encoding of this name.

    :return: The DER encoded form of this name.
    :rtype: :py:class:`bytes`
  summary: Return the DER encoding of this name
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: X509Name.get_components
  kind: method
  ns: scrapy.utils
  description: |-
    Returns the components of this name, as a sequence of 2-tuples.

    :return: The components of this name.
    :rtype: :py:class:`list` of ``name, value`` tuples.
  summary: Returns the components of this name, as a sequence of 2-tuples
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: X509Name.hash
  kind: method
  ns: scrapy.utils
  description: |-
    Return an integer representation of the first four bytes of the
    MD5 digest of the DER representation of the name.

    This is the Python equivalent of OpenSSL's ``X509_NAME_hash``.

    :return: The (integer) hash of this name.
    :rtype: :py:class:`int`
  summary: Return an integer representation of the first four bytes of the
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: ffi_buf_to_string
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: buf
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: get_openssl_version
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: get_temp_key_info
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: ssl_object
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: to_unicode
  kind: function
  ns: scrapy.utils
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: x509name_to_string
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: x509name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: trackref
  kind: module
  ns: scrapy.utils
  description: |-
    This module provides some functions and classes to record and report
    references to live object instances.

    If you want live objects for a particular class to be tracked, you only have to
    subclass from object_ref (instead of object).

    About performance: This library has a minimal performance impact when enabled,
    and no performance penalty at all when disabled (as object_ref becomes just an
    alias to object in that case).
  summary: This module provides some functions and classes to record and report
  signatures: null
  inherits_from: null
- name: DefaultDict
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.defaultdict.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: NoneType
  kind: class
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: format_live_refs
  kind: function
  ns: scrapy.utils
  description: Return a tabular representation of tracked objects
  summary: Return a tabular representation of tracked objects
  signatures:
  - kind: positional
    name: ignore
    default: <class 'NoneType'>
    rest: false
  - type: '?'
  inherits_from: null
- name: get_oldest
  kind: function
  ns: scrapy.utils
  description: Get the oldest object for a specific class name
  summary: Get the oldest object for a specific class name
  signatures:
  - kind: positional
    name: class_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: iter_all
  kind: function
  ns: scrapy.utils
  description: Iterate over all objects of the same class by its class name
  summary: Iterate over all objects of the same class by its class name
  signatures:
  - kind: positional
    name: class_name
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: live_refs
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: print_live_refs
  kind: function
  ns: scrapy.utils
  description: Print tracked objects
  summary: Print tracked objects
  signatures:
  - type: '?'
  inherits_from: null
- name: url
  kind: module
  ns: scrapy.utils
  description: |-
    This module contains general purpose URL functions not found in the standard
    library.

    Some of the functions that used to be imported from this module have been moved
    to the w3lib.url module. Always import those from there instead.
  summary: This module contains general purpose URL functions not found in the standard
  signatures: null
  inherits_from: null
- name: Iterable
  kind: callable
  ns: scrapy.utils
  description: A generic version of collections.abc.Iterable.
  summary: A generic version of collections
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Optional
  kind: callable
  ns: scrapy.utils
  description: Optional[X] is equivalent to Union[X, None].
  summary: Optional[X] is equivalent to Union[X, None]
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: TYPE_CHECKING
  kind: const
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: Type
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.type.

    builtins.type or typing.Type can be used to annotate class objects.
    For example, suppose we have the following classes::

        class User: ...  # Abstract base for User classes
        class BasicUser(User): ...
        class ProUser(User): ...
        class TeamUser(User): ...

    And a function that takes a class argument that's a subclass of
    User and returns an instance of the corresponding class::

        U = TypeVar('U', bound=User)
        def new_user(user_class: Type[U]) -> U:
            user = user_class()
            # (Here we could write the user object to a database)
            return user

        joe = new_user(BasicUser)

    At this point the type checker knows that joe has type BasicUser.
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Union
  kind: callable
  ns: scrapy.utils
  description: |-
    Union type; Union[X, Y] means either X or Y.

    On Python 3.10 and higher, the | operator
    can also be used to denote unions;
    X | Y means the same thing to the type checker as Union[X, Y].

    To define a union, use e.g. Union[int, str]. Details:
    - The arguments must be types and there must be at least one.
    - None as an argument is a special case and is replaced by
      type(None).
    - Unions of unions are flattened, e.g.::

        assert Union[Union[int, str], float] == Union[int, str, float]

    - Unions of a single argument vanish, e.g.::

        assert Union[int] == int  # The constructor actually returns int

    - Redundant arguments are skipped, e.g.::

        assert Union[int, str, int] == Union[int, str]

    - When comparing unions, the argument order is ignored, e.g.::

        assert Union[int, str] == Union[str, int]

    - You cannot subclass or instantiate a union.
    - You can use Optional[X] as a shorthand for Union[X, None].
  summary: Union type; Union[X, Y] means either X or Y
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: UrlT
  kind: callable
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: add_http_if_no_scheme
  kind: function
  ns: scrapy.utils
  description: Add http as the default scheme if it is missing from the url.
  summary: Add http as the default scheme if it is missing from the url
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: add_or_replace_parameter
  kind: function
  ns: scrapy.utils
  description: |-
    Add or remove a parameter to a given url

    >>> import w3lib.url
    >>> w3lib.url.add_or_replace_parameter('http://www.example.com/index.php', 'arg', 'v')
    'http://www.example.com/index.php?arg=v'
    >>> w3lib.url.add_or_replace_parameter('http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3', 'arg4', 'v4')
    'http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3&arg4=v4'
    >>> w3lib.url.add_or_replace_parameter('http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3', 'arg3', 'v3new')
    'http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3new'
    >>>
  summary: Add or remove a parameter to a given url
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: name
    default: null
    rest: false
  - kind: positional
    name: new_value
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: add_or_replace_parameters
  kind: function
  ns: scrapy.utils
  description: |-
    Add or remove a parameters to a given url

    >>> import w3lib.url
    >>> w3lib.url.add_or_replace_parameters('http://www.example.com/index.php', {'arg': 'v'})
    'http://www.example.com/index.php?arg=v'
    >>> args = {'arg4': 'v4', 'arg3': 'v3new'}
    >>> w3lib.url.add_or_replace_parameters('http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3', args)
    'http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3new&arg4=v4'
    >>>
  summary: Add or remove a parameters to a given url
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: new_parameters
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: any_to_uri
  kind: function
  ns: scrapy.utils
  description: |-
    If given a path name, return its File URI, otherwise return it
    unmodified
  summary: If given a path name, return its File URI, otherwise return it
  signatures:
  - kind: positional
    name: uri_or_path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: canonicalize_url
  kind: function
  ns: scrapy.utils
  description: |-
    Canonicalize the given url by applying the following procedures:

    - make the URL safe
    - sort query arguments, first by key, then by value
    - normalize all spaces (in query arguments) '+' (plus symbol)
    - normalize percent encodings case (%2f -> %2F)
    - remove query arguments with blank values (unless `keep_blank_values` is True)
    - remove fragments (unless `keep_fragments` is True)

    The url passed can be bytes or unicode, while the url returned is
    always a native str (bytes in Python 2, unicode in Python 3).

    >>> import w3lib.url
    >>>
    >>> # sorting query arguments
    >>> w3lib.url.canonicalize_url('http://www.example.com/do?c=3&b=5&b=2&a=50')
    'http://www.example.com/do?a=50&b=2&b=5&c=3'
    >>>
    >>> # UTF-8 conversion + percent-encoding of non-ASCII characters
    >>> w3lib.url.canonicalize_url('http://www.example.com/r\u00e9sum\u00e9')
    'http://www.example.com/r%C3%A9sum%C3%A9'
    >>>

    For more examples, see the tests in `tests/test_url.py`.
  summary: 'Canonicalize the given url by applying the following procedures:'
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: keep_blank_values
    default: 'True'
    rest: false
  - kind: positional
    name: keep_fragments
    default: 'False'
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: cast
  kind: function
  ns: scrapy.utils
  description: |-
    Cast a value to a type.

    This returns the value unchanged.  To the type checker this
    signals that the return value has the designated type, but at
    runtime we intentionally don't check anything (we want this
    to be as fast as possible).
  summary: Cast a value to a type
  signatures:
  - kind: positional
    name: typ
    default: null
    rest: false
  - kind: positional
    name: val
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: escape_ajax
  kind: function
  ns: scrapy.utils
  description: |-
    Return the crawlable url according to:
    https://developers.google.com/webmasters/ajax-crawling/docs/getting-started

    >>> escape_ajax("www.example.com/ajax.html#!key=value")
    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'
    >>> escape_ajax("www.example.com/ajax.html?k1=v1&k2=v2#!key=value")
    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'
    >>> escape_ajax("www.example.com/ajax.html?#!key=value")
    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'
    >>> escape_ajax("www.example.com/ajax.html#!")
    'www.example.com/ajax.html?_escaped_fragment_='

    URLs that are not "AJAX crawlable" (according to Google) returned as-is:

    >>> escape_ajax("www.example.com/ajax.html#key=value")
    'www.example.com/ajax.html#key=value'
    >>> escape_ajax("www.example.com/ajax.html#")
    'www.example.com/ajax.html#'
    >>> escape_ajax("www.example.com/ajax.html")
    'www.example.com/ajax.html'
  summary: 'Return the crawlable url according to:'
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: file_uri_to_path
  kind: function
  ns: scrapy.utils
  description: |-
    Convert File URI to local filesystem path according to:
    http://en.wikipedia.org/wiki/File_URI_scheme
  summary: 'Convert File URI to local filesystem path according to:'
  signatures:
  - kind: positional
    name: uri
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: guess_scheme
  kind: function
  ns: scrapy.utils
  description: |-
    Add an URL scheme if missing: file:// for filepath-like input or
    http:// otherwise.
  summary: 'Add an URL scheme if missing: file:// for filepath-like input or'
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: is_url
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: parse_data_uri
  kind: function
  ns: scrapy.utils
  description: 'Parse a data: URI into :class:`ParseDataURIResult`.'
  summary: 'Parse a data: URI into :class:`ParseDataURIResult`'
  signatures:
  - kind: positional
    name: uri
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: parse_url
  kind: function
  ns: scrapy.utils
  description: |-
    Return urlparsed url from the given argument (which could be an already
    parsed url)
  summary: Return urlparsed url from the given argument (which could be an already
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - type: '?'
  inherits_from: null
- name: path_to_file_uri
  kind: function
  ns: scrapy.utils
  description: |-
    Convert local filesystem path to legal File URIs as described in:
    http://en.wikipedia.org/wiki/File_URI_scheme
  summary: 'Convert local filesystem path to legal File URIs as described in:'
  signatures:
  - kind: positional
    name: path
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: safe_download_url
  kind: function
  ns: scrapy.utils
  description: |-
    Make a url for download. This will call safe_url_string
    and then strip the fragment, if one exists. The path will
    be normalised.

    If the path is outside the document root, it will be changed
    to be within the document root.
  summary: Make a url for download
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: utf8
    rest: false
  - kind: positional
    name: path_encoding
    default: utf8
    rest: false
  - type: '?'
  inherits_from: null
- name: safe_url_string
  kind: function
  ns: scrapy.utils
  description: |-
    Return a URL equivalent to *url* that a wide range of web browsers and
    web servers consider valid.

    *url* is parsed according to the rules of the `URL living standard`_,
    and during serialization additional characters are percent-encoded to make
    the URL valid by additional URL standards.

    .. _URL living standard: https://url.spec.whatwg.org/

    The returned URL should be valid by *all* of the following URL standards
    known to be enforced by modern-day web browsers and web servers:

    -   `URL living standard`_

    -   `RFC 3986`_

    -   `RFC 2396`_ and `RFC 2732`_, as interpreted by `Java 8s java.net.URI
        class`_.

    .. _Java 8s java.net.URI class: https://docs.oracle.com/javase/8/docs/api/java/net/URI.html
    .. _RFC 2396: https://www.ietf.org/rfc/rfc2396.txt
    .. _RFC 2732: https://www.ietf.org/rfc/rfc2732.txt
    .. _RFC 3986: https://www.ietf.org/rfc/rfc3986.txt

    If a bytes URL is given, it is first converted to `str` using the given
    encoding (which defaults to 'utf-8'). If quote_path is True (default),
    path_encoding ('utf-8' by default) is used to encode URL path component
    which is then quoted. Otherwise, if quote_path is False, path component
    is not encoded or quoted. Given encoding is used for query string
    or form data.

    When passing an encoding, you should use the encoding of the
    original page (the page from which the URL was extracted from).

    Calling this function on an already "safe" URL will return the URL
    unmodified.
  summary: Return a URL equivalent to *url* that a wide range of web browsers and
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: utf8
    rest: false
  - kind: positional
    name: path_encoding
    default: utf8
    rest: false
  - kind: positional
    name: quote_path
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: strip_url
  kind: function
  ns: scrapy.utils
  description: |-
    Strip URL string from some of its components:

    - ``strip_credentials`` removes "user:password@"
    - ``strip_default_port`` removes ":80" (resp. ":443", ":21")
      from http:// (resp. https://, ftp://) URLs
    - ``origin_only`` replaces path component with "/", also dropping
      query and fragment components ; it also strips credentials
    - ``strip_fragment`` drops any #fragment component
  summary: 'Strip URL string from some of its components:'
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: strip_credentials
    default: 'True'
    rest: false
  - kind: positional
    name: strip_default_port
    default: 'True'
    rest: false
  - kind: positional
    name: origin_only
    default: 'False'
    rest: false
  - kind: positional
    name: strip_fragment
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: to_unicode
  kind: function
  ns: scrapy.utils
  description: |-
    Return the unicode representation of a bytes object ``text``. If
    ``text`` is already an unicode object, return it as-is.
  summary: Return the unicode representation of a bytes object ``text``
  signatures:
  - kind: positional
    name: text
    default: null
    rest: false
  - kind: positional
    name: encoding
    default: None
    rest: false
  - kind: positional
    name: errors
    default: strict
    rest: false
  - type: '?'
  inherits_from: null
- name: url_has_any_extension
  kind: function
  ns: scrapy.utils
  description: Return True if the url ends with one of the extensions provided
  summary: Return True if the url ends with one of the extensions provided
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: extensions
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: url_is_from_any_domain
  kind: function
  ns: scrapy.utils
  description: Return True if the url belongs to any of the given domains
  summary: Return True if the url belongs to any of the given domains
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: domains
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: url_is_from_spider
  kind: function
  ns: scrapy.utils
  description: Return True if the url belongs to the given spider
  summary: Return True if the url belongs to the given spider
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: spider
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: url_query_cleaner
  kind: function
  ns: scrapy.utils
  description: |-
    Clean URL arguments leaving only those passed in the parameterlist keeping order

    >>> import w3lib.url
    >>> w3lib.url.url_query_cleaner("product.html?id=200&foo=bar&name=wired", ('id',))
    'product.html?id=200'
    >>> w3lib.url.url_query_cleaner("product.html?id=200&foo=bar&name=wired", ['id', 'name'])
    'product.html?id=200&name=wired'
    >>>

    If `unique` is ``False``, do not remove duplicated keys

    >>> w3lib.url.url_query_cleaner("product.html?d=1&e=b&d=2&d=3&other=other", ['d'], unique=False)
    'product.html?d=1&d=2&d=3'
    >>>

    If `remove` is ``True``, leave only those **not in parameterlist**.

    >>> w3lib.url.url_query_cleaner("product.html?id=200&foo=bar&name=wired", ['id'], remove=True)
    'product.html?foo=bar&name=wired'
    >>> w3lib.url.url_query_cleaner("product.html?id=2&foo=bar&name=wired", ['id', 'foo'], remove=True)
    'product.html?name=wired'
    >>>

    By default, URL fragments are removed. If you need to preserve fragments,
    pass the ``keep_fragments`` argument as ``True``.

    >>> w3lib.url.url_query_cleaner('http://domain.tld/?bla=123#123123', ['bla'], remove=True, keep_fragments=True)
    'http://domain.tld/#123123'
  summary: Clean URL arguments leaving only those passed in the parameterlist keeping order
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: parameterlist
    default: ()
    rest: false
  - kind: positional
    name: sep
    default: '&'
    rest: false
  - kind: positional
    name: kvsep
    default: =
    rest: false
  - kind: positional
    name: remove
    default: 'False'
    rest: false
  - kind: positional
    name: unique
    default: 'True'
    rest: false
  - kind: positional
    name: keep_fragments
    default: 'False'
    rest: false
  - type: '?'
  inherits_from: null
- name: url_query_parameter
  kind: function
  ns: scrapy.utils
  description: |-
    Return the value of a url parameter, given the url and parameter name

    General case:

    >>> import w3lib.url
    >>> w3lib.url.url_query_parameter("product.html?id=200&foo=bar", "id")
    '200'
    >>>

    Return a default value if the parameter is not found:

    >>> w3lib.url.url_query_parameter("product.html?id=200&foo=bar", "notthere", "mydefault")
    'mydefault'
    >>>

    Returns None if `keep_blank_values` not set or 0 (default):

    >>> w3lib.url.url_query_parameter("product.html?id=", "id")
    >>>

    Returns an empty string if `keep_blank_values` set to 1:

    >>> w3lib.url.url_query_parameter("product.html?id=", "id", keep_blank_values=1)
    ''
    >>>
  summary: Return the value of a url parameter, given the url and parameter name
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: parameter
    default: null
    rest: false
  - kind: positional
    name: default
    default: None
    rest: false
  - kind: positional
    name: keep_blank_values
    default: '0'
    rest: false
  - type: '?'
  inherits_from: null
- name: urldefrag
  kind: function
  ns: scrapy.utils
  description: |-
    Removes any existing fragment from URL.

    Returns a tuple of the defragmented URL and the fragment.  If
    the URL contained no fragments, the second element is the
    empty string.
  summary: Removes any existing fragment from URL
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: urlparse
  kind: function
  ns: scrapy.utils
  description: |-
    Parse a URL into 6 components:
    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    The result is a named 6-tuple with fields corresponding to the
    above. It is either a ParseResult or ParseResultBytes object,
    depending on the type of the url parameter.

    The username, password, hostname, and port sub-components of netloc
    can also be accessed as attributes of the returned object.

    The scheme argument provides the default value of the scheme
    component when no scheme is found in url.

    If allow_fragments is False, no attempt is made to separate the
    fragment component from the previous component, which can be either
    path or query.

    Note that % escapes are not expanded.
  summary: 'Parse a URL into 6 components:'
  signatures:
  - kind: positional
    name: url
    default: null
    rest: false
  - kind: positional
    name: scheme
    default: null
    rest: false
  - kind: positional
    name: allow_fragments
    default: 'True'
    rest: false
  - type: '?'
  inherits_from: null
- name: urlunparse
  kind: function
  ns: scrapy.utils
  description: |-
    Put a parsed URL back together again.  This may result in a
    slightly different, but equivalent URL, if the URL that was parsed
    originally had redundant delimiters, e.g. a ? with an empty query
    (the draft states that these are equivalent).
  summary: Put a parsed URL back together again
  signatures:
  - kind: positional
    name: components
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: versions
  kind: module
  ns: scrapy.utils
  description: null
  summary: ''
  signatures: null
  inherits_from: null
- name: List
  kind: callable
  ns: scrapy.utils
  description: A generic version of list.
  summary: A generic version of list
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: Tuple
  kind: callable
  ns: scrapy.utils
  description: |-
    Deprecated alias to builtins.tuple.

    Tuple[X, Y] is the cross-product type of X and Y.

    Example: Tuple[T1, T2] is a tuple of two elements corresponding
    to type variables T1 and T2.  Tuple[int, float, str] is a tuple
    of an int, a float and a string.

    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].
  summary: Deprecated alias to builtins
  signatures:
  - kind: positional
    name: self
    default: null
    rest: false
  - type: '?'
  inherits_from: null
- name: get_openssl_version
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
- name: scrapy_components_versions
  kind: function
  ns: scrapy.utils
  description: null
  summary: ''
  signatures:
  - type: '?'
  inherits_from: null
