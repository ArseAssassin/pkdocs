---
name: OpenTSDB
slug: opentsdb
text_format: markdown
generator: src:devdocs
generator_command: src:devdocs
version: null
copyright: |-
  © 2010–2016 The OpenTSDB Authors
  Licensed under the GNU LGPLv2.1+ and GPLv3+ licenses.
  http://opentsdb.net/docs/build/html/index.html
homepage: http://opentsdb.net/

---
- name: /api/aggregators
  id: api_http/aggregators
  summary: This endpoint simply lists the names of implemented aggregation functions used in timeseries queries
  description: "# /api/aggregators\n\nThis endpoint simply lists the names of implemented aggregation functions used in timeseries queries.\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\nThis endpoint does not require any parameters via query string or body.\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/aggregators\n```\n\n## Response\n\nThe response is an array of strings that are the names of aggregation functions that can be used in a timeseries query.\n\n### Example Response\n\n``` javascript\n[\n  \"min\",\n  \"sum\",\n  \"max\",\n  \"avg\",\n  \"dev\"\n]\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/aggregators.html](http://opentsdb.net/docs/build/html/api_http/aggregators.html)"
- name: /api/annotation
  id: api_http/annotation/index
  summary: These endpoints provides a means of adding, editing or deleting annotations stored in the OpenTSDB backend
  description: "# /api/annotation\n\nThese endpoints provides a means of adding, editing or deleting annotations stored in the OpenTSDB backend. Annotations are very basic objects used to record a note of an arbitrary event at some point, optionally associated with a timeseries. Annotations are not meant to be used as a tracking or event based system, rather they are useful for providing links to such systems by displaying a notice on graphs or via API query calls.\n\nWhen creating, modifying or deleting annotations, all changes will be propagated to the search plugin if configured.\n\n## Annotation API Endpoints\n\n- [/api/annotation/bulk](bulk)\n\nThe default `/annotation` endpoint deals with one notation at a time. The `/annotation/bulk` endpoint allows for adding or updating multiple annotations at a time.\n\n## Verbs\n\n- GET - Retrieve a single annotation\n- POST - Create or modify an annotation\n- PUT - Create or replace an annotation\n- DELETE - Delete an annotation\n\n## Requests\n\nAll annotations are identified by the `startTime` field and optionally the `tsuid` field. Each note can be global, meaning it is associated with all timeseries, or it can be local, meaning it's associated with a specific tsuid. If the tsuid is not supplied or has an empty value, the annotation is considered to be a global note.\n\n| Name        | Data Type | Required | Description                                                                                                                                    | Default | QS          | RW  | Example                          |\n|-------------|-----------|----------|------------------------------------------------------------------------------------------------------------------------------------------------|---------|-------------|-----|----------------------------------|\n| startTime   | Integer   | Required | A Unix epoch timestamp, in seconds, marking the time when the annotation event should be recorded                                              |         | start_time  | RW  | 1369141261                       |\n| endTime     | Integer   | Optional | An optional end time for the event if it has completed or been resolved                                                                        | 0       | end_time    | RW  | 1369141262                       |\n| tsuid       | String    | Optional | A TSUID if the annotation is associated with a timeseries. This may be null or empty if the note was for a global event                        |         | tsuid       | RW  | 000001000001000001               |\n| description | String    | Optional | A brief description of the event. As this may appear on GnuPlot graphs, the description should be very short, ideally less than 25 characters. |         | description | RW  | Network Outage                   |\n| notes       | String    | Optional | Detailed notes about the event                                                                                                                 |         | notes       | RW  | Switch \\#5 died and was replaced |\n| custom      | Map       | Optional | A key/value map to store custom fields and values                                                                                              | null    |             | RW  | *See Below*                      |\n\nNote\n\nCustom fields cannot be passed via query string. You must use the `POST` or `PUT` verbs.\n\nWarning\n\nIf your request uses `PUT`, any fields that you do not supply with the request will be overwritten with their default values. For example, the `description` field will be set to an empty string and the `custom` field will be reset to `null`.\n\n### Example GET Request\n\n``` python\nhttp://localhost:4242/api/annotation?start_time=1369141261&tsuid=000001000001000001\n```\n\n### Example POST Request\n\n``` javascript\n{\n  \"startTime\":\"1369141261\",\n  \"tsuid\":\"000001000001000001\",\n  \"description\": \"Testing Annotations\",\n  \"notes\": \"These would be details about the event, the description is just a summary\",\n  \"custom\": {\n    \"owner\": \"jdoe\",\n    \"dept\": \"ops\"\n  }\n}\n```\n\n## Response\n\nA successful response to a `GET`, `POST` or `PUT` request will return the full object with the requested changes. Successful `DELETE` calls will return with a `204` status code and no body content. When modifying data, if no changes were present, i.e. the call did not provide any data to store, the response will be a `304` without any body content. If the requested annotation did not exist in the system, a `404` will be returned with an error message. If invalid data was supplied a `400` error will be returned.\n\n### Example Response\n\n``` javascript\n{\n  \"tsuid\": \"000001000001000001\",\n  \"description\": \"Testing Annotations\",\n  \"notes\": \"These would be details about the event, the description is just a summary\",\n  \"custom\": {\n    \"owner\": \"jdoe\",\n    \"dept\": \"ops\"\n  },\n  \"endTime\": 0,\n  \"startTime\": 1369141261\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/annotation/index.html](http://opentsdb.net/docs/build/html/api_http/annotation/index.html)"
- name: /api/annotation/bulk
  id: api_http/annotation/bulk
  summary: 'NOTE: (Version 2.1) The bulk endpoint enables adding, updating or deleting multiple annotations in a single call'
  description: "# /api/annotation/bulk\n\n*NOTE: (Version 2.1)* The bulk endpoint enables adding, updating or deleting multiple annotations in a single call. Annotation updates must be sent over PUT or POST as content data. Query string requests are not supported for `POST` or `GET`. Each annotation is processed individually and synchronized with the backend. If one of the annotations has an error, such as a missing field, an exception will be returned and some of the annotations may not be written to storage. In such an event, the errant annotation should be fixed and all annotations sent again.\n\nAnnotations may also be deleted in bulk for a specified time span. If you supply a list of of one or more TSUIDs, annotations with a `start`` ``time` that falls within the specified timespan and belong to those TSUIDs will be removed. Alternatively the `global` flag can be set and any global annotations (those not associated with a time series) will be deleted within the range.\n\n## Verbs\n\n- POST - Create or modify annotations\n- PUT - Create or replace annotations\n- DELETE - Delete annotations within a time range\n\n## Requests\n\nFields for posting or updating annotations are documented at [*/api/annotation*](index)\n\nFields for a bulk delete request are defined below:\n\n| Name      | Data Type | Required | Description                                                                                                                                                                                             | Default | QS         | RW  | Example                                |\n|-----------|-----------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|------------|-----|----------------------------------------|\n| startTime | Integer   | Required | A timestamp for the start of the request. The timestamp may be relative or absolute as per [*Dates and Times*](../../user_guide/query/dates).                                                           |         | start_time | RO  | 1369141261                             |\n| endTime   | Integer   | Optional | An optional end time for the event if it has completed or been resolved. The timestamp may be relative or absolute as per [*Dates and Times*](../../user_guide/query/dates).                            |         | end_time   | RO  | 1369141262                             |\n| tsuids    | Array     | Optional | A list of TSUIDs with annotations that should be deleted. This may be empty or null (for JSON) in which case the `global` flag should be set. When using the query string, separate TSUIDs with commas. |         | tsuids     | RO  | 000001000001000001, 000001000001000002 |\n| global    | Boolean   | Optional | Whether or not global annotations should be deleted for the range                                                                                                                                       | false   | global     | RO  | true                                   |\n\nWarning\n\nIf your request uses `PUT`, any fields that you do not supply with the request will be overwritten with their default values. For example, the `description` field will be set to an empty string and the `custom` field will be reset to `null`.\n\n### Example POST/PUT Request\n\n``` javascript\n[\n  {\n  \"startTime\":\"1369141261\",\n  \"tsuid\":\"000001000001000001\",\n  \"description\": \"Testing Annotations\",\n  \"notes\": \"These would be details about the event, the description is just a summary\",\n  \"custom\": {\n    \"owner\": \"jdoe\",\n    \"dept\": \"ops\"\n  }\n  },\n  {\n  \"startTime\":\"1369141261\",\n  \"tsuid\":\"000001000001000002\",\n  \"description\": \"Second annotation on different TSUID\",\n  \"notes\": \"Additional details\"\n  }\n]\n```\n\n### Example DELETE QS Request\n\n``` python\n/api/annotation/bulk?start_time=1d-ago&end_time=1h-ago&method_override=delete&tsuids=000001000001000001,000001000001000002\n```\n\n### Example DELETE Request\n\n``` javascript\n{\n  \"tsuids\": [\n    \"000001000001000001\",\n    \"000001000001000002\"\n  ],\n  \"global\": false,\n  \"startTime\": 1389740544690,\n  \"endTime\": 1389823344698,\n  \"totalDeleted\": 0\n}\n```\n\n## Response\n\nA successful response to a `POST` or `PUT` request will return the list of annotations after synchronization (i.e. if issuing a `POST` call, existing objects will be merged with the new objects). Delete requests will return an object with the delete query and a `totalDeleted` field with an integer number reflecting the total number of annotations deleted. If invalid data was supplied a `400` error will be returned along with the specific annotation that caused the error in the `details` field of the error object.\n\n### Example POST/PUT Response\n\n``` javascript\n[\n  {\n    \"tsuid\": \"000001000001000001\",\n    \"description\": \"Testing Annotations\",\n    \"notes\": \"These would be details about the event, the description is just a summary\",\n    \"custom\": {\n      \"owner\": \"jdoe\",\n      \"dept\": \"ops\"\n    },\n    \"endTime\": 0,\n    \"startTime\": 1369141261\n  },\n  {\n    \"tsuid\": \"000001000001000002\",\n    \"description\": \"Second annotation on different TSUID\",\n    \"notes\": \"Additional details\",\n    \"custom\": null,\n    \"endTime\": 0,\n    \"startTime\": 1369141261\n  }\n]\n```\n\n### Example DELETE Response\n\n``` javascript\n{\n  \"tsuids\": [\n    \"000001000001000001\",\n    \"000001000001000002\"\n  ],\n  \"global\": false,\n  \"startTime\": 1389740544690,\n  \"endTime\": 1389823344698,\n  \"totalDeleted\": 42\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/annotation/bulk.html](http://opentsdb.net/docs/build/html/api_http/annotation/bulk.html)"
- name: /api/config
  id: api_http/config/index
  summary: This endpoint returns information about the running configuration of the TSD
  description: "# /api/config\n\nThis endpoint returns information about the running configuration of the TSD. It is read only and cannot be used to set configuration options.\n\n## Conf API Endpoints\n\n- [/api/config/filters](filters)\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\nThis endpoint does not require any parameters via query string or body.\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/config\n```\n\n## Response\n\nThe response is a hash map of configuration properties and values.\n\n### Example Response\n\n``` javascript\n{\n  \"tsd.search.elasticsearch.tsmeta_type\": \"tsmetadata\",\n  \"tsd.storage.flush_interval\": \"1000\",\n  \"tsd.network.tcp_no_delay\": \"true\",\n  \"tsd.search.tree.indexer.enable\": \"true\",\n  \"tsd.http.staticroot\": \"/usr/local/opentsdb/staticroot/\",\n  \"tsd.network.bind\": \"0.0.0.0\",\n  \"tsd.network.worker_threads\": \"\",\n  \"tsd.storage.hbase.zk_quorum\": \"localhost\",\n  \"tsd.network.port\": \"4242\",\n  \"tsd.rpcplugin.DummyRPCPlugin.port\": \"42\",\n  \"tsd.search.elasticsearch.hosts\": \"localhost\",\n  \"tsd.network.async_io\": \"true\",\n  \"tsd.rtpublisher.plugin\": \"net.opentsdb.tsd.RabbitMQPublisher\",\n  \"tsd.search.enableindexer\": \"false\",\n  \"tsd.rtpublisher.rabbitmq.user\": \"guest\",\n  \"tsd.search.enable\": \"false\",\n  \"tsd.search.plugin\": \"net.opentsdb.search.ElasticSearch\",\n  \"tsd.rtpublisher.rabbitmq.hosts\": \"localhost\",\n  \"tsd.core.tree.enable_processing\": \"false\",\n  \"tsd.stats.canonical\": \"true\",\n  \"tsd.http.cachedir\": \"/tmp/opentsdb/\",\n  \"tsd.http.request.max_chunk\": \"16384\",\n  \"tsd.http.show_stack_trace\": \"true\",\n  \"tsd.core.auto_create_metrics\": \"true\",\n  \"tsd.storage.enable_compaction\": \"true\",\n  \"tsd.rtpublisher.rabbitmq.pass\": \"guest\",\n  \"tsd.core.meta.enable_tracking\": \"true\",\n  \"tsd.mq.enable\": \"true\",\n  \"tsd.rtpublisher.rabbitmq.vhost\": \"/\",\n  \"tsd.storage.hbase.data_table\": \"tsdb\",\n  \"tsd.storage.hbase.uid_table\": \"tsdb-uid\",\n  \"tsd.http.request.enable_chunked\": \"true\",\n  \"tsd.core.plugin_path\": \"/usr/local/opentsdb/plugins\",\n  \"tsd.storage.hbase.zk_basedir\": \"/hbase\",\n  \"tsd.rtpublisher.enable\": \"false\",\n  \"tsd.rpcplugin.DummyRPCPlugin.hosts\": \"localhost\",\n  \"tsd.storage.hbase.tree_table\": \"tsdb-tree\",\n  \"tsd.network.keep_alive\": \"true\",\n  \"tsd.network.reuse_address\": \"true\",\n  \"tsd.rpc.plugins\": \"net.opentsdb.tsd.DummyRpcPlugin\"\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/config/index.html](http://opentsdb.net/docs/build/html/api_http/config/index.html)"
- name: /api/config/filters
  id: api_http/config/filters
  summary: (Version 2.2 and later) This endpoint lists the various filters loaded by the TSD and some information about how to use them
  description: "# /api/config/filters\n\n**(Version 2.2 and later)** This endpoint lists the various filters loaded by the TSD and some information about how to use them.\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\nThis endpoint does not require any parameters via query string or body.\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/config/filters\n```\n\n## Response\n\nThe response is a map of filter names or types and sub maps of examples and descriptions. The examples show how to use them in both URI and JSON queries.\n\n### Example Response\n\n``` javascript\n{\n  \"iliteral_or\": {\n    \"examples\": \"host=iliteral_or(web01),  host=iliteral_or(web01|web02|web03)  {\\\"type\\\":\\\"iliteral_or\\\",\\\"tagk\\\":\\\"host\\\",\\\"filter\\\":\\\"web01|web02|web03\\\",\\\"groupBy\\\":false}\",\n    \"description\": \"Accepts one or more exact values and matches if the series contains any of them. Multiple values can be included and must be separated by the | (pipe) character. The filter is case insensitive and will not allow characters that TSDB does not allow at write time.\"\n  },\n  \"wildcard\": {\n    \"examples\": \"host=wildcard(web*),  host=wildcard(web*.tsdb.net)  {\\\"type\\\":\\\"wildcard\\\",\\\"tagk\\\":\\\"host\\\",\\\"filter\\\":\\\"web*.tsdb.net\\\",\\\"groupBy\\\":false}\",\n    \"description\": \"Performs pre, post and in-fix glob matching of values. The globs are case sensitive and multiple wildcards can be used. The wildcard character is the * (asterisk). At least one wildcard must be present in the filter value. A wildcard by itself can be used as well to match on any value for the tag key.\"\n  },\n  \"not_literal_or\": {\n    \"examples\": \"host=not_literal_or(web01),  host=not_literal_or(web01|web02|web03)  {\\\"type\\\":\\\"not_literal_or\\\",\\\"tagk\\\":\\\"host\\\",\\\"filter\\\":\\\"web01|web02|web03\\\",\\\"groupBy\\\":false}\",\n    \"description\": \"Accepts one or more exact values and matches if the series does NOT contain any of them. Multiple values can be included and must be separated by the | (pipe) character. The filter is case sensitive and will not allow characters that TSDB does not allow at write time.\"\n  },\n  \"not_iliteral_or\": {\n    \"examples\": \"host=not_iliteral_or(web01),  host=not_iliteral_or(web01|web02|web03)  {\\\"type\\\":\\\"not_iliteral_or\\\",\\\"tagk\\\":\\\"host\\\",\\\"filter\\\":\\\"web01|web02|web03\\\",\\\"groupBy\\\":false}\",\n    \"description\": \"Accepts one or more exact values and matches if the series does NOT contain any of them. Multiple values can be included and must be separated by the | (pipe) character. The filter is case insensitive and will not allow characters that TSDB does not allow at write time.\"\n  },\n  \"not_key\": {\n    \"examples\": \"host=not_key()  {\\\"type\\\":\\\"not_key\\\",\\\"tagk\\\":\\\"host\\\",\\\"filter\\\":\\\"\\\",\\\"groupBy\\\":false}\",\n    \"description\": \"Skips any time series with the given tag key, regardless of the value. This can be useful for situations where a metric has inconsistent tag sets. NOTE: The filter value must be null or an empty string.\"\n  },\n  \"iwildcard\": {\n    \"examples\": \"host=iwildcard(web*),  host=iwildcard(web*.tsdb.net)  {\\\"type\\\":\\\"iwildcard\\\",\\\"tagk\\\":\\\"host\\\",\\\"filter\\\":\\\"web*.tsdb.net\\\",\\\"groupBy\\\":false}\",\n    \"description\": \"Performs pre, post and in-fix glob matching of values. The globs are case insensitive and multiple wildcards can be used. The wildcard character is the * (asterisk). Case insensitivity is achieved by dropping all values to lower case. At least one wildcard must be present in the filter value. A wildcard by itself can be used as well to match on any value for the tag key.\"\n  },\n  \"literal_or\": {\n    \"examples\": \"host=literal_or(web01),  host=literal_or(web01|web02|web03)  {\\\"type\\\":\\\"literal_or\\\",\\\"tagk\\\":\\\"host\\\",\\\"filter\\\":\\\"web01|web02|web03\\\",\\\"groupBy\\\":false}\",\n    \"description\": \"Accepts one or more exact values and matches if the series contains any of them. Multiple values can be included and must be separated by the | (pipe) character. The filter is case sensitive and will not allow characters that TSDB does not allow at write time.\"\n  },\n  \"regexp\": {\n    \"examples\": \"host=regexp(.*)  {\\\"type\\\":\\\"regexp\\\",\\\"tagk\\\":\\\"host\\\",\\\"filter\\\":\\\".*\\\",\\\"groupBy\\\":false}\",\n    \"description\": \"Provides full, POSIX compliant regular expression using the built in Java Pattern class. Note that an expression containing curly braces {} will not parse properly in URLs. If the pattern is not a valid regular expression then an exception will be raised.\"\n  }\n```\n\n}\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/config/filters.html](http://opentsdb.net/docs/build/html/api_http/config/filters.html)"
- name: /api/dropcaches
  id: api_http/dropcaches
  summary: This endpoint purges the in-memory data cached in OpenTSDB
  description: "# /api/dropcaches\n\nThis endpoint purges the in-memory data cached in OpenTSDB. This includes all UID to name and name to UID maps for metrics, tag names and tag values.\n\nNote\n\nThis endpoint does not purge the on-disk temporary cache where graphs and other files are stored.\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\nThis endpoint does not require any parameters via query string or body.\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/dropcaches\n```\n\n## Response\n\nThe response is a hash map of information. Unless something goes wrong, this should always result in a `status` of `200` and a message of `Caches`` ``dropped`.\n\n### Example Response\n\n``` javascript\n{\n  \"message\": \"Caches dropped\",\n  \"status\": \"200\"\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/dropcaches.html](http://opentsdb.net/docs/build/html/api_http/dropcaches.html)"
- name: /api/put
  id: api_http/put
  summary: This endpoint allows for storing data in OpenTSDB over HTTP as an alternative to the Telnet interface
  description: "# /api/put\n\nThis endpoint allows for storing data in OpenTSDB over HTTP as an alternative to the Telnet interface. Put requests can only be performed via content associated with the POST method. The format of the content is dependent on the serializer selected. However there are some common parameters and responses as documented below.\n\nTo save on bandwidth, the put API allows clients to store multiple data points in a single request. The data points do not have to be related in any way. Each data point is processed individually and an error with one piece of data will not affect the storing of good data. This means if your request has 100 data points and 1 of them has an error, 99 data points will still be written and one will be rejected. See the Response section below for details on determining what data point was not stored.\n\nNote\n\nIf the content you provide with the request cannot be parsed, such JSON content missing a quotation mark or curly brace, then all of the datapoints will be discarded. The API will return an error with details about what went wrong.\n\nWhile the API does support multiple data points per request, the API will not return until every one has been processed. That means metric and tag names/values must be verified, the value parsed and the data queued for storage. If your put request has a large number of data points, it may take a long time for the API to respond, particularly if OpenTSDB has to assign UIDs to tag names or values. Therefore it is a good idea to limit the maximum number of data points per request; 50 per request is a good starting point.\n\nAnother recommendation is to enable keep-alives on your HTTP client so that you can re-use your connection to the server every time you put data.\n\nNote\n\nWhen using HTTP for puts, you may need to enable support for chunks if your HTTP client automatically breaks large requests into smaller packets. For example, CURL will break up messages larger than 2 or 3 data points and by default, OpenTSDB disables chunk support. Enable it by setting `tsd.http.request.enable_chunked` to true in the config file.\n\nNote\n\nIf the `tsd.mode` is set to `ro`, the `/api/put` endpoint will be unavailable and all calls will return a 404 error.\n\n## Verbs\n\n- POST\n\n## Requests\n\nSome query string parameters can be supplied that alter the response to a put request:\n\n| Name         | Data Type | Required | Description                                                                                                                                                                                                                                                                                                               | Default | QS           | RW  | Example                           |\n|--------------|-----------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|--------------|-----|-----------------------------------|\n| summary      | Present   | Optional | Whether or not to return summary information                                                                                                                                                                                                                                                                              | false   | summary      |     | /api/put?summary                  |\n| details      | Present   | Optional | Whether or not to return detailed information                                                                                                                                                                                                                                                                             | false   | details      |     | /api/put?details                  |\n| sync         | Boolean   | Optional | Whether or not to wait for the data to be flushed to storage before returning the results.                                                                                                                                                                                                                                | false   | sync         |     | /api/put?sync                     |\n| sync_timeout | Integer   | Optional | A timeout, in milliseconds, to wait for the data to be flushed to storage before returning with an error. When a timeout occurs, using the `details` flag will tell how many data points failed and how many succeeded. `sync` must also be given for this to take effect. A value of 0 means the write will not timeout. | 0       | sync_timeout |     | /api/put/?sync&sync_timeout=60000 |\n\nIf both `detailed` and `summary` are present in a query string, the API will respond with `detailed` information.\n\nThe fields and examples below refer to the default JSON serializer.\n\n| Name      | Data Type              | Required | Description                                                                                                                                      | Default | QS  | RW  | Example          |\n|-----------|------------------------|----------|--------------------------------------------------------------------------------------------------------------------------------------------------|---------|-----|-----|------------------|\n| metric    | String                 | Required | The name of the metric you are storing                                                                                                           |         |     | W   | sys.cpu.nice     |\n| timestamp | Integer                | Required | A Unix epoch style timestamp in seconds or milliseconds. The timestamp must not contain non-numeric characters.                                  |         |     | W   | 1365465600       |\n| value     | Integer, Float, String | Required | The value to record for this data point. It may be quoted or not quoted and must conform to the OpenTSDB value rules: `../../user_guide/writing` |         |     | W   | 42.5             |\n| tags      | Map                    | Required | A map of tag name/tag value pairs. At least one pair must be supplied.                                                                           |         |     | W   | {\"host\":\"web01\"} |\n\n### Example Single Data Point Put\n\nYou can supply a single data point in a request:\n\n``` javascript\n{\n  \"metric\": \"sys.cpu.nice\",\n  \"timestamp\": 1346846400,\n  \"value\": 18,\n  \"tags\": {\n     \"host\": \"web01\",\n     \"dc\": \"lga\"\n  }\n}\n```\n\n### Example Multiple Data Point Put\n\nMultiple data points must be encased in an array:\n\n``` javascript\n[\n  {\n    \"metric\": \"sys.cpu.nice\",\n    \"timestamp\": 1346846400,\n    \"value\": 18,\n    \"tags\": {\n       \"host\": \"web01\",\n       \"dc\": \"lga\"\n    }\n  },\n  {\n    \"metric\": \"sys.cpu.nice\",\n    \"timestamp\": 1346846400,\n    \"value\": 9,\n    \"tags\": {\n       \"host\": \"web02\",\n       \"dc\": \"lga\"\n    }\n  }\n]\n```\n\n## Response\n\nBy default, the put endpoint will respond with a `204` HTTP status code and no content if all data points were stored successfully. If one or more datapoints had an error, the API will return a `400` with an error message in the content.\n\nFor debugging purposes, you can ask for the response to include a summary of how many data points were stored successfully and failed, or get details about what data points could not be stored and why so that you can fix your client code. Also, errors with a data point will be logged in the TSD's log file so you can look there for issues.\n\nFields present in `summary` or `detailed` responses include:\n\n| Name    | Data Type | Description                                                                                  |\n|---------|-----------|----------------------------------------------------------------------------------------------|\n| success | Integer   | The number of data points that were queued successfully for storage                          |\n| failed  | Integer   | The number of data points that could not be queued for storage                               |\n| errors  | Array     | A list of data points that failed be queued and why. Present in the `details` response only. |\n\n### Example Response with Summary\n\n``` javascript\n{\n  \"failed\": 1,\n  \"success\": 0\n}\n```\n\n### Example Response With Details\n\n``` javascript\n{\n  \"errors\": [\n    {\n      \"datapoint\": {\n        \"metric\": \"sys.cpu.nice\",\n        \"timestamp\": 1365465600,\n        \"value\": \"NaN\",\n        \"tags\": {\n          \"host\": \"web01\"\n        }\n      },\n      \"error\": \"Unable to parse value to a number\"\n    }\n  ],\n  \"failed\": 1,\n  \"success\": 0\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/put.html](http://opentsdb.net/docs/build/html/api_http/put.html)"
- name: /api/query
  id: api_http/query/index
  summary: Probably the most useful endpoint in the API, /api/query enables extracting data from the storage system in various formats determined by the serializer selected
  description: "# /api/query\n\nProbably the most useful endpoint in the API, `/api/query` enables extracting data from the storage system in various formats determined by the serializer selected. Queries can be submitted via the 1.0 query string format or body content.\n\n## Query API Endpoints\n\n- [/api/query/exp](exp)\n- [/api/query/gexp](gexp)\n- [/api/query/last](last)\n\nThe `/query` endpoint is documented below. As of 2.2 data matching a query can be deleted by using the `DELETE` verb. The configuration parameter `tsd.http.query.allow_delete` must be enabled to allow deletions. Data that is deleted will be returned in the query results. Executing the query a second time should return empty results.\n\nWarning\n\nDeleting data is permanent. Also beware that when deleting, some data outside the boundaries of the start and end times may be deleted as data is stored on an hourly basis.\n\n## Verbs\n\n- GET\n- POST\n- DELETE\n\n## Requests\n\nRequest parameters include:\n\n| Name                 | Data Type       | Required | Description                                                                                                                                                                                                                                                                                  | Default        | QS                 | RW  | Example     |\n|----------------------|-----------------|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|--------------------|-----|-------------|\n| start                | String, Integer | Required | The start time for the query. This can be a relative or absolute timestamp. See [*Querying or Reading Data*](../../user_guide/query/index) for details.                                                                                                                                      |                | start              |     | 1h-ago      |\n| end                  | String, Integer | Optional | An end time for the query. If not supplied, the TSD will assume the local system time on the server. This may be a relative or absolute timestamp. See [*Querying or Reading Data*](../../user_guide/query/index) for details.                                                               | *current time* | end                |     | 1s-ago      |\n| queries              | Array           | Required | One or more sub queries used to select the time series to return. These may be metric `m` or TSUID `tsuids` queries                                                                                                                                                                          |                | m or tsuids        |     | *See below* |\n| noAnnotations        | Boolean         | Optional | Whether or not to return annotations with a query. The default is to return annotations for the requested timespan but this flag can disable the return. This affects both local and global notes and overrides `globalAnnotations`                                                          | false          | no_annotations     |     | false       |\n| globalAnnotations    | Boolean         | Optional | Whether or not the query should retrieve global annotations for the requested timespan                                                                                                                                                                                                       | false          | global_annotations |     | true        |\n| msResolution (or ms) | Boolean         | Optional | Whether or not to output data point timestamps in milliseconds or seconds. The msResolution flag is recommended. If this flag is not provided and there are multiple data points within a second, those data points will be down sampled using the query's aggregation function.             | false          | ms                 |     | true        |\n| showTSUIDs           | Boolean         | Optional | Whether or not to output the TSUIDs associated with timeseries in the results. If multiple time series were aggregated into one set, multiple TSUIDs will be returned in a sorted manner                                                                                                     | false          | show_tsuids        |     | true        |\n| showSummary          | Boolean         | Optional | Whether or not to show a summary of timings surrounding the query in the results. This creates another object in the map that is unlike the data point objects.                                                                                                                              | false          | show_summary       |     | true        |\n| showQuery            | Boolean         | Optional | Whether or not to return the original sub query with the query results. If the request contains many sub queries then this is a good way to determine which results belong to which sub query. Note that in the case of a `*` or wildcard query, this can produce a lot of duplicate output. | false          | show_query         |     | true        |\n| delete               | Boolean         | Optional | Can be passed to the JSON with a POST to delete any data points that match the given query.                                                                                                                                                                                                  | false          |                    | W   | true        |\n\n### Sub Queries\n\nAn OpenTSDB query requires at least one sub query, a means of selecting which time series should be included in the result set. There are two types:\n\n- **Metric Query** - The full name of a metric is supplied along with an optional list of tags. This is optimized for aggregating multiple time series into one result.\n- **TSUID Query** - A list of one or more TSUIDs that share a common metric. This is optimized for fetching individual time series where aggregation is not required.\n\nA query can include more than one sub query and any mixture of the two types. When submitting a query via content body, if a list of TSUIDs is supplied, the metric and tags for that particular sub query will be ignored.\n\nEach sub query can retrieve individual or groups of timeseries data, performing aggregation or grouping calculations on each set. Fields for each sub query include:\n\n| Name                 | Data Type | Required | Description                                                                                                                                                                                                                                                                                                                             | Default     | Example     |\n|----------------------|-----------|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|-------------|\n| aggregator           | String    | Required | The name of an aggregation function to use. See [*/api/aggregators*](../aggregators)                                                                                                                                                                                                                                                    |             | sum         |\n| metric               | String    | Required | The name of a metric stored in the system                                                                                                                                                                                                                                                                                               |             | sys.cpu.0   |\n| rate                 | Boolean   | Optional | Whether or not the data should be converted into deltas before returning. This is useful if the metric is a continuously incrementing counter and you want to view the rate of change between data points.                                                                                                                              | false       | true        |\n| rateOptions          | Map       | Optional | Monotonically increasing counter handling options                                                                                                                                                                                                                                                                                       | *See below* | *See below* |\n| downsample           | String    | Optional | An optional downsampling function to reduce the amount of data returned.                                                                                                                                                                                                                                                                | *See below* | 5m-avg      |\n| tags                 | Map       | Optional | To drill down to specific timeseries or group results by tag, supply one or more map values in the same format as the query string. Tags are converted to filters in 2.2. See the notes below about conversions. Note that if no tags are specified, all metrics in the system will be aggregated into the results. *Deprecated in 2.2* |             | *See Below* |\n| filters *(2.2)*      | List      | Optional | Filters the time series emitted in the results. Note that if no filters are specified, all time series for the given metric will be aggregated into the results.                                                                                                                                                                        |             | *See Below* |\n| explicitTags *(2.3)* | Boolean   | Optional | Returns the series that include only the tag keys provided in the filters.                                                                                                                                                                                                                                                              | false       | true        |\n\n*Rate Options*\n\nWhen passing rate options in a query string, the options must be enclosed in curly braces. For example: `m=sum:rate{counter,,1000}:if.octets.in`. If you wish to use the default `counterMax` but do want to supply a `resetValue`, you must add two commas as in the previous example. Additional fields in the `rateOptions` object include the following:\n\n| Name       | Data Type | Required | Description                                                                                                                                                                              | Default            | Example |\n|------------|-----------|----------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|---------|\n| counter    | Boolean   | Optional | Whether or not the underlying data is a monotonically increasing counter that may roll over                                                                                              | false              | true    |\n| counterMax | Integer   | Optional | A positive integer representing the maximum value for the counter.                                                                                                                       | Java Long.MaxValue | 65535   |\n| resetValue | Integer   | Optional | An optional value that, when exceeded, will cause the aggregator to return a `0` instead of the calculated rate. Useful when data sources are frequently reset to avoid spurious spikes. | 0                  | 65000   |\n\n*Downsampling*\n\nDownsample specifications const if an interval, a unit of time, an aggregator and (as of 2.2) an optional fill policy. The format of a downsample spec is:\n\n``` python\n<interval><units>-<aggregator>[-<fill policy>]\n```\n\nFor example:\n\n``` python\n1h-sum\n30m-avg-nan\n24h-max-zero\n```\n\nSee [*Aggregators*](../../user_guide/query/aggregators) for a list of supported fill policies.\n\n*Filters*\n\nNew for 2.2, OpenTSDB includes expanded and plugable filters across tag key and value combinations. For a list of filters loaded in the TSD, see [*/api/config/filters*](../config/filters). For descriptions of the built-in filters see [*Filters*](../../user_guide/query/filters). Filters can be used in both query string and POST formatted queries. Multiple filters on the same tag key are allowed and when processed, they are *ANDed* together e.g. if we have two filters `host=literal_or(web01)` and `host=literal_or(web02)` the query will always return empty. If two or more filters are included for the same tag key and one has group by enabled but another does not, then group by will effectively be true for all filters on that tag key. Fields for POST queries pertaining to filters include:\n\n| Name    | Data Type | Required | Description                                                                                                                                                 | Default | Example           |\n|---------|-----------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|-------------------|\n| type    | String    | Required | The name of the filter to invoke. See [*/api/config/filters*](../config/filters)                                                                            |         | regexp            |\n| tagk    | String    | Required | The tag key to invoke the filter on                                                                                                                         |         | host              |\n| filter  | String    | Required | The filter expression to evaluate and depends on the filter being used                                                                                      |         | web.\\*.mysite.com |\n| groupBy | Boolean   | Optional | Whether or not to group the results by each value matched by the filter. By default all values matching the filter will be aggregated into a single series. | false   | true              |\n\nFor URI queries, the type precedes the filter expression in parentheses. The format is `<tagk>=<type>(<filter_expression>)`. Whether or not results are grouped depends on which curly bracket the filter is in. Two curly braces are now supported per metric query. The first set is the *group by* filter and the second is a *non group by* filter, e.g. `{host=wildcard(web*)}{colo=regexp(sjc.*)}`. This specifies any metrics where the colo matches the regex expression \"sjc.\\*\" and the host tag value starts with \"web\" and the results are grouped by host. If you only want to filter without grouping then the first curly set must be empty, e.g. `{}{host=wildcard(web*),colo=regexp(sjc.*)}`. This specifies nany metrics where colo matches the regex expression \"sjc.\\*\" and the host tag value starts with \"web\" and the results are not grouped.\n\nNote\n\nRegular expression, wildcard filters with a pre/post/in-fix or literal ors with many values can cause queries to return slower as each row of data must be resolved to their string values then processed.\n\nNote\n\nWhen submitting a JSON query to OpenTSDB 2.2 or later, use either `tags` OR `filters`. Only one will take effect and the order is indeterminate as the JSON parser may deserialize one before the other. We recommend using filters for all future queries.\n\n*Filter Conversions*\n\nValues in the POST query `tags` map and the *group by* curly brace of URI queries are automatically converted to filters to provide backwards compatibility with existing systems. The auto conversions include:\n\n| Example                       | Description                                                                                                        |\n|-------------------------------|--------------------------------------------------------------------------------------------------------------------|\n| `<tagk>=*`                    | Wildcard filter, effectively makes sure the tag key is present in the series                                       |\n| `<tagk>=value`                | Case sensitive literal OR filter                                                                                   |\n| `<tagk>=value1|value2|valueN` | Case sensitive literal OR filter                                                                                   |\n| `<tagk>=va*`                  | Case insensitive wildcard filter. An asterisk (star) with any other strings now becomes a wildcard filter shortcut |\n\n### Metric Query String Format\n\nThe full specification for a metric query string sub query is as follows:\n\n``` python\nm=<aggregator>:[rate[{counter[,<counter_max>[,<reset_value>]]]}:][<down_sampler>:][explicit_tags:]<metric_name>[{<tag_name1>=<grouping filter>[,...<tag_nameN>=<grouping_filter>]}][{<tag_name1>=<non grouping filter>[,...<tag_nameN>=<non_grouping_filter>]}]\n```\n\nIt can be a little daunting at first but you can break it down into components. If you're ever confused, try using the built-in GUI to plot a graph the way you want it, then look at the URL to see how the query is formatted. Changes to any of the form fields will update the URL (which you can actually copy and paste to share with other users). For examples, please see [*Query Examples*](../../user_guide/query/examples).\n\n### TSUID Query String Format\n\nTSUID queries are simpler than Metric queries. Simply pass a list of one or more hexadecimal encoded TSUIDs separated by commas:\n\n``` python\ntsuid=<aggregator>:<tsuid1>[,...<tsuidN>]\n```\n\n### Example Query String Requests\n\n``` python\nhttp://localhost:4242/api/query?start=1h-ago&m=sum:rate:proc.stat.cpu{host=foo,type=idle}\nhttp://localhost:4242/api/query?start=1h-ago&tsuid=sum:000001000002000042,000001000002000043\n```\n\n### Example Content Request\n\nPlease see the serializer documentation for request information: [*HTTP Serializers*](../serializers/index). The following examples pertain to the default JSON serializer.\n\n``` javascript\n{\n  \"start\": 1356998400,\n  \"end\": 1356998460,\n  \"queries\": [\n    {\n      \"aggregator\": \"sum\",\n      \"metric\": \"sys.cpu.0\",\n      \"rate\": \"true\",\n      \"tags\": {\n        \"host\": \"*\",\n        \"dc\": \"lga\"\n      }\n    },\n    {\n      \"aggregator\": \"sum\",\n      \"tsuids\": [\n        \"000001000002000042\",\n        \"000001000002000043\"\n        ]\n      }\n    }\n  ]\n}\n```\n\n2.2 query with filters\n\n``` javascript\n{\n  \"start\": 1356998400,\n  \"end\": 1356998460,\n  \"queries\": [\n    {\n      \"aggregator\": \"sum\",\n      \"metric\": \"sys.cpu.0\",\n      \"rate\": \"true\",\n      \"filters\": [\n        {\n           \"type\":\"wildcard\",\n           \"tagk\":\"host\",\n           \"filter\":\"*\",\n           \"groupBy\":true\n        },\n        {\n           \"type\":\"literal_or\",\n           \"tagk\":\"dc\",\n           \"filter\":\"lga|lga1|lga2\",\n           \"groupBy\":false\n        },\n      ]\n    },\n    {\n      \"aggregator\": \"sum\",\n      \"tsuids\": [\n        \"000001000002000042\",\n        \"000001000002000043\"\n        ]\n      }\n    }\n  ]\n}\n```\n\n## Response\n\nThe output generated for a query depends heavily on the chosen serializer [*HTTP Serializers*](../serializers/index). A request may result in multiple sets of data returned, particularly if the request included multiple queries or grouping was requested. Some common fields included with each data set in the response will be:\n\n| Name              | Description                                                                                                                                                                                                                                                                                                 |\n|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| metric            | Name of the metric retrieved for the time series                                                                                                                                                                                                                                                            |\n| tags              | A list of tags only returned when the results are for a single time series. If results are aggregated, this value may be null or an empty map                                                                                                                                                               |\n| aggregatedTags    | If more than one timeseries were included in the result set, i.e. they were aggregated, this will display a list of tag names that were found in common across all time series.                                                                                                                             |\n| dps               | Retrieved data points after being processed by the aggregators. Each data point consists of a timestamp and a value, the format determined by the serializer.                                                                                                                                               |\n| annotations       | If the query retrieved annotations for timeseries over the requested timespan, they will be returned in this group. Annotations for every timeseries will be merged into one set and sorted by `start_time`. Aggregator functions do not affect annotations, all annotations will be returned for the span. |\n| globalAnnotations | If requested by the user, the query will scan for global annotations during the timespan and the results returned in this group                                                                                                                                                                             |\n\nUnless there was an error with the query, you will generally receive a `200` status with content. However if your query couldn't find any data, it will return an empty result set. In the case of the JSON serializer, the result will be an empty array:\n\n``` javascript\n[]\n```\n\nFor the JSON serializer, the timestamp will always be a Unix epoch style integer followed by the value as an integer or a floating point. For example, the default output is `\"dps\"{\"<timestamp>\":<value>}`. By default the timestamps will be in seconds. If the `msResolution` flag is set, then the timestamps will be in milliseconds.\n\n### Example Aggregated Default Response\n\n``` javascript\n[\n  {\n    \"metric\": \"tsd.hbase.puts\",\n    \"tags\": {},\n    \"aggregatedTags\": [\n      \"host\"\n    ],\n    \"annotations\": [\n      {\n        \"tsuid\": \"00001C0000FB0000FB\",\n        \"description\": \"Testing Annotations\",\n        \"notes\": \"These would be details about the event, the description is just a summary\",\n        \"custom\": {\n          \"owner\": \"jdoe\",\n          \"dept\": \"ops\"\n        },\n        \"endTime\": 0,\n        \"startTime\": 1365966062\n      }\n    ],\n    \"globalAnnotations\": [\n      {\n        \"description\": \"Notice\",\n        \"notes\": \"DAL was down during this period\",\n        \"custom\": null,\n        \"endTime\": 1365966164,\n        \"startTime\": 1365966064\n      }\n    ],\n    \"tsuids\": [\n      \"0023E3000002000008000006000001\"\n    ],\n    \"dps\": {\n      \"1365966001\": 25595461080,\n      \"1365966061\": 25595542522,\n      \"1365966062\": 25595543979,\n...\n      \"1365973801\": 25717417859\n    }\n  }\n]\n```\n\n### Example Aggregated Array Response\n\n``` javascript\n[\n  {\n    \"metric\": \"tsd.hbase.puts\",\n    \"tags\": {},\n    \"aggregatedTags\": [\n      \"host\"\n    ],\n    \"dps\": [\n      [\n        1365966001,\n        25595461080\n      ],\n      [\n        1365966061,\n        25595542522\n      ],\n...\n      [\n        1365974221,\n        25722266376\n      ]\n    ]\n  }\n]\n```\n\n### Example Multi-Set Response\n\nFor the following example, two TSDs were running and the query was: `http://localhost:4242/api/query?start=1h-ago&m=sum:tsd.hbase.puts{host=*}`. This returns two explicit time series.\n\n``` javascript\n[\n  {\n    \"metric\": \"tsd.hbase.puts\",\n    \"tags\": {\n      \"host\": \"tsdb-1.mysite.com\"\n    },\n    \"aggregatedTags\": [],\n    \"dps\": {\n      \"1365966001\": 3758788892,\n      \"1365966061\": 3758804070,\n...\n      \"1365974281\": 3778141673\n    }\n  },\n  {\n    \"metric\": \"tsd.hbase.puts\",\n    \"tags\": {\n      \"host\": \"tsdb-2.mysite.com\"\n    },\n    \"aggregatedTags\": [],\n    \"dps\": {\n      \"1365966001\": 3902179270,\n      \"1365966062\": 3902197769,\n...\n      \"1365974281\": 3922266478\n    }\n  }\n]\n```\n\n### Example With Show Summary and Query\n\n``` javascript\n[\n  {\n    \"metric\": \"tsd.hbase.puts\",\n    \"tags\": {},\n    \"aggregatedTags\": [\n      \"host\"\n    ],\n    \"query\": {\n      \"aggregator\": \"sum\",\n      \"metric\": \"tsd.hbase.puts\",\n      \"tsuids\": null,\n      \"downsample\": null,\n      \"rate\": true,\n      \"explicitTags\": false,\n      \"filters\": [\n        {\n          \"tagk\": \"host\",\n          \"filter\": \"*\",\n          \"group_by\": true,\n          \"type\": \"wildcard\"\n        }\n      ],\n      \"rateOptions\": null,\n      \"tags\": { }\n    },\n    \"dps\": {\n      \"1365966001\": 25595461080,\n      \"1365966061\": 25595542522,\n      \"1365966062\": 25595543979,\n...\n      \"1365973801\": 25717417859\n    }\n  },\n  {\n    \"statsSummary\": {\n      \"datapoints\": 0,\n      \"rawDatapoints\": 56,\n      \"aggregationTime\": 0,\n      \"serializationTime\": 20,\n      \"storageTime\": 6,\n      \"timeTotal\": 26\n    }\n  }\n]\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/query/index.html](http://opentsdb.net/docs/build/html/api_http/query/index.html)"
- name: /api/query/exp
  id: api_http/query/exp
  summary: This endpoint allows for querying data using expressions
  description: "# /api/query/exp\n\nThis endpoint allows for querying data using expressions. The query is broken up into different sections.\n\nTwo set operations (or Joins) are allowed. The union of all time series ore the intersection.\n\nFor example we can compute \"a + b\" with a group by on the host field. Both metrics queried alone would emit a time series per host, e.g. maybe one for \"web01\", \"web02\" and \"web03\". Lets say metric \"a\" has values for all 3 hosts but metric \"b\" is missing \"web03\".\n\nWith the intersection operator, the expression will effectively add \"a.web01 + b.web01\" and \"a.web02 + b.web02\" but will skip emitting anything for \"web03\". Be aware of this if you see fewer outputs that you expected or you see errors about no series available after intersection.\n\nWith the union operator the expression will add the `web01` and `web02` series but for metric \"b\", it will substitute the metric's fill policy value for the results.\n\nNote\n\nSupported as of version 2.3\n\n## Verbs\n\n- POST\n\n## Requests\n\nThe various sections implemented include:\n\n### \"time\"\n\nThe time section is required and is a single JSON object. This affects the time range and optional reductions for all metrics requested.\n\n| Name        | Data Type | Required | Description                                                                                                   | Default | Example                     |\n|-------------|-----------|----------|---------------------------------------------------------------------------------------------------------------|---------|-----------------------------|\n| start       | Integer   | Required | The start time for the query. This may be relative, absolute human readable or absolute Unix Epoch.           |         | 1h-ago, 2015/05/05-00:00:00 |\n| aggregator  | String    | Required | The global aggregation function to use for all metrics. It may be overridden on a per metric basis.           |         | sum                         |\n| end         | Integer   | Optional | The end time for the query. If left out, the end is *now*                                                     | now     | 1h-ago, 2015/05/05-00:00:00 |\n| downsampler | Object    | Optional | Reduces the number of data points returned. The format is defined below                                       | None    | See below                   |\n| rate        | Boolean   | Optional | Whether or not to calculate all metrics as rates, i.e. value per second. This is computed before expressions. | false   | true                        |\n\nE.g.\n\n``` javascript\n\"time\":{ \"start\":\"1h-ago\", \"end\":\"10m-ago\", \"downsampler\":{\"interval\":\"15m\",\"aggregator\":\"max\"}\n```\n\n**Downsampler**\n\n| Name       | Data Type | Required | Description                                                                                                   | Default | Example   |\n|------------|-----------|----------|---------------------------------------------------------------------------------------------------------------|---------|-----------|\n| interval   | String    | Required | A downsampling interval, i.e. what time span to rollup raw values into. The format is `<#><unit>`, e.g. `15m` |         | 1h        |\n| aggregator | String    | Required | The aggregation function to use for reducing the data points                                                  |         | avg       |\n| fillPolicy | Object    | Optional | A policy to use for filling buckets that are missing data points                                              | None    | See Below |\n\n**Fill Policies**\n\nThese are used to replace \"missing\" values, i.e. when a data point was expected but couldn't be found in storage.\n\n| Name   | Data Type | Required | Description                                                              | Default | Example |\n|--------|-----------|----------|--------------------------------------------------------------------------|---------|---------|\n| policy | String    | Required | The name of a policy to use. The values are listed in the table below    |         | zero    |\n| value  | Double    | Optional | For scalar fills, an optional value that can be used during substitution | NaN     | 42      |\n\n| Name   | Description                                                                                                                                                                                                                                                                                                                                                      |\n|--------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| nan    | Emits a NaN if all values in the aggregation function were NaN or \"missing\". For aggregators, NaNs are treated as \"sentinel\" values that cause the function to skip over the values. Note that if a series emits a NaN in an expression, the NaN is infectious and will cause the output of that expression to be NaN. At serialization the NaN will be emitted. |\n| null   | Emits a Null at serialization time. During computation the values are treated as NaNs.                                                                                                                                                                                                                                                                           |\n| zero   | Emits a zero when the value is missing                                                                                                                                                                                                                                                                                                                           |\n| scalar | Emits a user defined value when a data point is missing. Must specify the value with `value`. The value can be an integer or floating point.                                                                                                                                                                                                                     |\n\nNote that if you try to supply a value that is incompatible with the type the query will throw an exception. E.g. supplying a value with the NaN that isn't NaN will throw an error.\n\nE.g.\n\n``` javascript\n{\"policy\":\"scalar\",\"value\":\"1\"}\n```\n\n### \"filters\"\n\nFilters are for selecting various time series based on the tag keys and values. At least one filter must be specified (for now) with at least an aggregation function supplied. Fields include:\n\n| Name | Data Type | Required | Description                                                                   | Default | Example   |\n|------|-----------|----------|-------------------------------------------------------------------------------|---------|-----------|\n| id   | String    | Required | A unique ID for the filter. Cannot be the same as any metric or expression ID |         | f1        |\n| tags | Array     | Optional | A list of filters on tag values                                               | None    | See below |\n\nE.g.\n\n``` javascript\n\"filters\":[\n  \"id\":\"f1\",\n  \"tags\":[\n  {\n    \"type\":\"wildcard\",\n    \"tagk\":\"host\",\n    \"filter\":\"*\",\n    \"groupBy\":true\n  },\n  {\n    \"type\":\"literal_or\",\n    \"tagk\":\"colo\",\n    \"filter\":\"lga\",\n    \"groupBy\":false\n  }\n   ]\n  ]\n```\n\n**Filter Fields**\n\nWithin the \"tags\" field you can have one or more filter. The list of filters can be found via the [*/api/config/filters*](../config/filters) endpoint.\n\n| Name    | Data Type | Required | Description                                                                                                                                                                                                                                     | Default | Example          |\n|---------|-----------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|------------------|\n| type    | String    | Required | The name of the filter from the API                                                                                                                                                                                                             |         | regexp           |\n| tagk    | String    | Required | The tag key name such as *host* or *colo* that we filter on                                                                                                                                                                                     |         | host             |\n| filter  | String    | Required | The value to filter on. This depends on the filter in use. See the API for details                                                                                                                                                              |         | web.\\*mysite.com |\n| groupBy | Boolean   | Optional | Whether or not to group results by the tag values matching this filter. E.g. grouping by host will return one result per host. Not grouping by host would aggregate (using the aggregation function) all results for the metric into one series | false   | true             |\n\n### \"metrics\"\n\nThe metrics list determines which metrics are included in the expression. There must be at least one metric.\n\n| Name       | Data Type | Required | Description                                                                                                                                | Default             | Example         |\n|------------|-----------|----------|--------------------------------------------------------------------------------------------------------------------------------------------|---------------------|-----------------|\n| id         | String    | Required | A unique ID for the metric. This MUST be a simple string, no punctuation or spaces                                                         |                     | cpunice         |\n| filter     | String    | Required | The filter to use when fetching this metric. It must match a filter in the filters array                                                   |                     | f1              |\n| metric     | String    | Required | The name of a metric in OpenTSDB                                                                                                           |                     | system.cpu.nice |\n| aggregator | String    | Optional | An optional aggregation function to overload the global function in `time` for just this metric                                            | `time`'s aggregator | count           |\n| fillPolicy | Object    | Optional | If downsampling is not used, this can be included to determine what to emit in calculations. It will also override the downsampling policy | zero fill           | See above       |\n\nE.g.\n\n``` javascript\n{\"id\":\"cpunice\", \"filter\":\"f1\", \"metric\":\"system.cpu.nice\"}\n```\n\n### \"expressions\"\n\nA list of one or more expressions over the metrics. The variables in an expression **MUST** refer to either a metric ID field or an expression ID field. Nested expressions are supported but exceptions will be thrown if a self reference or circular dependency is detected. So far only basic operations are supported such as addition, subtraction, multiplication, division, modulo\n\n| Name       | Data Type | Required | Description                                                                                                | Default | Example      |\n|------------|-----------|----------|------------------------------------------------------------------------------------------------------------|---------|--------------|\n| id         | String    | Required | A unique ID for the expression                                                                             |         | cpubusy      |\n| expr       | String    | Required | The expression to execute                                                                                  |         | a + b / 1024 |\n| join       | Object    | Optional | The set operation or \"join\" to perform for series across sets.                                             | union   | See below    |\n| fillPolicy | Object    | Optional | An optional fill policy for the expression when it is used in a nested expression and doesn't have a value | NaN     | See above    |\n\nE.g.\n\n``` javascript\n{\n  \"id\": \"cpubusy\",\n  \"expr\": \"(((a + b + c + d + e + f + g) - g) / (a + b + c + d + e + f + g)) * 100\",\n  \"join\": {\n    \"operator\": \"intersection\",\n    \"useQueryTags\": true,\n    \"includeAggTags\": false\n  }\n}\n```\n\n**Joins**\n\nThe join object controls how the various time series for a given metric are merged within an expression. The two basic operations supported at this time are the union and intersection operators. Additional flags control join behavior.\n\n| Name           | Data Type | Required | Description                                                                                        | Default | Example      |\n|----------------|-----------|----------|----------------------------------------------------------------------------------------------------|---------|--------------|\n| operator       | String    | Required | The operator to use, either union or intersection                                                  |         | intersection |\n| useQueryTags   | Boolean   | Optional | Whether or not to use just the tags explicitly defined in the filters when computing the join keys | false   | true         |\n| includeAggTags | Boolean   | Optional | Whether or not to include the tag keys that were aggregated out of a series in the join key        | true    | false        |\n\n### \"outputs\"\n\nThese determine the output behavior and allow you to eliminate some expressions from the results or include the raw metrics. By default, if this section is missing, all expressions and only the expressions will be serialized. The field is a list of one or more output objects. More fields will be added later with flags to affect the output.\n\n| Name  | Data Type | Required | Description                             | Default | Example     |\n|-------|-----------|----------|-----------------------------------------|---------|-------------|\n| id    | String    | Required | The ID of the metric or expression      |         | e           |\n| alias | String    | Optional | An optional descriptive name for series |         | System Busy |\n\nE.g.\n\n``` javascript\n{\"id\":\"e\", \"alias\":\"System Busy\"}\n```\n\nNote\n\nThe `id` field for all objects can not contain spaces, special characters or periods at this time.\n\n**Complete Example**\n\n``` javascript\n{\n   \"time\": {\n     \"start\": \"1y-ago\",\n     \"aggregator\":\"sum\"\n   },\n   \"filters\": [\n     {\n       \"tags\": [\n         {\n           \"type\": \"wildcard\",\n           \"tagk\": \"host\",\n           \"filter\": \"web*\",\n           \"groupBy\": true\n         }\n       ],\n       \"id\": \"f1\"\n     }\n   ],\n   \"metrics\": [\n     {\n       \"id\": \"a\",\n       \"metric\": \"sys.cpu.user\",\n       \"filter\": \"f1\",\n       \"fillPolicy\":{\"policy\":\"nan\"}\n     },\n     {\n       \"id\": \"b\",\n       \"metric\": \"sys.cpu.iowait\",\n       \"filter\": \"f1\",\n       \"fillPolicy\":{\"policy\":\"nan\"}\n     }\n   ],\n   \"expressions\": [\n     {\n       \"id\": \"e\",\n       \"expr\": \"a + b\"\n     },\n     {\n     \"id\":\"e2\",\n     \"expr\": \"e * 2\"\n     },\n     {\n     \"id\":\"e3\",\n     \"expr\": \"e2 * 2\"\n     },\n     {\n     \"id\":\"e4\",\n     \"expr\": \"e3 * 2\"\n     },\n     {\n     \"id\":\"e5\",\n     \"expr\": \"e4 + e2\"\n     }\n  ],\n  \"outputs\":[\n    {\"id\":\"e5\", \"alias\":\"Mega expression\"},\n    {\"id\":\"a\", \"alias\":\"CPU User\"}\n  ]\n }\n```\n\n## Response\n\nThe output will contain a list of objects in the `outputs` array with the results in an array of arrays representing each time series followed by meta data for each series and the query overall. Also included is the original query and some summary statistics. The fields include:\n\n| Name       | Description                                                                                                                                                                           |\n|------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| id         | The expression ID the output matches                                                                                                                                                  |\n| dps        | The array of results. Each sub array starts with the timestamp in ms as the first (offset 0) value. The remaining values are the results for each series when a group by was applied. |\n| dpsMeta    | Meta data around the query including the first and last timestamps, number of result \"sets\", or sub arrays, and the number of series represented.                                     |\n| datapoints | The total number of data points returned to the user after aggregation                                                                                                                |\n| meta       | Data about each time series in the result set. The fields are below                                                                                                                   |\n\nThe meta section contains ordered information about each time series in the output arrays. The first element in the array will always have a `metrics` value of `timestamp` and no other data.\n\n| Name           | Description                                                                                              |\n|----------------|----------------------------------------------------------------------------------------------------------|\n| index          | The index in the data point arrays that the meta refers to                                               |\n| metrics        | The different metric names included in the expression                                                    |\n| commonTags     | Tag keys and values that were common across all time series that were aggregated in the resulting series |\n| aggregatedTags | Tag keys that appeared in all series in the resulting series but had different values                    |\n| dps            | The number of data points emitted                                                                        |\n| rawDps         | The number of raw values wrapped into the result                                                         |\n\n### Example Responses\n\n``` javascript\n{\n  \"outputs\": [\n    {\n      \"id\": \"Mega expression\",\n      \"dps\": [\n        [\n          1431561600000,\n          1010,\n          1030\n        ],\n        [\n          1431561660000,\n          \"NaN\",\n          \"NaN\"\n        ],\n        [\n          1431561720000,\n          \"NaN\",\n          \"NaN\"\n        ],\n        [\n          1431561780000,\n          1120,\n          1140\n        ]\n      ],\n      \"dpsMeta\": {\n        \"firstTimestamp\": 1431561600000,\n        \"lastTimestamp\": 1431561780000,\n        \"setCount\": 4,\n        \"series\": 2\n      },\n      \"meta\": [\n        {\n          \"index\": 0,\n          \"metrics\": [\n            \"timestamp\"\n          ]\n        },\n        {\n          \"index\": 1,\n          \"metrics\": [\n            \"sys.cpu\",\n            \"sys.iowait\"\n          ],\n          \"commonTags\": {\n            \"host\": \"web01\"\n          },\n          \"aggregatedTags\": []\n        },\n        {\n          \"index\": 2,\n          \"metrics\": [\n            \"sys.cpu\",\n            \"sys.iowait\"\n          ],\n          \"commonTags\": {\n            \"host\": \"web02\"\n          },\n          \"aggregatedTags\": []\n        }\n      ]\n    },\n    {\n      \"id\": \"sys.cpu\",\n      \"dps\": [\n        [\n          1431561600000,\n          1,\n          2\n        ],\n        [\n          1431561660000,\n          3,\n          0\n        ],\n        [\n          1431561720000,\n          5,\n          0\n        ],\n        [\n          1431561780000,\n          7,\n          8\n        ]\n      ],\n      \"dpsMeta\": {\n        \"firstTimestamp\": 1431561600000,\n        \"lastTimestamp\": 1431561780000,\n        \"setCount\": 4,\n        \"series\": 2\n      },\n      \"meta\": [\n        {\n          \"index\": 0,\n          \"metrics\": [\n            \"timestamp\"\n          ]\n        },\n        {\n          \"index\": 1,\n          \"metrics\": [\n            \"sys.cpu\"\n          ],\n          \"commonTags\": {\n            \"host\": \"web01\"\n          },\n          \"aggregatedTags\": []\n        },\n        {\n          \"index\": 2,\n          \"metrics\": [\n            \"sys.cpu\"\n          ],\n          \"commonTags\": {\n            \"host\": \"web02\"\n          },\n          \"aggregatedTags\": []\n        }\n      ]\n    }\n  ],\n  \"statsSummary\": {\n    \"datapoints\": 0,\n    \"rawDatapoints\": 0,\n    \"aggregationTime\": 0,\n    \"serializationTime\": 33,\n    \"storageTime\": 77,\n    \"timeTotal\": 148.63\n  },\n  \"query\": {\n    \"name\": null,\n    \"time\": {\n      \"start\": \"1y-ago\",\n      \"end\": null,\n      \"timezone\": null,\n      \"downsampler\": null,\n      \"aggregator\": \"sum\"\n    },\n    \"filters\": [\n      {\n        \"id\": \"f1\",\n        \"tags\": [\n          {\n            \"tagk\": \"host\",\n            \"filter\": \"web*\",\n            \"group_by\": true,\n            \"type\": \"wildcard\"\n          }\n        ]\n      }\n    ],\n    \"metrics\": [\n      {\n        \"metric\": \"sys.cpu\",\n        \"id\": \"a\",\n        \"filter\": \"f1\",\n        \"aggregator\": null,\n        \"fillPolicy\": {\n          \"policy\": \"nan\",\n          \"value\": \"NaN\"\n        },\n        \"timeOffset\": null\n      },\n      {\n        \"metric\": \"sys.iowait\",\n        \"id\": \"b\",\n        \"filter\": \"f1\",\n        \"aggregator\": null,\n        \"fillPolicy\": {\n          \"policy\": \"nan\",\n          \"value\": \"NaN\"\n        },\n        \"timeOffset\": null\n      }\n    ],\n    \"expressions\": [\n      {\n        \"id\": \"e\",\n        \"expr\": \"a + b\"\n      },\n      {\n        \"id\": \"e2\",\n        \"expr\": \"e * 2\"\n      },\n      {\n        \"id\": \"e3\",\n        \"expr\": \"e2 * 2\"\n      },\n      {\n        \"id\": \"e4\",\n        \"expr\": \"e3 * 2\"\n      },\n      {\n        \"id\": \"e5\",\n        \"expr\": \"e4 + e2\"\n      }\n    ],\n    \"outputs\": [\n      {\n        \"id\": \"e5\",\n        \"alias\": \"Woot!\"\n      },\n      {\n        \"id\": \"a\",\n        \"alias\": \"Woot!2\"\n      }\n    ]\n   }\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/query/exp.html](http://opentsdb.net/docs/build/html/api_http/query/exp.html)"
- name: /api/query/gexp
  id: api_http/query/gexp
  summary: Graphite is an excellent storage system for time series data with a number of built in functions to manipulate the data
  description: "# /api/query/gexp\n\nGraphite is an excellent storage system for time series data with a number of built in functions to manipulate the data. To support transitions from Graphite to OpenTSDB, the `/api/query/gexp` endpoint supports URI queries *similar* but not *identical* to Graphite\\`s expressions. Graphite functions are generally formatted as `func(<series>[,`` ``param1][,`` ``paramN])` with the ability to nest functions. TSD\\`s implementation follows the same pattern but uses an `m` style query (e.g. `sum:proc.stat.cpu{host=foo,type=idle}`) in place of the `<series>`. Nested functions are supported.\n\nTSDB implements a subset of Graphite functions though we hope to add more in the future. For a list of Graphite functions and descriptions, see the [Documentation](http://graphite.readthedocs.org/en/latest/functions.html). TSD supported functions appear below.\n\nNote\n\nSupported as of version 2.3\n\n## Verbs\n\n- GET\n\n## Requests\n\nQueries can only be executed via GET using the URI at this time. (In the future, the [*/api/query/exp*](exp) endpoint will support more flexibility.) This is an extension of the main [*/api/query*](index) endpoint so parameters in the request table are also supported here. Additional parameters include:\n\n| Name | Data Type | Required | Description                                                                                                                                  | Example                              |\n|------|-----------|----------|----------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------|\n| exp  | String    | Required | The Graphite style expression to execute. The first parameter of a function must either be another function or a URI formatted **Sub Query** | scale(sum:if.bytes_in{host=\\*},1024) |\n\n### Example Query String Requests\n\n``` python\nhttp://localhost:4242/api/query/gexp?start=1h-ago&exp=scale(sum:if.bytes_in{host=*},1024)\n```\n\n## Response\n\nThe output is identical to [*/api/query*](index).\n\n## Functions\n\nFunctions that accept a single metric query will operate across each time series result. E.g. if a query includes a group by on host such as `scale(sum:if.bytes_in{host=*},1024)`, and multiple hosts exist with that metric, then a series for each host will be emitted and the function applied. For functions that take multiple metrics, a union is performed across each metric and the function is executed across each resulting series with matching tags. E.g with the query `sum(sum:if.bytes_in{host=*},sum:if.bytes_out{host=*})`, assume two hosts exist, `web01` and `web02`. In this case, the output will be `if.bytes_in{host=web01}`` ``+`` ``if.bytes_out{host=web01}` and `if.bytes_in{host=web02}`` ``+`` ``if.bytes_out{host=web02}`. Missing series in any metric result set will be filled with the default fill value of the function.\n\nCurrently supported expressions include:\n\n### absolute(\\<metric\\>)\n\nEmits the results as absolute values, converting negative values to positive.\n\n### diffSeries(\\<metric\\>\\[,\\<metricN\\>\\])\n\nReturns the difference of all series in the list. Performs a UNION across tags in each metric result sets, defaulting to a fill value of zero. A maximum of 26 series are supported at this time.\n\n### divideSeries(\\<metric\\>\\[,\\<metricN\\>\\])\n\nReturns the quotient of all series in the list. Performs a UNION across tags in each metric result sets, defaulting to a fill value of zero. A maximum of 26 series are supported at this time.\n\n### highestCurrent(\\<metric\\>,\\<n\\>)\n\nSorts all resulting time series by their most recent value and emits `n` number of series with the highest values. `n` must be a positive integer value.\n\n### highestMax(\\<metric\\>,\\<n\\>)\n\nSorts all resulting time series by the maximum value for the time span and emits `n` number of series with the highest values. `n` must be a positive integer value.\n\n### movingAverage(\\<metric\\>,\\<window\\>)\n\nEmits a sliding window moving average for each data point and series in the metric. The `window` parameter may either be a positive integer that reflects the number of data points to maintain in the window (non-timed) or a time span specified by an integer followed by time unit such as `` `60s` `` or `` `60m` `` or `` `24h` ``. Timed windows must be in single quotes.\n\n### multiplySeries(\\<metric\\>\\[,\\<metricN\\>\\])\n\nReturns the product of all series in the list. Performs a UNION across tags in each metric result sets, defaulting to a fill value of zero. A maximum of 26 series are supported at this time.\n\n### scale(\\<metric\\>,\\<factor\\>)\n\nMultiplies each series by the factor where the factor can be a positive or negative floating point or integer value.\n\n### sumSeries(\\<metric\\>\\[,\\<metricN\\>\\])\n\nReturns the sum of all series in the list. Performs a UNION across tags in each metric result sets, defaulting to a fill value of zero. A maximum of 26 series are supported at this time.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/query/gexp.html](http://opentsdb.net/docs/build/html/api_http/query/gexp.html)"
- name: /api/query/last
  id: api_http/query/last
  summary: This endpoint (2.1 and later) provides support for accessing the latest value of individual time series
  description: "# /api/query/last\n\nThis endpoint (2.1 and later) provides support for accessing the latest value of individual time series. It provides an optimization over a regular query when only the last data point is required. Locating the last point can be done with the timestamp of the meta data counter or by scanning backwards from the current system time.\n\nNote\n\nIn order for this endpoint to function with metric string queries by scanning for matching time series, the meta data table must exist and have been populated with counters or TSMeta objects using one of the methods specified in [*Metadata*](../../user_guide/metadata). You must set either `tsd.core.meta.enable_tsuid_tracking` or `tsd.core.meta.enable_realtime_ts`. Queries with a backscan parameter will skip the meta table.\n\nSimilar to the standard query endpoint, there are two methods to use in selecting which time series should return data:\n\n- **Metric Query** - Similar to a regular metric query, you can send a metric name and optionally a set of tag pairs. If the real-time meta has been enabled, the TSD will scan the meta data table to see if any time series match the query. For each time series that matches, it will scan for the latest data point and return it. However if meta is disabled, then the TSD will attempt a lookup for the exact set of metric and tags provided as long as a backscan value is given (as of 2.1.1).\n- **TSUID Query** - If you know the TSUIDs for the time series that you want to access data for, simply provide a list of TSUIDs.\n\nAdditionally there are two ways to find the last data point for each time series located:\n\n- **Counter Method** - If no backscan value is given and meta is enabled, the default is to lookup the data point counter in the meta data table for each time series. This counter records the time when the latest data point was written by a TSD. The endpoint looks up the timestamp and \"gets\" the proper data row, fetching the last point in the row. This will work most of the time, however please be aware that if you backfill older data (via an import or simply putting a data point with an old timestamp) the counter column timestamp may not be accurate. This method is best used for continuously updated data.\n- **Back Scan** - Alternatively you can specify a number of hours to scan back in time starting at the current system time of the TSD where the query is being executed. For example, if you specify a back scan time of 24 hours, the TSD will first look for data in the row with the current hour. If that row is empty, it will look for data one hour before that. It will keep doing that until it finds a data point or it exceeds the hour limit. This method is useful if you often write data points out of order in time. Also note that the larger the backscan value, the longer it may take for queries to complete as they may scan further back in time for data.\n\nAll queries will return results only for time series that matched the query and for which a data point was found. The results are a list of individual data points per time series. Aggregation cannot be performed on individual data points as the timestamps may not align and the TSD will only return a single point so interpolation is impossible.\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\nCommon parameters include:\n\n| Name         | Data Type | Required | Description                                                                                                                            | Default | QS                   | RW  | Example |\n|--------------|-----------|----------|----------------------------------------------------------------------------------------------------------------------------------------|---------|----------------------|-----|---------|\n| queries      | Array     | Required | A list of one or more queries used to determine which time series to fetch the last data point for.                                    |         | timeseries \\| tsuids |     |         |\n| resolveNames | Boolean   | Optional | Whether or not to resolve the TSUIDs of results to their metric and tag names.                                                         | false   | resolve              |     | true    |\n| backScan     | Integer   | Optional | A number of hours to search in the past for data. If set to 0 then the timestamp of the meta data counter for the time series is used. | 0       | back_scan            |     | 24      |\n\nNote that you can mix multiple metric and TSUID queries in one request.\n\n### Metric Query String Format\n\nThe full specification for a metric query string sub query is as follows:\n\n``` python\ntimeseries=<metric_name>[{<tag_name1>=<tag_value1>[,...<tag_nameN>=<tag_valueN>]}]\n```\n\nIt is similar to a regular metric query but does not allow for aggregations, rates, down sampling or grouping operators. Note that if you supply a backscan value to avoid the meta table, then you must supply all of the tags and values to match the exact time series you are looking for. Backscan does not currently filter on the metric and tags given but will look for the specific series.\n\n### TSUID Query String Format\n\nTSUID queries are simpler than Metric queries. Simply pass a list of one or more hexadecimal encoded TSUIDs separated by commas:\n\n``` python\ntsuids=<tsuid1>[,...<tsuidN>]\n```\n\n### Example Query String Requests\n\n``` python\nhttp://localhost:4242/api/query/last?timeseries=proc.stat.cpu{host=foo,type=idle}&timeseries=proc.stat.mem{host=foo,type=idle}\nhttp://localhost:4242/api/query/last?tsuids=000001000002000003,000001000002000004&back_scan=24&resolve=true\n```\n\n### Example Content Request\n\n``` javascript\n{\n  \"queries\": [\n    {\n      \"metric\": \"sys.cpu.0\",\n      \"tags\": {\n        \"host\": \"web01\",\n        \"dc\": \"lga\"\n      }\n    },\n    {\n      \"tsuids\": [\n        \"000001000002000042\",\n        \"000001000002000043\"\n        ]\n      }\n    }\n  ],\n  \"resolveNames\":true,\n  \"backScan\":24\n}\n```\n\n## Response\n\nThe output will be an array of 0 or more data points depending on the data that was found. If a data point for a particular time series was not located within the time specified, it will not appear in the output. Output fields depend on whether or not the `resolve` flag was set.\n\n| Name      | Description                                                                         |\n|-----------|-------------------------------------------------------------------------------------|\n| metric    | Name of the metric for the time series. Only returned if `resolve` was set to true. |\n| tags      | A list of tags for the time series. Only returned if `resolve` was set to true.     |\n| timestamp | A Unix epoch timestamp, in milliseconds, when the data point was written            |\n| value     | The value of the data point enclosed in quotation marks as a string                 |\n| tsuid     | The hexadecimal TSUID for the time series                                           |\n\nUnless there was an error with the query, you will generally receive a `200` status with content. However if your query couldn't find any data, it will return an empty result set. In the case of the JSON serializer, the result will be an empty array:\n\n``` javascript\n[]\n```\n\n### Example Responses\n\n``` javascript\n[\n  {\n    \"timestamp\": 1377118201000,\n    \"value\": \"1976558550\",\n    \"tsuid\": \"0023E3000002000008000006000001\"\n  },\n  {\n    \"timestamp\": 1377118201000,\n    \"value\": \"1654587485\",\n    \"tsuid\": \"0023E3000002000008000006001656\"\n  }\n]\n```\n\n``` javascript\n[\n  {\n    \"metric\": \"tsd.hbase.rpcs\",\n    \"timestamp\": 1377186301000,\n    \"value\": \"2723265185\",\n    \"tags\": {\n      \"type\": \"put\",\n      \"host\": \"tsd1\"\n    },\n    \"tsuid\": \"0023E3000002000008000006000001\"\n  },\n  {\n    \"metric\": \"tsd.hbase.rpcs\",\n    \"timestamp\": 1377186301000,\n    \"value\": \"580720\",\n    \"tags\": {\n      \"type\": \"put\",\n      \"host\": \"tsd2\"\n    },\n    \"tsuid\": \"0023E3000002000008000006017438\"\n  }\n]\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/query/last.html](http://opentsdb.net/docs/build/html/api_http/query/last.html)"
- name: /api/rollup
  id: api_http/rollup
  summary: This endpoint allows for storing rolled up and/or pre-aggregated data in OpenTSDB over HTTP
  description: "# /api/rollup\n\nThis endpoint allows for storing rolled up and/or pre-aggregated data in OpenTSDB over HTTP. For details on rollups and pre-aggs, please see the user guide: [*Rollup And Pre-Aggregates*](../user_guide/rollups).\n\nAlso see the [*/api/put*](put) documentation for notes and common parameters that are shared with the `/api/rollup` endpoint. This page lays out the differences between the two.\n\n## Verbs\n\n- POST\n\n## Requests\n\nRollup and pre-aggregate values are extensions of the `put` object with three additional fields. For completeness, all fields are listed below:\n\n| Name              | Data Type              | Required   | Description                                                                                                                                                                                                                            | Default | QS  | RW    | Example          |\n|-------------------|------------------------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|-----|-------|------------------|\n| metric            | String                 | Required   | The name of the metric you are storing                                                                                                                                                                                                 |         |     | W     | sys.cpu.nice     |\n| timestamp         | Integer                | Required   | A Unix epoch style timestamp in seconds or milliseconds. The timestamp must not contain non-numeric characters.                                                                                                                        |         |     | W     | 1365465600       |\n| value             | Integer, Float, String | Required   | The value to record for this data point. It may be quoted or not quoted and must conform to the OpenTSDB value rules: `../../user_guide/writing`                                                                                       |         |     | W     | 42.5             |\n| tags              | Map                    | Required   | A map of tag name/tag value pairs. At least one pair must be supplied.                                                                                                                                                                 |         |     | W     | {\"host\":\"web01\"} |\n| interval          | String                 | Optional\\* | A time interval reflecting what timespan the **rollup** value represents. The interval consists of `<amount><unit>` similar to a downsampler or relative query timestamp. E.g. `6h` for 5 hours of data, `30m` for 30 minutes of data. |         |     | W     | 1h               |\n| aggregator        | String                 | Optional\\* | An aggregation function used to generate the **rollup** value. Must match a supplied TSDB aggregator.                                                                                                                                  |         |     | W     | SUM              |\n| groupByAggregator | String                 | Optional\\* | An aggregation function used to generate the **pre-aggregate** value. Must match a supplied TSDB aggregator.                                                                                                                           |         | W   | COUNT |                  |\n\nWhile the aggregators and interval are marked as optional above, at least one of the combinations documented below must be satisfied for data to be recorded.\n\n| interval | aggregator | groupByAggregator | Description                                                                             |\n|----------|------------|-------------------|-----------------------------------------------------------------------------------------|\n| Set      | Set        | Empty             | Data represents a *raw* or *non-pre-aggregated* **rollup** over the interval.           |\n| Empty    | Empty      | Set               | Data represents a *raw* **pre-aggregated** value that has not been rolled up over time. |\n| Set      | Set        | Set               | Data represents a *rolled up* *pre-aggregated* value.                                   |\n\n### Example Single Data Point Put\n\nYou can supply a single data point in a request:\n\n``` javascript\n{\n  \"metric\": \"sys.cpu.nice\",\n  \"timestamp\": 1346846400,\n  \"value\": 18,\n  \"tags\": {\n     \"host\": \"web01\",\n     \"dc\": \"lga\"\n  },\n  \"interval\": \"1h\",\n  \"aggregator\": \"SUM\",\n  \"groupByAggregator\", \"SUM\"\n}\n```\n\n### Example Multiple Data Point Put\n\nMultiple data points must be encased in an array:\n\n``` javascript\n[\n  {\n    \"metric\": \"sys.cpu.nice\",\n    \"timestamp\": 1346846400,\n    \"value\": 18,\n    \"tags\": {\n       \"host\": \"web01\",\n       \"dc\": \"lga\"\n    },\n    \"interval\": \"1h\",\n    \"aggregator\": \"SUM\",\n    \"groupByAggregator\", \"SUM\"\n  },\n  {\n    \"metric\": \"sys.cpu.nice\",\n    \"timestamp\": 1346846400,\n    \"value\": 9,\n    \"tags\": {\n       \"host\": \"web02\",\n       \"dc\": \"lga\"\n    },\n    \"interval\": \"1h\",\n    \"aggregator\": \"SUM\",\n    \"groupByAggregator\", \"SUM\"\n  }\n]\n```\n\n## Response\n\nResponses are handled in the same was as for the [*/api/put*](put) endpoint.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/rollup.html](http://opentsdb.net/docs/build/html/api_http/rollup.html)"
- name: /api/search
  id: api_http/search/index
  summary: This endpoint provides a basic means of searching OpenTSDB meta data
  description: "# /api/search\n\nThis endpoint provides a basic means of searching OpenTSDB meta data. Lookups can be performed against the `tsdb-meta` table when enabled. Optionally, a search plugin can be installed to send and retreive information from an external search indexing service such as Elastic Search. It is up to each search plugin to implement various parts of this endpoint and return data in a consistent format. The type of object searched and returned depends on the endpoint chosen.\n\nNote\n\nIf the plugin is not configured or enabled, endpoints other than `/api/search/lookup` will return an exception.\n\n## Search API Endpoints\n\n- [*/api/search/lookup*](lookup)\n- /api/search/tsmeta - [TSMETA Response](#tsmeta-endpoint)\n- /api/search/tsmeta_summary - [TSMETA_SUMMARY Response](#tsmeta-summary-endpoint)\n- /api/search/tsuids - [TSUIDS Response](#tsuids-endpoint)\n- /api/search/uidmeta - [UIDMETA Response](#uidmeta-endpoint)\n- /api/search/annotation - [Annotation Response](#annotation-endpoint)\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\nParameters used by the search endpoint include:\n\n| Name       | Data Type | Required | Description                                                                                                                                                                                 | Default | QS          | RW  | Example                            |\n|------------|-----------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|-------------|-----|------------------------------------|\n| query      | String    | Optional | The string based query to pass to the search engine. This will be parsed by the engine or plugin to perform the actual search. Allowable values depends on the plugin. Ignored for lookups. |         | query       |     | name:sys.cpu.\\*                    |\n| limit      | Integer   | Optional | Limits the number of results returned per query so as not to override the TSD or search engine. Allowable values depends on the plugin. Ignored for lookups.                                | 25      | limit       |     | 100                                |\n| startIndex | Integer   | Optional | Used in combination with the `limit` value to page through results. Allowable values depends on the plugin. Ignored for lookups.                                                            | 0       | start_index |     | 42                                 |\n| metric     | String    | Optional | The name of a metric or a wildcard for lookup queries                                                                                                                                       | \\*      | metric      |     | tsd.hbase.rpcs                     |\n| tags       | Array     | Optional | One or more key/value objects with tag names and/or tag values for lookup queries. See [*/api/search/lookup*](lookup)                                                                       |         | tags        |     | See [*/api/search/lookup*](lookup) |\n\n### Example Request\n\nQuery String:\n\n``` python\nhttp://localhost:4242/api/search/tsmeta?query=name:*&limit=3&start_index=0\n```\n\nPOST:\n\n``` javascript\n{\n  \"query\": \"name:*\",\n  \"limit\": 4,\n  \"startIndex\": 5\n}\n```\n\n## Response\n\nDepending on the endpoint called, the output will change slightly. However common fields include:\n\n[TABLE]\n\nThis endpoint will almost always return a `200` with content body. If the query doesn't match any results, the `results` field will be an empty array and `totalResults` will be 0. If an error occurs, such as the plugin being disabled or not configured, an exception will be returned.\n\n## TSMETA Response\n\nThe TSMeta endpoint returns a list of matching TSMeta objects.\n\n``` javascript\n{\n  \"type\": \"TSMETA\",\n  \"query\": \"name:*\",\n  \"metric\": \"*\",\n  \"tags\": [],\n  \"limit\": 2,\n  \"time\": 675,\n  \"results\": [\n    {\n      \"tsuid\": \"0000150000070010D0\",\n      \"metric\": {\n        \"uid\": \"000015\",\n        \"type\": \"METRIC\",\n        \"name\": \"app.apache.connections\",\n        \"description\": \"\",\n        \"notes\": \"\",\n        \"created\": 1362655264,\n        \"custom\": null,\n        \"displayName\": \"\"\n      },\n      \"tags\": [\n        {\n          \"uid\": \"000007\",\n          \"type\": \"TAGK\",\n          \"name\": \"fqdn\",\n          \"description\": \"\",\n          \"notes\": \"\",\n          \"created\": 1362655264,\n          \"custom\": null,\n          \"displayName\": \"\"\n        },\n        {\n          \"uid\": \"0010D0\",\n          \"type\": \"TAGV\",\n          \"name\": \"web01.mysite.com\",\n          \"description\": \"\",\n          \"notes\": \"\",\n          \"created\": 1362720007,\n          \"custom\": null,\n          \"displayName\": \"\"\n        }\n      ],\n      \"description\": \"\",\n      \"notes\": \"\",\n      \"created\": 1362740528,\n      \"units\": \"\",\n      \"retention\": 0,\n      \"max\": 0,\n      \"min\": 0,\n      \"displayName\": \"\",\n      \"dataType\": \"\",\n      \"lastReceived\": 0,\n      \"totalDatapoints\": 0\n    },\n    {\n      \"tsuid\": \"0000150000070010D5\",\n      \"metric\": {\n        \"uid\": \"000015\",\n        \"type\": \"METRIC\",\n        \"name\": \"app.apache.connections\",\n        \"description\": \"\",\n        \"notes\": \"\",\n        \"created\": 1362655264,\n        \"custom\": null,\n        \"displayName\": \"\"\n      },\n      \"tags\": [\n        {\n          \"uid\": \"000007\",\n          \"type\": \"TAGK\",\n          \"name\": \"fqdn\",\n          \"description\": \"\",\n          \"notes\": \"\",\n          \"created\": 1362655264,\n          \"custom\": null,\n          \"displayName\": \"\"\n        },\n        {\n          \"uid\": \"0010D5\",\n          \"type\": \"TAGV\",\n          \"name\": \"web02.mysite.com\",\n          \"description\": \"\",\n          \"notes\": \"\",\n          \"created\": 1362720007,\n          \"custom\": null,\n          \"displayName\": \"\"\n        }\n      ],\n      \"description\": \"\",\n      \"notes\": \"\",\n      \"created\": 1362882263,\n      \"units\": \"\",\n      \"retention\": 0,\n      \"max\": 0,\n      \"min\": 0,\n      \"displayName\": \"\",\n      \"dataType\": \"\",\n      \"lastReceived\": 0,\n      \"totalDatapoints\": 0\n    }\n  ],\n  \"startIndex\": 0,\n  \"totalResults\": 9688066\n}\n```\n\n## TSMETA_SUMMARY Response\n\nThe TSMeta Summary endpoint returns just the basic information associated with a timeseries including the TSUID, the metric name and tags. The search is run against the same index as the TSMeta query but returns a subset of the data.\n\n``` javascript\n{\n  \"type\": \"TSMETA_SUMMARY\",\n  \"query\": \"name:*\",\n  \"metric\": \"*\",\n  \"tags\": [],\n  \"limit\": 3,\n  \"time\": 565,\n  \"results\": [\n    {\n      \"tags\": {\n        \"fqdn\": \"web01.mysite.com\"\n      },\n      \"metric\": \"app.apache.connections\",\n      \"tsuid\": \"0000150000070010D0\"\n    },\n    {\n      \"tags\": {\n        \"fqdn\": \"web02.mysite.com\"\n      },\n      \"metric\": \"app.apache.connections\",\n      \"tsuid\": \"0000150000070010D5\"\n    },\n    {\n      \"tags\": {\n        \"fqdn\": \"web03.mysite.com\"\n      },\n      \"metric\": \"app.apache.connections\",\n      \"tsuid\": \"0000150000070010D6\"\n    }\n  ],\n  \"startIndex\": 0,\n  \"totalResults\": 9688066\n}\n```\n\n## TSUIDS Response\n\nThe TSUIDs endpoint returns a list of TSUIDS that match the query. The search is run against the same index as the TSMeta query but returns a subset of the data.\n\n``` javascript\n{\n  \"type\": \"TSUIDS\",\n  \"query\": \"name:*\",\n  \"metric\": \"*\",\n  \"tags\": [],\n  \"limit\": 3,\n  \"time\": 517,\n  \"results\": [\n    \"0000150000070010D0\",\n    \"0000150000070010D5\",\n    \"0000150000070010D6\"\n  ],\n  \"startIndex\": 0,\n  \"totalResults\": 9688066\n}\n```\n\n## UIDMETA Response\n\nThe UIDMeta endpoint returns a list of UIDMeta objects that match the query.\n\n``` javascript\n{\n  \"type\": \"UIDMETA\",\n  \"query\": \"name:*\",\n  \"metric\": \"*\",\n  \"tags\": [],\n  \"limit\": 3,\n  \"time\": 517,\n  \"results\": [\n    {\n      \"uid\": \"000007\",\n      \"type\": \"TAGK\",\n      \"name\": \"fqdn\",\n      \"description\": \"\",\n      \"notes\": \"\",\n      \"created\": 1362655264,\n      \"custom\": null,\n      \"displayName\": \"\"\n    },\n    {\n      \"uid\": \"0010D0\",\n      \"type\": \"TAGV\",\n      \"name\": \"web01.mysite.com\",\n      \"description\": \"\",\n      \"notes\": \"\",\n      \"created\": 1362720007,\n      \"custom\": null,\n      \"displayName\": \"\"\n    },\n    {\n      \"uid\": \"0010D5\",\n      \"type\": \"TAGV\",\n      \"name\": \"web02.mysite.com\",\n      \"description\": \"\",\n      \"notes\": \"\",\n      \"created\": 1362720007,\n      \"custom\": null,\n      \"displayName\": \"\"\n    }\n  ],\n  \"startIndex\": 0,\n  \"totalResults\": 9688066\n}\n```\n\n## Annotation Response\n\nThe Annotation endpoint returns a list of Annotation objects that match the query.\n\n``` javascript\n{\n  \"type\": \"ANNOTATION\",\n  \"query\": \"description:*\",\n  \"metric\": \"*\",\n  \"tags\": [],\n  \"limit\": 25,\n  \"time\": 80,\n  \"results\": [\n    {\n      \"tsuid\": \"000001000001000001\",\n      \"description\": \"Testing Annotations\",\n      \"notes\": \"These would be details about the event, the description is just a summary\",\n      \"custom\": {\n        \"owner\": \"jdoe\",\n        \"dept\": \"ops\"\n      },\n      \"endTime\": 0,\n      \"startTime\": 1369141261\n    }\n  ],\n  \"startIndex\": 0,\n  \"totalResults\": 1\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/search/index.html](http://opentsdb.net/docs/build/html/api_http/search/index.html)"
- name: /api/search/lookup
  id: api_http/search/lookup
  summary: Lookup queries use either the meta data table or the main data table to determine what time series are associated with a given metric, tag name, tag value, tag pair or combination thereof
  description: "# /api/search/lookup\n\nNote\n\nAvailable in 2.1\n\nLookup queries use either the meta data table or the main data table to determine what time series are associated with a given metric, tag name, tag value, tag pair or combination thereof. For example, if you want to know what metrics are available for a tag pair `host=web01` you can execute a lookup to find out. Lookups do not require a search plugin to be installed.\n\nNote\n\nLookups are performed against the `tsdb-meta` table. You must enable real-time meta data creation or perform a `metasync` using the `uid` command in order to retreive data from a lookup. Lookups can be executed against the raw data table using the CLI command only: [*search*](../../user_guide/cli/search)\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\nParameters used by the lookup endpoint include:\n\n| Name       | Data Type | Required | Description                                                                                               | Default | QS       | RW  | Example                 |\n|------------|-----------|----------|-----------------------------------------------------------------------------------------------------------|---------|----------|-----|-------------------------|\n| query      | String    | Required | A lookup query as defined below.                                                                          |         | m        |     | tsd.hbase.rpcs{type=\\*} |\n| useMeta    | Boolean   | Optional | Whether or not to use the meta data table or the raw data table. The raw table will be much slower.       | False   | use_meta |     | True                    |\n| limit      | Integer   | Optional | The maximum number of items returned in the result set. Currently the limit is ignored for lookup queries | 25      |          |     | 100                     |\n| startIndex | Integer   | Optional | Ignored for lookup queries, always the default.                                                           | 0       |          |     | 10                      |\n\n### Lookup Queries\n\nA lookup query consists of at least one metric, tag name (tagk) or tag value (tagv). Each value must be a literal name in the UID table. If a given name cannot be resolved to a UID, an exception will be returned. Only one metric can be supplied per query but multiple tagk, tagv or tag pairs may be provided.\n\nNormally, tags a provided in the format `<tagk>=<tagv>` and a value is required on either side of the equals sign. However for lookups, one value may an asterisk `*`, i.e. `<tagk>=*` or `*=<tagv>`. In these cases, the asterisk acts as a wildcard meaning any time series with the given tagk or tagv will be returned. For example, if we issue a query for `host=*` then we will get all of the time series with a `host` tagk such as `host=web01` and `host=web02`.\n\nFor complex queries with multiple values, each type is `AND`'d with the other types and `OR`'d with it's own type.\n\n``` python\n<metric> AND (<tagk1>=[<tagv1>] OR <tagk1>=[<tagv2>]) AND ([<tagk2>]=<tagv3> OR [<tagk2>]=<tagv4>)\n```\n\nFor example, the query `tsd.hbase.rpcs{type=*,host=tsd1,host=tsd2,host=tsd3}` would return only the time series with the metric `tsd.hbase.rpcs` and the `type` tagk with any value and a `host` tag with either `tsd1` or `tsd2` or `tsd3`. Unlike a data query, you may supply multiple tagks with the same name as seen in the example above. Wildcards always take priority so if your query looked like `tsd.hbase.rpcs{type=*,host=tsd1,host=tsd2,host=*}`, then the query would effectively be treated as `tsd.hbase.rpcs{type=*,host=*}`.\n\nTo retreive a list of all time series with a specific tag value, e.g. a particular host, you could issue a query like `{*=web01}` that will return all time series with a tag value of `web01`. This can be useful in debugging tag name issues such as some series having `host=web01` or `server=web01`.\n\n### Example Request\n\nQuery String:\n\n``` python\nhttp://localhost:4242/api/search/lookup?m=tsd.hbase.rpcs{type=*}\n```\n\nPOST:\n\nJSON requests follow the search query format on the [*/api/search*](index) page. Limits and startNote that tags are supplied as a list of objects. The value for the `key` should be a `tagk` and the value for `value` should be a `tagv` or wildcard.\n\n``` javascript\n{\n  \"metric\": \"tsd.hbase.rpcs\",\n  \"tags\":[\n    {\n      \"key\": \"type\",\n      \"value\": \"*\"\n    }\n  ]\n}\n```\n\n## Response\n\nDepending on the endpoint called, the output will change slightly. However common fields include:\n\n| Name         | Data Type | Description                                                                                               | Example     |\n|--------------|-----------|-----------------------------------------------------------------------------------------------------------|-------------|\n| type         | String    | The type of query submitted, i.e. the endpoint called.                                                    | LOOKUP      |\n| query        | String    | Ignored for lookup queries.                                                                               |             |\n| limit        | Integer   | The maximum number of items returned in the result set. Currently the limit is ignored for lookup queries | 25          |\n| startIndex   | Integer   | Ignored for lookup queries, always the default.                                                           | 0           |\n| metric       | String    | The metric used for the lookup                                                                            | \\*          |\n| tags         | Array     | The list of tag pairs used for the lookup. May be an empty list.                                          | \\[ \\]       |\n| time         | Integer   | The amount of time it took, in milliseconds, to complete the query                                        | 120         |\n| totalResults | Integer   | The total number of results matched by the query                                                          | 1024        |\n| results      | Array     | The result set with the TSUID, metric and tags for each series.                                           | *See Below* |\n\nThis endpoint will almost always return a `200` with content body. If the query doesn't match any results, the `results` field will be an empty array and `totalResults` will be 0. If an error occurs, such as a failure to resolve a metric or tag name to a UID, an exception will be returned.\n\n## Example Response\n\n``` javascript\n{\n  \"type\": \"LOOKUP\",\n  \"metric\": \"tsd.hbase.rpcs\",\n  \"tags\":[\n    {\n      \"key\": \"type\",\n      \"value\": \"*\"\n    }\n  ]\n  \"limit\": 3,\n  \"time\": 565,\n  \"results\": [\n    {\n      \"tags\": {\n        \"fqdn\": \"web01.mysite.com\"\n      },\n      \"metric\": \"app.apache.connections\",\n      \"tsuid\": \"0000150000070010D0\"\n    },\n    {\n      \"tags\": {\n        \"fqdn\": \"web02.mysite.com\"\n      },\n      \"metric\": \"app.apache.connections\",\n      \"tsuid\": \"0000150000070010D5\"\n    },\n    {\n      \"tags\": {\n        \"fqdn\": \"web03.mysite.com\"\n      },\n      \"metric\": \"app.apache.connections\",\n      \"tsuid\": \"0000150000070010D6\"\n    }\n  ],\n  \"startIndex\": 0,\n  \"totalResults\": 9688066\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/search/lookup.html](http://opentsdb.net/docs/build/html/api_http/search/lookup.html)"
- name: /api/serializers
  id: api_http/serializers
  summary: This endpoint lists the serializer plugins loaded by the running TSD
  description: "# /api/serializers\n\nThis endpoint lists the serializer plugins loaded by the running TSD. Information given includes the name, implemented methods, content types and methods.\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\nNo parameters are available, this is a read-only endpoint that simply returns system data.\n\n## Response\n\nThe response is an array of serializer objects. Each object has the following fields:\n\n| Field Name | Data Type       | Description                                                                                                                                                                                                                                                                                                                                                                    | Example           |\n|------------|-----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|\n| serializer | String          | The name of the serializer, suitable for use in the query string `serializer=<serializer_name>` parameter                                                                                                                                                                                                                                                                      | xml               |\n| formatters | Array\\<String\\> | An array of methods or endpoints that the serializer implements to convert response data. These usually map to an endpoint such as `/api/suggest` mapping to `Suggest`. If the serializer does not implement a certain method, the default formatter will respond. Each name also ends with the API version supported, e.g. `V1` will support version 1 API calls.             | \"Error\",\"Suggest\" |\n| parsers    | Array\\<String\\> | An array of methods or endpoints that the serializer implements to parse user input in the HTTP request body. These usually map to an endpoint such as `/api/suggest` will map to `Suggest`. If a serializer does not implement a parser, the default serializer will be used. Each name also ends with the API version supported, e.g. `V1` will support version 1 API calls. | \"Suggest\",\"Put\"   |\n\nThis endpoint should always return data with the JSON serializer as the default. If you think the TSD should have other formatters listed, check the plugin documentation to make sure you have the proper plugin and it's located in the right directory.\n\n## Example Response\n\n``` javascript\n[\n  {\n    \"formatters\": [\n      \"SuggestV1\",\n      \"SerializersV1\",\n      \"ErrorV1\",\n      \"ErrorV1\",\n      \"NotFoundV1\"\n    ],\n    \"serializer\": \"json\",\n    \"parsers\": [\n      \"SuggestV1\"\n    ],\n    \"class\": \"net.opentsdb.tsd.HttpJsonSerializer\",\n    \"response_content_type\": \"application/json; charset=UTF-8\",\n    \"request_content_type\": \"application/json\"\n  }\n]\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/serializers.html](http://opentsdb.net/docs/build/html/api_http/serializers.html)"
- name: /api/stats
  id: api_http/stats/index
  summary: This endpoint provides a list of statistics for the running TSD
  description: "# /api/stats\n\nThis endpoint provides a list of statistics for the running TSD. Sub endpoints return details about other TSD components such as the JVM, thread states or storage client. All statistics are read only.\n\n- [/api/stats/jvm](jvm)\n- [/api/stats/query](query)\n- [/api/stats/region_clients](region_clients)\n- [/api/stats/threads](threads)\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\nNo parameters available.\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/stats\n```\n\n## Response\n\nThe response is an array of objects. Fields in the response include:\n\n| Name      | Data Type | Description                                                                      | Example                       |\n|-----------|-----------|----------------------------------------------------------------------------------|-------------------------------|\n| metric    | String    | Name of the metric the statistic is recording                                    | tsd.connectionmgr.connections |\n| timestamp | Integer   | Unix epoch timestamp, in seconds, when the statistic was collected and displayed | 1369350222                    |\n| value     | Integer   | The numeric value for the statistic                                              | 42                            |\n| tags      | Map       | A list of key/value tag name/tag value pairs                                     | *See Below*                   |\n\n### Example Response\n\n``` javascript\n[\n  {\n    \"metric\": \"tsd.connectionmgr.connections\",\n    \"timestamp\": 1369350222,\n    \"value\": \"1\",\n    \"tags\": {\n      \"host\": \"wtdb-1-4\"\n    }\n  },\n  {\n    \"metric\": \"tsd.connectionmgr.exceptions\",\n    \"timestamp\": 1369350222,\n    \"value\": \"0\",\n    \"tags\": {\n      \"host\": \"wtdb-1-4\"\n    }\n  },\n  {\n    \"metric\": \"tsd.rpc.received\",\n    \"timestamp\": 1369350222,\n    \"value\": \"0\",\n    \"tags\": {\n      \"host\": \"wtdb-1-4\",\n      \"type\": \"telnet\"\n    }\n  }\n]\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/stats/index.html](http://opentsdb.net/docs/build/html/api_http/stats/index.html)"
- name: /api/stats/jvm
  id: api_http/stats/jvm
  summary: The threads endpoint is used for debugging the TSD's JVM process and includes stats about the garbage collector, system load and memory usage
  description: "# /api/stats/jvm\n\nThe threads endpoint is used for debugging the TSD's JVM process and includes stats about the garbage collector, system load and memory usage. (v2.2)\n\nNote\n\nThe information printed will change depending on the JVM you are running the TSD under. In particular, the pools and GC sections will differ quite a bit.\n\n## Verbs\n\n- GET\n\n## Requests\n\nNo parameters available.\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/stats/jvm\n```\n\n## Response\n\nThe response is an object with multiple sub objects. Top level objects include\n\n| Name    | Data Type | Description                                                                                                                    |\n|---------|-----------|--------------------------------------------------------------------------------------------------------------------------------|\n| os      | Object    | Information about the system                                                                                                   |\n| gc      | Object    | Information about the various garbage collectors such as how many times GC occurred and how long the process spent collecting. |\n| runtime | Object    | Details about the JVM including version and vendor, start timestamp (in millieconds) and the uptime.                           |\n| pools   | Object    | Details about each of the memory pools, particularly when used with a generational collector.                                  |\n| memory  | Object    | Information about the JVM's memory usage.                                                                                      |\n\n### Example Response\n\n``` javascript\n{\n  \"os\": {\n    \"systemLoadAverage\": 4.85\n  },\n  \"gc\": {\n    \"parNew\": {\n      \"collectionTime\": 26027510,\n      \"collectionCount\": 361039\n    },\n    \"concurrentMarkSweep\": {\n      \"collectionTime\": 333710,\n      \"collectionCount\": 396\n    }\n  },\n  \"runtime\": {\n    \"startTime\": 1441069233346,\n    \"vmVersion\": \"24.60-b09\",\n    \"uptime\": 1033439220,\n    \"vmVendor\": \"Oracle Corporation\",\n    \"vmName\": \"Java HotSpot(TM) 64-Bit Server VM\"\n  },\n  \"pools\": {\n    \"cMSPermGen\": {\n      \"collectionUsage\": {\n        \"init\": 21757952,\n        \"used\": 30044544,\n        \"committed\": 50077696,\n        \"max\": 85983232\n      },\n      \"usage\": {\n        \"init\": 21757952,\n        \"used\": 30045408,\n        \"committed\": 50077696,\n        \"max\": 85983232\n      },\n      \"type\": \"NON_HEAP\",\n      \"peakUsage\": {\n        \"init\": 21757952,\n        \"used\": 30045408,\n        \"committed\": 50077696,\n        \"max\": 85983232\n      }\n    },\n    \"parSurvivorSpace\": {\n      \"collectionUsage\": {\n        \"init\": 157024256,\n        \"used\": 32838400,\n        \"committed\": 157024256,\n        \"max\": 157024256\n      },\n      \"usage\": {\n        \"init\": 157024256,\n        \"used\": 32838400,\n        \"committed\": 157024256,\n        \"max\": 157024256\n      },\n      \"type\": \"HEAP\",\n      \"peakUsage\": {\n        \"init\": 157024256,\n        \"used\": 157024256,\n        \"committed\": 157024256,\n        \"max\": 157024256\n      }\n    },\n    \"codeCache\": {\n      \"collectionUsage\": null,\n      \"usage\": {\n        \"init\": 2555904,\n        \"used\": 8754368,\n        \"committed\": 8978432,\n        \"max\": 50331648\n      },\n      \"type\": \"NON_HEAP\",\n      \"peakUsage\": {\n        \"init\": 2555904,\n        \"used\": 8767040,\n        \"committed\": 8978432,\n        \"max\": 50331648\n      }\n    },\n    \"cMSOldGen\": {\n      \"collectionUsage\": {\n        \"init\": 15609561088,\n        \"used\": 1886862056,\n        \"committed\": 15609561088,\n        \"max\": 15609561088\n      },\n      \"usage\": {\n        \"init\": 15609561088,\n        \"used\": 5504187904,\n        \"committed\": 15609561088,\n        \"max\": 15609561088\n      },\n      \"type\": \"HEAP\",\n      \"peakUsage\": {\n        \"init\": 15609561088,\n        \"used\": 11849865176,\n        \"committed\": 15609561088,\n        \"max\": 15609561088\n      }\n    },\n    \"parEdenSpace\": {\n      \"collectionUsage\": {\n        \"init\": 1256259584,\n        \"used\": 0,\n        \"committed\": 1256259584,\n        \"max\": 1256259584\n      },\n      \"usage\": {\n        \"init\": 1256259584,\n        \"used\": 825272064,\n        \"committed\": 1256259584,\n        \"max\": 1256259584\n      },\n      \"type\": \"HEAP\",\n      \"peakUsage\": {\n        \"init\": 1256259584,\n        \"used\": 1256259584,\n        \"committed\": 1256259584,\n        \"max\": 1256259584\n      }\n    }\n  },\n  \"memory\": {\n    \"objectsPendingFinalization\": 0,\n    \"nonHeapMemoryUsage\": {\n      \"init\": 24313856,\n      \"used\": 38798912,\n      \"committed\": 59056128,\n      \"max\": 136314880\n    },\n    \"heapMemoryUsage\": {\n      \"init\": 17179869184,\n      \"used\": 6351794296,\n      \"committed\": 17022844928,\n      \"max\": 17022844928\n    }\n  }\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/stats/jvm.html](http://opentsdb.net/docs/build/html/api_http/stats/jvm.html)"
- name: /api/stats/query
  id: api_http/stats/query
  summary: This endpoint can be used for tracking and troubleshooting queries executed against a TSD
  description: "# /api/stats/query\n\nThis endpoint can be used for tracking and troubleshooting queries executed against a TSD. It maintains an unbounded list of currently executing queries as well as a list of up to 256 completed queries (rotating the oldest queries out of memory). Information about each query includes the original query, request headers, response code, timing and an exception if thrown. (v2.2)\n\n## Verbs\n\n- GET\n\n## Requests\n\nNo parameters available.\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/stats/query\n```\n\n## Response\n\nThe response includes two arrays. `completed` lists the 256 most recent queries that have finished execution, whether successfully or with an error. The `running` array contains a list of queries currently executing. If this list is growing, the TSD is under heavy load. Note that the running list will not contain an exception, response code or timing details.\n\nFor information on the various sections and data from the stats endpoint, see [*Query Details and Stats*](../../user_guide/query/stats).\n\n### Example Response\n\n``` javascript\n{\n    \"completed\": [{\n        \"query\": {\n            \"start\": \"1455531250181\",\n            \"end\": null,\n            \"timezone\": null,\n            \"options\": null,\n            \"padding\": false,\n            \"queries\": [{\n                \"aggregator\": \"zimsum\",\n                \"metric\": \"tsd.connectionmgr.bytes.written\",\n                \"tsuids\": null,\n                \"downsample\": \"1m-avg\",\n                \"rate\": true,\n                \"filters\": [{\n                    \"tagk\": \"colo\",\n                    \"filter\": \"*\",\n                    \"group_by\": true,\n                    \"type\": \"wildcard\"\n                }, {\n                    \"tagk\": \"env\",\n                    \"filter\": \"prod\",\n                    \"group_by\": true,\n                    \"type\": \"literal_or\"\n                }, {\n                    \"tagk\": \"role\",\n                    \"filter\": \"frontend\",\n                    \"group_by\": true,\n                    \"type\": \"literal_or\"\n                }],\n                \"rateOptions\": {\n                    \"counter\": true,\n                    \"counterMax\": 9223372036854775807,\n                    \"resetValue\": 1,\n                    \"dropResets\": false\n                },\n                \"tags\": {\n                    \"role\": \"literal_or(frontend)\",\n                    \"env\": \"literal_or(prod)\",\n                    \"colo\": \"wildcard(*)\"\n                }\n            }, {\n                \"aggregator\": \"zimsum\",\n                \"metric\": \"tsd.hbase.rpcs.cumulative_bytes_received\",\n                \"tsuids\": null,\n                \"downsample\": \"1m-avg\",\n                \"rate\": true,\n                \"filters\": [{\n                    \"tagk\": \"colo\",\n                    \"filter\": \"*\",\n                    \"group_by\": true,\n                    \"type\": \"wildcard\"\n                }, {\n                    \"tagk\": \"env\",\n                    \"filter\": \"prod\",\n                    \"group_by\": true,\n                    \"type\": \"literal_or\"\n                }, {\n                    \"tagk\": \"role\",\n                    \"filter\": \"frontend\",\n                    \"group_by\": true,\n                    \"type\": \"literal_or\"\n                }],\n                \"rateOptions\": {\n                    \"counter\": true,\n                    \"counterMax\": 9223372036854775807,\n                    \"resetValue\": 1,\n                    \"dropResets\": false\n                },\n                \"tags\": {\n                    \"role\": \"literal_or(frontend)\",\n                    \"env\": \"literal_or(prod)\",\n                    \"colo\": \"wildcard(*)\"\n                }\n            }],\n            \"delete\": false,\n            \"noAnnotations\": false,\n            \"globalAnnotations\": false,\n            \"showTSUIDs\": false,\n            \"msResolution\": false,\n            \"showQuery\": false,\n            \"showStats\": false,\n            \"showSummary\": false\n        },\n        \"exception\": \"null\",\n        \"executed\": 1,\n        \"user\": null,\n        \"requestHeaders\": {\n            \"Accept-Language\": \"en-US,en;q=0.8\",\n            \"Host\": \"tsdhost:4242\",\n            \"Content-Length\": \"440\",\n            \"Referer\": \"http://tsdhost:8080/dashboard/db/tsdfrontend\",\n            \"Accept-Encoding\": \"gzip, deflate\",\n            \"X-Forwarded-For\": \"192.168.0.2\",\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.109 Safari/537.36\",\n            \"Origin\": \"http://tsdhost:8080\",\n            \"Content-Type\": \"application/json;charset=UTF-8\",\n            \"Accept\": \"application/json, text/plain, */*\"\n        },\n        \"numRunningQueries\": 0,\n        \"httpResponse\": {\n            \"code\": 200,\n            \"reasonPhrase\": \"OK\"\n        },\n        \"queryStartTimestamp\": 1455552844368,\n        \"queryCompletedTimestamp\": 1455552844621,\n        \"sentToClient\": true,\n        \"stats\": {\n            \"avgAggregationTime\": 2.11416,\n            \"avgHBaseTime\": 200.267711,\n            \"avgQueryScanTime\": 242.037174,\n            \"avgScannerTime\": 200.474122,\n            \"avgScannerUidToStringTime\": 0.0,\n            \"avgSerializationTime\": 2.124153,\n            \"emittedDPs\": 716,\n            \"maxAggregationTime\": 2.093369,\n            \"maxHBaseTime\": 241.708782,\n            \"maxQueryScanTime\": 240.637231,\n            \"maxScannerUidtoStringTime\": 0.0,\n            \"maxSerializationTime\": 2.103411,\n            \"maxUidToStringTime\": 0.059345,\n            \"processingPreWriteTime\": 253.050907,\n            \"successfulScan\": 40,\n            \"totalTime\": 256.568992,\n            \"uidPairsResolved\": 0\n        }\n    }],\n    \"running\": []\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/stats/query.html](http://opentsdb.net/docs/build/html/api_http/stats/query.html)"
- name: /api/stats/region_clients
  id: api_http/stats/region_clients
  summary: Returns information about the various HBase region server clients in AsyncHBase
  description: "# /api/stats/region_clients\n\nReturns information about the various HBase region server clients in AsyncHBase. This helps to identify issues with a particular region server. (v2.2)\n\n## Verbs\n\n- GET\n\n## Requests\n\nNo parameters available.\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/stats/region_clients\n```\n\n## Response\n\nThe response is an array of objects. Fields in the response include:\n\n| Name                 | Data Type | Description                                                                                                                                                                                                                               | Example          |\n|----------------------|-----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|\n| pendingBreached      | Integer   | The total number of times writes to a new region client were discarded because it's pending RPC buffer was full. This should almost always be zero and a positive value indicates the TSD took a long time to connect to a region server. | 0                |\n| writesBlocked        | Integer   | How many RPCs (batched or individual) in total were blocked due to the connection's send buffer being full. A positive value indicates a slow HBase server or poor network performance.                                                   | 0                |\n| inflightBreached     | Integer   | The total number of times RPCs were blocked due to too many outstanding RPCs waiting for a response from HBase. A positive value indicates the region server is slow or network performance is poor.                                      | 0                |\n| dead                 | Boolean   | Whether or not the region client is marked as dead due to a connection close event (such as region server going down)                                                                                                                     | false            |\n| rpcsInFlight         | Integer   | The current number of RPCs sent to HBase and awaiting a response.                                                                                                                                                                         | 10               |\n| rpcsSent             | Integer   | The total number of RPCs sent to HBase.                                                                                                                                                                                                   | 424242           |\n| rpcResponsesUnknown  | Integer   | The total number of responses received from HBase for which we couldn't find an RPC. This may indicate packet corruption or an incompatible HBase version.                                                                                | 0                |\n| pendingBatchedRPCs   | Integer   | The number of RPCs queued in the batched RPC awaiting the next flush or the batch limit.                                                                                                                                                  | 0                |\n| endpoint             | String    | The IP and port of the region server in the format '/\\<ip\\>:\\<port\\>'                                                                                                                                                                     | /127.0.0.1:35008 |\n| rpcResponsesTimedout | Integer   | The total number of responses from HBase for RPCs that have previously timedout. This means HBase may be catching up and responding to stale RPCs.                                                                                        | 0                |\n| rpcid                | Integer   | The ID of the last RPC sent to HBase. This may be a negative number                                                                                                                                                                       | 42               |\n| rpcsTimedout         | Integer   | The total number of RPCs that have timed out. This may indicate a slow region server, poor network performance or GC issues with the TSD.                                                                                                 | 0                |\n| pendingRPCs          | Integer   | The number of RPCs queued and waiting for the connection handshake with the region server to complete                                                                                                                                     | 0                |\n\n### Example Response\n\n``` javascript\n[\n  {\n    \"pendingBreached\": 0,\n    \"writesBlocked\": 0,\n    \"inflightBreached\": 0,\n    \"dead\": false,\n    \"rpcsInFlight\": 0,\n    \"rpcsSent\": 35704,\n    \"rpcResponsesUnknown\": 0,\n    \"pendingBatchedRPCs\": 452,\n    \"endpoint\": \"/127.0.0.1:35008\",\n    \"rpcResponsesTimedout\": 0,\n    \"rpcid\": 35703,\n    \"rpcsTimedout\": 0,\n    \"pendingRPCs\": 0\n  }\n]\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/stats/region_clients.html](http://opentsdb.net/docs/build/html/api_http/stats/region_clients.html)"
- name: /api/stats/threads
  id: api_http/stats/threads
  summary: The threads endpoint is used for debugging the TSD and providing insight into the state and execution of various threads without having to resort to a JStack trace
  description: "# /api/stats/threads\n\nThe threads endpoint is used for debugging the TSD and providing insight into the state and execution of various threads without having to resort to a JStack trace. (v2.2)\n\n## Verbs\n\n- GET\n\n## Requests\n\nNo parameters available.\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/stats/threads\n```\n\n## Response\n\nThe response is an array of objects. Fields in the response include:\n\n| Name        | Data Type       | Description                                                | Example             |\n|-------------|-----------------|------------------------------------------------------------|---------------------|\n| threadID    | Integer         | Numeric ID of the thread                                   | 1                   |\n| priority    | Integer         | Execution priority for the thread                          | 5                   |\n| name        | String          | String name of the thread, usually assigned by default     | New I/O worker \\#23 |\n| interrupted | Boolean         | Whether or not the thread was interrupted                  | false               |\n| state       | String          | One of the valid Java thread states                        | RUNNABLE            |\n| stack       | Array\\<String\\> | A stack trace showing where execution is currently located | *See Below*         |\n\n### Example Response\n\n``` javascript\n[\n  {\n    \"threadID\": 33,\n    \"priority\": 5,\n    \"name\": \"AsyncHBase I/O Worker #23\",\n    \"interrupted\": false,\n    \"state\": \"RUNNABLE\",\n    \"stack\": [\n      \"sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\",\n      \"sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)\",\n      \"sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)\",\n      \"sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)\",\n      \"sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)\",\n      \"org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\",\n      \"org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)\",\n      \"org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\",\n      \"org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\",\n      \"org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\",\n      \"org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\",\n      \"org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\",\n      \"java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\",\n      \"java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\",\n      \"java.lang.Thread.run(Thread.java:695)\"\n    ]\n  },\n  {\n    \"threadID\": 6,\n    \"priority\": 9,\n    \"name\": \"Signal Dispatcher\",\n    \"interrupted\": false,\n    \"state\": \"RUNNABLE\",\n    \"stack\": []\n  },\n  {\n    \"threadID\": 21,\n    \"priority\": 5,\n    \"name\": \"AsyncHBase I/O Worker #11\",\n    \"interrupted\": false,\n    \"state\": \"RUNNABLE\",\n    \"stack\": [\n      \"sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\",\n      \"sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)\",\n      \"sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)\",\n      \"sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)\",\n      \"sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)\",\n      \"org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\",\n      \"org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)\",\n      \"org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\",\n      \"org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\",\n      \"org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\",\n      \"org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\",\n      \"org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\",\n      \"java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\",\n      \"java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\",\n      \"java.lang.Thread.run(Thread.java:695)\"\n    ]\n  },\n  {\n    \"threadID\": 2,\n    \"priority\": 10,\n    \"name\": \"Reference Handler\",\n    \"interrupted\": false,\n    \"state\": \"WAITING\",\n    \"stack\": [\n      \"java.lang.Object.wait(Native Method)\",\n      \"java.lang.Object.wait(Object.java:485)\",\n      \"java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)\"\n    ]\n  },\n  {\n    \"threadID\": 44,\n    \"priority\": 5,\n    \"name\": \"OpenTSDB Timer TSDB Timer #1\",\n    \"interrupted\": false,\n    \"state\": \"TIMED_WAITING\",\n    \"stack\": [\n      \"java.lang.Thread.sleep(Native Method)\",\n      \"org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:483)\",\n      \"org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:392)\",\n      \"org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\",\n      \"java.lang.Thread.run(Thread.java:695)\"\n    ]\n  }\n]\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/stats/threads.html](http://opentsdb.net/docs/build/html/api_http/stats/threads.html)"
- name: /api/suggest
  id: api_http/suggest
  summary: This endpoint provides a means of implementing an "auto-complete" call that can be accessed repeatedly as a user types a request in a GUI
  description: "# /api/suggest\n\nThis endpoint provides a means of implementing an \"auto-complete\" call that can be accessed repeatedly as a user types a request in a GUI. It does not offer full text searching or wildcards, rather it simply matches the entire string passed in the query on the first characters of the stored data. For example, passing a query of `type=metrics&q=sys` will return the top 25 metrics in the system that start with `sys`. Matching is case sensitive, so `sys` will not match `System.CPU`. Results are sorted alphabetically.\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\n| Name | Data Type | Required | Description                                                                                           | Default | QS   | RW  | Example |\n|------|-----------|----------|-------------------------------------------------------------------------------------------------------|---------|------|-----|---------|\n| type | String    | Required | The type of data to auto complete on. Must be one of the following: **metrics**, **tagk** or **tagv** |         | type |     | metrics |\n| q    | String    | Optional | A string to match on for the given type                                                               |         | q    |     | web     |\n| max  | Integer   | Optional | The maximum number of suggested results to return. Must be greater than 0                             | 25      | max  |     | 10      |\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/suggest?type=metrics&q=sys&max=10\n```\n\n**JSON Content**\n\n``` javascript\n{\n  \"type\":\"metrics\",\n  \"q\":\"sys\",\n  \"max\":10\n}\n```\n\n## Response\n\nThe response is an array of strings of the given type that match the query. If nothing was found to match the query, an empty array will be returned.\n\n### Example Response\n\n``` javascript\n[\n  \"sys.cpu.0.nice\",\n  \"sys.cpu.0.system\",\n  \"sys.cpu.0.user\",\n  \"sys.cpu.1.nice\",\n  \"sys.cpu.1.system\",\n  \"sys.cpu.1.user\"\n]\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/suggest.html](http://opentsdb.net/docs/build/html/api_http/suggest.html)"
- name: /api/tree
  id: api_http/tree/index
  summary: Trees are meta data used to organize time series in a heirarchical structure for browsing similar to a typical file system
  description: "# /api/tree\n\nTrees are meta data used to organize time series in a heirarchical structure for browsing similar to a typical file system. A number of endpoints under the `/tree` root allow working with various tree related data:\n\n## Tree API Endpoints\n\n- [/api/tree/branch](branch)\n- [/api/tree/collisions](collisions)\n- [/api/tree/notmatched](notmatched)\n- [/api/tree/rule](rule)\n- [/api/tree/rules](rules)\n- [/api/tree/test](test)\n\nThe `/tree` endpoint allows for creating or modifying a tree definition. Tree definitions include configuration and meta data accessible via this endpoint, as well as the rule set accessiable via `/tree/rule` or `/tree/rules`.\n\nNote\n\nWhen creating a tree it will have the `enabled` field set to `false` by default. After creating a tree you should add rules then use the `tree/test` endpoint with a few TSUIDs to make sure the resulting tree will be what you expected. After you have verified the results, you can set the `enabled` field to `true` and new TSMeta objects or a tree synchronization will start to populate branches.\n\n## Verbs\n\n- GET - Retrieve one or more tree definitions\n- POST - Edit tree fields\n- PUT - Replace tree fields\n- DELETE - Delete the results of a tree and/or the tree definition\n\n## Requests\n\nThe following fields can be used for all tree endpoint requests:\n\n| Name          | Data Type | Required   | Description                                                                                                                                                                  | Default | QS             | RW  | Example                          |\n|---------------|-----------|------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|----------------|-----|----------------------------------|\n| treeId        | Integer   | Required\\* | Used to fetch or modify a specific tree. [\\*](#id1)When creating a new tree, the `tree` value must not be present.                                                           |         | treeid         | RO  | 1                                |\n| name          | String    | Required\\* | A brief, descriptive name for the tree. [\\*](#id3)Required only when creating a tree.                                                                                        |         | name           | RW  | Network Infrastructure           |\n| description   | String    | Optional   | A longer description of what the tree contains                                                                                                                               |         | description    | RW  | Tree containing all network gear |\n| notes         | String    | Optional   | Detailed notes about the tree                                                                                                                                                |         | notes          | RW  |                                  |\n| strictMatch   | Boolean   | Optional   | Whether or not timeseries should be included in the tree if they fail to match one or more rule levels.                                                                      | false   | strict_match   | RW  | true                             |\n| enabled       | Boolean   | Optional   | Whether or not TSMeta should be processed through the tree. By default this is set to `false` so that you can setup rules and test some objects before building branches.    | false   | enabled        | RW  | true                             |\n| storeFailures | Boolean   | Optional   | Whether or not collisions and 'not matched' TSUIDs should be recorded. This can create very wide rows.                                                                       | false   | store_failures | RW  | true                             |\n| definition    | Boolean   | Optional   | Used only when `DELETE` ing a tree, if this flag is set to true, then the entire tree definition will be deleted along with all branches, collisions and not matched entries | false   | definition     |     | true                             |\n\n## Response\n\nA successful response to a `GET`, `POST` or `PUT` request will return tree objects with optinally requested changes. Successful `DELETE` calls will return with a `204` status code and no body content. When modifying data, if no changes were present, i.e. the call did not provide any data to store, the resposne will be a `304` without any body content. If the requested tree did not exist in the system, a `404` will be returned with an error message. If invalid data was supplied an `400` error will be returned.\n\nAll **Request** fields will be present in the response in addition to others:\n\n| Name    | Data Type | Description                                                                                                                                       | Example        |\n|---------|-----------|---------------------------------------------------------------------------------------------------------------------------------------------------|----------------|\n| rules   | Map       | A map or dictionary with rules defined for the tree organized by `level` and `order`. If no rules have been defined yet, the value will be `null` | *See Examples* |\n| created | Integer   | A Unix Epoch timestamp in seconds when the tree was originally created.                                                                           | 1350425579     |\n\n## GET\n\nA `GET` request to `/api/tree` without a tree ID will return a list of all of the trees configured in the system. The results will include configured rules for each tree. If no trees have been configured yet, the list will be empty.\n\n### Example GET All Trees Query\n\n``` python\nhttp://localhost:4242/api/tree\n```\n\n### Example Response\n\n``` javascript\n[\n  {\n    \"name\": \"Test Tree\",\n    \"description\": \"My Description\",\n    \"notes\": \"Details\",\n    \"rules\": {\n      \"0\": {\n        \"0\": {\n          \"type\": \"TAGK\",\n          \"field\": \"host\",\n          \"regex\": \"\",\n          \"separator\": \"\",\n          \"description\": \"Hostname rule\",\n          \"notes\": \"\",\n          \"level\": 0,\n          \"order\": 0,\n          \"treeId\": 1,\n          \"customField\": \"\",\n          \"regexGroupIdx\": 0,\n          \"displayFormat\": \"\"\n        }\n      },\n      \"1\": {\n        \"0\": {\n          \"type\": \"METRIC\",\n          \"field\": \"\",\n          \"regex\": \"\",\n          \"separator\": \"\",\n          \"description\": \"\",\n          \"notes\": \"Metric rule\",\n          \"level\": 1,\n          \"order\": 0,\n          \"treeId\": 1,\n          \"customField\": \"\",\n          \"regexGroupIdx\": 0,\n          \"displayFormat\": \"\"\n        }\n      }\n    },\n    \"created\": 1356998400,\n    \"treeId\": 1,\n    \"strictMatch\": false,\n    \"storeFailures\": false,\n    \"enabled\": true\n  },\n  {\n    \"name\": \"2nd Tree\",\n    \"description\": \"Other Tree\",\n    \"notes\": \"\",\n    \"rules\": {\n      \"0\": {\n        \"0\": {\n          \"type\": \"TAGK\",\n          \"field\": \"host\",\n          \"regex\": \"\",\n          \"separator\": \"\",\n          \"description\": \"\",\n          \"notes\": \"\",\n          \"level\": 0,\n          \"order\": 0,\n          \"treeId\": 2,\n          \"customField\": \"\",\n          \"regexGroupIdx\": 0,\n          \"displayFormat\": \"\"\n        }\n      },\n      \"1\": {\n        \"0\": {\n          \"type\": \"METRIC\",\n          \"field\": \"\",\n          \"regex\": \"\",\n          \"separator\": \"\",\n          \"description\": \"\",\n          \"notes\": \"\",\n          \"level\": 1,\n          \"order\": 0,\n          \"treeId\": 2,\n          \"customField\": \"\",\n          \"regexGroupIdx\": 0,\n          \"displayFormat\": \"\"\n        }\n      }\n    },\n    \"created\": 1368964815,\n    \"treeId\": 2,\n    \"strictMatch\": false,\n    \"storeFailures\": false,\n    \"enabled\": false\n  }\n]\n```\n\nTo fetch a specific tree, supply a [\\`\\`](#id5)treeId' value. The response will include the tree object if found. If the requested tree does not exist, a 404 exception will be returned.\n\n### Example GET Single Tree\n\n``` python\nhttp://localhost:4242/api/treeId?tree=1\n```\n\n### Example Response\n\n``` javascript\n{\n  \"name\": \"2nd Tree\",\n  \"description\": \"Other Tree\",\n  \"notes\": \"\",\n  \"rules\": {\n    \"0\": {\n      \"0\": {\n        \"type\": \"TAGK\",\n        \"field\": \"host\",\n        \"regex\": \"\",\n        \"separator\": \"\",\n        \"description\": \"\",\n        \"notes\": \"\",\n        \"level\": 0,\n        \"order\": 0,\n        \"treeId\": 2,\n        \"customField\": \"\",\n        \"regexGroupIdx\": 0,\n        \"displayFormat\": \"\"\n      }\n    },\n    \"1\": {\n      \"0\": {\n        \"type\": \"METRIC\",\n        \"field\": \"\",\n        \"regex\": \"\",\n        \"separator\": \"\",\n        \"description\": \"\",\n        \"notes\": \"\",\n        \"level\": 1,\n        \"order\": 0,\n        \"treeId\": 2,\n        \"customField\": \"\",\n        \"regexGroupIdx\": 0,\n        \"displayFormat\": \"\"\n      }\n    }\n  },\n  \"created\": 1368964815,\n  \"treeId\": 2,\n  \"strictMatch\": false,\n  \"storeFailures\": false,\n  \"enabled\": false\n}\n```\n\n## POST/PUT\n\nUsing the `POST` or `PUT` methods, you can create a new tree or edit most of the fields for an existing tree. New trees require a `name` value and for the `treeId'`` ``value`` ``to`` ``be`` ``empty.`` ``Existing`` ``trees`` ``require`` ``a`` ``valid`` ```` ``treeId ``` ID and any fields that require modification. A successful request will return the modified tree object.\n\nNote\n\nA new tree will not have any rules. Your next call should probably be to `/tree/rule` or `/tree/rules`.\n\n### Example POST Create Request\n\n``` python\nhttp://localhost:4242/api/tree?name=Network%20Tree&method_override=post\n```\n\n### Example Response\n\n``` javascript\n{\n  \"name\": \"Network\",\n  \"description\": \"\",\n  \"notes\": \"\",\n  \"rules\": null,\n  \"created\": 1368964815,\n  \"treeId\": 3,\n  \"strictMatch\": false,\n  \"storeFailures\": false,\n  \"enabled\": false\n}\n```\n\n### Example POST Edit Request\n\n``` python\nhttp://localhost:4242/api/tree?treeId=3&description=Network%20Device%20Information&method_override=post\n```\n\n### Example Response\n\n``` javascript\n{\n  \"name\": \"Network\",\n  \"description\": \"Network Device Information\",\n  \"notes\": \"\",\n  \"rules\": null,\n  \"created\": 1368964815,\n  \"treeId\": 3,\n  \"strictMatch\": false,\n  \"storeFailures\": false,\n  \"enabled\": false\n}\n```\n\n## DELETE\n\nUsing the `DELETE` method will remove only collisions, not matched entries and branches for the given tree from storage. This endpoint starts a delete. Because the delete can take some time, the endpoint will return a successful 204 response without data if the delete completed. If the tree was not found, it will return a 404. If you want to delete the tree definition itself, you can supply the `defintion` flag in the query string with a value of `true` and the tree and rule definitions will be removed as well.\n\nWarning\n\nThis method cannot be undone. Once executed, the purge will continue running unless the TSD is shutdown.\n\nNote\n\nBefore executing a `DELETE` query, you should make sure that a manual tree syncronization is not running somehwere on your data. If it is, there may be some orphaned branches or leaves stored during the purge. Use the \\_\\_\\_\\_\\_ CLi tool sometime after the delete to cleanup left over branches or leaves.\n\n### Example DELETE Request\n\n``` python\nhttp://localhost:4242/api/tree?tree=1&method_override=delete\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/tree/index.html](http://opentsdb.net/docs/build/html/api_http/tree/index.html)"
- name: /api/tree/branch
  id: api_http/tree/branch
  summary: A branch represents a level in the tree heirarchy and contains information about child branches and/or leaves
  description: "# /api/tree/branch\n\nA branch represents a level in the tree heirarchy and contains information about child branches and/or leaves. Branches are immutable from an API perspective and can only be created or modified by processing a TSMeta through tree rules via a CLI command or when a new timeseries is encountered or a TSMeta object modified. Therefore the `branch` endpoint only supports the `GET` verb.\n\nA branch is identified by a `branchId`, a hexadecimal encoded string that represents the ID of the tree it belongs to as well as the IDs of each parent the branch stems from. All branches stem from the **ROOT** branch of a tree and this is usually the starting place when browsing. To fetch the **ROOT** just call this endpoingt with a valid `treeId`. The root branch ID is also a 4 character encoding of the tree ID.\n\n## Verbs\n\n- GET\n\n## Requests\n\nThe following fields can be used to request a branch. Only one or the other may be used.\n\n| Name   | Data Type | Required | Description                                                                                                     | Default | QS     | RW  | Example      |\n|--------|-----------|----------|-----------------------------------------------------------------------------------------------------------------|---------|--------|-----|--------------|\n| treeId | Integer   | Optional | Used to fetch the root branch of the tree. If used in combination with a branchId, the tree ID will be ignored. |         | treeid | RO  | 1            |\n| branch | String    | Required | A hexadecimal representation of the branch ID, required for all but the root branch request                     |         | branch | RO  | 000183A21C8F |\n\n## Response\n\nA successful response to a request will return the branch object using the requested serializer. If the requested tree or branch did not exist in the system, a `404` will be returned with an error message.\n\nFields returned with the response include:\n\n| Name        | Data Type | Description                                                              | Example            |\n|-------------|-----------|--------------------------------------------------------------------------|--------------------|\n| treeId      | Integer   | The ID of the tree the branch belongs to                                 | 1                  |\n| displayName | String    | Name of the branch as determined by the rule set                         | sys                |\n| branchId    | String    | Hexadecimal encoded ID of the branch                                     | 00010001BECD       |\n| depth       | Integer   | Depth of the branch within the tree, starting at *0* for the root branch | 1                  |\n| path        | Map       | List of parent branch names and their depth.                             | *See Below*        |\n| branches    | Array     | An array of child branch objects. May be `null`.                         | *See Below*        |\n| leaves      | Array     | An array of child leaf objects. May be `null`.                           | *See Leaves Below* |\n\n**Leaves**\n\nIf a branch contains child leaves, i.e. timeseries stored in OpenTSDB, their metric, tags, TSUID and display name will be contained in the results. Leaf fields are as follows:\n\n| Name        | Data Type | Description                                                | Example            |\n|-------------|-----------|------------------------------------------------------------|--------------------|\n| metric      | String    | The name of the metric for the timeseries                  | sys.cpu.0          |\n| tags        | Map       | A list of tag names and values representing the timeseries | *See Below*        |\n| tsuid       | String    | Hexadecimal encoded timeseries ID                          | 000001000001000001 |\n| displayName | String    | A name as parsed by the rule set                           | user               |\n\n## GET\n\n### Example Root GET Query\n\n``` python\nhttp://localhost:4242/api/tree/branch?treeid=1\n```\n\n### Example Response\n\n``` javascript\n{\n  \"leaves\": null,\n  \"branches\": [\n    {\n      \"leaves\": null,\n      \"branches\": null,\n      \"path\": {\n        \"0\": \"ROOT\",\n        \"1\": \"sys\"\n      },\n      \"treeId\": 1,\n      \"displayName\": \"sys\",\n      \"branchId\": \"00010001BECD\",\n      \"depth\": 1\n    }\n  ],\n  \"path\": {\n    \"0\": \"ROOT\"\n  },\n  \"treeId\": 1,\n  \"displayName\": \"ROOT\",\n  \"branchId\": \"0001\",\n  \"depth\": 0\n}\n```\n\n### Example Branch GET Query\n\n``` python\nhttp://localhost:4242/api/tree/branch?branchid=00010001BECD000181A8\n```\n\n### Example Response\n\n``` javascript\n{\n  \"leaves\": [\n    {\n      \"metric\": \"sys.cpu.0.user\",\n      \"tags\": {\n        \"host\": \"web01\"\n      },\n      \"tsuid\": \"000001000001000001\",\n      \"displayName\": \"user\"\n    }\n  ],\n  \"branches\": [\n    {\n      \"leaves\": null,\n      \"branches\": null,\n      \"path\": {\n        \"0\": \"ROOT\",\n        \"1\": \"sys\",\n        \"2\": \"cpu\",\n        \"3\": \"mboard\"\n      },\n      \"treeId\": 1,\n      \"displayName\": \"mboard\",\n      \"branchId\": \"00010001BECD000181A8BF992A99\",\n      \"depth\": 3\n    }\n  ],\n  \"path\": {\n    \"0\": \"ROOT\",\n    \"1\": \"sys\",\n    \"2\": \"cpu\"\n  },\n  \"treeId\": 1,\n  \"displayName\": \"cpu\",\n  \"branchId\": \"00010001BECD000181A8\",\n  \"depth\": 2\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/tree/branch.html](http://opentsdb.net/docs/build/html/api_http/tree/branch.html)"
- name: /api/tree/collisions
  id: api_http/tree/collisions
  summary: When processing a TSMeta, if the resulting leaf would overwrite an existing leaf with a different TSUID, a collision will be recorded
  description: "# /api/tree/collisions\n\nWhen processing a TSMeta, if the resulting leaf would overwrite an existing leaf with a different TSUID, a collision will be recorded. This endpoint allows retreiving a list of the TSUIDs that were not included in a tree due to collisions. It is useful for debugging in that if you find a TSUID in this list, you can pass it through the `/tree/test` endpoint to get details on why the collision occurred.\n\nNote\n\nCalling this endpoint without a list of one or more TSUIDs will return all collisions in the tree. If you have a large number of timeseries in your system, the response can potentially be very large. Thus it is best to use this endpoint with specific TSUIDs.\n\nNote\n\nIf `storeFailures` is diabled for the tree, this endpoint will not return any data. Collisions will still appear in the TSD's logs.\n\n## Verbs\n\n- GET\n\n## Requests\n\nThe following fields are used for this endpoint\n\n| Name   | Data Type | Required | Description                                                                                                                                     | Default | QS     | RW  | Example                              |\n|--------|-----------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------|---------|--------|-----|--------------------------------------|\n| treeId | Integer   | Required | The ID of the tree to pass the TSMeta objects through                                                                                           |         | treeid |     | 1                                    |\n| tsuids | String    | Required | A list of one or more TSUIDs to search for collision entries. If requesting testing of more than one TSUID, they should be separted by a comma. |         | tsuids |     | 000001000001000001,00000200000200002 |\n\n## Response\n\nA successful response will return a map of key/value pairs where the unrecorded TSUID as the key and the existing leave's TSUID as the value. The response will only return collisions that were found. If one or more of the TSUIDs requested did not result in a collision, it will not be returned with the result. This may mean that the TSMeta has not been processed yet. Note that if no collisions have occurred or the tree hasn't processed any data yet, the result set will be empty. If the requested tree did not exist in the system, a `404` will be returned with an error message. If invalid data was supplied a `400` error will be returned.\n\n### Example Request\n\n> [http://localhost:4242/api/tree/collisions?treeId=1&tsuids=010101,020202](http://localhost:4242/api/tree/collisions?treeId=1&tsuids=010101,020202)\n\n### Example Response\n\n``` javascript\n{\n  \"010101\": \"AAAAAA\",\n  \"020202\": \"BBBBBB\"\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/tree/collisions.html](http://opentsdb.net/docs/build/html/api_http/tree/collisions.html)"
- name: /api/tree/notmatched
  id: api_http/tree/notmatched
  summary: When processing a TSMeta, if the tree has strictMatch enabled and the meta fails to match on a rule in any level of the set, a not matched entry will be recorded
  description: "# /api/tree/notmatched\n\nWhen processing a TSMeta, if the tree has `strictMatch` enabled and the meta fails to match on a rule in any level of the set, a *not matched* entry will be recorded. This endpoint allows for retrieving the list of TSUIDs that failed to match a rule set. It is useful for debugging in that if you find a TSUID in this list, you can pass it through the `/tree/test` endpoint to get details on why the meta failed to match.\n\nNote\n\nCalling this endpoint without a list of one or more TSUIDs will return all non-matched TSUIDs in the tree. If you have a large number of timeseries in your system, the response can potentially be very large. Thus it is best to use this endpoint with specific TSUIDs.\n\nNote\n\nIf `storeFailures` is diabled for the tree, this endpoint will not return any data. Not Matched entries will still appear in the TSD's logs.\n\n## Verbs\n\n- GET\n\n## Requests\n\nThe following fields are used for this endpoint\n\n| Name   | Data Type | Required | Description                                                                                                                                       | Default | QS     | RW  | Example                              |\n|--------|-----------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------|---------|--------|-----|--------------------------------------|\n| treeId | Integer   | Required | The ID of the tree to pass the TSMeta objects through                                                                                             |         | treeid |     | 1                                    |\n| tsuids | String    | Required | A list of one or more TSUIDs to search for not-matched entries. If requesting testing of more than one TSUID, they should be separted by a comma. |         | tsuids |     | 000001000001000001,00000200000200002 |\n\n## Response\n\nA successful response will return a map of key/value pairs where the unrecorded TSUID as the key and a message about which rule failed to match as the value. The response will only return not matched entries that were found. If one or more of the TSUIDs requested did not result in a not matched entry, it will not be returned with the result. This may mean that the TSMeta has not been processed yet. Note that if no failed matches have occurred or the tree hasn't processed any data yet, the result set will be empty. If the requested tree did not exist in the system, a `404` will be returned with an error message. If invalid data was supplied a `400` error will be returned.\n\n### Example Request\n\n> [http://localhost:4242/api/tree/notmatched?treeId=1&tsuids=010101,020202](http://localhost:4242/api/tree/notmatched?treeId=1&tsuids=010101,020202)\n\n### Example Response\n\n``` javascript\n{\n  \"010101\": \"Failed rule 0:0\",\n  \"020202\": \"Failed rule 1:1\"\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/tree/notmatched.html](http://opentsdb.net/docs/build/html/api_http/tree/notmatched.html)"
- name: /api/tree/rule
  id: api_http/tree/rule
  summary: Each rule in a tree is an individual object in storage, thus the /api/tree/rule endpoint allows for easy modification of a single rule in the set
  description: "# /api/tree/rule\n\nEach rule in a tree is an individual object in storage, thus the `/api/tree/rule` endpoint allows for easy modification of a single rule in the set. Rules are addressed by their `tree` ID, `level` and `order` and all requests require these three parameters.\n\nNote\n\nIf a manual tree synchronization is running somewhere or there is a large number of TSMeta objects being created or edited, the tree rule may be cached and modifications to a tree's rule set may take some time to propagate. If you make any modifications to the rule set, other than to meta information such as the `description` and `notes`, you may want to flush the tree data and perform a manual synchronization so that branches and leaves reflect the new rules.\n\n## Verbs\n\n- GET - Retrieve one or more rules\n- POST - Create or modify a rule\n- PUT - Create or replace a rule\n- DELETE - Delete a rule\n\n## Requests\n\nThe following fields can be used for all rule endpoint requests:\n\n| Name          | Data Type | Required   | Description                                                                                                                                         | Default | QS              | RW  | Example                                        |\n|---------------|-----------|------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|---------|-----------------|-----|------------------------------------------------|\n| treeId        | Integer   | Required   | The tree the requested rule belongs to                                                                                                              |         | treeid          | RO  | 1                                              |\n| level         | Integer   | Required   | The level in the rule heirarchy where the rule resides. Must be 0 or greater.                                                                       | 0       | level           | RW  | 2                                              |\n| order         | Integer   | Required   | The order within a level where the rule resides. Must be 0 or greater.                                                                              | 0       | order           | RW  | 1                                              |\n| description   | String    | Optional   | A brief description of the rule's purpose                                                                                                           |         | description     | RW  | Split the metric by dot                        |\n| notes         | String    | Optional   | Detailed notes about the rule                                                                                                                       |         | notes           | RW  |                                                |\n| type          | String    | Required\\* | The type of rule represented. See [*Trees*](../../user_guide/trees). [\\*](#id1)Required when creating a new rule.                                   |         | type            | RW  | METRIC                                         |\n| field         | String    | Optional   | The name of a field for the rule to operate on                                                                                                      |         | field           | RW  | host                                           |\n| customField   | String    | Optional   | The name of a `TSMeta` custom field for the rule to operate on. Note that the `field` value must also be configured or an exception will be raised. |         | custom_field    | RW  | owner                                          |\n| regex         | String    | Optional   | A regular expression pattern to process the associated field or custom field value through.                                                         |         | regex           | RW  | ^.\\*\\\\(\\[a-zA-Z\\]{3,4})\\[0-9\\]{0,1}\\\\.\\*\\\\.\\*$ |\n| separator     | String    | Optional   | If the field value should be split into multiple branches, provide the separation character.                                                        |         | separator       | RW  | \\\\                                             |\n| regexGroupIdx | Integer   | Optional   | A group index for extracting a portion of a pattern from the given regular expression pattern. Must be 0 or greater.                                | 0       | regex_group_idx | RW  | 1                                              |\n| displayFormat | String    | Optional   | A display format string to alter the `display_name` value of the resulting branch or leaf. See [*Trees*](../../user_guide/trees)                    |         | display_format  | RW  | Port: {ovalue}                                 |\n\nNote\n\nWhen supplying a `separator` or a `regex` value, you must supply a valid regular expression. For separators, the most common use is to split dotted metrics into branches. E.g. you may want \"sys.cpu.0.user\" to be split into \"sys\", \"cpu\", \"0\" and \"user\" branches. You cannot supply just a \".\" for the separator value as that will not match properly. Instead, escape the period via \".\". Note that if you are supplying JSON via a POST request, you must escape the backslash as well and supply \"\\\\\". GET request responses will escape all backslashes.\n\n## Response\n\nA successful response to a `GET`, `POST` or `PUT` request will return the full rule object with optional requested changes. Successful `DELETE` calls will return with a `204` status code and no body content. When modifying data, if no changes were present, i.e. the call did not provide any data to store, the resposne will be a `304` without any body content. If the requested tree or rule did not exist in the system, a `404` will be returned with an error message. If invalid data was supplied a `400` error will be returned.\n\n## GET\n\nA `GET` request requires a specific tree ID, rule level and order. Otherwise a `400` will be returned. To fetch all of the rules for a tree, use the `/api/tree` endpoint with a [\\`\\`](#id3)treeId' value.\n\n### Example GET Query\n\n``` python\nhttp://localhost:4242/api/tree/rule?treeId=1&level=0&order=0\n```\n\n### Example Response\n\n``` javascript\n{\n  \"type\": \"METRIC\",\n  \"field\": \"\",\n  \"regex\": \"\",\n  \"separator\": \"\\\\.\",\n  \"description\": \"Split the metric on periods\",\n  \"notes\": \"\",\n  \"level\": 1,\n  \"order\": 0,\n  \"treeId\": 1,\n  \"customField\": \"\",\n  \"regexGroupIdx\": 0,\n  \"displayFormat\": \"\"\n}\n```\n\n## POST/PUT\n\nUsing the `POST` or `PUT` methods, you can create a new rule or edit an existing rule. New rules require a `type` value. Existing trees require a valid `treeId` ID and any fields that require modification. A successful request will return the modified rule object. Note that if a rule exists at the given level and order, any changes will be merged with or overwrite the existing rule.\n\n### Example Query String Request\n\n``` python\nhttp://localhost:4242/api/tree/rule?treeId=1&level=0&order=0&type=METRIC&separator=\\.&method_override=post\n```\n\n### Example Content Request\n\n``` javascript\n{\n  \"type\": \"METRIC\",\n  \"separator\": \"\\\\.\",\n  \"description\": \"Split the metric on periods\",\n  \"level\": 1,\n  \"order\": 0,\n  \"treeId\": 1\n}\n```\n\n### Example Response\n\n``` javascript\n{\n  \"type\": \"METRIC\",\n  \"field\": \"\",\n  \"regex\": \"\",\n  \"separator\": \"\\\\.\",\n  \"description\": \"Split the metric on periods\",\n  \"notes\": \"\",\n  \"level\": 1,\n  \"order\": 0,\n  \"treeId\": 1,\n  \"customField\": \"\",\n  \"regexGroupIdx\": 0,\n  \"displayFormat\": \"\"\n}\n```\n\n## DELETE\n\nUsing the `DELETE` method will remove a rule from a tree. A successful deletion will respond with a `204` status code and no content body. If the rule did not exist, a `404` error will be returned.\n\nWarning\n\nThis method cannot be undone.\n\n### Example DELETE Request\n\n``` python\nhttp://localhost:4242/api/tree/rule?treeId=1&level=0&order=0&method_override=delete\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/tree/rule.html](http://opentsdb.net/docs/build/html/api_http/tree/rule.html)"
- name: /api/tree/rules
  id: api_http/tree/rules
  summary: The rules endpoint is used for bulk merging, replacing or deleting the entire ruleset of a tree
  description: "# /api/tree/rules\n\nThe rules endpoint is used for bulk merging, replacing or deleting the entire ruleset of a tree. Instead of calling the `tree/rule` endpoint multiple times for a single rule, you can supply a list of rules that will be merged into, or replace, the current rule set. Note that the `GET` verb is not supported for this endpoint. To fetch the ruleset, load the tree via the `/tree` endpoint. Also, all data must be provided in request content, query strings are not supported.\n\n## Verbs\n\n- POST - Merge rule sets\n- PUT - Replace the entire rule set\n- DELETE - Delete a rule\n\n## Requests\n\nA request to store data must be an array of objects in the content of the request. The same fields as required for the [*/api/tree/rule*](rule) endpoint are supported.\n\n## Response\n\nA successful response to a `POST` or `PUT` request will return a `204` response code without body content. Successful `DELETE` calls will return with a `204` status code and no body content. If a tree does not have any rules, the `DELETE` request will still return a `204`. When modifying data, if no changes were present, i.e. the call did not provide any data to store, the response will be a `304` without any body content. If the requested tree did not exist in the system, a `404` will be returned with an error message. If invalid data was supplied a `400` error will be returned.\n\n## POST/PUT\n\nIssuing a `POST` will merge the given rule set with any that already exist. This means that if a rule already exists for one of the given rules, only the fields given will be modified in the existing rule. Using the `PUT` method will replace *all* of the rules for the given tree with the new set. Any existing rules for the tree will be deleted before the new rules are stored.\n\nNote\n\nAll of the rules in the request array must belong to the same `treeId` or a `400` exception will be returned. Likewise, all of the rules will pass validation and must include the `level` and `order` fields.\n\n### Example POST Request\n\n``` javascript\nhttp://localhost:4242/api/tree/rule?treeId=1&level=0&order=0&type=METRIC&separator=.&method_override=post\n```\n\n### Example Content Request\n\n``` javascript\n[\n  {\n    \"treeId\": 1,\n    \"level\": 0,\n    \"order\": 0,\n    \"type\": \"METRIC\",\n    \"description\": \"Metric split rule\",\n    \"split\": \"\\\\.\"\n  },\n  {\n    \"treeId\": 1,\n    \"level\": 0,\n    \"order\": 1,\n    \"type\": \"tagk\",\n    \"field\": \"fqdn\",\n    \"description\": \"Hostname for the device\"\n  },\n  {\n    \"treeId\": 1,\n    \"level\": 1,\n    \"order\": 0,\n    \"type\": \"tagk\",\n    \"field\": \"department\"\n    \"description\": \"Department that owns the device\"\n  }\n]\n```\n\n## DELETE\n\nUsing the `DELETE` method will remove all rules from a tree. A successful deletion will respond with a `204` status code and no content body. If the tree did not exist, a `404` error will be returned.\n\nWarning\n\nThis method cannot be undone.\n\n### Example DELETE Request\n\n``` python\nhttp://localhost:4242/api/tree/rules?treeId=1&method_override=delete\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/tree/rules.html](http://opentsdb.net/docs/build/html/api_http/tree/rules.html)"
- name: /api/tree/test
  id: api_http/tree/test
  summary: For debugging a rule set, the test endpoint can be used to run a TSMeta object through a tree's rules and determine where in the heirarchy the leaf would appear
  description: "# /api/tree/test\n\nFor debugging a rule set, the test endpoint can be used to run a TSMeta object through a tree's rules and determine where in the heirarchy the leaf would appear. Or find out why a timeseries failed to match on a rule set or collided with an existing timeseries. The only method supported is `GET` and no changes will be made to the actual tree in storage when using this endpoint.\n\nThe `messages` field of the response contains information about what occurred during processing. If the TSUID did not exist or an error occurred, the reason will be found in this field. During processing, each rule that the TSMeta is processed through will generate a message. If a rule matched on the TSMeta successfully or failed, the reason will be recorded.\n\n## Verbs\n\n- GET\n\n## Requests\n\nThe following fields are required for this endpoint.\n\n| Name   | Data Type | Required | Description                                                                                                                         | Default | QS     | RW  | Example                              |\n|--------|-----------|----------|-------------------------------------------------------------------------------------------------------------------------------------|---------|--------|-----|--------------------------------------|\n| treeId | Integer   | Required | The ID of the tree to pass the TSMeta objects through                                                                               |         | treeid |     | 1                                    |\n| tsuids | String    | Required | A list of one or more TSUIDs to fetch TSMeta for. If requesting testing of more than one TSUID, they should be separted by a comma. |         | tsuids |     | 000001000001000001,00000200000200002 |\n\n## Response\n\nA successful response will return a list of JSON objects with a number of items including the TSMeta object, messages about the processing steps taken and a resulting branch. There will be one object for each TSUID requested with the TSUID as the object name. If the requested tree did not exist in the system, a `404` will be returned with an error message. If invalid data was supplied a `400` error will be returned.\n\nFields found in the response include:\n\n| Name     | Data Type        | Description                                                | Example     |\n|----------|------------------|------------------------------------------------------------|-------------|\n| messages | Array of Strings | A list of messages for each level and rule of the rule set | *See Below* |\n| meta     | Object           | The TSMeta object loaded from storage                      | *See Below* |\n| branch   | Object           | The full tree if successfully parsed                       | *See Below* |\n\n### Example Request\n\n> [http://localhost:4242/api/tree/test?treeId=1&tsuids=000001000001000001000002000002](http://localhost:4242/api/tree/test?treeId=1&tsuids=000001000001000001000002000002)\n\n### Example Response\n\n``` javascript\n{\n  \"000001000001000001000002000002\": {\n    \"messages\": [\n      \"Processing rule: [1:0:0:TAGK]\",\n      \"Matched tagk [host] for rule: [1:0:0:TAGK]\",\n      \"Processing rule: [1:1:0:METRIC]\",\n      \"Depth [3] Adding leaf [name: sys.cpu.0 tsuid: 000001000001000001000002000002] to parent branch [Name: [web-01.lga.mysite.com]]\"\n    ],\n    \"meta\": {\n      \"tsuid\": \"000001000001000001000002000002\",\n      \"metric\": {\n        \"uid\": \"000001\",\n        \"type\": \"METRIC\",\n        \"name\": \"sys.cpu.0\",\n        \"description\": \"\",\n        \"notes\": \"\",\n        \"created\": 1368979404,\n        \"custom\": null,\n        \"displayName\": \"\"\n      },\n      \"tags\": [\n        {\n          \"uid\": \"000001\",\n          \"type\": \"TAGK\",\n          \"name\": \"host\",\n          \"description\": \"\",\n          \"notes\": \"\",\n          \"created\": 1368979404,\n          \"custom\": null,\n          \"displayName\": \"\"\n        },\n        {\n          \"uid\": \"000001\",\n          \"type\": \"TAGV\",\n          \"name\": \"web-01.lga.mysite.com\",\n          \"description\": \"\",\n          \"notes\": \"\",\n          \"created\": 1368979404,\n          \"custom\": null,\n          \"displayName\": \"\"\n        },\n        {\n          \"uid\": \"000002\",\n          \"type\": \"TAGK\",\n          \"name\": \"type\",\n          \"description\": \"\",\n          \"notes\": \"\",\n          \"created\": 1368979404,\n          \"custom\": null,\n          \"displayName\": \"\"\n        },\n        {\n          \"uid\": \"000002\",\n          \"type\": \"TAGV\",\n          \"name\": \"user\",\n          \"description\": \"\",\n          \"notes\": \"\",\n          \"created\": 1368979404,\n          \"custom\": null,\n          \"displayName\": \"\"\n        }\n      ],\n      \"description\": \"\",\n      \"notes\": \"\",\n      \"created\": 0,\n      \"units\": \"\",\n      \"retention\": 0,\n      \"max\": \"NaN\",\n      \"min\": \"NaN\",\n      \"displayName\": \"\",\n      \"lastReceived\": 0,\n      \"totalDatapoints\": 0,\n      \"dataType\": \"\"\n    },\n    \"branch\": {\n      \"leaves\": null,\n      \"branches\": [\n        {\n          \"leaves\": [\n            {\n              \"metric\": \"\",\n              \"tags\": null,\n              \"tsuid\": \"000001000001000001000002000002\",\n              \"displayName\": \"sys.cpu.0\"\n            }\n          ],\n          \"branches\": null,\n          \"path\": {\n            \"0\": \"ROOT\",\n            \"1\": \"web-01.lga.mysite.com\"\n          },\n          \"treeId\": 1,\n          \"displayName\": \"web-01.lga.mysite.com\",\n          \"branchId\": \"0001247F7202\",\n          \"numLeaves\": 1,\n          \"numBranches\": 0,\n          \"depth\": 1\n        }\n      ],\n      \"path\": {\n        \"0\": \"ROOT\"\n      },\n      \"treeId\": 1,\n      \"displayName\": \"ROOT\",\n      \"branchId\": \"0001\",\n      \"numLeaves\": 0,\n      \"numBranches\": 1,\n      \"depth\": 0\n    }\n  }\n}\n```\n\n### Example Error Response\n\n``` javascript\n{\n  \"000001000001000001000002000003\": {\n    \"branch\": null,\n    \"messages\": [\n      \"Unable to locate TSUID meta data\"\n    ],\n    \"meta\": null\n  }\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/tree/test.html](http://opentsdb.net/docs/build/html/api_http/tree/test.html)"
- name: /api/uid
  id: api_http/uid/index
  summary: Every metric, tag name and tag value is associated with a unique identifier (UID)
  description: "# /api/uid\n\nEvery metric, tag name and tag value is associated with a unique identifier (UID). Internally, the UID is a binary array assigned to a text value the first time it is encountered or via an explicit assignment request. This endpoint provides utilities for managing UIDs and their associated data. Please see the UID endpoint TOC below for information on what functions are implemented.\n\nUIDs exposed via the API are encoded as hexadecimal strings. The UID `42` would be expressed as `00002A` given the default UID width of 3 bytes.\n\nYou may also edit meta data associated with timeseries or individual UID objects via the UID endpoint.\n\n## UID API Endpoints\n\n- [/api/uid/assign](assign)\n- [/api/uid/tsmeta](tsmeta)\n- [/api/uid/uidmeta](uidmeta)\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/uid/index.html](http://opentsdb.net/docs/build/html/api_http/uid/index.html)"
- name: /api/uid/assign
  id: api_http/uid/assign
  summary: This endpoint enables assigning UIDs to new metrics, tag names and tag values
  description: "# /api/uid/assign\n\nThis endpoint enables assigning UIDs to new metrics, tag names and tag values. Multiple types and names can be provided in a single call and the API will process each name individually, reporting which names were assigned UIDs successfully, along with the UID assigned, and which failed due to invalid characters or had already been assigned. Assignment can be performed via query string or content data.\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\nEach request must have one or more of the following fields:\n\n| Name   | Data Type | Required | Description                           | Default | QS     | RW  | Example   |\n|--------|-----------|----------|---------------------------------------|---------|--------|-----|-----------|\n| metric | String    | Optional | A list of metric names for assignment |         | metric | RW  | sys.cpu.0 |\n| tagk   | String    | Optional | A list of tag names for assignment    |         | tagk   | RW  | host      |\n| tagv   | String    | Optional | A list of tag values for assignment   |         | tagv   | RW  | web01     |\n\nWhen making a query string request, multiple names for a given type can be supplied in a comma separated fashion. E.g. `metric=sys.cpu.0,sys.cpu.1,sys.cpu.2,sys.cpu.3`. Naming conventions apply: see \\_\\_\\_\\_\\_\\_\\_.\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/uid/assign?metric=sys.cpu.0,sys.cpu.1&tagk=host&tagv=web01,web02,web03\n```\n\n**JSON Content**\n\n``` javascript\n{\n  \"metric\": [\n    \"sys.cpu.0\",\n    \"sys.cpu.1\",\n    \"illegal!character\"\n  ],\n  \"tagk\": [\n    \"host\"\n  ],\n  \"tagv\": [\n    \"web01\",\n    \"web02\",\n    \"web03\"\n  ]\n}\n```\n\n## Response\n\nThe response will contain a map of successful assignments along with the hex encoded UID value. If one or more values were not assigned, a separate map will contain a list of the values and the reason why they were not assigned. Maps with the type name and `<type>_errors` will be generated only if one or more values for that type were provided.\n\nWhen all values are assigned, the endpoint returns a 200 status code but if any value failed assignment, it will return a 400.\n\n### Example Response\n\n``` javascript\n{\n  \"metric\": {},\n  \"metric_errors\": {\n    \"sys.cpu.0\": \"Name already exists with UID: 000042\",\n    \"sys.cpu.1\": \"Name already exists with UID: 000043\",\n    \"illegal!character\": \"Invalid metric (illegal!character): illegal character: !\",\n  },\n  \"tagv\": {},\n  \"tagk_errors\": {\n    \"host\": \"Name already exists with UID: 0007E5\"\n  },\n  \"tagk\": {\n    \"web01\": \"000012\",\n    \"web02\": \"000013\",\n    \"web03\": \"000014\"\n  }\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/uid/assign.html](http://opentsdb.net/docs/build/html/api_http/uid/assign.html)"
- name: /api/uid/tsmeta
  id: api_http/uid/tsmeta
  summary: This endpoint enables searching, editing or deleting timeseries meta data information, that is meta data associated with a specific timeseries associated with a metric and one or more tag name/value pairs
  description: "# /api/uid/tsmeta\n\nThis endpoint enables searching, editing or deleting timeseries meta data information, that is meta data associated with a specific timeseries associated with a *metric* and one or more *tag name/value* pairs. Some fields are set by the TSD but others can be set by the user. When using the `POST` method, only the fields supplied with the request will be stored. Existing fields that are not included will be left alone. Using the `PUT` method will overwrite all user mutable fields with given values or defaults if a given field is not provided.\n\nPlease note that deleting a meta data entry will not delete the data points stored for the timeseries. Neither will it remove the UID assignments or associated UID meta objects.\n\n## Verbs\n\n- GET - Lookup one or more TS meta data\n- POST - Updates only the fields provided\n- PUT - Overwrites all user configurable meta data fields\n- DELETE - Deletes the TS meta data\n\n## GET Requests\n\nA GET request can lookup the TS meta objects for one or more time series if they exist in the storage system. Two types of queries are supported:\n\n- **tsuid** - A single hexadecimal TSUID may be supplied and a meta data object will be returned if located. The results will include a single object.\n- **metric** - *(Version 2.1)* Similar to a data point query, you can supply a metric and one or more tag pairs. Any TS meta data matching the query will be returned. The results will be an array of one or more objects. Only one metric query may be supplied per call and wild cards or grouping operators are not supported.\n\n### Example TSUID GET Request\n\n``` python\nhttp://localhost:4242/api/uid/tsmeta?tsuid=00002A000001000001\n```\n\n### Example Metric GET Request\n\n``` python\nhttp://localhost:4242/api/uid/tsmeta?m=sys.cpu.nice&dc=lga\n```\n\n## POST/PUT Requests\n\nBy default, you may only write data to a TS meta object if it already exists. TS meta data is created via the meta sync CLI command or in real-time as data points are written. If you attempt to write data to the tsmeta endpoint for a TSUID that does not exist, an error will be returned and no data will be saved.\n\nFields that can be supplied with a request include:\n\n| Name        | Data Type | Required | Description                                                                                                                                                                                                  | Default | QS           | RW  | Example               |\n|-------------|-----------|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|--------------|-----|-----------------------|\n| tsuid       | String    | Required | A hexadecimal representation of the timeseries UID                                                                                                                                                           |         | tsuid        | RO  | 00002A000001000001    |\n| description | String    | Optional | A brief description of what the UID represents                                                                                                                                                               |         | description  | RW  | System processor time |\n| displayName | String    | Optional | A short name that can be displayed in GUIs instead of the default name                                                                                                                                       |         | display_name | RW  | System CPU Time       |\n| notes       | String    | Optional | Detailed notes about what the UID represents                                                                                                                                                                 |         | notes        | RW  | Details               |\n| custom      | Map       | Optional | A key/value map to store custom fields and values                                                                                                                                                            | null    |              | RW  | *See Below*           |\n| units       | String    | Optional | Units reflective of the data stored in the timeseries, may be used in GUIs or calculations                                                                                                                   |         | units        | RW  | Mbps                  |\n| dataType    | String    | Optional | The kind of data stored in the timeseries such as `counter`, `gauge`, `absolute`, etc. These may be defined later but they should be similar to Data Source Types in an [RRD](http://oss.oetiker.ch/rrdtool) |         | data_type    | RW  | counter               |\n| retention   | Integer   | Optional | The number of days of data points to retain for the given timeseries. **Not Implemented**. When set to 0, the default, data is retained indefinitely.                                                        | 0       | retention    | RW  | 365                   |\n| max         | Float     | Optional | An optional maximum value for this timeseries that may be used in calculations such as percent of maximum. If the default of `NaN` is present, the value is ignored.                                         | NaN     | max          | RW  | 1024                  |\n| min         | Float     | Optional | An optional minimum value for this timeseries that may be used in calculations such as percent of minimum. If the default of `NaN` is present, the value is ignored.                                         | NaN     | min          | RW  | 0                     |\n\nNote\n\nCustom fields cannot be passed via query string. You must use the `POST` or `PUT` verbs.\n\nWarning\n\nIf your request uses `PUT`, any fields that you do not supply with the request will be overwritten with their default values. For example, the `description` field will be set to an emtpy string and the `custom` field will be reset to `null`.\n\nWith OpenTSDB 2.1 you may supply a metric style query and, if UIDs exist for the given metric and tags, a new TS meta object will be stored. Data may be supplied via POST for the fields above as per a normal request, however the `tsuid` field must be left empty. Additionally two query string parameters must be supplied:\n\n- **m** - A metric and tags similar to a GET request or data point query\n- **create** - A flag with a value of `true`\n\nFor example:\n\n``` python\nhttp://localhost:4242/api/uid/tsmeta?display_name=Testing&m=sys.cpu.nice{host=web01,dc=lga}&create=true&method_override=post\n```\n\nIf a TS meta object already exists in storage for the given metric and tags, the fields will be updated or overwritten.\n\n### Example POST or PUT Request\n\n*Query String:*\n\n``` python\nhttp://localhost:4242/api/uid/tsmeta?tsuid=00002A000001000001&method_override=post&display_name=System%20CPU%20Time\n```\n\n*JSON Content:*\n\n``` javascript\n{\n  \"tsuid\":\"00002A000001000001\",\n  \"displayName\":\"System CPU Time for Webserver 01\",\n  \"custom\": {\n    \"owner\": \"Jane Doe\",\n    \"department\": \"Operations\",\n    \"assetTag\": \"12345\"\n  }\n}\n```\n\n### Example DELETE Request\n\n*Query String:*\n\n``` python\nhttp://localhost:4242/api/uid/tsmeta?tsuid=00002A000001000001&method_override=delete\n```\n\n*JSON Content:*\n\n``` javascript\n{\n  \"tsuid\":\"00002A000001000001\"\n}\n```\n\n## Response\n\nA successful response to a `GET`, `POST` or `PUT` request will return the full TS meta data object with any given changes. Successful `DELETE` calls will return with a `204` status code and no body content. When modifying data, if no changes were present, i.e. the call did not provide any data to store, the resposne will be a `304` without any body content. If the requested TSUID did not exist in the system, a `404` will be returned with an error message. If invalid data was supplied an error will be returned.\n\nAll **Request** fields will be present in the response in addition to others:\n\n| Name            | Data Type        | Description                                                                                                                                                                                                                                                            | Example     |\n|-----------------|------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|\n| metric          | UIDMeta          | A UID meta data object representing information about the UID                                                                                                                                                                                                          | *See Below* |\n| tags            | Array of UIDMeta | A list of tag name / tag value UID meta data objects associated with the timeseries. The `tagk` UID will be first followed by it's corresponding `tagv` object.                                                                                                        | *See Below* |\n| created         | Integer          | A Unix epoch timestamp, in seconds, when the timeseries was first recorded in the system. Note that if the TSD was upgraded or meta data recently enabled, this value may not be accurate. Run the [*uid*](../../user_guide/cli/uid) utility to synchronize meta data. | 1350425579  |\n| lastReceived    | Integer          | A Unix epoch timestamp, in seconds, when a data point was last recieved. This is only updated on TSDs where meta data is enabled and it is not updated for every data point so there may be some lag.                                                                  | 1350425579  |\n| totalDatapoints | Integer          | The total number of data points recorded for the timeseries. NOTE: This may not be accurate unless you have enabled metadata tracking since creating the TSDB tables.                                                                                                  | 3242322     |\n\n### Example Response\n\n``` javascript\n{\n  \"tsuid\": \"00002A000001000001\",\n  \"metric\": {\n    \"uid\": \"00002A\",\n    \"type\": \"METRIC\",\n    \"name\": \"sys.cpu.0\",\n    \"description\": \"System CPU Time\",\n    \"notes\": \"\",\n    \"created\": 1350425579,\n    \"custom\": null,\n    \"displayName\": \"\"\n  },\n  \"tags\": [\n    {\n      \"uid\": \"000001\",\n      \"type\": \"TAGK\",\n      \"name\": \"host\",\n      \"description\": \"Server Hostname\",\n      \"notes\": \"\",\n      \"created\": 1350425579,\n      \"custom\": null,\n      \"displayName\": \"Hostname\"\n    },\n    {\n      \"uid\": \"000001\",\n      \"type\": \"TAGV\",\n      \"name\": \"web01.mysite.com\",\n      \"description\": \"Website hosting server\",\n      \"notes\": \"\",\n      \"created\": 1350425579,\n      \"custom\": null,\n      \"displayName\": \"Web Server 01\"\n    }\n  ],\n  \"description\": \"Measures CPU activity\",\n  \"notes\": \"\",\n  \"created\": 1350425579,\n  \"units\": \"\",\n  \"retention\": 0,\n  \"max\": \"NaN\",\n  \"min\": \"NaN\",\n  \"custom\": {\n    \"owner\": \"Jane Doe\",\n    \"department\": \"Operations\",\n    \"assetTag\": \"12345\"\n  },\n  \"displayName\": \"\",\n  \"dataType\": \"absolute\",\n  \"lastReceived\": 1350425590,\n  \"totalDatapoints\", 12532\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/uid/tsmeta.html](http://opentsdb.net/docs/build/html/api_http/uid/tsmeta.html)"
- name: /api/uid/uidmeta
  id: api_http/uid/uidmeta
  summary: This endpoint enables editing or deleting UID meta data information, that is meta data associated with metrics, tag names and tag values
  description: "# /api/uid/uidmeta\n\nThis endpoint enables editing or deleting UID meta data information, that is meta data associated with *metrics*, *tag names* and *tag values*. Some fields are set by the TSD but others can be set by the user. When using the `POST` method, only the fields supplied with the request will be stored. Existing fields that are not included will be left alone. Using the `PUT` method will overwrite all user mutable fields with given values or defaults if a given field is not provided.\n\nNote\n\nDeleting a meta data entry will not delete the UID assignment nor will it delete any data points or associated timeseries information. Deletion only removes the specified meta data object, not the actual value. If you query for the same UID, you'll see the default meta data with empty fields.\n\n## Verbs\n\n- GET - Query string only\n- POST - Updates only the fields provided\n- PUT - Overwrites all user configurable meta data fields\n- DELETE - Deletes the UID meta data\n\n## Requests\n\nFields that can be supplied with a request include:\n\n| Name        | Data Type | Required | Description                                                            | Default | QS           | RW  | Example               |\n|-------------|-----------|----------|------------------------------------------------------------------------|---------|--------------|-----|-----------------------|\n| uid         | String    | Required | A hexadecimal representation of the UID                                |         | uid          | RO  | 00002A                |\n| type        | String    | Required | The type of UID, must be `metric`, `tagk` or `tagv`                    |         | type         | RO  | metric                |\n| description | String    | Optional | A brief description of what the UID represents                         |         | description  | RW  | System processor time |\n| displayName | String    | Optional | A short name that can be displayed in GUIs instead of the default name |         | display_name | RW  | System CPU Time       |\n| notes       | String    | Optional | Detailed notes about what the UID represents                           |         | notes        | RW  | Details               |\n| custom      | Map       | Optional | A key/value map to store custom fields and values                      | null    |              | RW  | *See Below*           |\n\nNote\n\nCustom fields cannot be passed via query string. You must use the `POST` or `PUT` verbs.\n\nWarning\n\nIf your request uses `PUT`, any fields that you do not supply with the request will be overwritten with their default values. For example, the `description` field will be set to an emtpy string and the `custom` field will be reset to `null`.\n\n### Example GET Request\n\n``` python\nhttp://localhost:4242/api/uid/uidmeta?uid=00002A&type=metric\n```\n\n### Example POST or PUT Request\n\n*Query String:*\n\n``` python\nhttp://localhost:4242/api/uid/uidmeta?uid=00002A&type=metric&method=post&display_name=System%20CPU%20Time\n```\n\n*JSON Content:*\n\n``` javascript\n{\n  \"uid\":\"00002A\",\n  \"type\":\"metric\",\n  \"displayName\":\"System CPU Time\",\n  \"custom\": {\n    \"owner\": \"Jane Doe\",\n    \"department\": \"Operations\",\n    \"assetTag\": \"12345\"\n  }\n}\n```\n\n### Example DELETE Request\n\n*Query String:*\n\n``` python\nhttp://localhost:4242/api/uid/uidmeta?uid=00002A&type=metric&method=delete\n```\n\n*JSON Content:*\n\n``` javascript\n{\n  \"uid\":\"00002A\",\n  \"type\":\"metric\"\n}\n```\n\n## Response\n\nA successful response to a `GET`, `POST` or `PUT` request will return the full UID meta data object with any given changes. Successful `DELETE` calls will return with a `204` status code and no body content. When modifying data, if no changes were present, i.e. the call did not provide any data to store, the response will be a `304` without any body content. If the requested UID did not exist in the system, a `404` will be returned with an error message. If invalid data was supplied an error will be returned.\n\nAll **Request** fields will be present in the response in addition to a couple of others:\n\n| Name    | Data Type | Description                                                                                                                                       | Example    |\n|---------|-----------|---------------------------------------------------------------------------------------------------------------------------------------------------|------------|\n| name    | String    | The name of the UID as given when the data point was stored or the UID assigned                                                                   | sys.cpu.0  |\n| created | Integer   | A Unix epoch timestamp in seconds when the UID was first created. If the meta data was not stored when the UID was assigned, this value may be 0. | 1350425579 |\n\n### Example Response\n\n``` javascript\n{\n  \"uid\": \"00002A\",\n  \"type\": \"TAGV\",\n  \"name\": \"web01.mysite.com\",\n  \"description\": \"Website hosting server\",\n  \"notes\": \"This server needs a new boot disk\",\n  \"created\": 1350425579,\n  \"custom\": {\n    \"owner\": \"Jane Doe\",\n    \"department\": \"Operations\",\n    \"assetTag\": \"12345\"\n  },\n  \"displayName\": \"Webserver 01\"\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/uid/uidmeta.html](http://opentsdb.net/docs/build/html/api_http/uid/uidmeta.html)"
- name: /api/version
  id: api_http/version
  summary: This endpoint returns information about the running version of OpenTSDB
  description: "# /api/version\n\nThis endpoint returns information about the running version of OpenTSDB.\n\n## Verbs\n\n- GET\n- POST\n\n## Requests\n\nThis endpoint does not require any parameters via query string or body.\n\n### Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/api/version\n```\n\n## Response\n\nThe response is a hash map of version properties and values.\n\n### Example Response\n\n``` javascript\n{\n  \"timestamp\": \"1362712695\",\n  \"host\": \"localhost\",\n  \"repo\": \"/opt/opentsdb/build\",\n  \"full_revision\": \"11c5eefd79f0c800b703ebd29c10e7f924c01572\",\n  \"short_revision\": \"11c5eef\",\n  \"user\": \"localuser\",\n  \"repo_status\": \"MODIFIED\",\n  \"version\": \"2.0.0\"\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/version.html](http://opentsdb.net/docs/build/html/api_http/version.html)"
- name: /s
  id: api_http/s
  summary: This endpoint was introduced in 1.0 as a means of accessing static files on the local system
  description: "# /s\n\nThis endpoint was introduced in 1.0 as a means of accessing static files on the local system. `/s` will be maintained in the future and will not be deprecated. The static root is definied in the config file as `tsd.http.staticroot` or CLI via `--staticroot`.\n\nBy default, static files will be returned with a header telling clients to cache them for 1 year. Any file that contains `nocache` in the name (e.g. `queryui.nocache.js`, the idiom used by GWT) will not include the cache header.\n\nNote\n\nThe TSD will attempt to return the correct **Content-Type** header for the requested file. However the TSD code doesn't support very many formats at this time, just HTML, JSON, Javascript and PNG. Let us know what formats you need or issue a pull request with your patches.\n\nWarning\n\nThe code for this endpoint is very simple and does not include any security. Thus you should make sure that permissions on your static root directory are secure so that users can't write malicious files and serve them out of OpenTSDB. Users shouldn't be able to write files via OpenTSDB, but take precautions just to be safe.\n\n## Verbs\n\nAll verbs are supported and simply ignored\n\n## Requests\n\nQuery string and content body requests are ignored. Rather the requested file is a component of the path, e.g. `/s/index.html` will return the contents of the `index.html` file.\n\n## Example Request\n\n**Query String**\n\n``` python\nhttp://localhost:4242/s/queryui.nocache.js\n```\n\n## Response\n\nThe response will be the contents of the requested file with appropriate HTTP headers configured.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/s.html](http://opentsdb.net/docs/build/html/api_http/s.html)"
- name: Additional Resources
  id: resources
  summary: These are just some of the awesome front-ends, utilities, libraries and resources created by the OpenTSDB community
  description: "# Additional Resources\n\nThese are just some of the awesome front-ends, utilities, libraries and resources created by the OpenTSDB community. Please let us know if you have a project you'd like to see listed and if you don't see something you need, search for it on Github (new projects are popping up all the time) or your favorite search engine.\n\n## Monitoring\n\n- [Bosun](https://bosun.org/) - A monitoring and alerting system built on OpenTSDB from the folks at [Stack Exchange](http://stackexchange.com/).\n\n## Docker Images\n\n- [petergrace/opentsdb-docker](https://registry.hub.docker.com/u/petergrace/opentsdb-docker/) - A prebuilt Docker image with HBase and OpenTSDB already configured and ready to run! If you have Docker installed, execute `docker`` ``run`` ``-d`` ``-p`` ``4242:4242`` ``petergrace/opentsdb-docker` to create an opentsdb instance running on port 4242.\n- [opower/opentsdb](https://registry.hub.docker.com/u/opower/opentsdb/) - A Docker image containing OpenTSDB, HBase, and tcollector. Comes in both 2.0.1 and 2.1 versions (latest defaults to 2.1). Execute `docker`` ``run`` ``-d`` ``-p`` ``4242:4242`` ``opower/opentsdb` to create an OpenTSDB instance running on port 4242.\n\n## Front Ends\n\n- [Status Wolf](https://github.com/box/StatusWolf) - A PHP and MySQL based dashboard for creating and storing dynamic custom graphs with OpenTSDB data including anonmaly detection.\n- [Metrilyx](https://github.com/Ticketmaster/metrilyx-2.0) - A Python and Django based dashboard system with dynamic graphs from Ticketmaster.\n- [Opentsdb-Dashboard](https://github.com/clover/opentsdb-dashboard) - An HBase based dashboard system for OpenTSDB 1.x from Clover.\n- [TSDash](https://github.com/facebook/tsdash) - A Java based UI and dashboard from Facebook.\n- [OpenTSDB Dashboard](https://github.com/turn/opentsdb-dashboard) - A JQuery based dashboard from Turn.\n- [Grafana](http://grafana.org) - A dashboard and graph editor with OpenTSDB support.\n- [Graphite OpenTSDB Finder](https://github.com/mikebryant/graphite-opentsdb-finder) - A Graphite plugin to load TSDB data.\n\n## Utilities\n\n- [opentsdbjsonproxy](https://github.com/noca/opentsdbjsonproxy) - An HTTP proxy to convert 1.x ASCII output from the `/q` endpoint to JSON for use with High Charts or other libraries.\n- [Collectd-opentsdb](https://github.com/auxesis/collectd-opentsdb) - A Collectd plugin to emmit stats to a TSD.\n- [Collectd-opentsdb Java](https://github.com/dotcloud/collectd-opentsdb) - A Collectd plugin to that uses the OpenTSDB Java API to push data to a TSD.\n- TSD_proxy \\<[https://github.com/aravind/tsd_proxy](https://github.com/aravind/tsd_proxy)\\>\\`\\_ - A buffering write proxy for OpenTSDB and alternate DBs.\n- [Vacuumetrix](https://github.com/99designs/vacuumetrix) - Utility to pull data from various cloud services or APIs and store the results in backends such as Graphite, Ganglia and OpenTSDB.\n- [JuJu Deployment Charm](https://github.com/charms/opentsdb) - Utility to compile OpenTSDB from GIT and deploy on a cluster.\n- [Statsd Publisher](https://github.com/danslimmon/statsd-opentsdb-backend) - A statsd backend to publish data to a TSD.\n- [OpenTSDB Proxy](https://github.com/nimbusproject/opentsdbproxy) - A Django based proxy with authentication and SSL support to run in front of the TSDs.\n- [Puppet Module](https://github.com/mburger/puppet-opentsdb) - A puppet deployment module.\n- [Flume Module](https://github.com/yandex/opentsdb-flume) - Write data from Flume to a TSD.\n- [Chef Cookbook](https://github.com/looztra/opentsdb-cookbook) - Deploy from source via Chef.\n- - [OpenTSDB Cookbook](https://github.com/acaiafa/opentsdb-cookbook) - A Chef cookbook for CentOS or Ubuntu.\n- [Coda Hale Metrics Reporter](https://github.com/sps/metrics-opentsdb) - Writes data to OpenTSDB from the Java Metrics library.\n- [Alternative Coda Hale Metrics Reporter](https://github.com/stuart-warren/metrics-opentsdb) - Writes data to OpenTSDB from the Java Metrics library.\n- [opentsdb-snmp](https://github.com/frogmaster/opentsdb-snmp) - Fetches data from SNMP enabled devices and writes to OpenTSDB.\n- [proxTSDB](https://github.com/worldline/proxyTSDB) - A metric data gateway capable of buffering data to RAM or disk if the TSD is down.\n- [OpenTSDB Pig UDFs](https://github.com/santosh-d3vpl3x/opentsdb-udfs) - Integrate OpenTSDB with Apache Pig for large data set processing.\n\n## Clients\n\n- [R Client](https://github.com/holstius/opentsdbr) - A client to pull data from OpenTSDB into R.\n- [Erlang Client](https://github.com/bradfordw/gen_opentsdb) - A simple client to publish data to a TSD from Erlang.\n- [time-series](https://github.com/opower/time-series) - A Ruby client that supports both reading and writing to OpenTSDB 2.x - contains support for synthetic time series calculations.\n- [Ruby](https://github.com/j05h/continuum) - A read-only client for querying data from the 1.x API.\n- [Ruby](https://github.com/johnewart/ruby-opentsdb) A write-only client for pushing data to a TSD.\n- [Go](https://github.com/bzub/go-opentsdb) - Work with OpenTSDB data in Go.\n- [Potsdb](https://pypi.python.org/pypi/potsdb) - A Python client for writing data.\n- [vert.x OpenTsDb](https://github.com/cyngn/vertx-opentsdb) - A library to write data to OpenTSDB from Vert.x.\n\n## References to OpenTSDB\n\n- [HBase in Action](http://www.manning.com/dimidukkhurana/) (Manning Publications) - Chapter 7: HBase by Example: OpenTSDB\n- [Professional NoSQL](http://www.wrox.com/WileyCDA/WroxTitle/Professional-NoSQL.productCd-047094224X.html) (Wrox Publishing) - Mentioned in Chapter 17: Tools and Utilities\n- [OSCon Data 2011](http://www.youtube.com/watch?v=WlsyqhrhRZA) - Presentation from Benoit Sigoure\n- [Percona Live 2013](http://www.slideshare.net/geoffanderson/monitoring-mysql-with-opentsdb-19982758) Presentation from Geoffrey Anderson\n- [HBaseCon 2013](http://www.hbasecon.com/sessions/opentsdb-at-scale/) - Presentation from Jonathan Creasy and Geoffrey Anderson\n- [Strata 2011](http://strataconf.com/strata2011/public/schedule/detail/16996) - Presentation by Benoit Sigoure\n\n## Statistical Analysis Tools\n\n- [GnuPlot](http://www.gnuplot.info/) - Graphing library used by OpenTSDB\n- [R](http://www.r-project.org/) - Statistical computing framework\n- [SciPy](http://www.scipy.org/) - Python libraries for dealing with numbers (Pandas library has time series support)\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/resources.html](http://opentsdb.net/docs/build/html/resources.html)"
- name: Aggregators
  id: user_guide/query/aggregators
  summary: OpenTSDB was designed to efficiently combine multiple, distinct time series during query execution
  description: "# Aggregators\n\nOpenTSDB was designed to efficiently combine multiple, distinct time series during query execution. But how do you merge individual time series into a single series of data? Aggregation functions provide the means of mathematically merging the different data series into one, giving you a choice of various mathematical operations. Since OpenTSDB doesn't know whether or not a query will return multiple time series, an aggregation function is always required just in case.\n\nAggregators have two methods of operation:\n\n## Aggregation\n\nSince OpenTSDB doesn't know whether a query will return multiple time series until it scans through all of the data, an aggregation function must be specified for every query just in case. When more than one series is found, the two series are **aggregated** together into a single time series. For each timestamp in the different time series, the aggregator will perform it's computation for each value in every time series at that timestamp. That is, the aggregator will work *across* all of the time series at each timestamp. The following table illustrates the `sum` aggregator as it works across time series `A` and `B` to produce series `Output`.\n\n| series | ts0 | ts0+10s | ts0+20s | ts0+30s | ts0+40s | ts0+50s |\n|--------|-----|---------|---------|---------|---------|---------|\n| A      | 5   | 5       | 10      | 15      | 20      | 5       |\n| B      | 10  | 5       | 20      | 15      | 10      | 0       |\n| Output | 15  | 10      | 30      | 30      | 30      | 5       |\n\nFor timestamp `ts0` the data points for `A` and `B` are summed, i.e. `5`` ``+`` ``10`` ``==`` ``15`. Next, the two values for `ts1` are summed together to get `10` and so on. Each aggregation function will perform a different mathematical operation.\n\n### Interpolation\n\nIn the example above, both time series `A` and `B` had data points at every time stamp, they lined up neatly. However what happens when two series do not line up? It can be difficult, and sometimes undesired, to synchronize all sources of data to write at the exact same time. For example, if we have 10,000 servers sending 100 system metrics every 5 minutes, that would be a burst of 10M data points in a single second. We would need a pretty beefy network and cluster to accommodate that traffic. Not to mention the system would be sitting idle for the rest of 5 minutes. Instead it makes much more sense to splay the writes over time so that we have an average of 3,333 writes per second to reduce our hardware and network requirements.\n\nMissing Data\n\nBy \"missing\" we simply mean that a time series does not have a data point for the timestamp requested. Usually the data is simply time shifted before or after the requested timestamp, but it could actually be missing if the source or the TSD encountered an error and the data wasn't recorded.\n\nHow do you *sum* or find the *avg* of a number and something that doesn't exist? One option is to simply ignore the data points for all time series at the time stamp where any series is missing data. But if you have two time series and they are simply miss-aligned, your query would return an empty data set even though there is good data in storage, so that's not very useful.\n\nAnother option is to define a scalar value (e.g. `0` or the maximum value for a Long) to use whenever a data point is missing. OpenTSDB 2.0 provides a few aggregation methods that substitute a scalar value for missing data points. These are useful when working with distinct value time series such as the number of sales in at a given time.\n\nHowever sometimes it doesn't make sense to define a scalar for missing data. Often you may be recording a monotonically increasing counter such as the number of bytes transmitted from a network interface. With a counter, we can use **interpolation** to make a guess as to what the value would be at that point in time. Interpolation takes two points and the time span between them to calculate a *best guess* value at the time stamp requested.\n\nTake a look at these two time series where the data is simply offset by 10 seconds:\n\n| series | ts0 | ts0+10s | ts0+20s | ts0+30s | ts0+40s | ts0+50s | ts0+60s |\n|--------|-----|---------|---------|---------|---------|---------|---------|\n| A      | na  | 5       | na      | 15      | na      | 5       | na      |\n| B      | 10  | na      | 20      | na      | 10      | na      | 20      |\n\nWhen OpenTSDB is calculating an aggregation it starts at the first data point found for any series, in this case it will be the data for `B` at `ts0`. We request a value for `A` at `ts0` but there isn't any data there. We know that there is data for `A` at `ts0+10s` but since we don't have any value before that, we can't make a guess as to what it would be. Thus we simply return the value for `B`.\n\nNext we run across a value for `A` at time `ts0+10s`. We request a value for `ts0+10s` from time series `B` but there isn't one. But `B` knows there is a value at `ts0+20s` and we had a value at `ts0` so we can now calculate a guess for `ts0+10s`. The formula for linear interpolation is `y`` ``=`` ``y0`` ``+`` ``(y1`` ``-`` ``y0)`` ``*`` ``((x`` ``-`` ``x0)`` ``/`` ``(x1`` ``-`` ``x0))` where, for series `B`, `y0`` ``=`` ``10`, `y1`` ``=`` ``20`, `x`` ``=`` ``ts0+10s`` ``(or`` ``10)`, `x0`` ``=`` ``ts0`` ``(or`` ``0)` and `x1`` ``=`` ``ts0+20s`` ``(or`` ``20)`. Thus we have `y`` ``=`` ``10`` ``+`` ``(20`` ``-`` ``10)`` ``*`` ``((10`` ``-`` ``0)`` ``/`` ``(20`` ``-`` ``0)` which will reduce to `y`` ``=`` ``10`` ``+`` ``10`` ``*`` ``(10`` ``/`` ``20)` further reducing to `y`` ``=`` ``10`` ``+`` ``10`` ``*`` ``.5` and `y`` ``=`` ``10`` ``+`` ``5`. Therefore `B` will give us a *guestimated* value of `15` at `ts0+10s`.\n\nIteration continues over every timestamp for which a data point is found for every series returned as a part of the query. The resulting series, using the **sum** aggregator, will look like this:\n\n| series         | ts0 | ts0+10s | ts0+20s | ts0+30s | ts0+40s | ts0+50s | ts0+60s |\n|----------------|-----|---------|---------|---------|---------|---------|---------|\n| A              | na  | 5       | na      | 15      | na      | 5       | na      |\n| B              | 10  | na      | 20      | na      | 10      | na      | 20      |\n| Interpolated A | na  |         | 10      |         | 10      |         |         |\n| Interpolated B |     | 15      |         | 15      |         | 15      | na      |\n| Summed Result  | 10  | 20      | 30      | 25      | 20      | 20      | 20      |\n\n**More Examples:** For the graphically inclined we have the following examples. An imaginary metric named `m` is recorded in OpenTSDB. The \"sum of m\" is the blue line at the top resulting from a query like `start=1h-ago&m=sum:m`. It's made of the sum of the red line for `host=foo` and the green line for `host=bar`:\n\nIt seems intuitive from the image above that if you \"stack up\" the red line and the green line, you'd get the blue line. At any discrete point in time, the blue line has a value that is equal to the sum of the value of the red line and the value of the green line at that time. Without interpolation, you get something rather unintuitive that is harder to make sense of, and which is also a lot less meaningful and useful:\n\nNotice how the blue line drops down to the green data point at 18:46:48. No need to be a mathematician or to have taken advanced maths classes to see that interpolation is needed to properly aggregate multiple time series together and get meaningful results.\n\nAt the moment OpenTSDB primarily supports [linear interpolation](http://en.wikipedia.org/wiki/Linear_interpolation) (sometimes shortened \"lerp\") along with some aggregators that will simply substitute zeros or the max or min value. Patches are welcome for those who would like to add other interpolation methods.\n\nInterpolation is only performed at query time when more than one time series are found to match a query. Many metrics collection systems interpolate on *write* so that you original value is never recorded. OpenTSDB stores your original value and lets you retrieve it at any time.\n\nHere is another slightly more complicated example that came from the mailing list, depicting how multiple time series are aggregated by average:\n\n[](../../_images/aggregation_average.png)\n\nThe thick blue line with triangles is the an aggregation with the `avg` function of multiple time series as per the query `start=1h-ago&m=avg:duration_seconds`. As we can see, the resulting time series has one data point at each timestamp of all the underlying time series it aggregates, and that data point is computed by taking the average of the values of all the time series at that timestamp. This is also true for the lonely data point of the squared-purple time series, that temporarily boosted the average until the next data point.\n\nNote\n\nAggregation functions return integer or double values based on the input data points. If both source values are integers in storage, the resulting calculations will be integers. This means any fractional values resulting from the computation will be lopped off, no rounding will occur. If either data point is a floating point value, the result will be a floating point. However if downsampling or rates are enabled, the result will always be a float.\n\n## Downsampling\n\nThe second method of operation for aggregation functions is `downsampling`. Since OpenTSDB stores data at the original resolution indefinitely, requesting data for a long time span can return millions of points. This can cause a burden on bandwidth or graphing libraries so it's common to request data at a lower resolution for longer spans. Downsampling breaks the long span of data into smaller spans and merges the data for the smaller span into a single data point. Aggregation functions will perform the same calculation as for an aggregation process but instead of working across data points for multiple time series at a single time stamp, downsampling works across multiple data points within a single time series over a given time span.\n\nFor example, take series `A` and `B` in the first table under **Aggregation**. The data points cover a 50 second time span. Let's say we want to downsample that to 30 seconds. This will give us two data points for each series:\n\n| series            | ts0 | ts0+10s | ts0+20s | ts0+30s | ts0+40s | ts0+50s |\n|-------------------|-----|---------|---------|---------|---------|---------|\n| A                 | 5   | 5       | 10      | 15      | 20      | 5       |\n| A Downsampled     |     |         |         | 35      |         | 25      |\n| B                 | 10  | 5       | 20      | 15      | 10      | 0       |\n| B Downsampled     |     |         |         | 50      |         | 10      |\n| Aggregated Result |     |         |         | 85      |         | 35      |\n\nFor early versions of OpenTSDB, the actual time stamps for the new data points will be an average of the time stamps for each data point in the time span. As of 2.1 and later, the timestamp for each point is aligned to the start of a time bucket based on a modulo of the current time and the downsample interval.\n\nNote that when a query specifies a down sampling function and multiple time series are returned, downsampling occurs **before** aggregation. I.e. now that we have `A`` ``Downsampled` and `B`` ``Downsampled` we can aggregate the two series to come up with the aggregated result on the bottom line.\n\n## Fill Policies\n\nWith version 2.2 you can specify a fill policy when downsampling to substitute values for use in cross-series aggregations when data points are \"missing\". Because OpenTSDB does not impose constraints on time alignment or when values are supposed to exist, such constraints must be specified at query time. At serialization time, if all series are missing values for an expected timestamp, nothing is emitted. For example, if a series is writing data every minute from T0 to T4, but for some reason the source fails to write data at T3, only 4 values will be serialized when the user may expect 5. With fill policies you can now choose what value is emitted for T3.\n\nWhen aggregating multiple series OpenTSDB generally performs linear interpolation when a series is missing a value at a timestamp present in one or more other series. Some aggregators substitute specific values such as zero, min or max values. With fill policies you can modify aggregation behavior by flagging a missing value as a NaN or a scalar such as zero. When a NaN is emitted for a series, it is skipped for all calculations. For example, if a query asks for the average of a metric and one or more series are missing values, substituting a 0 would drive down the average and lerping introduces non-extant values. However with NaNs we can flag the value as missing and skip it in the calculation.\n\nAvailable polices include:\n\n- None (`none`) - The default behavior that does not emit missing values during serialization and performs linear interpolation (or otherwise specified interpolation) when aggregating series.\n- NaN (`nan`) - Emits a `NaN` in the serialization output when all values are missing in a series. Skips series in aggregations when the value is missing.\n- Null (`null`) - Same behavior as NaN except that during serialization it emits a `null` instead of a `NaN`.\n- Zero (`zero`) - Substitutes a zero when a timestamp is missing. The zero value will be incorporated in aggregated results.\n\n(The terms in parentheses can be used in downsampling specifications, e.g. `1h-sum-nan`)\n\nAn example with the NaN fill policy and downsampling on 10 seconds:\n\n| series         | ts0 | ts0+10s | ts0+20s | ts0+30s | ts0+40s | ts0+50s | ts0+60s |\n|----------------|-----|---------|---------|---------|---------|---------|---------|\n| A              | na  | na      | na      | 15      | na      | 5       | na      |\n| B              | 10  | na      | 20      | na      | na      | na      | 20      |\n| Interpolated A | NaN | NaN     | NaN     |         | NaN     |         | NaN     |\n| Interpolated B |     | NaN     |         | NaN     | NaN     | NaN     |         |\n| Summed Result  | 10  | NaN     | 20      | 15      | NaN     | 5       | 20      |\n\n## Available Aggregators\n\nThe following is a description of the aggregation functions available in OpenTSDB.\n\n| Aggregator | Description                                                                                   | Interpolation        |\n|------------|-----------------------------------------------------------------------------------------------|----------------------|\n| avg        | Averages the data points                                                                      | Linear Interpolation |\n| count      | The number of raw data points in the set                                                      | Zero if missing      |\n| dev        | Calculates the standard deviation                                                             | Linear Interpolation |\n| ep50r3     | Calculates the estimated 50th percentile with the R-3 method \\*                               | Linear Interpolation |\n| ep50r7     | Calculates the estimated 50th percentile with the R-7 method \\*                               | Linear Interpolation |\n| ep75r3     | Calculates the estimated 75th percentile with the R-3 method \\*                               | Linear Interpolation |\n| ep75r7     | Calculates the estimated 75th percentile with the R-7 method \\*                               | Linear Interpolation |\n| ep90r3     | Calculates the estimated 90th percentile with the R-3 method \\*                               | Linear Interpolation |\n| ep90r7     | Calculates the estimated 90th percentile with the R-7 method \\*                               | Linear Interpolation |\n| ep95r3     | Calculates the estimated 95th percentile with the R-3 method \\*                               | Linear Interpolation |\n| ep95r7     | Calculates the estimated 95th percentile with the R-7 method \\*                               | Linear Interpolation |\n| ep99r3     | Calculates the estimated 99th percentile with the R-3 method \\*                               | Linear Interpolation |\n| ep99r7     | Calculates the estimated 99th percentile with the R-7 method \\*                               | Linear Interpolation |\n| ep999r3    | Calculates the estimated 999th percentile with the R-3 method \\*                              | Linear Interpolation |\n| ep999r7    | Calculates the estimated 999th percentile with the R-7 method \\*                              | Linear Interpolation |\n| first      | Returns the first data point in the set. Only useful for downsampling, not aggregation. (2.3) | Indeterminate        |\n| last       | Returns the last data point in the set. Only useful for downsampling, not aggregation. (2.3)  | Indeterminate        |\n| mimmin     | Selects the smallest data point                                                               | Maximum if missing   |\n| mimmax     | Selects the largest data point                                                                | Minimum if missing   |\n| min        | Selects the smallest data point                                                               | Linear Interpolation |\n| max        | Selects the largest data point                                                                | Linear Interpolation |\n| none       | Skips group by aggregation of all time series. (2.3)                                          | Zero if missing      |\n| p50        | Calculates the 50th percentile                                                                | Linear Interpolation |\n| p75        | Calculates the 75th percentile                                                                | Linear Interpolation |\n| p90        | Calculates the 90th percentile                                                                | Linear Interpolation |\n| p95        | Calculates the 95th percentile                                                                | Linear Interpolation |\n| p99        | Calculates the 99th percentile                                                                | Linear Interpolation |\n| p999       | Calculates the 999th percentile                                                               | Linear Interpolation |\n| sum        | Adds the data points together                                                                 | Linear Interpolation |\n| zimsum     | Adds the data points together                                                                 | Zero if missing      |\n\n\\* For percentile calculations, see the [Wikipedia](http://en.wikipedia.org/wiki/Quantile) article. For high cardinality calculations, using the estimated percentiles may be more performant.\n\n### Avg\n\nCalculates the average of all values across the time span or across multiple time series. This function will perform linear interpolation across time series. It's useful for looking at gauge metrics. Note that even though the calculation will usually result in a float, if the data points are recorded as integers, an integer will be returned losing some precision.\n\n### Count\n\nReturns the number of data points stored in the series or range. When used to aggregate multiple series, zeros will be substituted. It's best to use this when downsampling.\n\n### Dev\n\nCalculates the [standard deviation](http://en.wikipedia.org/wiki/Standard_deviation) across a span or time series. This function will perform linear interpolation across time series. It's useful for looking at gauge metrics. Note that even though the calculation will usually result in a float, if the data points are recorded as integers, an integer will be returned losing some precision.\n\n### Estimated Percentiles\n\nCalculates various percentiles using a choice of algorithms. These are useful for series with many data points as some data may be kicked out of the calculation. When used to aggregate multiple series, the function will perform linear interpolation. See [Wikipedia](http://en.wikipedia.org/wiki/Quantile) for details. Implementation is through the [Apache Math library.](http://commons.apache.org/proper/commons-math/)\n\n### First & Last\n\n(2.3) These aggregators will return the first or the last data point in the downsampling interval. E.g. if a downsample bucket consists of the series `2,`` ``6,`` ``1,`` ``7` then the `first` aggregator will return `1` and `last` will return `7`. Note that this aggregator is only useful for downsamplers. When used as a group-by aggregator, the results are indeterminate as the ordering of time series retrieved from storage and held in memory is not consistent from TSD to TSD or execution to execution.\n\n### Max\n\nThe inverse of `min`, it returns the largest data point from all of the time series or within a time span. This function will perform linear interpolation across time series. It's useful for looking at the upper bounds of gauge metrics.\n\n### MimMin\n\nThe \"maximum if missing minimum\" function returns only the smallest data point from all of the time series or within the time span. This function will *not* perform interpolation, instead it will return the maximum value for the type of data specified if the value is missing. This will return the Long.MaxValue for integer points or Double.MaxValue for floating point values. See [Primitive Data Types](http://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html) for details. It's useful for looking at the lower bounds of gauge metrics.\n\n### MimMax\n\nThe \"minimum if missing maximum\" function returns only the largest data point from all of the time series or within the time span. This function will *not* perform interpolation, instead it will return the minimum value for the type of data specified if the value is missing. This will return the Long.MinValue for integer points or Double.MinValue for floating point values. See [Primitive Data Types](http://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html) for details. It's useful for looking at the upper bounds of gauge metrics.\n\n### Min\n\nReturns only the smallest data point from all of the time series or within the time span. This function will perform linear interpolation across time series. It's useful for looking at the lower bounds of gauge metrics.\n\n### None\n\n(2.3) Skips group by aggregation. This aggregator is useful for fetching the *raw* data from storage as it will return a result set for every time series matching the filters. Note that the query will throw an exception if used with a downsampler.\n\n### Percentiles\n\nCalculates various percentiles. When used to aggregate multiple series, the function will perform linear interpolation. Implementation is through the [Apache Math library.](http://commons.apache.org/proper/commons-math/)\n\n### Sum\n\nCalculates the sum of all data points from all of the time series or within the time span if down sampling. This is the default aggregation function for the GUI as it's often the most useful when combining multiple time series such as gauges or counters. It performs linear interpolation when data points fail to line up. If you have a distinct series of values that you want to sum and you do not need interpolation, look at `zimsum`\n\n### ZimSum\n\nCalculates the sum of all data points at the specified timestamp from all of the time series or within the time span. This function does *not* perform interpolation, instead it substitutes a `0` for missing data points. This can be useful when working with discrete values.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/query/aggregators.html](http://opentsdb.net/docs/build/html/user_guide/query/aggregators.html)"
- name: Alerting with Nagios
  id: user_guide/utilities/nagios
  summary: OpenTSDB is great, but it's not (yet) a full monitoring platform
  description: "# Alerting with Nagios\n\nOpenTSDB is great, but it's not (yet) a full monitoring platform. Now that you have a bunch of metrics in OpenTSDB, you want to start sending alerts when thresholds are getting too high. It's easy!\n\nIn the `tools` directory is a Python script `check_tsd`. This script queries OpenTSDB and returns Nagios compatible output that gives you OK/WARNING/CRITICAL state.\n\n## Parameters\n\n``` python\nOptions:\n  -h, --help      show this help message and exit\n  -H HOST, --host=HOST  Hostname to use to connect to the TSD.\n  -p PORT, --port=PORT  Port to connect to the TSD instance on.\n  -m METRIC, --metric=METRIC\n            Metric to query.\n  -t TAG, --tag=TAG   Tags to filter the metric on.\n  -d SECONDS, --duration=SECONDS\n            How far back to look for data. Default 600s.\n  -D METHOD, --downsample=METHOD\n            Downsample function, e.g. one of avg, min, sum, max ... etc\n  -W SECONDS, --downsample-window=SECONDS\n            Window size over which to downsample.\n  -a METHOD, --aggregator=METHOD\n            Aggregation method: avg, min, sum (default), max .. etc\n  -x METHOD, --method=METHOD\n            Comparison method: gt, ge, lt, le, eq, ne.\n  -r, --rate      Use rate value as comparison operand.\n  -w THRESHOLD, --warning=THRESHOLD\n            Threshold for warning.  Uses the comparison method.\n  -c THRESHOLD, --critical=THRESHOLD\n            Threshold for critical.  Uses the comparison method.\n  -v, --verbose     Be more verbose.\n  -T SECONDS, --timeout=SECONDS\n            How long to wait for the response from TSD.\n  -E, --no-result-ok  Return OK when TSD query returns no result.\n  -I SECONDS, --ignore-recent=SECONDS\n            Ignore data points that are that are that recent.\n  -P PERCENT, --percent-over=PERCENT\n            Only alarm if PERCENT of the data points violate the\n            threshold.\n  -N UTC, --now=UTC   Set unix timestamp for \"now\", for testing\n  -S, --ssl       Make queries to OpenTSDB via SSL (https)\n```\n\nFor a complete list of downsample & aggregation modes, see [http://opentsdb.net/docs/build/html/user_guide/query/aggregators.html#available-aggregators](../query/aggregators#available-aggregators)\n\n## Nagios Setup\n\nDrop the script into your Nagios path and set up a command like this:\n\n``` python\ndefine command{\n    command_name check_tsd\n    command_line $USER1$/check_tsd -H $HOSTADDRESS$ $ARG1$\n}\n```\n\nThen define a host in nagios for your TSD server(s). You can give it a check_command that is guaranteed to always return something if the backend is healthy.\n\n``` python\ndefine host{\n    host_name         tsd\n    address         tsd\n    check_command       check_tsd!-d 60 -m rate:tsd.rpc.received -t type=put -x lt -c 1\n    [...]\n}\n```\n\nThen define some service checks for the things you want to monitor.\n\n``` python\ndefine service{\n    host_name             tsd\n    service_description       Apache too many internal errors\n    check_command           check_tsd!-d 300 -m rate:apache.stats.hits -t status=500 -w 1 -c 2\n    [...]\n}\n```\n\n## Testing\n\nIf you have want to test your parameters against some specific point in time, you can use the `--now`` ``<UTC>` parameter to specify an explicit unix timestamp which is used as the current timestamp instead of the actual current time. If set, the script will fetch data starting at `UTC`` ``-`` ``duration`, ending at `UTC`.\n\nTo see the values retreived, and potentially ignored (due to duration), use the `--verbose` option.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/utilities/nagios.html](http://opentsdb.net/docs/build/html/user_guide/utilities/nagios.html)"
- name: Bigtable
  id: user_guide/backends/bigtable
  summary: Google Cloud Platform provides hosting of Google's Bigtable database, the original inspiration of HBase and many NoSQL storage systems
  description: "# Bigtable\n\n[Google Cloud Platform](https://cloud.google.com/) provides hosting of Google's Bigtable database, the original inspiration of HBase and many NoSQL storage systems. Because HBase is so similar to Bigtable, running OpenTSDB 2.3 and later with Google's backend is simple. Indeed, the schemas (see [*HBase Schema*](hbase)) are exactly the same so all you have to do is create your Bigtable instance, create your TSDB tables using the Bigtable HBase shell, and fire up the TSDs.\n\nNote\n\nThe clients for Bigtable are in beta and undergoing a number of changes. Performance should improve as we adjust the code and uncover new tuning parameters. Please help us out on the mailing list or by modifying the code in GitHub.\n\n## Setup\n\n1.  Setup your Google Cloud Platform account.\n2.  Follow the steps in [Creating a Cloud Bigtable Cluster](https://cloud.google.com/bigtable/docs/creating-cluster).\n3.  Follow the steps in [HBase Shell Quickstart](https://cloud.google.com/bigtable/docs/hbase-shell-quickstart), paying attention to where you download your JSON key file.\n4.  Set the HBASE_HOME environment variable to your Bigtable shell directory, make sure the HBASE_CLASSPATH, JAVA_HOME, and GOOGLE_APPLICATION_CREDENTIALS environment variables have been set according to the values in Creating a Cloud BigTable Cluster document, and run the src/create_table.sh script. If the script fails to launch the shell, try running the shell manually and execute the create statements substituting the proper values.\n5.  Build TSDB by executing sh build-bigtable.sh (or if you prefer Maven, sh build-bigtable.sh pom.xml)\n6.  Prepare the opentsdb.conf file with the required and/or optional configuration parameters below.\n7.  Run the TSD via build/tsdb tsd --config=\\<path\\>/opentsdb.conf\n\n## Configuration\n\nThe following is a table of required and optional parameters to run OpenTSDB with Bigtable. These are in addition to the standard TSD configuration parameters from [*Configuration*](../configuration).\n\n| Property                                    | Type    | Required | Description                                                                                                                                                                                         | Default |\n|---------------------------------------------|---------|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|\n| google.bigtable.project.id                  | String  | Required | The project ID hosting your Bigtable cluster.                                                                                                                                                       |         |\n| google.bigtable.cluster.name                | String  | Required | The cluster ID you assigned to your Bigtable cluster at creation.                                                                                                                                   |         |\n| google.bigtable.zone.name                   | String  | Required | The zone where your Bigtable cluster is operating; chosen at creation.                                                                                                                              |         |\n| hbase.client.connection.impl                | String  | Required | The class that will be used to implement the HBase API AsyncBigtable will use as a shim between the Bigtable client and OpenTSDB. Set this to com.google.cloud.bigtable.hbase1_0.BigtableConnection |         |\n| google.bigtable.auth.service.account.enable | Boolean | Required | Whether or not to use a Google cloud service account to connect. Set this to true                                                                                                                   | false   |\n| google.bigtable.auth.json.keyfile           | String  | Required | The full path to the JSON formatted key file associated with the service account you want to use for Bigtable access. Download this from your cloud console.                                        |         |\n| google.bigtable.grpc.channel.count          | Integer | Optional | The number of sockets opened to the Bigtable API for handling RPCs. For higher throughput consider increasing the channel count.                                                                    | 4       |\n\nNote\n\nGoogle's Bigtable client communicates with their servers over HTTP2 with TLS using ALPN. As Java 7 and 8 (dunno about 9) lack native ALPN support, a [library](http://www.eclipse.org/jetty/documentation/current/alpn-chapter.html) must be loaded at JVM start to modify the JVM's bytecode. The build script for OpenTSDB will attempt to detect your JDK version and download the proper version of ALPN but if you have a custom JVM or something other than Hotspot or OpenJDK you may run into issues. Try different versions of the alpn-boot JAR to see what works for you.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/backends/bigtable.html](http://opentsdb.net/docs/build/html/user_guide/backends/bigtable.html)"
- name: Cassandra
  id: user_guide/backends/cassandra
  summary: Cassandra is an eventually consistent key value store similar to HBase and Google`s Bigtable
  description: "# Cassandra\n\nCassandra is an eventually consistent key value store similar to HBase and Google\\`s Bigtable. It implements a distributed hash map with column families originally it supported a Thrift based API very close to HBase\\`s. Lately Cassandra has moved towards a SQL like query language with much more flexibility around data types, joints and filters. Thankfully the Thrift interface is still there so it\\`s easy to convert the OpenTSDB HBase schema and calls to Cassandra at a low level through the AsyncHBase `HBaseClient` API. [AsyncCassandra](https://github.com/OpenTSDB/asynccassandra) is a shim between OpenTSDB and Cassandra for trying out TSDB with an alternate backend.\n\n## Setup\n\n1.  Setup a Cassandra cluster using the `ByteOrderedPartitioner`. This is critical as we require the row keys to be sorted. Because this setting affects the entire node, you may need to setup a cluster dedicated to OpenTSDB.\n2.  Create the proper keyspsaces and column families by using the cassandra-cli script:\n\n``` python\ncreate keyspace tsdb;\nuse tsdb;\ncreate column family t with comparator = BytesType;\n\ncreate keyspace tsdbuid;\nuse tsdbuid;\ncreate column family id with comparator = BytesType;\ncreate column family name with comparator = BytesType;\n```\n\n3.  Build TSDB by executing sh build-cassandra.sh (or if you prefer Maven, sh build-cassandra.sh pom.xml)\n4.  Modify your opentsdb.conf file with the asynccassandra.seeds parameter, e.g. asynccassandra.seeds=127.0.0.1:9160.\n5.  In the config file, set tsd.storage.hbase.uid_table=tsdbuid\n6.  Run the tsd via build/tsdb tsd --config=\\<path\\>/opentsdb.conf\n\nIf you intend to use meta data or tree features, repeat the keyspace creation with the proper table name.\n\n## Configuration\n\nThe following is a table with required and optional parameters to run OpenTSDB with Cassandra. These are in addition to the standard TSD configuration parameters from [*Configuration*](../configuration)\n\n| Property             | Type    | Required | Description                                                                               | Default |\n|----------------------|---------|----------|-------------------------------------------------------------------------------------------|---------|\n| asynccassandra.seeds | String  | Required | The list of nodes in your Cassandra cluster. These can be formatted \\<hostname\\>:\\<port\\> |         |\n| asynccassandra.port  | Integer | Optional | An optional port to use for all nodes if not configured in the seeds setting.             | 9160    |\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/backends/cassandra.html](http://opentsdb.net/docs/build/html/user_guide/backends/cassandra.html)"
- name: clean_cache.sh
  id: user_guide/utilities/clean_cache
  summary: OpenTSDB uses a directory for caching graphs and gnuplot scripts
  description: "# clean_cache.sh\n\nOpenTSDB uses a directory for caching graphs and gnuplot scripts. Unfortunately it doesn't clean up after itself at this time so a simple shell script is included to purge all files in the directory if drive where the directory resides drops below 10% of free space. Simply add this script as a cron entry and set it to run as often as you like.\n\nWarning\n\nThis script will purge all files in the directory. Don't store anything important in the temp directory.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/utilities/clean_cache.html](http://opentsdb.net/docs/build/html/user_guide/utilities/clean_cache.html)"
- name: CLI Tools
  id: user_guide/cli/index
  summary: OpenTSDB consists of a single JAR file that uses a shell script to determine what actiosn the user wants to take
  description: "# CLI Tools\n\nOpenTSDB consists of a single JAR file that uses a shell script to determine what actiosn the user wants to take. While the most common action is to start the TSD with the `tsd` command so that it can run all the time and process RPCs, other commands are available to work with OpenTSDB data. These commands include:\n\n- [uid](uid)\n- [mkmetric](mkmetric)\n- [import](import)\n- [query](query)\n- [fsck](fsck)\n- [scan](scan)\n- [search](search)\n- [tsd](tsd)\n\nAccessing a CLI tool is performed from the location of the `tsdb` file, built after compiling OpenTSDB. By default the tsdb file will be located in the `build` directory so you may access it via `./build/tsdb`. Provide the name of the CLI utility as in `./build/tsdb`` ``tsd`.\n\n## Common Parameters\n\nAll command line utilities share some common command line parameters:\n\n| Name        | Data Type | Description                                                                                                                                                                     | Default                                 | Example                             |\n|-------------|-----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------|-------------------------------------|\n| --config    | String    | The full or relative path to an OpenTSDB [*Configuration*](../configuration) file. If this parameter is not provided, the command will attempt to load the default config file. | See [*Configuration*](../configuration) | --config=/usr/local/tempconfig.conf |\n| --table     | String    | Name of the HBase table where datapoints are stored                                                                                                                             | tsdb                                    | --table=prod-tsdb                   |\n| --uidtable  | String    | Name of the HBase table where UID information is stored                                                                                                                         | tsdb-uid                                | --uidtable=prod-tsdb-uid            |\n| --verbose   | Boolean   | For some CLI tools, this command will allow for INFO and above logging per the logback.xml config. Otherwise without this flag, some tools may only log WARNing messages.       |                                         |                                     |\n| --zkbasedir | String    | Path under which is the znode for the -ROOT- region                                                                                                                             | /hbase                                  | --zkbasedir=/prod/hbase             |\n| --read-only | Boolean   | Sets the mode for OpenTSDB                                                                                                                                                      | false                                   | --read-only                         |\n| --zkquorum  | String    | Specification of the ZooKeeper quorum to use, i.e. a list of servers and/or ports in the ZooKeeper cluster                                                                      | localhost                               | --zkquorum=zkhost1,zkhost2,zkhost3  |\n\n## Site-specific Configuration\n\nThe common parameters above are required by all the CLI commands. It can be tedious to manually type them over and over again. You can instead store typically used values in a file `./tsdb.local`. This file is expected to be a shell script and will be sourced by `./tsdb` if it exists.\n\n*Setting default values for common parameters*\n\nIf, for example, your ZooKeeper quorum is behind the DNS name \"zookeeper.example.com\" (a name with 5 A records), instead of always passing `--zkquorum=zookeeper.example.com` to the CLI tool each time you use it, you can create `./tsdb.local` with the following contents:\n\n``` bash\n#!/bin/bash\nMY_ARGS='--zkquorum=zookeeper'\nset x $MY_ARGS \"$@\"\nshift\n```\n\n*Overriding the timezone of the TSD*\n\nServers are frequently using UTC as their timezone. By default, the TSD renders graphs using the local timezone of the server. You can override this to have graphs in your local time by specifying a timezone in `./tsdb.local`. For example, if you're in California, this will force the TSD to use your timezone:\n\n``` bash\necho export TZ=PST8PDT >>tsdb.local\n```\n\nOn most Linux and BSD systems, you can look under `/usr/share/zoneinfo` for names of timezones supported on your system.\n\n*Changing JVM parameters*\n\nYou might want to adjust JVM parameters, for instance to turn on GC activity logging or to set the size of various memory regions. In order to do so, simply set the variable JVMARGS in `./tsdb.local`.\n\nHere is an example that is recommended for production use:\n\n``` bash\nGCARGS=\"-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps\\\n -XX:+PrintTenuringDistribution -Xloggc:/tmp/tsd-gc-`date +%s`.log\"\nif test -t 0; then # if stdin is a tty, don't turn on GC logging.\n  GCARGS=\nfi\n# The Sun JDK caches all name resolution results forever, which is stupid.\n# This forces you to restart your application if any of the backends change\n# IP. Instead tell it to cache names for only 10 minutes at most.\nFIX_DNS='-Dsun.net.inetaddr.ttl=600'\nJVMARGS=\"$JVMARGS $GCARGS $FIX_DNS\"\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/cli/index.html](http://opentsdb.net/docs/build/html/user_guide/cli/index.html)"
- name: Configuration
  id: user_guide/configuration
  summary: OpenTSDB can be configured via a file on the local system, via command line arguments or a combination or both
  description: "# Configuration\n\nOpenTSDB can be configured via a file on the local system, via command line arguments or a combination or both.\n\n## Configuration File\n\nThe configuration file conforms to the Java properties specification. Configuration names are lower-case, dotted strings without spaces. Each name is followed by an equals sign, then the value for the property. All OpenTSDB properties start with `tsd.` Comments or inactive configuration lines are blocked by a hash symbol `#`. For example:\n\n``` python\n# List of Zookeeper hosts that manage the HBase cluster\ntsd.storage.hbase.zk_quorum = 192.168.1.100\n```\n\nwill configure the TSD to connect to Zookeeper on `192.168.1.100`.\n\nWhen combining configuration files and command line arguments, the order of processing is as follows:\n\n1.  Default values are loaded\n2.  Configuration file values are loaded, overriding default values\n3.  Command line parameters are loaded, overriding config file and default values\n\n## File Locations\n\nYou can use the `--config` command line argument to specify the full path to a configuration file. Otherwise if not specified, OpenTSDB and some of the command-line tools will attempt to search for a valid configuration file in the following locations:\n\n- ./opentsdb.conf\n- /etc/opentsdb.conf\n- /etc/opentsdb/opentsdb.conf\n- /opt/opentsdb/opentsdb.conf\n\nIn the event that a valid configuration file cannot be found and the required properties are not set, the TSD will not start. Please see the properties table below for a list of required configuration settings.\n\n## Properties\n\nThe following is a table of configuration options for all tools. When applicable, the corresponding command line override is provided. Please note that individual command line tools may have their own values so see their documentation for details.\n\nNote\n\nFor additional parameters used for tuning the AsyncHBase client, see [AsyncHBase Configuration](http://opentsdb.github.io/asynchbase/docs/build/html/configuration.html)\n\n| Property                                                    | Type    | Required | Description                                                                                                                                                                                                                                                                                                                                            | Default                                                                                                                                     | CLI              |\n|-------------------------------------------------------------|---------|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|------------------|\n| tsd.core.auto_create_metrics                                | Boolean | Optional | Whether or not a data point with a new metric will assign a UID to the metric. When false, a data point with a metric that is not in the database will be rejected and an exception will be thrown.                                                                                                                                                    | False                                                                                                                                       | --auto-metric    |\n| tsd.core.auto_create_tagks *(2.1)*                          | Boolean | Optional | Whether or not a data point with a new tag name will assign a UID to the tagk. When false, a data point with a tag name that is not in the database will be rejected and an exception will be thrown.                                                                                                                                                  | True                                                                                                                                        |                  |\n| tsd.core.auto_create_tagvs *(2.1)*                          | Boolean | Optional | Whether or not a data point with a new tag value will assign a UID to the tagv. When false, a data point with a tag value that is not in the database will be rejected and an exception will be thrown.                                                                                                                                                | True                                                                                                                                        |                  |\n| tsd.core.connections.limit *(2.3)*                          | Integer | Optional | Sets the maximum number of connections a TSD will handle, additional connections are immediately closed.                                                                                                                                                                                                                                               | 0                                                                                                                                           |                  |\n| tsd.core.enable_api *(2.3)*                                 | Boolean | Optional | Whether or not to allow the 2.x HTTP API to function. When disabled, calls to endpoints such as `/api/query` or `/api/suggest` will return a 404.                                                                                                                                                                                                      | True                                                                                                                                        | --disable-api    |\n| tsd.core.enable_ui *(2.3)*                                  | Boolean | Optional | Whether or not to allow the built-in GUI and legacy HTTP API to function. When disabled, calls to the root endpoint or other such as `/logs` or `/suggest` will return a 404.                                                                                                                                                                          | True                                                                                                                                        | --disable-ui     |\n| tsd.core.meta.enable_realtime_ts                            | Boolean | Optional | Whether or not to enable real-time TSMeta object creation. See [*Metadata*](metadata)                                                                                                                                                                                                                                                                  | False                                                                                                                                       |                  |\n| tsd.core.meta.enable_realtime_uid                           | Boolean | Optional | Whether or not to enable real-time UIDMeta object creation. See [*Metadata*](metadata)                                                                                                                                                                                                                                                                 | False                                                                                                                                       |                  |\n| tsd.core.meta.enable_tsuid_incrementing                     | Boolean | Optional | Whether or not to enable tracking of TSUIDs by incrementing a counter every time a data point is recorded. See [*Metadata*](metadata) (Overrides \"tsd.core.meta.enable_tsuid_tracking\")                                                                                                                                                                | False                                                                                                                                       |                  |\n| tsd.core.meta.enable_tsuid_tracking                         | Boolean | Optional | Whether or not to enable tracking of TSUIDs by storing a `1` with the current timestamp every time a data point is recorded. See [*Metadata*](metadata)                                                                                                                                                                                                | False                                                                                                                                       |                  |\n| tsd.core.plugin_path                                        | String  | Optional | A path to search for plugins when the TSD starts. If the path is invalid, the TSD will fail to start. Plugins can still be enabled if they are in the class path.                                                                                                                                                                                      |                                                                                                                                             |                  |\n| tsd.core.preload_uid_cache *(2.1)*                          | Boolean | Optional | Enables pre-population of the UID caches when starting a TSD.                                                                                                                                                                                                                                                                                          | False                                                                                                                                       |                  |\n| tsd.core.preload_uid_cache.max_entries *(2.1)*              | Integer | Optional | The number of rows to scan for UID pre-loading.                                                                                                                                                                                                                                                                                                        | 300,000                                                                                                                                     |                  |\n| tsd.core.storage_exception_handler.enable *(2.2)*           | Boolean | Optional | Whether or not to enable the configured storage exception handler plugin.                                                                                                                                                                                                                                                                              | False                                                                                                                                       |                  |\n| tsd.core.storage_exception_handler.plugin *(2.2)*           | String  | Optional | The full class name of the storage exception handler plugin you wish to use.                                                                                                                                                                                                                                                                           |                                                                                                                                             |                  |\n| tsd.core.timezone                                           | String  | Optional | A localized timezone identification string used to override the local system timezone used when converting absolute times to UTC when executing a query. This does not affect incoming data timestamps. E.g. America/Los_Angeles                                                                                                                       | System Configured                                                                                                                           |                  |\n| tsd.core.tree.enable_processing                             | Boolean | Optional | Whether or not to enable processing new/edited TSMeta through tree rule sets                                                                                                                                                                                                                                                                           | false                                                                                                                                       |                  |\n| tsd.core.uid.random_metrics *(2.2)*                         | Boolean | Optional | Whether or not to randomly assign UIDs to new metrics as they are created                                                                                                                                                                                                                                                                              | false                                                                                                                                       |                  |\n| tsd.core.bulk.allow_out_of_order_timestamps [\\*](#id1)(2.3) | Boolean | Optional | Whether or not to allow out-of-order values when bulk importing data from a text file.                                                                                                                                                                                                                                                                 | false                                                                                                                                       |                  |\n| tsd.http.cachedir                                           | String  | Required | The full path to a location where temporary files can be written. E.g. /tmp/opentsdb                                                                                                                                                                                                                                                                   |                                                                                                                                             | --cachedir       |\n| tsd.http.query.allow_delete                                 | Boolean | Optional | Whether or not to allow deleting data points from storage during query time.                                                                                                                                                                                                                                                                           | False                                                                                                                                       |                  |\n| tsd.query.enable_fuzzy_filter                               | Boolean | Optional | Whether or not to enable the FuzzyRowFilter for HBase when making queries using the `explicitTags` flag.                                                                                                                                                                                                                                               | True                                                                                                                                        |                  |\n| tsd.http.request.cors_domains                               | String  | Optional | A comma separated list of domain names to allow access to OpenTSDB when the `Origin` header is specified by the client. If empty, CORS requests are passed through without validation. The list may not contain the public wildcard `*` and specific domains at the same time.                                                                         |                                                                                                                                             |                  |\n| tsd.http.request.cors_headers *(2.1)*                       | String  | Optional | A comma separated list of headers sent to clients when executing a CORs request. The literal value of this option will be passed to clients.                                                                                                                                                                                                           | Authorization, Content-Type, Accept, Origin, User-Agent, DNT, Cache-Control, X-Mx-ReqToken, Keep-Alive, X-Requested-With, If-Modified-Since |                  |\n| tsd.http.request.enable_chunked                             | Boolean | Optional | Whether or not to enable incoming chunk support for the HTTP RPC                                                                                                                                                                                                                                                                                       | false                                                                                                                                       |                  |\n| tsd.http.request.max_chunk                                  | Integer | Optional | The maximum request body size to support for incoming HTTP requests when chunking is enabled.                                                                                                                                                                                                                                                          | 4096                                                                                                                                        |                  |\n| tsd.http.rpc.plugins *(2.2)*                                | String  | Optional | A comma delimited list of RPC plugins to load when starting a TSD. Must contain the entire class name.                                                                                                                                                                                                                                                 |                                                                                                                                             |                  |\n| tsd.http.show_stack_trace                                   | Boolean | Optional | Whether or not to return the stack trace with an API query response when an exception occurs.                                                                                                                                                                                                                                                          | false                                                                                                                                       |                  |\n| tsd.http.staticroot                                         | String  | Required | Location of a directory where static files, such as JavaScript files for the web interface, are located. E.g. /opt/opentsdb/staticroot                                                                                                                                                                                                                 |                                                                                                                                             | --staticroot     |\n| tsd.mode *(2.1)*                                            | String  | Optional | Whether or not the TSD will allow writing data points. Must be either `rw` to allow writing data or `ro` to block data point writes. Note that meta data such as UIDs can still be written/modified.                                                                                                                                                   | rw                                                                                                                                          |                  |\n| tsd.network.async_io                                        | Boolean | Optional | Whether or not to use NIO or traditional blocking IO                                                                                                                                                                                                                                                                                                   | True                                                                                                                                        | --async-io       |\n| tsd.network.backlog                                         | Integer | Optional | The connection queue depth for completed or incomplete connection requests depending on OS. The default may be limited by the 'somaxconn' kernel setting or set by Netty to 3072.                                                                                                                                                                      | See Description                                                                                                                             | --backlog        |\n| tsd.network.bind                                            | String  | Optional | An IPv4 address to bind to for incoming requests. The default is to listen on all interfaces. E.g. 127.0.0.1                                                                                                                                                                                                                                           | 0.0.0.0                                                                                                                                     | --bind           |\n| tsd.network.keep_alive                                      | Boolean | Optional | Whether or not to allow keep-alive connections                                                                                                                                                                                                                                                                                                         | True                                                                                                                                        |                  |\n| tsd.network.port                                            | Integer | Required | The TCP port to use for accepting connections                                                                                                                                                                                                                                                                                                          |                                                                                                                                             | --port           |\n| tsd.network.reuse_address                                   | Boolean | Optional | Whether or not to allow reuse of the bound port within Netty                                                                                                                                                                                                                                                                                           | True                                                                                                                                        |                  |\n| tsd.network.tcp_no_delay                                    | Boolean | Optional | Whether or not to disable TCP buffering before sending data                                                                                                                                                                                                                                                                                            | True                                                                                                                                        |                  |\n| tsd.network.worker_threads                                  | Integer | Optional | The number of asynchronous IO worker threads for Netty                                                                                                                                                                                                                                                                                                 | *\\#CPU cores \\* 2*                                                                                                                          | --worker-threads |\n| tsd.no_diediedie *(2.1)*                                    | Boolean | Optional | Enable or disable the `diediedie` HTML and ASCII commands to shutdown a TSD.                                                                                                                                                                                                                                                                           | False                                                                                                                                       |                  |\n| tsd.query.allow_simultaneous_duplicates *(2.2)*             | Boolean | Optional | Whether or not to allow simultaneous duplicate queries from the same host. If disabled, a second query that comes in matching one already running will receive an exception.                                                                                                                                                                           | False                                                                                                                                       |                  |\n| tsd.query.filter.expansion_limit *(2.2)*                    | Integer | Optional | The maximum number of tag values to include in the regular expression sent to storage during scanning for data. A larger value means more computation on the HBase region servers.                                                                                                                                                                     | 4096                                                                                                                                        |                  |\n| tsd.query.skip_unresolved_tagvs *(2.2)*                     | Boolean | Optional | Whether or not to continue querying when the query includes a tag value that hasn't been assigned a UID yet and may not exist.                                                                                                                                                                                                                         | False                                                                                                                                       |                  |\n| tsd.query.timeout *(2.2)*                                   | Integer | Optional | How long, in milliseconds, before canceling a running query. A value of 0 means queries will not timeout.                                                                                                                                                                                                                                              | 0                                                                                                                                           |                  |\n| tsd.rollups.enable *(2.4)*                                  | Boolean | Optional | Whether or not to enable rollup and pre-aggregation storage and writing.                                                                                                                                                                                                                                                                               | false                                                                                                                                       |                  |\n| tsd.rollups.tag_raw *(2.4)*                                 | Boolean | Optional | Whether or not to tag non-rolled-up and non-pre-aggregated values with the tag key configured in `tsd.rollups.agg_tag_key` and value configured in `tsd.rollups.raw_agg_tag_value`                                                                                                                                                                     | false                                                                                                                                       |                  |\n| tsd.rollups.agg_tag_key *(2.4)*                             | String  | Optional | A special key to tag pre-aggregated data with when writing to storage                                                                                                                                                                                                                                                                                  | \\_aggregate                                                                                                                                 |                  |\n| tsd.rollups.raw_agg_tag_value *(2.4)*                       | String  | Optional | A special tag value to non-rolled-up and non-pre-aggregated data with when writing to storage. `tsd.rollups.tag_raw` must be set to true.                                                                                                                                                                                                              | RAW                                                                                                                                         |                  |\n| tsd.rollups.block_derived *(2.4)*                           | Boolean | Optional | Whether or not to block storing derived aggregations such as `AVG` and `DEV`.                                                                                                                                                                                                                                                                          | true                                                                                                                                        |                  |\n| tsd.rpc.plugins                                             | String  | Optional | A comma delimited list of RPC plugins to load when starting a TSD. Must contain the entire class name.                                                                                                                                                                                                                                                 |                                                                                                                                             |                  |\n| tsd.rpc.telnet.return_errors *(2.4)*                        | Boolean | Optional | Whether or not to return errors to the Telnet style socket when writing data via `put` or `rollup`                                                                                                                                                                                                                                                     | true                                                                                                                                        |                  |\n| tsd.rtpublisher.enable                                      | Boolean | Optional | Whether or not to enable a real time publishing plugin. If true, you must supply a valid `tsd.rtpublisher.plugin` class name                                                                                                                                                                                                                           | False                                                                                                                                       |                  |\n| tsd.rtpublisher.plugin                                      | String  | Optional | The class name of a real time publishing plugin to instantiate. If `tsd.rtpublisher.enable` is set to false, this value is ignored. E.g. net.opentsdb.tsd.RabbitMQPublisher                                                                                                                                                                            |                                                                                                                                             |                  |\n| tsd.search.enable                                           | Boolean | Optional | Whether or not to enable search functionality. If true, you must supply a valid `tsd.search.plugin` class name                                                                                                                                                                                                                                         | False                                                                                                                                       |                  |\n| tsd.search.plugin                                           | String  | Optional | The class name of a search plugin to instantiate. If `tsd.search.enable` is set to false, this value is ignored. E.g. net.opentsdb.search.ElasticSearch                                                                                                                                                                                                |                                                                                                                                             |                  |\n| tsd.stats.canonical                                         | Boolean | Optional | Whether or not the FQDN should be returned with statistics requests. The default stats are returned with `host=<hostname>` which is not guaranteed to perform a lookup and return the FQDN. Setting this to true will perform a name lookup and return the FQDN if found, otherwise it may return the IP. The stats output should be `fqdn=<hostname>` | false                                                                                                                                       |                  |\n| tsd.storage.compaction.flush_interval *(2.2)*               | Integer | Optional | How long, in seconds, to wait in between compaction queue flush calls                                                                                                                                                                                                                                                                                  | 10                                                                                                                                          |                  |\n| tsd.storage.compaction.flush_speed *(2.2)*                  | Integer | Optional | A multiplier used to determine how quickly to attempt flushing the compaction queue. E.g. a value of 2 means it will try to flush the entire queue within 30 minutes. A value of 1 would take an hour.                                                                                                                                                 | 2                                                                                                                                           |                  |\n| tsd.storage.compaction.max_concurrent_flushes *(2.2)*       | Integer | Optional | The maximum number of compaction calls inflight to HBase at any given time                                                                                                                                                                                                                                                                             | 10000                                                                                                                                       |                  |\n| tsd.storage.compaction.min_flush_threshold *(2.2)*          | Integer | Optional | Size of the compaction queue that must be exceeded before flushing is triggered                                                                                                                                                                                                                                                                        | 100                                                                                                                                         |                  |\n| tsd.storage.enable_appends *(2.2)*                          | Boolean | Optional | Whether or not to append data to columns when writing data points instead of creating new columns for each value. Avoids the need for compactions after each hour but can use more resources on HBase.                                                                                                                                                 | False                                                                                                                                       |                  |\n| tsd.storage.enable_compaction                               | Boolean | Optional | Whether or not to enable compactions                                                                                                                                                                                                                                                                                                                   | True                                                                                                                                        |                  |\n| tsd.storage.fix_duplicates *(2.1)*                          | Boolean | Optional | Whether or not to accept the last written value when parsing data points with duplicate timestamps. When enabled in conjunction with compactions, a compacted column will be written with the latest data points.                                                                                                                                      | False                                                                                                                                       |                  |\n| tsd.storage.flush_interval                                  | Integer | Optional | How often, in milliseconds, to flush the data point storage write buffer                                                                                                                                                                                                                                                                               | 1000                                                                                                                                        | --flush-interval |\n| tsd.storage.hbase.data_table                                | String  | Optional | Name of the HBase table where data points are stored                                                                                                                                                                                                                                                                                                   | tsdb                                                                                                                                        | --table          |\n| tsd.storage.hbase.meta_table                                | String  | Optional | Name of the HBase table where meta data are stored                                                                                                                                                                                                                                                                                                     | tsdb-meta                                                                                                                                   |                  |\n| tsd.storage.hbase.prefetch_meta *(2.2)*                     | Boolean | Optional | Whether or not to prefetch the regions for the TSDB tables before starting the network interface. This can improve performance.                                                                                                                                                                                                                        | False                                                                                                                                       |                  |\n| tsd.storage.hbase.tree_table                                | String  | Optional | Name of the HBase table where tree data are stored                                                                                                                                                                                                                                                                                                     | tsdb-tree                                                                                                                                   |                  |\n| tsd.storage.hbase.uid_table                                 | String  | Optional | Name of the HBase table where UID information is stored                                                                                                                                                                                                                                                                                                | tsdb-uid                                                                                                                                    | --uidtable       |\n| tsd.storage.hbase.zk_basedir                                | String  | Optional | Path under which the znode for the -ROOT- region is located                                                                                                                                                                                                                                                                                            | /hbase                                                                                                                                      | --zkbasedir      |\n| tsd.storage.hbase.zk_quorum                                 | String  | Optional | A comma-separated list of ZooKeeper hosts to connect to, with or without port specifiers. E.g. `192.168.1.1:2181,192.168.1.2:2181`                                                                                                                                                                                                                     | localhost                                                                                                                                   | --zkquorum       |\n| tsd.storage.repair_appends *(2.2)*                          | Boolean | Optional | Whether or not to re-write appended data point columns at query time when the columns contain duplicate or out of order data.                                                                                                                                                                                                                          | False                                                                                                                                       |                  |\n| tsd.storage.max_tags *(2.2)*                                | Integer | Optional | The maximum number of tags allowed per data point. **NOTE** Please be aware of the performance tradeoffs of overusing tags `writing`                                                                                                                                                                                                                   | 8                                                                                                                                           |                  |\n| tsd.storage.salt.buckets *(2.2)*                            | Integer | Optional | The number of salt buckets used to distribute load across regions. **NOTE** Changing this value after writing data may cause TSUID based queries to fail.                                                                                                                                                                                              | 20                                                                                                                                          |                  |\n| tsd.storage.salt.width *(2.2)*                              | Integer | Optional | The width, in bytes, of the salt prefix used to indicate which bucket a time series belongs in. A value of 0 means salting is disabled. **WARNING** Do not change after writing data to HBase or you will corrupt your tables and not be able to query any more.                                                                                       | 0                                                                                                                                           |                  |\n| tsd.storage.uid.width.metric *(2.2)*                        | Integer | Optional | The width, in bytes, of metric UIDs. **WARNING** Do not change after writing data to HBase or you will corrupt your tables and not be able to query any more.                                                                                                                                                                                          | 3                                                                                                                                           |                  |\n| tsd.storage.uid.width.tagk *(2.2)*                          | Integer | Optional | The width, in bytes, of tag name UIDs. **WARNING** Do not change after writing data to HBase or you will corrupt your tables and not be able to query any more.                                                                                                                                                                                        | 3                                                                                                                                           |                  |\n| tsd.storage.uid.width.tagv *(2.2)*                          | Integer | Optional | The width, in bytes, of tag value UIDs. **WARNING** Do not change after writing data to HBase or you will corrupt your tables and not be able to query any more.                                                                                                                                                                                       | 3                                                                                                                                           |                  |\n\n## Data Types\n\nSome configuration values require special consideration:\n\n- Booleans - The following literals will parse to `True`:\n\n  - `1`\n  - `true`\n  - `yes`\n\n  Any other values will result in a `False`. Parsing is case insensitive\n\n- Strings - Strings, even those with spaces, do not require quotation marks, but some considerations apply:\n\n  - Special characters must be escaped with a backslash include: `#`, `!`, `=`, and `:` E.g.:\n\n    ``` python\n    my.property = Hello World\\!\n    ```\n\n  - Unicode characters must be escaped with their hexadecimal representation, e.g.:\n\n    ``` python\n    my.property = \\u0009\n    ```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/configuration.html](http://opentsdb.net/docs/build/html/user_guide/configuration.html)"
- name: Dates and Times
  id: user_guide/query/dates
  summary: OpenTSDB supports a number of date and time formats when querying for data
  description: "# Dates and Times\n\nOpenTSDB supports a number of date and time formats when querying for data. The following formats are supported in queries submitted through the GUI, CliQuery tool or HTTP API. Every query requires a **start time** and an optional **end time**. If the end time is not specified, the current time on the system where the TSD is running will be used.\n\n## Relative\n\nIf you don't know the exact timestamp to request you can submit a time in the past relative to the time on the system where the TSD is running. Relative times follow the format `<amount><time`` ``unit>-ago` where `<amount>` is the number of time units and `<time`` ``unit>` is the unit of time, such as hours, days, etc. For example, if we provide a **start time** of `1h-ago` and leave out the **end time**, our query will return data start at 1 hour ago to the current time. Possible units of time include:\n\n- ms - Milliseconds\n- s - Seconds\n- m - Minutes\n- h - Hours\n- d - Days (24 hours)\n- w - Weeks (7 days)\n- n - Months (30 days)\n- y - Years (365 days)\n\nNote\n\nRelative times do not account for leap seconds, leap years or time zones. They simply calculate the number of seconds in the past from the current time.\n\n## Absolute Unix Time\n\nInternally, all data is associated with a Unix (or POSIX) style timestamp. Unix times are defined as the number of seconds that have elapsed since January 1st, 1970 at 00:00:00 UTC time. Timestamps are represented as a positive integer such as `1364410924`, representing `ISO`` ``8601:2013-03-27T19:02:04Z`. Since calls to store data in OpenTSDB require a Unix timestamp, it makes sense to support the format in queries. Thus you can supply an integer for a start or end time in a query.\n\nQueries using Unix timestamps can also support millisecond precision by simply appending three digits. For example providing a start time of `1364410924000` and an end time of `1364410924250` will return data within a 250 millisecond window. Millisecond timestamps may also be supplied with a period separating the seconds from the milliseconds as in `1364410924.250`. Any integers with 13 (or 14) characters will be treated as a millisecond timestamp. Anything 10 characters or less represent seconds. Milliseconds may only be supplied with 3 digit precision. If your tool outputs more than 3 digits you must truncate or round the value.\n\n## Absolute Formatted Time\n\nSince calculating a Unix time in your head is pretty difficult, OpenTSDB also supports human readable absolute date and times. Supported formats include:\n\n- yyyy/MM/dd-HH:mm:ss\n- yyyy/MM/dd HH:mm:ss\n- yyyy/MM/dd-HH:mm\n- yyyy/MM/dd HH:mm\n- yyyy/MM/dd\n\n`yyyy` represents the year as a four digit value, e.g. `2013`. `MM` represents the month of year starting at `01` for January to `12` for December. `dd` represents the day of the month starting at `01`. `HH` represents the hour of day in 24 hour format starting at `00` to `23`. `mm` represents the minutes starting at `00` to `59` and `ss` represents seconds starting at `00` to `59`. All months, days, hours, minutes and seconds that are single digits must be preceeded by a 0, e.g. the 5th day of the month must be given as `05`. When supplying on the data without a time, the system will assume midnight of the given day.\n\nExamples include `2013/01/23-12:50:42` or `2013/01/23`. Formatted times are converted from the default timezone of the host running the TSD to UTC. HTTP API queries can accept a user supplied time zone to override the local zone.\n\nNote\n\nWhen using the CliQuery tool, you must use the format that separates the date from the time with a dash. This is because the command line is split on spaces, so if you put a space in the timestamp, it will fail to parse execute properly.\n\n## Time Zones\n\nWhen converting human readable timestamps, OpenTSDB will convert to UTC from the timezone configured on the system where the TSD is running. While many servers are configured to UTC, and we recommend that all systems running OpenTSDB use UTC, sometimes a local timezone is used.\n\nQueries via query string to the HTTP API can specify a `tz` parameter with a timezone identification string in a format applicable to the localization settings of the system running the TSD. For example, we could specify `tz=America/Los_Angeles` to convert our timestamp from Los Angeles local time to UTC.\n\nAlternatively, if you are unable to change the system timezone, you can provide an override via the config file `tsd.core.timezone` property.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/query/dates.html](http://opentsdb.net/docs/build/html/user_guide/query/dates.html)"
- name: Definitions
  id: user_guide/definitions
  summary: When it comes to timeseries data, there are lots of terms tossed about that can lead to some confusion
  description: "# Definitions\n\nWhen it comes to timeseries data, there are lots of terms tossed about that can lead to some confusion. This page is a sort of glossary that helps to define words related to the use of OpenTSDB.\n\n## Cardinality\n\nCardinality is a mathematical term defined as the number of elements in a set. In database lingo, it's often used to refer to the number of unique items in an index. With regards to OpenTSDB it can refer to:\n\n- The number of unique time series for a given metric\n- The number of unique tag values associated with a tag name\n\nDue to the nature of the OpenTSDB storage schema, metrics with higher cardinality may take longer return results during query execution than those with lower cardinality. E.g. we may have metric `foo` with the tag name `datacenter` and there are 100 possible values for datacenter. Then we have metric `bar` with the tag `host` and 50,000 possible values for host. Metric `bar` has a higher cardinality than `foo`: 50,000 possible time series for `bar` an only 100 for `foo`.\n\n## Compaction\n\nAn OpenTSDB compaction takes multiple columns in an HBase row and merges them into a single column to reduce disk space. This is not to be confused with HBase compactions where multiple edits to a region are merged into one. OpenTSDB compactions can occur periodically for a TSD after data has been written, or during a query.\n\n## Data Point\n\nEach of the metrics above can be recorded as a number at a specific time. For example, we could record that Sue worked 8 hours at the end of each day. Or that \"mylogo.jpg\" was downloaded 400 times in the past hour. Thus a datapoint consists of:\n\n- A metric\n- A numeric value\n- A timestamp when the value was recorded\n- One or more sets of tags\n\n## Metric\n\nA metric is simply the name of a quantitative measurement. Metrics include things like:\n\n- hours worked by an employee\n- webserver downloads of a file\n- snow accumulation in a region\n\nNote\n\nNotice that the `metric` did not include a specific number or a time. That is becaue a `metric` is just a label of what you are measuring. The actual measurements are called `datapoints`, as you'll see later.\n\nUnfortunately OpenTSDB requires metrics to be named as a single, long word without spaces. Thus metrics are usually recorded using \"dotted notation\". For example, the metrics above would have names like:\n\n- hours.worked\n- webserver.downloads\n- accumulation.snow\n\n## Tags\n\nA `metric` should be descriptive of what is being measured, but with OpenTSDB, it should not be too specific. Instead, it is better to use `tags` to differentiate and organize different items that may share a common metric. Tags are pairs of words that provide a means of associating a metric with a specific item. Each pair consists of a `tagk` that represents the group or category of the following `tagv` that represents a specific item, object, location or other noun.\n\nExpanding on the metric examples above:\n\n- A business may have four employees, Sue, John, Kelly and Paul. Therefore we may configure a `tagk` of `employee` with their names as the `tagv`. These would be recorded as `employee=sue`, `employee=john` etc.\n- Webservers usually have many files so we could have a `tagk` of `file` to arrive at `file=logo.jpg` or `file=index.php`\n- Snow falls in many regions so we may record a `tagk` of `region` to get `region=new_england` or `region=north_west`\n\n## Time Series\n\nA collection of two or more data points for a single metric and group of tag name/value pairs.\n\n## Timestamp\n\nTimestamps are simply the absolute time when a value for a given metric was recorded.\n\n## Value\n\nA value represents the actual numeric measurement of the given metric. One of our employees, Sue, worked 8 hours yesterday, thus the value would be `8`. There were 1,024 downloads of `logo.jpg` from our webserver in the past hour. And 12 inches of snow fell in New England today.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/definitions.html](http://opentsdb.net/docs/build/html/user_guide/definitions.html)"
- name: Deprecated HTTP API
  id: api_http/deprecated
  summary: Version 1.0 of OpenTSDB included a rudimentary HTTP API that allowed for querying data, suggesting metric or tag names and a means of accessing static files
  description: "# Deprecated HTTP API\n\nVersion 1.0 of OpenTSDB included a rudimentary HTTP API that allowed for querying data, suggesting metric or tag names and a means of accessing static files. The 1.0 API has been carried over to 2.0 for backwards compatibility though most of the calls have been deprecated. Below is a list of the different endpoints and how to use them.\n\nWarning\n\nVersion 3.0 may discard these deprecated methods so if you are developing tools against the HTTP API, make sure to use the 2.0 version.\n\nIf an endpoint is marked as (**Deprecated**) below, it should not be used for future development work.\n\n## Generalities\n\nMost of the endpoints can return data in one or more of the following formats:\n\n- Plain Test - Or ASCII, the default for many requests will return a simple page of data with the Content-Type `text/plain`\n- HTML - If a request is bad or there was an exception, the response will often be in HTML, hard-coded and not using templates\n- JSON - Many calls can respond in a JSON format when the `json` query string parameter is appended\n- PNG - Some requests, including exceptions and errors, can generate an image file. In these cases, an error is sent to GnuPlot and the resulting empty graph with a title consisting of the message is returned. Append the parameter `png` to the query string.\n\nThe correct Content-Type is returned for each response, e.g. `text/html;`` ``charset=UTF-8` for HTML, `application/json` for JSON and `image/png` for images.\n\nSelecting a different output format is done with the `png` or `json` query string parameter. The value for the parameter is ignored. For example you can request `http://localhost:4242/suggest?type=metrics&q=sys&json` to return JSON data.\n\n## /\n\nRequests the root which is the GWT generated OpenTSDB GUI. This endpoint only returns HTML and cannot return other data.\n\n## /aggregators (**Deprecated**)\n\nReturns a list of available aggregation functions in JSON format only. Other formats are ignored. This method does not accept any query string parameters.\n\nExample Request:\n\n``` python\nhttp://localhost:4242/aggregators\n```\n\nExample Response:\n\n``` python\n[\"min\",\"sum\",\"max\",\"avg\"]\n```\n\n## /diediedie (**Deprecated**)\n\nAccessing this endpoint causes the TSD to perform a graceful shutdown and exit. A graceful shutdown prevents data loss by flushing all the buffered edits to HBase before exiting. The endpoint does not return any data and does not accept any parameters.\n\n## /dropcaches (**Deprecated**)\n\nClears all internal caches such as the UID to name and name to UID maps. It should be used if you have renamed a metric, tagk or tagv.\n\n## /logs (**Deprecated**)\n\nReturns the latest lines logged by the TSD internally, returning the most recent entries first. OpenTSDB uses LogBack and the `src/logback.xml` file must have a Cyclic Buffer appender configured for this endpoint to function. The XML configuration determines how many lines will be returned with each call. Output defaults to plain text with message components separated by tabs, or it can be returned as JSON with the proper query string.\n\nThis endpoint can also change the logging level of \\_\\_\\_\\_\\_\\_ at runtime. The query string parameter to use is `level=<logging_level>`. For example, you can call `http://localhost:4242/logs?level=INFO` to set the log level to `INFO`. Valid parameter values are (from the most verbose to the least): `ALL` `TRACE` `DEBUG` `INFO` `WARN` `ERROR` `OFF` (names are case insensitive). Note that this method does not change the `logback.xml` configuration file and restarting the TSD will reload from that file.\n\n## /q (**Deprecated**)\n\nQueries the TSD for data.\n\n## /s\n\nServes static files, such as JavaScript generated by the GWT compiler or favicon.ico. The TSD needs a `--staticroot` or `tsd.http.staticroot` argument to start. This argument is the path to a directory that contains the files served by this end point.\n\nWhen a request for `GET`` ``/s/queryui.nocache.js` comes in, for instance, the file `${staticroot}/queryui.nocache.js` is sent to the browser.\n\nNote: The TSD will allow clients to cache static files for 1 year by default, and will report the age of the file on disk. If the file name contains nocache, then the TSD will tell clients to not cache the file (this idiom is used by GWT).\n\n## /stats (**Deprecated**)\n\nReturns statistics about the running TSD\n\n## /suggest (**Deprecated**)\n\nUsed for auto-complete calls to match metrics, tag names or tag values on the given string. Returns JSON data only.\n\nParameters:\n\n- type - The type of value to suggest. Must be either `metrics` for metrics, `tagk` for tag names or `tagv` for tag values.\n- q - The string to match on. The match is case-sensitive and only matches on the first characters of each type of data. For example, `type=metrics&q=sys` would only return the names of metrics that start with `sys` such as `sys.cpu.0.system`\n- max - An optional maximum number of results to return. The default is 25 and given values must be greater than 0.\n\nBoth parameters are required or you will receive an exception.\n\nExample Request:\n\n``` python\nhttp://localhost:4242/suggest?type=metrics&q=df\n```\n\nExample Response:\n\n``` python\n[\n  \"df.1kblocks.free\",\n  \"df.1kblocks.total\",\n  \"df.1kblocks.used\",\n  \"df.inodes.free\",\n  \"df.inodes.total\",\n  \"df.inodes.used\"\n]\n```\n\n## /version (**Deprecated**)\n\nReturns version information about the build of the running TSD. Can be returned in either the default of plain-text or JSON.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/deprecated.html](http://opentsdb.net/docs/build/html/api_http/deprecated.html)"
- name: Development
  id: development/index
  summary: OpenTSDB has a strong and growing base of users running TSDs in production
  description: "# Development\n\nOpenTSDB has a strong and growing base of users running TSDs in production. There are also a number of talented developers creating tools for OpenTSDB or contributing code directly to the project. If you are interested in helping, by adding new features, fixing bugs, adding tools or simply updating documentation, please read the guidelines below. Then sign the contributors agreement and send us a pull request!\n\nIf you are looking to integrate OpenTSDB with your application, the compiled JAVA library has a consistent and well documented API. Please see [JAVA API Documentation](http://opentsdb.net/docs/javadoc/index.html).\n\n## Guidelines\n\n- Please file [issues on GitHub](https://github.com/OpenTSDB/opentsdb/issues) after checking to see if anyone has posted a bug already. Make sure your bug reports contain enough details so they can be easily understood by others and quickly fixed.\n- Read the Development page for tips\n- The best way to contribute code is to fork the main repo and [send a pull request](https://help.github.com/articles/using-pull-requests) on GitHub.\n  - Bug fixes should be done in the `master` branch\n  - New features or major changes should be done in the `next` branch\n- Alternatively, you can send a plain-text patch to the [mailing list](https://groups.google.com/forum/#!forum/opentsdb).\n- Before your code changes can be included, please file the [Contribution License Agreement](https://docs.google.com/spreadsheet/embeddedform?formkey=dFNiOFROLXJBbFBmMkQtb1hNMWhUUnc6MQ).\n- Unlike, say, the Apache Software Foundation, we do not require every single code change to be attached to an issue. Feel free to send as many small fixes as you want.\n- Please break down your changes into as many small commits as possible.\n- Please *respect the coding style of the code* you're changing.\n  - Indent code with 2 spaces, no tabs\n  - Keep code to 80 columns\n  - Curly brace on the same line as `if`, `for`, `while`, etc\n  - Variables need descriptive names `like_this` (instead of the typical Java style of `likeThis`)\n  - Methods named `likeThis()` starting with lower case letters\n  - Classes named `LikeThis`, starting with upper case letters\n  - Use the `final` keyword as much as you can, particularly in method parameters and returns statements.\n  - Avoid checked exceptions as much as possible\n  - Always provide the most restrictive visibility to classes and members\n  - Javadoc all of your classes and methods. Some folks make use the Java API directly and we'll build docs for the site, so the more the merrier\n  - Don't add dependencies to the core OpenTSDB library unless absolutely necessary\n  - Add unit tests for any classes/methods you create and verify that your change doesn't break existing unit tests. We know UTs aren't fun, but they are useful\n\n## Details\n\n- [General Development](development)\n- [Plugins](plugins)\n- [HTTP API](http_api)\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/development/index.html](http://opentsdb.net/docs/build/html/development/index.html)"
- name: diediedie
  id: api_telnet/diediedie
  summary: This command will cause the running TSD to shutdown and the process to exit
  description: "# diediedie\n\nThis command will cause the running TSD to shutdown and the process to exit. Please use carefully.\n\nWarning\n\nAs stated, when this command executes, the TSD will shutdown. You'll have to restart it manually, using a script, or use something like Daemontools or Runit.\n\n## Request\n\nThe command format is:\n\n``` python\ndiediedie\n```\n\n## Response\n\nA response that it's cleaning up and exiting.\n\n### Example\n\n``` python\nCleaning up and exiting now.\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_telnet/diediedie.html](http://opentsdb.net/docs/build/html/api_telnet/diediedie.html)"
- name: dropcaches
  id: api_telnet/dropcaches
  summary: Purges the metric, tag key and tag value UID to string and string to UID maps
  description: "# dropcaches\n\nPurges the metric, tag key and tag value UID to string and string to UID maps.\n\n## Request\n\nThe command format is:\n\n``` python\ndropcaches\n```\n\n## Response\n\nAn acknowledgement after the caches have been purged.\n\n### Example\n\n``` python\nCaches dropped.\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_telnet/dropcaches.html](http://opentsdb.net/docs/build/html/api_telnet/dropcaches.html)"
- name: Filters
  id: user_guide/query/filters
  summary: In OpenTSDB 2.2 tag key and value filters were introduced
  description: "# Filters\n\nIn OpenTSDB 2.2 tag key and value filters were introduced. This makes it easier to extract only the data that you want from storage. The filter framework is plugable to allow for tying into external systems such as asset management or provisioning systems.\n\nMultiple filters on the same tag key are allowed and when processed, they are *ANDed* together e.g. if we have two filters `host=literal_or(web01)` and `host=literal_or(web02)` the query will always return empty. If two or more filters are included for the same tag key and one has group by enabled but another does not, then group by will effectively be true for all filters on that tag key.\n\nNote that some type of filters may cause queries to execute slower than others, e.g. the regex and wildcard filters. Before fetching data from storage, the filters are processed to create a database filter based on UIDs so using the case sensitive \"literal or\" filter is always faster than regex because we can resolve the strings to UIDs and send those to the storage system for filtering. Instead if you ask for regex or wildcards with pre, post or infix filtering the TSD must retrieve all of the rows from storage with the tag key UID, then for each unique row, resolve the UIDs back to strings and then run the filter over the results. Also, filter sets with a large list of literals will be processed post storage to avoid creating a massive filter for the backing store to process. This limit defaults to `4096` and can be configured via the `tsd.query.filter.expansion_limit` parameter.\n\n## Built-in Filters\n\n### literal_or\n\nTakes a single literal value or a pipe delimited list of values and returns any time series matching the results on a case sensitive bases. This is a very efficient filter as it can resolve the strings to UIDs and send that to the storage layer for pre-filtering.\n\n*Examples*\n\n`literal_or(web01|web02|web03)` `literal_or(web01)`\n\n### ilteral_or\n\nThe same as a `literal_or` but is case insensitive. Note that this is not efficient like the literal or as it must post-process all rows from storage.\n\n### not_literal_or\n\nCase sensitive `literal_or` that will return series that do **NOT** match the given list of values. Efficient as it can be pre-processed by storage.\n\n### not_iliteral_or\n\nCase insensitive `not_literal_or`.\n\n### wildcard\n\nProvides case sensitive postfix, prefix, infix and multi-infix filtering. The wildcard character is an asterisk (star) `*`. Multiple wildcards can be used. If only the asterisk is given, the filter effectively returns any time series that include the tag key (and is an efficient filter that can be pre-processed).\n\n*Examples* `wildcard(*mysite.com)` `wildcard(web*)` `wildcard(web*mysite.com)` `wildcard(web*mysite*)` `wildcard(*)`\n\n### iwildcard\n\nThe same as `wildcard` but case insensitive.\n\n### regexp\n\nFilters using POSIX compliant regular expressions post fetching from storage. The filter uses Java's built-in regular expression operation. Be careful to escape special characters depending on the query method used.\n\n*Examples* `regexp(web.*)` `regexp(web[0-9].mysite.com)`\n\n## Plugins\n\nAs developers add plugins we will list them here.\n\nTo develop a plugin, simply extend the `net.opentsdb.query.filter.TagVFilter` class, create JAR per the [*Plugins*](../../development/plugins) documentation and drop it in your plugins directory. On start, the TSD will search for the plugin and load it. If there was an error with the implementation the TSD will not start up and will log the exception.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/query/filters.html](http://opentsdb.net/docs/build/html/user_guide/query/filters.html)"
- name: fsck
  id: user_guide/cli/fsck
  summary: Similar to a file system check, the fsck command will scan and, optionally, attempt to repair problems with data points in OpenTSDB's data table
  description: "# fsck\n\nSimilar to a file system check, the fsck command will scan and, optionally, attempt to repair problems with data points in OpenTSDB's data table. The fsck command only operates on the `tsdb` storage table, scanning the entire data table or any rows of data that match on a given query. Fsck can be used to repair errors and also reclaim space by compacting rows that were not compacted by a TSD and variable-length encoding data points from previous versions of OpenTSDB.\n\nBy default, running fsck will only report errors found by the query. No changes are made to the underlying data unless you supply the `--fix` or `--fix-all` flags. Generally you should run an fsck without a fix flag first and verify issues found in the log file. If you're confident in the repairs, add a fix flag. Not all errors can be repaired automatically.\n\nWarning\n\nRunning fsck with `--fix` or `--fix-all` may delete data points, columns or entire rows and deleted data is unrecoverable unless you restore from a backup. (or perform some HBase trickery to restore the data before a major compaction)\n\nNote\n\nThis page documents the OpenTSDB 2.1 fsck utility. For previous versions, only the `--fix` flag is available and only data within a query may be fsckd.\n\n## Parameters\n\n``` bash\nfsck [flags] [START-DATE [END-DATE] query [queries...]]\n```\n\n| Name                     | Data Type         | Description                                                                                                                                                                                                                             | Default           | Example                     |\n|--------------------------|-------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|-----------------------------|\n| --fix                    | Flag              | Optional flag that will attempt to repair errors. By itself, fix will only repair sign extension bugs, 8 byte floats with 4 byte qualifiers and VLE stand-alone data points. Use in conjunction with other flags to repair more issues. | Not set           | --fix                       |\n| --fix-all                | Flag              | Sets all repair flags to attempt to fix all issues at once. **Use with caution**                                                                                                                                                        | Not set           | --fix                       |\n| --compact                | Flag              | Compacts non-compacted rows during a repair.                                                                                                                                                                                            | Not Set           | --compact                   |\n| --delete-bad-compacts    | Flag              | Removes columns that appear to be compacted but failed parsing. If a column parses properly but the final byte of the value is not set to a 0 or a 1, the column will be left alone.                                                    | Not Set           | --delete-bad-compacts       |\n| --delete-bad-rows        | Flag              | Removes any row that doesn't match the OpenTSDB row key format of a metric UID followed by a timestamp and tag UIDs.                                                                                                                    | Not Set           | --delete-bad-rows           |\n| --delete-bad-values      | Flag              | Removes any stand-alone data points that could not be repaired or did not conform to the OpenTSDB specification.                                                                                                                        | Not Set           | --delete-bad-values         |\n| --delete-orphans         | Flag              | Removes rows where one or more UIDs could not be resolved to a name.                                                                                                                                                                    | Not Set           | --delete-orphans            |\n| --delete-unknown_columns | Flag              | Removes any column that does not appear to be a compacted column, a stand-alone data point or a known or future OpenTSDB object.                                                                                                        | Not Set           | --delete-unknown-columns    |\n| --resolve-duplicates     | Flag              | Enables duplicate data point resolution by deleting all but the latest or oldest data point. Also see `--last-write-wins`.                                                                                                              | Not Set           | --resolve-duplicates        |\n| --last-write-wins        | Flag              | When set, deletes all but the most recently written data point when resolving duplicates. If the config value `tsd.storage.fix_duplicates` is set to true, then the latest data point will be kept regardless of this value.            | Not Set           | --last-write-wins           |\n| --full-scan              | Flag              | Scans the entire data table. **Note:** This can take a very long time to complete.                                                                                                                                                      | Not Set           | --full-scan                 |\n| --threads                | Integer           | The number of threads to use when performing a full scan. The default is twice the number of CPU cores.                                                                                                                                 | 2 x CPU Cores     | --threads=16                |\n| START-DATE               | String or Integer | Starting time for the query. This may be an absolute or relative time. See [*Dates and Times*](../query/dates) for details                                                                                                              |                   | 1h-ago                      |\n| END-DATE                 | String or Integer | Optional end time for the query. If not provided, the current time is used. This may be an absolute or relative time. See [*Dates and Times*](../query/dates) for details                                                               | Current timestamp | 2014/01/01-00:00:00         |\n| query                    | String            | One or more command line queries                                                                                                                                                                                                        |                   | sum tsd.hbase.rpcs type=put |\n\n## Examples\n\n**Query**\n\n``` bash\nfsck --fix 1h-ago now sum tsd.hbase.rpcs type=put sum tsd.hbase.rpcs type=scan\n```\n\n**Full Table**\n\n``` bash\nfsck --full-scan --threads=8 --fix --resolve-duplicates --compact\n```\n\n## Full Table Vs Queries\n\nUsing the `--full-scan` flag, the entire OpenTSDB `tsdb` data table will be scanned. By default the utility will launch `2`` ``x`` ``CPU`` ``core` threads for optimal performance. Data is stored with the metric UID as the start of each row key so the utility will determine the maximum metric UID and split up the main data table equally among threads. If your data is distributed among metrics fairly evenly, then each thread should complete in roughly the same amount of time. However some metrics usually have more data or time series than others so these threads may be running much longer than others. Future updates to OpenTSDB will be able to divy up the workload in a more efficient manner.\n\nAlternatively you can spcify a CLI query to fsck over a smaller timespan and look at a specific metric or time series. These queries will almost always complete much faster than a full scan and will uncover similar issues. However orphaned metrics will not found as the query will only operate on known time series. Orphans where tag names or values have been deleted will still be found.\n\nRegardless of the method used, fsck only looks at the most recent column value in HBase. If the table is configured to store multiple versions, older versions of a column are ignored.\n\n## Results\n\nThe results will be logged with settings in the `logback.xml` file. For long fscks, it's recommended to run in the background and configure LogBack to have plenty of space for writing data. On completion, statistics about the run will be printed. An example looks like:\n\n``` python\n2014-07-07 13:09:15,610 INFO  [main] Fsck: Starting full table scan\n2014-07-07 13:09:15,619 INFO  [main] Fsck: Max metric ID is [0]\n2014-07-07 13:09:15,619 INFO  [main] Fsck: Spooling up [1] worker threads\n2014-07-07 13:09:16,358 INFO  [main] Fsck: Thread [0] Finished\n2014-07-07 13:09:16,358 INFO  [main] Fsck: Key Values Processed: 301\n2014-07-07 13:09:16,358 INFO  [main] Fsck: Rows Processed: 1\n2014-07-07 13:09:16,359 INFO  [main] Fsck: Valid Datapoints: 300\n2014-07-07 13:09:16,359 INFO  [main] Fsck: Annotations: 1\n2014-07-07 13:09:16,359 INFO  [main] Fsck: Invalid Row Keys Found: 0\n2014-07-07 13:09:16,360 INFO  [main] Fsck: Invalid Rows Deleted: 0\n2014-07-07 13:09:16,360 INFO  [main] Fsck: Duplicate Datapoints: 0\n2014-07-07 13:09:16,360 INFO  [main] Fsck: Duplicate Datapoints Resolved: 0\n2014-07-07 13:09:16,361 INFO  [main] Fsck: Orphaned UID Rows: 0\n2014-07-07 13:09:16,361 INFO  [main] Fsck: Orphaned UID Rows Deleted: 0\n2014-07-07 13:09:16,361 INFO  [main] Fsck: Possible Future Objects: 0\n2014-07-07 13:09:16,362 INFO  [main] Fsck: Unknown Objects: 0\n2014-07-07 13:09:16,362 INFO  [main] Fsck: Unknown Objects Deleted: 0\n2014-07-07 13:09:16,362 INFO  [main] Fsck: Unparseable Datapoint Values: 0\n2014-07-07 13:09:16,362 INFO  [main] Fsck: Unparseable Datapoint Values Deleted: 0\n2014-07-07 13:09:16,363 INFO  [main] Fsck: Improperly Encoded Floating Point Values: 0\n2014-07-07 13:09:16,363 INFO  [main] Fsck: Improperly Encoded Floating Point Values Fixed: 0\n2014-07-07 13:09:16,363 INFO  [main] Fsck: Unparseable Compacted Columns: 0\n2014-07-07 13:09:16,364 INFO  [main] Fsck: Unparseable Compacted Columns Deleted: 0\n2014-07-07 13:09:16,364 INFO  [main] Fsck: Datapoints Qualified for VLE : 0\n2014-07-07 13:09:16,364 INFO  [main] Fsck: Datapoints Compressed with VLE: 0\n2014-07-07 13:09:16,365 INFO  [main] Fsck: Bytes Saved with VLE: 0\n2014-07-07 13:09:16,365 INFO  [main] Fsck: Total Errors: 0\n2014-07-07 13:09:16,366 INFO  [main] Fsck: Total Correctable Errors: 0\n2014-07-07 13:09:16,366 INFO  [main] Fsck: Total Errors Fixed: 0\n2014-07-07 13:09:16,366 INFO  [main] Fsck: Completed fsck in [1] seconds\n```\n\nFor the most part, these statistics should be self-explanatory. `Key`` ``Values`` ``Processed` indicates the number of individual columns in HBase. `VLE` referse to `variable`` ``length`` ``encoding`.\n\nDuring a run, progress will be reported every 5 seconds so that you know the utility is still working. You should see lines similar to the following:\n\n``` python\n10:14:00.518 INFO  [Fsck.run] - Processed 47689680000 rows, 449891670779 valid datapoints\n10:14:01.518 INFO  [Fsck.run] - Processed 47689730000 rows, 449892264237 valid datapoints\n10:14:02.519 INFO  [Fsck.run] - Processed 47689780000 rows, 449892880333 valid datapoints\n```\n\nAny time an error is found (and possibly fixed), the log will be updated immediately. Errors will usually include the column where the error was found in the output. Byte arrays are represented in either Java style signed bytes, e.g. `[0,`` ``1,`` ``-42]` or hex encoded strings, e.g. `00000000000000040000000000000005`. Short-hand references include (k) for the row key, (q) for the qualifier and (v) for the value.\n\n## Types of Errors and Fixes\n\nThe following is a list of errors and/or fixes that can be found or performed with fsck.\n\n### Bad Row Keys\n\nIf a row key is found that doesn't conform to the OpenTSDB data table specification `<metric_UID><base_timestamp><tagk1_UID><tagv1_UID>[...<tagkn_UID><tagvn_UID>]`, the entire row is considered invalid.\n\n``` bash\n2014-07-07 15:03:46,483 ERROR [Fsck #0] Fsck: Invalid row key.\n    Key: 000001\n```\n\n*Fix:*\n\nIf `--delete-bad-rows` is set, then the entire row will be removed from HBase.\n\n### Orphaned Rows\n\nIf a row key is parsed as a proper OpenTSDB row, then the UIDs for the time series ID (TSUID) of the row are resolved to their names. If any of the UIDs does not match a name in the `tsdb-uid` table, then the row is considered an orphan. This can happen if a UID is manually deleted from the UID table or a deletion does not complete properly.\n\n``` bash\n2014-07-07 15:08:45,057 ERROR [Fsck #0] Fsck: Unable to resolve the metric from the row key.\n    Key: 00000150E22700000001000001\n    No such unique ID for 'metric': [0, 0, 1]\n```\n\n*Fix:*\n\nIf `--delete-orphans` is set, then the entire row will be removed from HBase.\n\n### Compact Row\n\nWhile it's not strictly an error, fsck can be used to compact rows into a single column. Compacting rows saves storage space by merging multiple columns into one. This cuts down on HBase overhead. If a TSD that is configured to compact columns crashes, some rows may be missed and remain in stand-alone data point form. As compaction can consume resources, you can use fsck to compact rows when the load on your cluster is reduced.\n\nSpecifying the `--compact` flag along with `--fix` will compact any row that has stand-alone data points within the query range. During compaction, any data points from old OpenTSDB versions that qualify for VLE will be re-encoded.\n\nNote\n\nIf a row is repaired for any reason and has one or more compacted columns, the row will be re-compacted regardless of the `--compact` flag.\n\n### Bad Compacted Column Error\n\nThese errors occur when compacted column is found that cannot be parsed into individual data points. This can happen if the qualifier appears correct but the number of bytes in the value array do not match the lengths encoded in the qualifier. Compacted columns with their data points out of order are not considered bad columns. Instead, the column will be sorted properly and re-written if the `--fix` or `--fix-all` flags are present.\n\n``` bash\n2014-07-07 13:29:40,251 ERROR [Fsck #0] Fsck: Corrupted value: couldn't break down into individual values (consumed 20 bytes, but was expecting to consume 24): [k '00000150E22700000001000001' q '000700270033' v '00000000000000040000000000000005000000000000000600'], cells so far: [Cell([0, 7], [0, 0, 0, 0, 0, 0, 0, 4]), Cell([0, 39], [0, 0, 0, 0, 0, 0, 0, 5]), Cell([0, 51], [0, 0, 0, 0])]\n```\n\n*Fix:*\n\nThe only fix for this error is to delete the column by specifying the `--delete-bad-compacts` flag.\n\n### Compacted Last Byte Error\n\nThe last byte of a compacted value is for storing meta data. It will usually be `0` if all of the data points are encoded in seconds or milliseconds. If there is a mixture of seconds and milliseconds will be set to `1`. If the value is something else then it may be from a future version of OpenTSDB or the column may be invalid.\n\n``` bash\n18:13:35.979 [main] ERROR net.opentsdb.tools.Fsck - The last byte of a compacted should be 0 or 1. Either this value is corrupted or it was written by a future version of OpenTSDB.\n    [k '00000150E22700000001000001' q '00070027' v '00000000000000040000000000000005']\n```\n\n*Fix:*\n\nCurrently this is not repaired. You can manually set the last byte to 0 or 1 to prevent the error from being thrown. The `--delete-bad-compacts` flag will not remove these columns.\n\n### Value Too Long Or Short\n\nThis may occur if a value is recorded on greater than 8 bytes for a single data point column. Individual data points are stored on 2 or 4 byte qualifiers. This error cannot happen for a data point within a compacted column. If it was compacted, the column would throw a bad compacted column error as it wouldn't be parseable.\n\n``` bash\n2014-07-07 14:50:44,022 ERROR [Fsck #0] Fsck: This floating point value must be encoded either on 4 or 8 bytes, but it's on 9 bytes.\n    [k '00000150E22700000001000001' q 'F000020B' v '000000000000000005']\n```\n\n*Fix:*\n\n`--delete-bad-values` will remove the column.\n\n### Old Version Floats\n\nEarly OpenTSDB versions had a bug in the floating point value storage where the first 4 bytes of an 8 byte value were written with all bits set to 1. The value should be on the last four bytes as the qualifier encodes the length as four bytes. However if the invalid data was compacted, the data cannot be parsed properly and an error will be recorded.\n\n``` bash\n18:43:35.297 [main] ERROR net.opentsdb.tools.Fsck - Floating point value with 0xFF most significant bytes, probably caused by sign extension bug present in revisions [96908436..607256fc].\n    [k '00000150E22700000001000001' q '002B' v 'FFFFFFFF43FA6666']\n```\n\n*Fix:*\n\nThe `--fix` flag will repair these errors by rewriting the value without the first four bytes. The qualifier remains unchanged.\n\n### 4 Byte Floats with 8 Byte Value OK\n\nSome versions of OpenTSDB may have encoded floating point values on 8 bytes when setting the qualifier length to 4 bytes. The first four bytes should be 0. If the value was compacted, the compacted column will be invalid as parsing is no longer possible.\n\n``` bash\n2014-07-07 14:33:34,498 WARN  [Fsck #0] Fsck: Floating point value was marked as 4 bytes long but was actually 8 bytes long\n    [k '00000150E22700000001000001' q '000B' v '0000000040866666']\n```\n\n*Fix:*\n\nThe `--fix` flag will repair these errors by rewriting the value without the first four bytes. The qualifier remains unchanged.\n\n### 4 Byte Floats with 8 Byte Value Bad\n\nIn this case a value was encoded on 8 bytes with the first four bytes set to a non-zero value. It could be that the value is an 8 byte double since OpenTSDB never actually encoded on 8 bytes, the value is likely corrupt. If the value was compacted, the compacted column will be invalid as parsing is no longer possible.\n\n``` bash\n2014-07-07 14:37:02,717 ERROR [Fsck #0] Fsck: Floating point value was marked as 4 bytes long but was actually 8 bytes long and the first four bytes were not zeroed\n    [k '00000150E22700000001000001' q '002B' v 'FB02F40F43FA6666']\n```\n\n*Fix:*\n\nThe `--delete-bad-values` flag will remove the column. You could try parsing the value as a Double manually and see if it looks valid, otherwise it's likely a corrupt column.\n\n### Unknown Object\n\nOpenTSDB 2.0 supports objects such as annotations in the data table. If a column is found that doesn't match an OpenTSDB object, a compacted column or a stand-alone data point, it is considered an unknown object and can likely be deleted.\n\n``` bash\n2014-07-07 14:55:03,019 ERROR [Fsck #0] Fsck: Unknown qualifier, must be 2, 3, 5 or an even number of bytes.\n    [k '00000150E22700000001000001' q '00270401010101' v '0000000000000005']\n```\n\n*Fix:*\n\nThe `--delete-unknown-columns` flag will remove this column from the row.\n\n### Future Object\n\nObjects are encoded on 3 or 5 byte qualifiers and the type is determined by a prefix. If a prefix is found that OpenTSDB doesn't recognize, then it will report the object but it will not be deleted. Note that this may actually be an unknown or corrupted column as fsck only looks at the qualifier length and the first byte of the qualifier. If that is the case, you can safely delete this column manually.\n\n``` bash\n2014-07-07 14:57:15,858 WARN  [Fsck #0] Fsck: Found an object possibly from a future version of OpenTSDB\n    [k '00000150E22700000001000001' q '042704' v '467574757265204F626A656374']\n```\n\n*Fix:*\n\nFuture objects are left alone during fsck. Querying over the data with a TSD that doesn't support the object will throw an exception but versions that do support the object should procede normally.\n\n### Duplicate Timestamps\n\nDue to the use of encoding length and type for datapoints in qualifiers, it's possible to record a data point for the same timestamp with two different qualifiers. For example if you post an integer value for time `1` and then post a float value for time `1`, two different columns will be created. Duplicates can also happen if a row has been compacted and the TSD writes a new stand-alone column that matches a timestamp in the compacted column. At query time, an exception will be thrown as TSD does not know which value is the correct one.\n\n``` bash\n2014-07-07 15:22:43,231 ERROR [Fsck #0] Fsck: More than one column had a value for the same timestamp: (1356998400000)\n  row key: (00000150E22700000001000001)\n  write time: (1388534400000)  compacted: (false)  qualifier: [0, 7]  <--- Keep oldest\n  write time: (1388534400001)  compacted: (false)  qualifier: [0, 11]\n  write time: (1388534400002)  compacted: (false)  qualifier: [0, 3]\n  write time: (1388534400003)  compacted: (false)  qualifier: [0, 1]\n```\n\n*Fix:*\n\nIf `--resolve-duplicates` is set, then all data points except for the latest or the oldest value will be deleted. The fix applies to both stand-alone and compacted data points. If the `--last-write-wins` flag is set, then the latest value is saved. Without the `--last-write-wins` flag, then the oldest value is saved.\n\nNote\n\nIf the `tsd.storage.fix_duplicates` configuration value is set to `true` then the latest value will be saved regardless of `--last-write-wins`.\n\nNote\n\nWith compactions enabled, it is possible (though unlikely) that a data point is written while a row is being compacted. In this case, the compacted column will have a *later* timestamp than a data point written during the compaction. Therefore the default result of `--resolve-duplicates` will keep the stand-alone data point or, if last writes win, then the compacted value.\n\n### Variable-Length Encoding\n\nEarly OpenTSDB implementations always encoded integer values on 8 bytes. With 2.0, integers were written on the smallest number of bytes possible, either 1, 2, 4 or 8. During fsck, any 8 byte encoded integers detected will be re-written with VLE if the `--fix` or `--fix-all` flags are specified. This includes stand-alone and compacted values. At the end of a run, the number of bytes saved with VLE are displayed.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/cli/fsck.html](http://opentsdb.net/docs/build/html/user_guide/cli/fsck.html)"
- name: General Development
  id: development/development
  summary: OpenTSDB isn't laid out like a typical Java project, instead it's a bit more like a C or C++ environment
  description: "# General Development\n\nOpenTSDB isn't laid out like a typical Java project, instead it's a bit more like a C or C++ environment. This page is to help folks who want to modify OpenTSDB and provide updates back to the community.\n\n## Build System\n\nThere are almost as many build systems as there are developers so it's impossible to satisfy everyone no matter which system or layout is chosen. Autotools and GNU Make were chosen early on for OpenTSDB because of their flexibility, portability, and especially speed and popular usage. It's not the easiest to configure but for our needs, it's really not too difficult. We'll spell out what you need to change below and give tips for IDE users who want to setup an environment. Note that the build script can now compile a `pom.xml` file for compiling with Maven and work is underway to provide better Maven support. However you still have to modify `Makefile.am` if you add or remove classes or dependencies and such.\n\n## Building\n\nOpenTSDB is built using the standard `./configure`` ``&&`` ``make` model that is most commonly employed by many open-source projects. Fresh working copies checked out from Git must first be `./bootstraped`.\n\nAlternatively, there is a `build.sh` script you can run that makes as it takes care of all the steps for you. You can give it a Make target in argument, e.g. `./build.sh`` ``distcheck` (the default target is `all`).\n\n### Build Targets\n\nThe `build.sh` script will compile a JAR and the static GWT files for the front-end GUI if no parameters are passed. Additional parameters include:\n\n- **check** - Executes unit tests and reports on the results. You can specify executing the checks in a specific file via `test_SRC=<path>`, e.g. `./build.sh`` ``check`` ``test_SRC=test/uid/TestNoSuchUniqueId.java`\n- **pom.xml** - Compile a POM file to compile with Maven.\n- **dist** - Downloads dependencies, compiles OpenTSDB and creates a tarball for distribution\n- **distcheck** - Same as dist but also runs unit tests. This should be run before issuing pull requests to verify that everything performs correctly.\n- **debian** - Compiles OpenTSDB and generates a Debian package\n\n## Adding a Dependency\n\n*Please try your best not to*. We're extremely picky on the dependencies and will require a code review before we start depending on a new library. The goal isn't to re-invent the wheel either, but we are very mindful about the number and quality of dependent libraries we pull in. If you absolutely must add a new dependency, here are the steps:\n\n- Find the canonical source to download the dependent JAR file\n\n- Find or create the proper directory under `third_party/`\n\n- In that directory, create a `<depdencency>.jar.md5` file\n\n- Paste the MD5 hash of the entire jar in that file and save it\n\n- Create or edit the `include.mk` file and copy the header info from another directory's file\n\n- Add a `<DEPENDENCY>_VERSION`` ``:=`` ``<version>` e.g. `JACKSON_VERSION`` ``:=`` ``1.9.4`\n\n- Add a `<DEPENDENCY>`` ``:=`` ``third_parth/<DIR>/<dependency>$(<DEPENDENCY>_VERSION).jar` line e.g. `JACKSON_CORE`` ``:=`` ``third_party/jackson/jackson-core-lgpl-$(JACKSON_CORE_VERSION).jar`\n\n- Add the canonical source URL in the format `<DEPENDENCY>_BASE_URL`` ``:=`` ``<URL>` e.g. `JACKSON_CORE_BASE_URL`` ``:=`` ``http://repository.codehaus.org/org/codehaus/jackson/jackson-core-lgpl/$(JACKSON_VERSION)` and note that the JAR name will be appended to the end of the URL\n\n- Add the following lines\n\n  ``` python\n  $(<DEPENDENCY>): $(J<DEPENDENCY>).md5\n  set dummy ``$(<DEPENDENCY>_BASE_URL)`` ``$(<DEPENDENCY>)``; shift; $(FETCH_DEPENDENCY)\n  ```\n\n  e.g.\n\n  ``` python\n  $(JACKSON_CORE): $(JACKSON_CORE).md5\n  set dummy ``$(JACKSON_CORE_BASE_URL)`` ``$(JACKSON_CORE)``; shift; $(FETCH_DEPENDENCY)\n  ```\n\n- Add a line `THIRD_PARTY`` ``+=`` ``$(<DEPENDENCY>)` e.g. `THIRD_PARTY`` ``+=`` ``$(JACKSON_CORE)`\n\n- Next, back in the `third_party/` directory, edit the `include.mk` file and if you added a new directory for your dependency, insert a reference to the `.mk` file in the proper alphabetical position.\n\n- Edit `Makefile.am`\n\n  - Find the `tsdb_DEPS`` ``=`` ``\\` line\n\n  - Add your new dependency in the proper alphabetical position in the format `$(<DEPENDENCY>)`, e.g. `$(JACKSON_CORE>`. Note that if you put it the middle of the list, you must finish with the line continuation character, the backslash `\\`. If your dependency goes at the end, do not add the backslash.\n\n    Note\n\n    If the dependency is only used for unit tests, then add it to the `test_DEPS`` ``=`` ``\\` list\n\n  - Find the `pom.xml:`` ``pom.xml.in`` ``Makefile` line in the file\n\n  - Add a sed line such as `-e`` ``'s/@<DEPENDENCY>_VERSION@/$(<DEPENDENCY>_VERSION)/'`` ``\\` e.g. `-e`` ``'s/@JACKSON_VERSION@/$(JACKSON_VERSION)/'`` ``\\`\n\n    Note\n\n    Unit test dependencies go here as well as regular items\n\n- Edit `pom.xml.in`\n\n  - Find the `<dependencies>` XML section\n  - Copy and paste an existing dependency section and modify it for your variables\n\n- Now run a build via `./build.sh` and verify that it fetches your dependency and builds without errors. \\* Then run `./build.sh`` ``pom.xml` to verify that the POM is compiled properly and run a `mvn`` ``compile` to verify the Maven build works correctly.\n\n## Adding/Removing/Moving a Class\n\nThis is much easier than dealing with a dependency. You only need to modify `Makefile.am` and edit the `tsdb_SRC`` ``:=`` ``\\` or the `test_SRC`` ``:=`` ``\\` lists. If you're adding a class, put it in the proper alphabetical position and account for the proper directory and class name. It is case sensitive so make sure to get that right. If removing a class, just delete the line. If moving a class, add the new line and delete the old one. Be careful to handle the line continuation `\\` backslashes. The last class in each list should NOT end with a backslash, the rest need it.\n\nAfter editing, rebuild with `./build.sh` and verify that your class was compiled and included properly.\n\n## IDEs\n\nMany devs use an IDE to work on Java projects and despite OpenTSDB's non-java-standard directory layout, working with an IDE is pretty easy. Here are some steps to get up and running with Eclipse though they should work with other environments. This example assumes you're using Eclipse.\n\n- Clone the GIT repo to a location such as `/home/$USER/opentsdb`\n- Build the repo with `./build.sh` from the directory\n- Fire up Eclipse or your favorite IDE\n- Create a new Java project with a name like `opentsdb_dev` so that it winds up in `/home/$USER/opentsdb_dev`\n- Your dev directory should now have a `./src` directory\n- Create a `net` directory under `./src` so that you have `./src/net` (some IDEs may create a `./src/java` dir, so add `./src/java/net`)\n- Create a symlink to the GIT repo's `./src` directory from `./src/net/opentsdb`. E.g. `ln`` ``-s`` ``/home/$USER/opentsdb/src`` ``/home/$USER/opentsdb_dev/src/net/opentdsb`\n- Also, create a `tsd` directory under `./src` so that you have `./src/tsd`\n- Create a symlink to the GIT repo's `./src/tsd/client` directory from `./src/tsd/client`. E.g. `ln`` ``-s`` ``/home/$USER/opentsdb/src/tsd/client`` ``/home/$USER/opentsdb_dev/src/tsd/client`\n- If your IDE didn't, create a `./test` directory under your dev project folder. This will be used for unit tests.\n- Add a `net` directory under `./test` so you have `./test/net`\n- Create a symlink to the GIT repo's `./test` directory from `./test/net/opentsdb`. E.g. `ln`` ``-s`` ``/home/$USER/opentsdb/test`` ``/home/$USER/opentsdb_dev/test/net/opentdsb`\n- Refresh the directory lists in Eclipse and you should see all of the source files\n- Right click the `net.opentsdb.tsd.client` package under SRC and select `Build`` ``Path` then `Exclude` from the menu\n- Now add the downloaded dependencies by clicking Project -\\> Properties, click the `Java`` ``Build`` ``Path` menu item and click `Add`` ``External`` ``JARs` button.\n- Do that for each of the dependencies that were downloaded by the build script\n- Copy the file `./build/src/BuildData.java` from the GIT repo, post build, to your `./src/net/opentsdb/` directory\n- Now click Run (or Debug) -\\> Manage Configurations\n- Under Java Application, right click and select New from the pop-up\n- Under the Main tab, brows to your `opentsdb_dev` project\n- For the Main Class, search for `net.opentsdb.tools.TSDMain`\n- Under Arguments, add the runtime arguments to select your Zookeeper quorum and the static and cache directories\n- Run or Debug it and hopefully it worked\n- Now edit away and when you're ready to publish changes, follow the directions above about modifying the build system (if necessary), publish to your own GitHub fork, and issue a pull request.\n\nNote\n\nThis won't compile the GWT UI. If you want to do UI work and have made changes, recompile OpenTSDB or export it as a JAR from your IDE, then execute the following command (assuming the directory structure above):\n\n``` python\njava -cp ``<PATH_TO>gwt-dev-2.4.0.jar;<PATH_TO>gwt-user-2.4.0.jar;<PATH_TO>tsdb-1.1.0.jar;/home/$USER/opentsdb/src/net/opentsdb;/home/$USER/opentsdb/src`` com.google.gwt.dev.Compiler -ea -war <PATH_TO_STATIC_DIRECTORY> tsd.Queryui\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/development/development.html](http://opentsdb.net/docs/build/html/development/development.html)"
- name: GUI
  id: user_guide/guis/index
  summary: Currently OpenTSDB offers a simple built-in GUI accessible by opening your browser and navigating to the host and port where the TSD is running
  description: "# GUI\n\nCurrently OpenTSDB offers a simple built-in GUI accessible by opening your browser and navigating to the host and port where the TSD is running. For example, if you are running a TSD on your local computer on port 4242, simply navigate to `http://localhost:4242`. While the GUI won't win awards for beauty, it provides a quick means of building a useful graph with the data in your system.\n\n## Interface\n\nThere are three main areas of the GUI:\n\n1.  The notification area and tab area that serves as a menu\n2.  The query builder that lets you select what will be displayed and how\n3.  The graph area that displays query results\n\n### Menu\n\nThe menu is a group of tabs that can be clicked for different options.\n\n- Graph - This is the default that lets you issue a query and generate a graph\n- Stats - This tab will display a list of statistics about the running TSD. The same stats can be retrieved via the `/stats` or `/api/stats` endpoints.\n- Logs - If Logback is configured, this tab will show you a list of the latest 1,024 log entries for the TSD.\n- Version - Displays version information about the TSD\n\n### Errors\n\nWhen building a graph, if an error occurs, a message will appear above the menu. Click on the arrow to expand the message and determine what the error was.\n\n## Query Builder\n\nYou'll likely spend a lot of time in this area since there are a number of options to play with. You'll likely want to start by choosing one or more metrics and tags to graph.\n\nNote\n\nIf you start by picking a start and end time then as soon as you enter a metric, the TSD will start to graph *every time series for that metric*. This will show the `Loading`` ``Graph...` status and may take a long time before you can do anything else. So skip the times and choose your metrics first.\n\nNote\n\nAlso note that changes to any field in this section will cause a graph reload, so be aware if you're graph takes a long time to load.\n\n### Metrics Section\n\nThis area is where you choose the metrics, optional tags, aggregation function and a possible down sampler for your graph. Along the top are a pair of blue tabs. Each graph can display multiple metrics and the tabs organize the different sub queries. Each graph requires at least one metric so you'll choose that metric in the first tab. To add another metric to your graph, click the `+` tab and you'll be able to setup another sub query. If you have configured multiple metrics, simply click on the tab that corresponds to the metric you want to modify. The tab will display a subset of the metric name it is associated with.\n\nThe **Metric** box is where you'll choose a metric. This field auto-completes as you type just like a modern web browser. Auto-complete is generally case sensitive so only metrics matching the case provided will be displayed. By default, only the 25 top matching entries will be returned so you may not see all of the possible choices as you type. Either click on the entry you want when it appears or keep typing until you have entire metric in the box.\n\nRecall from the [*Querying or Reading Data*](../query/index) documentation that if you only provide a metric without any tags, *every time series with that metric* will be aggregated in the results. If you want to drill down, supply one or more **Tags** to filter or group the results. A new metric section will have two boxes next to **Tags**. The left box is for the tag name or `tagk` value, e.g. `host` or `symbol`. The right hand box is for the tag value or `tagv`, e.g. `webserver01` or `google`. When you add a tag, another pair of boxes will appear so that you can keep adding tags to filter as much as necessary.\n\nBoth tag name and value boxes also auto-complete in the same way as the **Metric** box. However each auto-complete will show *all* of the results for the name or value, not just the values that would apply to a specific metric or tag name. In future versions we may be able to implement such a mapping feature but currently you'll have to sort through all of the values.\n\nWith version 2.2, a checkbox to the right of each pair of check boxes is used to determine if the results should be grouped by the tag filter (checked) or aggregated (unchecked). The boxes are checked by default to exhibit the behavior of TSD prior to 2.2.\n\nThe tag value box can use grouping operators such as the `*` and the `|`. See [*Querying or Reading Data*](../query/index) for details. Tag value boxes can also use filters as of version 2.2. E.g. you can enter \"wildcard(webserver\\*)\" as a tag value and it will match all hosts starting with \"webserver\".\n\nThe **Rate** box allows you to convert all of the time series for the metric to a rate of change value. By default this option is turned off.\n\n**Rate ctr** Enables the rate options boxes below and indicate that the metric graphed is a monotonically increasing counter. If so, you can choose to supply a maximum value (**Rate Ctr Max**) for the counter so that when it rolls over, the graph will show the proper value instead of a negative number. Likewise you can choose to set a reset value (**Rate Ctr Reset**) to replace values with a zero if the rate is greater than the value. To avoid negative spikes it's generally save to set the rate counter with a reset value of 1.\n\nFor metrics or time series with different scales, you can select the **Right Axis** check box to add another axis to the right of the graph for the metric's time series. This can make graphs much more readable if the scales differ greatly.\n\nThe **Aggregator** box is a drop-down list of aggregation functions used to manipulate the data for multiple time series associated with the sub query. The default aggregator is *sum* but you can choose from a number of other options.\n\nThe **Downsample** section is used to reduce the number of data points displayed on the graph. By default, GnuPlot will place a character, such as the `+` or `x` at each data point of a graph. When the time span is wide and there are many data points, the graph can grow pretty thick and ugly. Use down sampling to reduce the number of points. Simply choose an aggregation function from the drop down list, then enter a time interval in the second box. The interval must follow the relative date format (without the `-ago` component). For example, to downsample on an hour, enter `1h`. The last selection box chooses a \"fill policy\" for the downsampled values when aggregated with other series. For graphing in the GUI, only the \"zero\" value makes a difference as it will substitute a zero for missing series. See [*Dates and Times*](../query/dates) for details.\n\nDownsampling Disabled\n\nDownsampling Enabled\n\n### Time Section\n\nThe time secion determines the timespan for all metrics and time series in your graph. The **Frome** time determines when your graph will start and the **End** time determines when it will stop. Both fields must be filled out for a query to execute. Times may be in human readable, absolute format or a relative format. See [*Dates and Times*](../query/dates) for details.\n\nClicking a time box will pop-up a utility to help you choose a time. Use the arrows at the top left of the box to navigate through the months, then click on a date. The relative links in the upper right are helpers to jump forward or backward 1 minute, 10 minutes, 1 hour, 1 day, 1 week or 30 days. The *now* link will update the time to the current time on your local system. The **HH** buttons let you choose an hour along with *AM* or *PM*. The MM buttons let you choose a normalized minute. You can also cut and paste a time into the any of the boxes or edit the times directly.\n\nNote\n\nUnix timestamps are not supported directly in the boxes. You can click in a box to display the calendar, then paste a Unix timestamp (in seconds) in the *UNIX Timestamp* box, then press the *TAB* key to convert to a human readable time stamp.\n\nIf the time stamp in a time box is invalid, the background will turn red. This may happen if your start time is greater than or equal to your end time.\n\nThe **To (now)** link will update the **End** box to the current time on your system.\n\nClick the **Autoreload** check box to automatically refresh your graph periodically. This can be very useful for monitoring displays where you want to have the graph displayed for a number of people. When checked, the **End** box will disappear and be replaced by an **Every:** box that lets you choose the refresh rate in seconds. The default is to refresh every 15 seconds.\n\n## Graphing\n\nWe'll make a quick detour here to talk about the actual graph section. Below the query building area is a spot where details about query results are displayed as well as the actual graph.\n\nA status line prints information about the results of a query including whether or not the results were cached in the TSD, how many raw data points were analyzed, how many data points were actually plotted (as per the results of aggregations and down sampling) and how long the query took to execute. When the browser is waiting for the results of a query, this message will show `Loading`` ``Graph...`.\n\nNote\n\nWhen using the built-in UI, graphs are cached on disk for 60 seconds. If auto-refresh is enabled and the default of 15s is used, the cached graph will be displayed until the 60 seconds have elapsed. If you have higher resolution data coming in and want to bypass the cache, simply append `&nocache` to the GUI URL.\n\nBelow the status line will be the actual graph. The graph is simply a PNG image generated by GnuPlot so you can copy the image and save it to your local machine or send it in an email.\n\nYou can also zoom in on a time range by clicking and dragging a red box across a section of the graph. Release and the query will be updated with the new time span. Note that the browser cursor doesn't change when you're over the graph, it will still remain the default arrow your browser or OS provides.\n\n### Graph Style\n\nBack in the query builder section you have the graphing style box to the right.\n\nThe **WxH** box alters the dimensions of the graph. Simply enter the `<width>x<height>` in pixels such as `1024x768` then tab or click in another box to update the graph.\n\nBelow that are a few tabs for altering different parts of the graph.\n\n### Axes Tab\n\nThis area deals with altering the Y axes of the graph. **Y** settings affect the axis on the left and **Y2** settings affect the axis on the right. Y2 settings are only enabled if at least one of the metrics has had the **Right Axis** check box checked.\n\nThe **Label** box will add the specified text to the graph alon the left or right Y axis. By default, no label is provided since OpenTSDB doesn't know what you're graphing.\n\nThe **Format** box can alter the numbers on the Y axis according to a custom algorithm or formatting. This can be useful to convert numbers to or from scientific notation and adjusting the scale for gigabytes if the data comes in as bytes. For example, you can supply a value of `%0.0f`` ``Reqs` and it will change the axis to show an integer value at each step with the string *Reqs* after it as in the following example.\n\nRead the [GnuPlot Manual](http://www.gnuplot.info/) for *Format Specifiers* to find out what is permissible.\n\nThe **Range** box allows you to effectively zoom horizontally, showing only the data points between a range of Y axis values. The format for this box is `[<starting`` ``value>:<optional`` ``end`` ``value>]`. For example, if I want to show only the data points with values between 700 and 800 I can enter `[700:800]`. This will produce a graph as below:\n\nThe **Log Scale** check box will set a base ten log scale on the Y axis. An example appears below.\n\n### Key Tab\n\nThe top half of the key tab's section deals with the location of the graph key. This is a series of buttons layed out to show you where the key will appear. A box surrounds some of the buttons indicating that the key will appear inside of the graph's box, overlaying the data. The default location is the top right inside of the graph box. Simply select a button to move the key box.\n\nBy default, the key lists all of the different labels vertically. The **Horizontal Layout** check box will lay out the key horizontally first, then vertically if the dimensions of the graph wouldn't support it.\n\nThe **Box** check box will toggle a box outline around the key. This is on by default.\n\nThe **No Key** check box will hide the key altogether.\n\n### Style Tab\n\nThe style tab currently has a single box, the **Smooth** check box. With this checked, the data point characters will be removed from the graph (showing the lines only) and the data will be smoothed with splines (at least three points need to be plotted). Some users prefer this over the default.\n\n## Saving Your Work\n\nAs you make changes via the GUI you'll see that the URL reflects your edits. You can copy the URL, save it or email it around and pull it back up to pick up where you were. Unfortunately OpenTSDB doesn't include a built in dashboard so you'll have to save the URL somewhere manually.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/guis/index.html](http://opentsdb.net/docs/build/html/user_guide/guis/index.html)"
- name: HBase Schema
  id: user_guide/backends/hbase
  summary: All OpenTSDB data points are stored in a single, massive table, named tsdb by default
  description: "# HBase Schema\n\n## Data Table Schema\n\nAll OpenTSDB data points are stored in a single, massive table, named `tsdb` by default. This is to take advantage of HBase's ordering and region distribution. All values are stored in the `t` column family.\n\n**Row Key** - Row keys are byte arrays comprised of an optional salt, the metric UID, a base timestamp and the UID for tagk/v pairs: `[salt]<metric_uid><timestamp><tagk1><tagv1>[...<tagkN><tagvN>]`. By default, UIDs are encoded on 3 bytes.\n\nWith salting enabled (as of OpenTSDB 2.2) the first byte (or bytes) are a hashed salt ID to better distribute data across multiple regions and/or region servers.\n\nThe timestamp is a Unix epoch value in seconds encoded on 4 bytes. Rows are broken up into hour increments, reflected by the timestamp in each row. Thus each timestamp will be normalized to an hour value, e.g. *2013-01-01 08:00:00*. This is to avoid stuffing too many data points in a single row as that would affect region distribution. Also, since HBase sorts on the row key, data for the same metric and time bucket, but with different tags, will be grouped together for efficient queries.\n\nSome example unsalted row keys, represented as hex are:\n\n``` python\n00000150E22700000001000001\n00000150E22700000001000001000002000004\n00000150E22700000001000002\n00000150E22700000001000003\n00000150E23510000001000001\n00000150E23510000001000001000002000004\n00000150E23510000001000002\n00000150E23510000001000003\n00000150E24320000001000001\n00000150E24320000001000001000002000004\n00000150E24320000001000002\n00000150E24320000001000003\n```\n\nwhere:\n\n``` python\n00000150E22700000001000001\n'----''------''----''----'\nmetric  time   tagk  tagv\n```\n\nThis represents a single metric but four time series across three hours. Note how there is one time series with two sets of tags:\n\n``` python\n00000150E22700000001000001000002000004\n'----''------''----''----''----''----'\nmetric  time   tagk  tagv  tagk  tagv\n```\n\nTag names (tagk) are sorted alphabetically before storage, so the \"host\" tag will always appear first in the row key/TSUID ahead of \"owner\".\n\n### Data Point Columns\n\nBy far the most common column are data points. These are the actual values recorded when data is sent to the TSD for storage.\n\n**Column Qualifiers** - The qualifier is comprised of 2 or 4 bytes that encode an offset from the row's base time and flags to determine if the value is an integer or a decimal value. Qualifiers encode an offset from the row base time as well as the format and length of the data stored.\n\nColumns with 2 byte qualifiers have an offset in seconds. The first 12 bits of the qualifier represent an integer that is a delta from the timestamp in the row key. For example, if the row key is normalized to `1292148000` and a data point comes in for `1292148123`, the recorded delta will be `123`. The last 4 bits are format flags\n\nColumns with 4 byte qualifiers have an offset in milliseconds. The first 4 *bits* of the qualifier will always be set to `1` or `F` in hex. The next 22 bits encode the offset in milliseconds as an unsigned integer. The next 2 bits are reserved and the final 4 bits are format flags.\n\nThe last 4 bits of either column type describe the data stored. The first bit is a flag that indicates whether or not the value is an integer or floating point. A value of 0 indicates an integer, 1 indicates a float. The last 3 bits indicate the length of the data, offset by 1. A value of `000` indicates a 1 byte value while `010` indicates a 2 byte value. The length must reflect a value of 1, 2, 4 or 8. Anything else indicates an error.\n\nFor example, `0100` means the column value is an 8 byte, signed integer. `1011` indicates the column value is a 4 byte floating point value So the qualifier for the data point at `1292148123` with an integer value of 4294967296 would have a qualifier of `0000011110110100` or `07B4` in hex.\n\n**Column Values** - 1 to 8 bytes encoded as indicated by the qualifier flag.\n\n### Compactions\n\nIf compactions have been enabled for a TSD, a row may be compacted after it's base hour has passed or a query has run over the row. Compacted columns simply squash all of the data points together to reduce the amount of overhead consumed by disparate data points. Data is initially written to individual columns for speed, then compacted later for storage efficiency. Once a row is compacted, the individual data points are deleted. Data may be written back to the row and compacted again later.\n\nNote\n\nThe OpenTSDB compaction process is entirely separate in scope and definition than the HBase idea of compactions.\n\n**Column Qualifiers** - The qualifier for a compacted column will always be an even number of bytes and is simply a concatenation of the qualifiers for every data point that was in the row. Since we know each data point qualifier is 2 bytes, it's simple to split this up. A qualifier in hex with 2 data points may look like `07B407D4`.\n\n**Column Values** - The value is also a concatenation of all of the individual data points. The qualifier is split first and the flags for each data point determine if the parser consumes 4 or 8 bytes\n\n### Annotations or Other Objects\n\nA row may store notes about the timeseries inline with the datapoints. Objects differ from data points by having an odd number of bytes in the qualifier.\n\n**Column Qualifiers** - The qualifier is on 3 or 5 bytes with the first byte an ID that denotes the column as a qualifier. The first byte will always have a hex value of `0x01` for annotations (future object types will have a different prefix). The remaining bytes encode the timestamp delta from the row base time in a manner similar to a data point, though without the flags. If the qualifier is 3 bytes in length, the offset is in seconds. If the qualifier is 5 bytes in length, the offset is in milliseconds. Thus if we record an annotation at `1292148123`, the delta will be `123` and the qualifier, in hex, will be `01007B`.\n\n**Column Values** - Annotation values are UTF-8 encoded JSON objects. Do not modify this value directly. The order of the fields is important, affecting CAS calls.\n\n### Append Data Points\n\nOpenTSDB 2.2 introduced the idea of writing numeric data points to OpenTSDB using the `append` method instead of the normal `put` method. This saves space in HBase by writing all data for a row in a single column, enabling the benefits of TSD compactions while avoiding problems with reading massive amounts of data back into TSDs and re-writing them to HBase. The drawback is that the schema is incompatible with regular data points and requires greater CPU usage on HBase region servers as they perform a read, modify, write operation for each value.\n\n**Row Key** - Same as regular values.\n\n**Column Qualifier** - The qualifier is always the object prefix `0x05` with an offset of 0 from the base time on two bytes. E.g. `0x050000`.\n\n**Column Values** - Each column value is the concatenation of original data point qualifier offsets and values in the format `<offset1><value1><offset2><value2>...<offsetN><valueN>`. Values can appear in any order and are sorted at query time (with the option to re-write the sorted result back to HBase.).\n\n## UID Table Schema\n\nA separate, smaller table called `tsdb-uid` stores UID mappings, both forward and reverse. Two columns exist, one named `name` that maps a UID to a string and another `id` mapping strings to UIDs. Each row in the column family will have at least one of three columns with mapping values. The standard column qualifiers are:\n\n- `metrics` for mapping metric names to UIDs\n- `tagk` for mapping tag names to UIDs\n- `tagv` for mapping tag values to UIDs.\n\nThe `name` family may also contain additional meta-data columns if configured.\n\n### `id` Column Family\n\n**Row Key** - This will be the string assigned to the UID. E.g. for a metric we may have a value of `sys.cpu.user` or for a tag value it may be `42`.\n\n**Column Qualifiers** - One of the standard column types above.\n\n**Column Value** - An unsigned integer encoded on 3 bytes by default reflecting the UID assigned to the string for the column type. If the UID length has been changed in the source code, the width may vary.\n\n### `name` Column Family\n\n**Row Key** - The unsigned integer UID encoded on 3 bytes by default. If the UID length has been changed in the source code, the width may be different.\n\n**Column Qualifiers** - One of the standard column types above OR one of `metrics_meta`, `tagk_meta` or `tagv_meta`.\n\n**Column Value** - For the standard qualifiers above, the string assigned to the UID. For a `*_meta` column, the value will be a UTF-8 encoded, JSON formatted UIDMeta Object as a string. Do not modify the column value outside of OpenTSDB. The order of the fields is important, affecting CAS calls.\n\n### UID Assignment Row\n\nWithin the `id` column family is a row with a single byte key of `\\x00`. This is the UID row that is incremented for the proper column type (metrics, tagk or tagv) when a new UID is assigned. The column values are 8 byte signed integers and reflect the maximum UID assigned for each type. On assignment, OpenTSDB calls HBase's atomic increment command on the proper column to fetch a new UID.\n\n## Meta Table Schema\n\nThis table is an index of the different time series stored in OpenTSDB and can contain meta-data for each series as well as the number of data points stored for each series. Note that data will only be written to this table if OpenTSDB has been configured to track meta-data or the user creates a TSMeta object via the API. Only one column family is used, the `name` family and currently there are two types of columns, the meta column and the counter column.\n\n### Row Key\n\nThis is the same as a data point table row key without the timestamp. E.g. `<metric_uid><tagk1><tagv1>[...<tagkN><tagvN>]`. It is shared for all column types.\n\n### TSMeta Column\n\nThese columns store UTF-8 encoded, JSON formatted objects similar to UIDMeta objects. The qualifier is always `ts_meta`. Do not modify these column values outside of OpenTSDB or it may break CAS calls.\n\n### Counter Column\n\nThese columns are atomic incrementers that count the number of data points stored for a time series. The qualifier is `ts_counter` and the value is an 8 byte signed integer.\n\n## Tree Table Schema\n\nThis table behaves as an index, organizing time series into a hierarchical structure similar to a file system for use with tools such as Graphite or other dashboards. A tree is defined by a set of rules that process a TSMeta object to determine where in the hierarchy, if at all, a time series should appear.\n\nEach tree is assigned a Unique ID consisting of an unsigned integer starting with `1` for the first tree. All rows related to a tree are prefixed with this ID encoded as a two byte array. E.g. `\\x00\\x01` for UID `1`.\n\n### Row Key\n\nTree definition rows are keyed with the ID of the tree on two bytes. Columns pertaining to the tree definition, as well as the root branch, appear in this row. Definitions are generated by the user.\n\nTwo special rows may be included. They are keyed on `<tree`` ``ID>\\x01` for the `collisions` row and `<tree`` ``ID>\\x02` for the `not`` ``matched` row. These are generated during tree processing and will be described later.\n\nThe remaining rows are branch and leaf rows containing information about the hierarchy. The rows are keyed on `<tree`` ``ID><branch`` ``ID>` where the `branch`` ``ID` is a concatenation of hashes of the branch display names. For example, if we have a flattened branch `dal.web01.myapp.bytes_sent` where each branch name is separated by a period, we would have 3 levels of branching. `dal`, `web01` and `myapp`. The leaf would be named `bytes_sent` and links to a TSUID. Hashing each branch name in Java returns a 4 byte integer and converting to hex for readability yields:\n\n- `dal` = x00x01x83x8F\n- `web01` = x06xBCx4Cx55\n- `myapp` = x06x38x7CxF5\n\nIf this branch belongs to tree `1`, the row key for `dal` would be `\\x00\\x01\\x00\\x01\\x83\\x8F`. The branch for `myapp` would be `\\x00\\x01\\x00\\x01\\x83\\x8F\\x06\\xBC\\x4C\\x55\\x06\\x38\\x7C\\xF5`. This schema allows for navigation by providing a row key filter using a prefix including the tree ID and current branch level and a wild-card to match any number of child branch levels (usually only one level down).\n\n### Tree Column\n\nA Tree is defined as a UTF-8 encoded JSON object in the `tree` column of a tree row (identified by the tree's ID). The object contains descriptions and configuration settings for processing time series through the tree. Do not modify this object outside of OpenTSDB as it may break CAS calls.\n\n### Rule Column\n\nIn the tree row there are 0 or more rule columns that define a specific processing task on a time series. These columns are also UTF-8 encoded JSON objects and are modified with CAS calls. The qualifier id of the format `rule:<level>:<order>` where `<level>` is the main processing order of a rule in the set (starting at 0) and `order` is the processing order of a rule (starting at 0) within a given level. For example `rule:1:0` defines a rule at level 1 and order 0.\n\n### Tree Collision Column\n\nIf collision storage is enabled for a tree, a column is recorded for each time series that would have created a leaf that was already created for a previous time series. These columns are used to debug rule sets and only appear rin the collision row for a tree. The qualifier is of the format `tree_collision:<tsuid>` where the TSUID is a byte array representing the time series identifier. This allows for a simple `getRequest` call to determine if a particular time series did not appear in a tree due to a collision. The value of a colission column is the byte array of the TSUID that was recorded as a leaf.\n\n### Not Matched Column\n\nSimilar to collisions, when enabled for a tree, a column can be recorded for each time series that failed to match any rules in the rule set and therefore, did not appear in the tree. These columns only appear in the not matched row for a tree. The qualifier is of the format `tree_not_matched:<TSUID>` where the TSUID is a byte array representing the time series identifier. The value of a not matched column is the byte array of the TSUID that failed to match a rule.\n\n### Branch Column\n\nBranch columns have the qualifier `branch` and contain a UTF-8 JSON encoded object describing the current branch and any child branches that may exist. A branch column may appear in any row except the collision or not matched columns. Branches in the tree definition row are the `root` branch and link to the first level of child branches. These links are used to traverse the heirarchy.\n\n### Leaf Column\n\nLeaves are mappings to specific time series and represent the end of a hierarchy. Leaf columns have a qualifier format of `leaf:<TSUID>` where the TUID is a byte array representing the time series identifier. The value of a leaf is a UTF-8 encoded JSON object describing the leaf. Leaves may appear in any row other than the collision or not matched rows.\n\n## Rollup Tables Schema\n\nAs of OpenTSDB 2.4 is the concept of rollup and pre-aggregation tables. While TSDB does a great job of storing raw values as long as you want, querying for wide timespans across massive amounts of raw data can slow queries to a crawl and potentially OOM a JVM. Instead, individual time series can be rolled up (or downsampled) by time and stored as separate values that allow for scanning much wider timespans at a lower resolution. Additionally, for metrics with high cardinalities, pre-aggregate groups can be stored to improve query speed dramatically.\n\nThere are three types of rolled up data:\n\n- Rollup - This is a downsampled value across time for a single time series. It's similar to using a downsampler in query where the time series may have a data point every minute but is downsampled to a data point every hour using the `sum` aggregation. In that case, the resulting rolled up value is the sum of 60 values. E.g. if the value for each 1 minute data point is `1` then the resulting rollup value would be `60`.\n- Pre-Aggregate - For a metric with high cardinality (many unique tag values), scanning for all of the series can be costly. Take a metric `system.interface.bytes.out` where there are 10,000 hosts spread across 5 data centers. If users often look at the total data output by data center ( the query would look similar to aggregation = sum and data_center = [\\*](#id2)) then pre-calculating the sum across each data center would result in 5 data points being fetched per time period from storage instead of 10K. The resulting pre-aggregate would have a different tag set than the raw time series. In the example above, each series would likely have a `host` tag along with a `data_center` tag. After pre-aggregation, the `host` tag would be dropped, leaving only the `data_center` tag.\n- Rolled-up Pre-Aggregate - Pre-aggregated data can also be rolled up on time similar to raw time series. This can improve query speed for wide time spans over pre-aggregated data.\n\n### Configuration\n\n**TODO** - Settle on a config. Rollup configs consist of a table name, interval span and rollup interval. Raw pre-aggs can be stored in the data table or rollup tables as well.\n\n### Pre-Aggregate Schema\n\nIn OpenTSDB's implementation, a new, users configurable tag is added to all time series when rollups are enabled. The default key is `_aggegate` with a value of `raw` or an aggregation function. The tag is used to differentiate pre-aggregated data from raw (original) values. Therefore pre-aggregated data is stored in the same manner as original time series and can either be written to the original data table or stored in a separate table for greater query performance.\n\n### Rollup Schema\n\nRolled up data must be stored in a separate table from the raw data as to avoid existing schema conflicts and to allow for more performant queries.\n\n**Row Key** - The row key for rollups is in the same format as the original data table.\n\n**Column Qualifier** - Columns are different for rolled up data and consist of `<aggregation_function>:<time`` ``offset><type`` ``+`` ``length>` where the aggregation function is an upper-case string consisting of the function name used to generate the rollup and time offset is an offset from the row base time and the type + length describes the column value encoding.\n\n- Aggregation Function - This is the name of a function such as `SUM`, `COUNT`, `MAX` or `MIN`.\n- Time Offset - This is an offset based on the rollup table config, generally on 2 bytes. The offset is not a specific number of seconds or minutes from the base, instead it's the index of an interval of an offset. For example, if the table is configured to store 1 day of data at a resolution of 1 hour per row, then the base timestamp of the row key will align on daily boundaries (on Unix epoch timestamps). Then there would be a potential of 24 offsets (1 for each hour in the day) for the row. A data point at midnight for the given day would have an offset of 0 whereas the 23:00 hour value would have an offset of 22. Since rollup timestamps are aligned to time boundaries, qualifiers can save a fair amount of space.\n- Type and Length - Similar to the original data table, the last 4 bits of each offset byte array contains the encoding of the data value including it's length and whether or not it's a floating point value.\n\nAn example column qualifier for the daily 1 hour interval table looks like:\n\n``` python\nSUM:0010\n'-''---'\nagg offset\n```\n\nWhere the aggregator is `SUM`, the offset is `1` and the length is 1 byte of an integer value.\n\n**Column Value** - The values are the same as in the main data table.\n\n**TODOs** - Some further work that's needed:\n\n- Compactions/Appends - The current schema does not support compacted or append data types. These can be implemented by denoting a single column per aggregation function (e.g. `SUM`, `COUNT`) and storing the offsets and values in the column value similar to the main data table.\n- Additional Data Types - Currently only numeric data points are written to the pre-agg and rollup tables. We need to support rolling up annotations and other types of data.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/backends/hbase.html](http://opentsdb.net/docs/build/html/user_guide/backends/hbase.html)"
- name: help
  id: api_telnet/help
  summary: Returns a list of the commands supported via the Telnet style API
  description: "# help\n\nReturns a list of the commands supported via the Telnet style API. This command does not modify TSD in any way.\n\n## Request\n\nThe command format is:\n\n``` python\nhelp\n```\n\n## Response\n\nA space separated list of commands supported.\n\n### Example\n\n``` python\navailable commands: put stats dropcaches version exit help diediedie\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_telnet/help.html](http://opentsdb.net/docs/build/html/api_telnet/help.html)"
- name: HTTP API
  id: api_http/index
  summary: OpenTSDB provides an HTTP based application programming interface to enable integration with external systems
  description: "# HTTP API\n\nOpenTSDB provides an HTTP based application programming interface to enable integration with external systems. Almost all OpenTSDB features are accessiable via the API such as querying timeseries data, managing metadata and storing data points. Please read this entire page for important information about standard API behavior before investigating individual endpoints.\n\n## Overview\n\nThe HTTP API is RESTful in nature but provides alternative access through various overrides since not all clients can adhere to a strict REST protocol. The default data exchange is via JSON though pluggable `formatters` may be accessed, via the request, to send or receive data in different formats. Standard HTTP response codes are used for all returned results and errors will be returned as content using the proper format.\n\n## Version 1.X to 2.x\n\nOpenTSDB 1.x had a simple HTTP API that provided access to common behaviors such as querying for data, auto-complete queries and static file requests. OpenTSDB 2.0 introduces a new, formalized API as documented here. The 1.0 API is still accessible though most calls are deprecated and may be removed in version 3. All 2.0 API calls start with `/api/`.\n\n## Serializers\n\n2.0 introduces pluggable serializers that allow for parsing user input and returning results in different formats such as XML or JSON. Serializers only apply to the 2.0 API calls, all 1.0 behave as before. For details on Serializers and options supported, please read [*HTTP Serializers*](serializers/index)\n\nAll API calls use the default JSON serializer unless overridden by query string or `Content-Type` header. To override:\n\n- **Query String** - Supply a parameter such as `serializer=<serializer_name>` where `<serializer_name>` is the hard-coded name of the serializer as shown in the `/api/serializers` `serializer` output field.\n\n  Warning\n\n  If a serializer isn't found that matches the `<serializer_name>` value, the query will return an error instead of processing further.\n\n- **Content-Type** - If a query string is not given, the TSD will parse the `Content-Type` header from the HTTP request. Each serializer may supply a content type and if matched to the incoming request, the proper serializer will be used. If a serializer isn't located that maps to the content type, the default serializer will be used.\n\n- **Default** - If no query string parameter is given or the content-type is missing or not matched, the default JSON serializer will be used.\n\nThe API documentation will display requests and responses using the JSON serializer. See plugin documentation for the ways in which serializers alter behavior.\n\nNote\n\nThe JSON specification states that fields can appear in any order, so do not assume the ordering in given examples will be preserved. Arrays may be sorted and if so, this will be documented.\n\n## Authentication/Permissions\n\nAs of yet, OpenTSDB lacks an authentication and access control system. Therefore no authentication is required when accessing the API. If you wish to limit access to OpenTSDB, use network ACLs or firewalls to block access. We do not recommend running OpenTSDB on a machine with a public IP Address.\n\n## Response Codes\n\nEvery request will be returned with a standard HTTP response code. Most responses will include content, particularly error codes that will include details in the body about what went wrong. Successful codes returned from the API include:\n\n| Code | Description                                                                                                                                                                                |\n|------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 200  | The request completed successfully                                                                                                                                                         |\n| 204  | The server has completed the request successfully but is not returning content in the body. This is primarily used for storing data points as it is not necessary to return data to caller |\n| 301  | This may be used in the event that an API call has migrated or should be forwarded to another server                                                                                       |\n\nCommon error response codes include:\n\n| Code | Description                                                                                                                                                                                                                             |\n|------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 400  | Information provided by the API user, via a query string or content data, was in error or missing. This will usually include information in the error body about what parameter caused the issue. Correct the data and try again.       |\n| 404  | The requested endpoint or file was not found. This is usually related to the static file endpoint.                                                                                                                                      |\n| 405  | The requested verb or method was not allowed. Please see the documentation for the endpoint you are attempting to access                                                                                                                |\n| 406  | The request could not generate a response in the format specified. For example, if you ask for a PNG file of the `logs` endpoing, you will get a 406 response since log entries cannot be converted to a PNG image (easily)             |\n| 408  | The request has timed out. This may be due to a timeout fetching data from the underlying storage system or other issues                                                                                                                |\n| 413  | The results returned from a query may be too large for the server's buffers to handle. This can happen if you request a lot of raw data from OpenTSDB. In such cases break your query up into smaller queries and run each individually |\n| 500  | An internal error occured within OpenTSDB. Make sure all of the systems OpenTSDB depends on are accessible and check the bug list for issues                                                                                            |\n| 501  | The requested feature has not been implemented yet. This may appear with formatters or when calling methods that depend on plugins                                                                                                      |\n| 503  | A temporary overload has occurred. Check with other users/applications that are interacting with OpenTSDB and determine if you need to reduce requests or scale your system.                                                            |\n\n## Errors\n\nIf an error occurs, the API will return a response with an error object formatted per the requested response type. Error object fields include:\n\n| Field Name | Data Type | Always Present | Description                                                                                                                                                                                              | Example                    |\n|------------|-----------|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------|\n| code       | Integer   | Yes            | The HTTP status code                                                                                                                                                                                     | 400                        |\n| message    | String    | Yes            | A descriptive error message about what went wrong                                                                                                                                                        | Missing required parameter |\n| details    | String    | Optional       | Details about the error, often a stack trace                                                                                                                                                             | Missing value: type        |\n| trace      | String    | Optional       | A JAVA stack trace describing the location where the error was generated. This can be disabled via the `tsd.http.show_stack_trace` configuration option. The default for TSD is to show the stack trace. | See below                  |\n\nAll errors will return with a valid HTTP status error code and a content body with error details. The default formatter returns error messages as JSON with the `application/json` content-type. If a different formatter was requested, the output may be different. See the formatter documentation for details.\n\n### Example Error Result\n\n``` javascript\n{\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Missing parameter <code>type</code>\",\n    \"trace\": \"net.opentsdb.tsd.BadRequestException: Missing parameter <code>type</code>\\r\\n\\tat net.opentsdb.tsd.BadRequestException.missingParameter(BadRequestException.java:78) ~[bin/:na]\\r\\n\\tat net.opentsdb.tsd.HttpQuery.getRequiredQueryStringParam(HttpQuery.java:250) ~[bin/:na]\\r\\n\\tat net.opentsdb.tsd.SuggestRpc.execute(SuggestRpc.java:63) ~[bin/:na]\\r\\n\\tat net.opentsdb.tsd.RpcHandler.handleHttpQuery(RpcHandler.java:172) [bin/:na]\\r\\n\\tat net.opentsdb.tsd.RpcHandler.messageReceived(RpcHandler.java:120) [bin/:na]\\r\\n\\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75) [netty-3.5.9.Final.jar:na]\\r\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:565) [netty-3.5.9.Final.jar:na]\n    ....\\r\\n\\tat java.lang.Thread.run(Unknown Source) [na:1.6.0_26]\\r\\n\"\n  }\n}\n```\n\nNote that the stack trace is truncated. Also, the trace will include system specific line endings (in this case `\\r\\n` for Windows). If displaying for a user or writing to a log, be sure to replace the `\\n` or `\\r\\n` and `\\r` characters with new lines and tabs.\n\n## Verbs\n\nThe HTTP API is RESTful in nature, meaning it does it's best to adhere to the REST protocol by using HTTP verbs to determine a course of action. For example, a `GET` request should only return data, a `PUT` or `POST` should modify data and `DELETE` should remove it. Documentation will reflect what verbs can be used on an endpoint and what they do.\n\nHowever in some situations, verbs such as `DELETE` and `PUT` are blocked by firewalls, proxies or not implemented in clients. Furthermore, most developers are used to using `GET` and `POST` exclusively. Therefore, while the OpenTSDB API supports extended verbs, most requests can be performed with just `GET` by adding the query string parameter `method_override`. This parameter allows clients to pass data for most API calls as query string values instead of body content. For example, you can delete an annotation by issuing a `GET` with a query string `/api/annotation?start_time=1369141261&tsuid=010101&method_override=delete`. The following table describes verb behavior and overrides.\n\n| Verb   | Description                                                                                                                                                            | Override               |\n|--------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------|\n| GET    | Used to retrieve data from OpenTSDB. Overrides can be provided to modify content. **Note**: Requests via GET can only use query string parameters; see the note below. | N/A                    |\n| POST   | Used to update or create an object in OpenTSDB using the content body from the request. Will use a formatter to parse the content body                                 | method_override=post   |\n| PUT    | Replace an entire object in the system with the provided content                                                                                                       | method_override=put    |\n| DELETE | Used to delete data from the system                                                                                                                                    | method_override=delete |\n\nIf a method is not supported for a given API call, the TSD will return a 405 error.\n\nNote\n\nThe HTTP specification states that there shouldn't be an association between data passed in a request body and the URI in a `GET` request. Thus OpenTSDB's API does not parse body content in `GET` requests. You can, however, provide a query string with data and an override for updating data in certain endpoints. But we recommend that you use `POST` for anything that writes data.\n\n## API Versioning\n\nOpenTSDB 2.0's API call calls are versioned so that users can upgrade with gauranteed backwards compatability. To access a specific API version, you craft a URL such as `/api/v<version>/<endpoint>` such as `/api/v2/suggest`. This will access version 2 of the `suggest` endpoint. Versioning starts at 1 for OpenTSDB 2.0.0. Requests for a version that does not exist will result in calls to the latest version. Also, if you do not supply an explicit version, such as `/api/suggest`, the latest version will be used.\n\n## Query String Vs. Body Content\n\nMost of the API endpoints support query string parameters, particularly those that fetch data from the system. However due to the complexities of encoding some characters, and particularly Unicode, all endpoints also support access via POST content using formatters. The default format is JSON so clients can use their favorite means of generating a JSON object and send it to the OpenTSDB API via a `POST` request. `POST` requests will generally provided greater flexibility in the fields offered and fully Unicode support than query strings.\n\n## Compressed Requests\n\nThe API can accept body content that has been compressed. Make sure to set the `Content-Encoding` header to `gzip` and pass the binary encoded data over the wire. This is particularly useful for posting data points to the `/api/put` endpoint. An example using curl:\n\n``` javascript\n$ gzip -9c clear-32k.json > gzip-32k.json\n\n$ file gzip-32k.json\ngzip-32k.json: gzip compressed data, was \"clear-32k.json\", from Unix, last modified: Thu Jan 16 15:31:55 2014\n\n$ ls -l gzip-32k.json\n-rw-r--r-- 1 root root 1666 févr.  4 09:57 gzip-32k.json\n\n$ curl -X POST --data-binary \"@gzip-32k.json\" --header \"Content-Type: application/json\" --header \"Content-Encoding: gzip\" http://mytsdb1:4242/api/put?details\n{\"errors\":[],\"failed\":0,\"success\":280}\n```\n\n## CORS\n\nOpenTSDB provides simple and preflight support for Cross-Origin Resource Sharing (CORS) requests. To enable CORS, you must supply either a wild card `*` or a comma separated list of specific domains in the `tsd.http.request.cors_domains` configuration setting and restart OpenTSDB. For example, you can supply a value of `*` or you could provide a list of domains such as `beeblebrox.com,www.beeblebrox.com,aurtherdent.com`. The domain list is case insensitive but must fully match any value sent by clients.\n\nWhen a `GET`, `POST`, `PUT` or `DELETE` request arrives with the `Origin` header set to a valid domain name, the server will compare the domain against the configured list. If the domain appears in the list or the wild card was set, the server will add the `Access-Control-Allow-Origin` and `Access-Control-Allow-Methods` headers to the response after processing is complete. The allowed methods will always be `GET,`` ``POST,`` ``PUT,`` ``DELETE`. It does not change per end point. If the request is a CORS preflight, i.e. the `OPTION` method is used, the response will be the same but with an empty content body and a 200 status code.\n\nIf the `Origin` domain did not match a domain in the configured list, the response will be a 200 status code and an Error (see above) for the content body stating that access was denied, regardless of whether the request was a preflight or a regular request. The request will not be processed any further.\n\nBy default, the `tsd.http.request.cors_domains` list is empty and CORS is diabled. Requests are passed through without appending CORS specific headers. If an `Options` request arrives, it will receive a 405 error message.\n\nNote\n\nDo not rely on CORS for security. It is exceedingly easy to spoof a domain in an HTTP request and OpenTSDB does not perform reverse lookups or domain validation. CORS is only implemented as a means to make it easier JavaScript developers to work with the API.\n\n## Documentation\n\nThe documentation for each endpoint listed below will contain details about how to use that endpoint. Eahc page will contain a description of the endpoint, what verbs are supported, the fields in a request, fields in a respone and examples.\n\nRequest Parameters are a list of field names that you can pass in with your request. Each table has the following information:\n\n- Name - The name of the field\n- Data Type - The type of data you need to supply. E.g. `String` should be text, `Integer` must be a whole number (positive or negative), `Float` should be a decimal number. The data type may also be a complex object such as an array or map of values or objects. If you see `Present` in this column then simply adding the parameter to the query string sets the value to `true`, the actual value of the parameter is ignored. For example `/api/put?summary` will effectively set `summary=true`. If you request `/api/put?summary=false`, the API will still consider the request as `summary=true`.\n- Required - Whether or not the parameter is required for a successful query. If the parameter is required, you'll see `Required` otherwise it will be `Optional`.\n- Description - A detailed description of the parameter including what values are allowed if applicable.\n- Default - The default value of the `Optional` parameter. If the data is required, this field will be blank.\n- QS - If the parameter can be supplied via query string, this field will have a `Yes` in it, otherwise it will have a `No` meaning the parameter can only be supplied as part of the request body content.\n- RW - Describes whether or not this parameter can result in an update to data stored in OpenTSDB. Possible values in this column are:\n  - *empty* - This means that the field is for queries only and does not, necessarily, represent a field in the response.\n  - **RO** - A field that appears in the response but is read only. The value passed along with a request will not alter the output field.\n  - **RW** or **W** - A field that **will** result in an update to the data stored in the system\n- Example - An example of the parameter value\n\n## Deprecated API\n\nRead [*Deprecated HTTP API*](deprecated)\n\n## API Endpoints\n\n- [/s](s)\n- [/api/aggregators](aggregators)\n- [/api/annotation](annotation/index)\n- [/api/config](config/index)\n- [/api/dropcaches](dropcaches)\n- [/api/put](put)\n- [/api/rollup](rollup)\n- [/api/query](query/index)\n- [/api/search](search/index)\n- [/api/serializers](serializers)\n- [/api/stats](stats/index)\n- [/api/suggest](suggest)\n- [/api/tree](tree/index)\n- [/api/uid](uid/index)\n- [/api/version](version)\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/index.html](http://opentsdb.net/docs/build/html/api_http/index.html)"
- name: HTTP API
  id: development/http_api
  summary: These are some notes on adding to the HTTP API
  description: "# HTTP API\n\nThese are some notes on adding to the HTTP API.\n\n## Reserved Query String Parameters\n\nThe following is a list of query string parameters that are used by OpenTSDB across the entire API. Don't try to overload their use please:\n\n| Parameter  | Description                                                                 |\n|------------|-----------------------------------------------------------------------------|\n| serializer | The name of a serializer to use for parsing input or formatting return data |\n| method     | Allows for overriding the HTTP verb when necessary                          |\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/development/http_api.html](http://opentsdb.net/docs/build/html/development/http_api.html)"
- name: HTTP Serializers
  id: api_http/serializers/index
  summary: OpenTSDB supports common data formats via Serializers, plugins that can parse different data formats from an HTTP request and return data in the same format in an HTTP response
  description: "# HTTP Serializers\n\nOpenTSDB supports common data formats via Serializers, plugins that can parse different data formats from an HTTP request and return data in the same format in an HTTP response. Below is a list of formatters included with OpenTSDB, descriptions and a list of formatter specific parameters.\n\n- [*JSON Serializer*](json) - The default formatter for OpenTSDB handles parsing JSON requests and returns all data as JSON.\n\nPlease see [*HTTP API*](../index) for details on selecting a serializer.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/serializers/index.html](http://opentsdb.net/docs/build/html/api_http/serializers/index.html)"
- name: import
  id: user_guide/cli/import
  summary: The import command enables bulk loading of time series data into OpenTSDB
  description: "# import\n\nThe import command enables bulk loading of time series data into OpenTSDB. You provide one or more files and OpenTSDB will parse and load the data. Data must be formatted in the Telnet `put` style with one data point per line in a text file. Each file may optionally be compressed with GZip and if so, must end with the `.gz` extension.\n\nFor more information on storing data in OpenTSDB, please see `../writing`\n\n## Parameters\n\n``` bash\nimport path [...paths]\n```\n\nPaths may be absolute or relative\n\nExample\n\n``` bash\nimport /home/hobbes/timeseries1.gz /home/hobbes/timeseries2.gz\n```\n\n## Input Format\n\nThe format is the same as the Telnet `put` interface.\n\n> \\<metric\\> \\<timestamp\\> \\<value\\> \\<tagk=tagv\\> \\[\\<tagkN=tagvN\\>\\]\n\nWhere:\n\n> - **metric** Is the name of the metric. Note that the metric name may not include spaces.\n> - **timestamp** Is the absolute timestamp of the data point in seconds or milliseconds\n> - **value** Is the value to store\n> - **tagk=tagv** Is a pair of one or more space sparate tag name and value pairs. Note that the tags may not have spaces in them.\n\nExample:\n\n> sys.cpu.user 1356998400 42 host=web01 cpu=0\n\nSuccessful processing will result in responses like:\n\n> 23:07:05.323 \\[main\\] INFO net.opentsdb.tools.TextImporter - Processed file in 22 ms, 2 data points (90.9 points/s)\n\nHowever if an error occurs, the importer will stop and the errant line will be printed. For example:\n\n> 23:07:06.375 \\[main\\] ERROR net.opentsdb.tools.TextImporter - Exception caught while processing file timeseries1.gz line=sys.cpu.system 1356998400 42 host=web02 novalue=\n\nWarning\n\nData points processed up to the error are written to storage. You should edit the file and clear all data points up to the line where the error occurred. If you fix the line and restart the import, conflicts may occur with the existing data. Future updates to OpenTSDB will handle this situation gracefully.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/cli/import.html](http://opentsdb.net/docs/build/html/user_guide/cli/import.html)"
- name: Installation
  id: installation
  summary: OpenTSDB may be compiled from source or installed from a package
  description: "# Installation\n\nOpenTSDB may be compiled from source or installed from a package. Releases can be found on [Github](https://github.com/OpenTSDB/opentsdb/releases).\n\n## Runtime Requirements\n\nTo actually run OpenTSDB, you'll need to meet the following:\n\n- A Linux system (or Windows with manual building)\n- Java Runtime Environment 1.6 or later\n- HBase 0.92 or later\n- GnuPlot 4.2 or later\n\n## Installation\n\nFirst, you need to setup HBase. If you are brand new to HBase and/or OpenTSDB we recommend you test with a stand-alone instance as this is the easiest to get up and running. The best place to start is to follow the [Apache Quick Start](https://hbase.apache.org/book/quickstart.html) guide. Alternatively you could try a packaged distribution such as [Cloudera's CDH](http://www.cloudera.com/content/cloudera/en/products-and-services/cloudera-express.html), [Hortonworks HDP](http://hortonworks.com/products/hdp-2/) or [\\`MapR\\<https://www.mapr.com/\\>\\`\\_](#id2).\n\nBefore proceeding with OpenTSDB, make certain that Zookeeper is accessible. One method is to simply telnet to the proper port and execute the `stats` command.\n\n``` python\nroot@host:~# telnet localhost 2181\nTrying 127.0.0.1...\nConnected to myhost.\nEscape character is '^]'.\nstats\nZookeeper version: 3.4.3-cdh4.0.1--1, built on 06/28/2012 23:59 GMT\nClients:\n\nLatency min/avg/max: 0/0/677\nReceived: 4684478\nSent: 4687034\nOutstanding: 0\nZxid: 0xb00187dd0\nMode: leader\nNode count: 127182\nConnection closed by foreign host.\n```\n\nIf you can't connect to Zookeeper, check IPs and name resolution. HBase can be finicky.\n\nIf HBase is running, you can choose to install OpenTSDB from a package (available under [Releases](https://github.com/OpenTSDB/opentsdb/releases) in Github) or from source using GIT or a source tarball.\n\n### Compiling From Source\n\nCompilation requirements include:\n\n- A Linux system\n- Java Development Kit 1.6 or later\n- GnuPlot 4.2 or later\n- Autotools\n- Make\n- Python\n- Git\n- An Internet connection\n\nDownload the latest version using `git`` ``clone` command or download a release from the site or Github. Then just run the `build.sh` script. This script helps run all the processes needed to compile OpenTSDB: it runs `./bootstrap` (only once, when you first check out the code), followed by `./configure` and `make`. The output of the build process is put into a `build` folder and JARs required by OpenTSDB will be downloaded.\n\n``` python\ngit clone git://github.com/OpenTSDB/opentsdb.git\ncd opentsdb\n./build.sh\n```\n\nIf compilation was successfully, you should have a tsdb jar file in `./build` along with a `tsdb` script. You can now execute command-line tool by invoking `./build/tsdb` or you can run `make`` ``install` to install OpenTSDB on your system. Should you ever change your mind, there is also `make`` ``uninstall`, so there are no strings attached.\n\nIf you need to distribute OpenTSDB to machines without an Internet connection, call `./build.sh`` ``dist` to wrap the build directory into a tarball that you can then copy to additional machines.\n\n### Source Layout\n\nThere are two main branches in the GIT repo. The `master` branch is the latest stable release along with any bug fixes that have been committed between releases. Currently, the `master` branch is OpenTSDB 2.0.1. The `next` branch is the next major or minor version of OpenTSDB with new features and development. When `next` is stable, it will be merged into `master`. Currently the `next` branch is 2.1.0 RC 1. Additional branches may be present and are used for testing or developing specific features.\n\n### Debian Package\n\nYou can generate a Debian package by calling `sh`` ``build.sh`` ``debian`. The package will be located at `./build/opentsdb-2.x.x/opentsdb-2.x.x_all.deb`. Then simply distribute the package and install it as you regularly would. For example `dpkg`` ``-i`` ``opentsdb-2.0.0_all.deb`.\n\nThe Debian package will create the following directories:\n\n- /etc/opentsdb - Configuration files\n- /tmp/opentsdb - Temporary cache files\n- /usr/share/opentsdb - Application files\n- /usr/share/opentsdb/bin - The \"tsdb\" startup script that launches a TSD or command line tools\n- /usr/share/opentsdb/lib - Java JAR library files\n- /usr/share/opentsdb/plugins - Location for plugin files and dependencies\n- /usr/share/opentsdb/static - Static files for the GUI\n- /usr/share/opentsdb/tools - Scripts and other tools\n- /var/log/opentsdb - Logs\n\nInstallation includes an init script at `/etc/init.d/opentsdb` that can start, stop and restart OpenTSDB. Simply call `service`` ``opentsdb`` ``start` to start the tsd and `service`` ``opentsdb`` ``stop` to gracefully shutdown. Note after install, the tsd will not be running so that you can edit the configuration file. Edit the config file, then start the TSD.\n\nThe Debian package also creates an `opentsdb` user and group for the TSD to run under for increased security. TSD only requires write permission to the temporary and logging directories. If you can't use the default locations, please change them in `/etc/opentsdb/opentsdb.conf` and `/etc/opentsdb/logback.xml` respectively and apply the proper permissions for the `opentsdb` user.\n\nIf you install OpenTSDB for the first time, you'll need to create the HBase tables using the script located at `/usr/share/opentsdb/tools/create_table.sh`. Follow the steps below.\n\n### Create Tables\n\nIf this is the first time that you are running OpenTSDB with your HBase instance, you first need to create the necessary HBase tables. A simple script is provided to create the proper tables with the ability to enable or disable compression. Execute:\n\n``` python\nenv COMPRESSION=NONE HBASE_HOME=path/to/hbase-0.94.X ./src/create_table.sh\n```\n\nwhere the `COMPRESSION` value is either `NONE`, `LZO`, `GZIP` or `SNAPPY`. This will create four tables: `tsdb`, `tsdb-uid`, `tsdb-tree` and `tsdb-meta`. If you're just evaluating OpenTSDB, don't worry about compression for now. In production and at scale, make sure you use a valid compression library as it will save on storage tremendously.\n\n### Start a TSD\n\nOpenTSDB 2.3 works off a configuration file that is shared between the daemon and command line tools. If you compiled from source, copy the `./src/opentsdb.conf` file to a proper directory as documented in [*Configuration*](user_guide/configuration) and edit the following, required settings:\n\n- **tsd.http.cachedir** - Path to write temporary files to\n- **tsd.http.staticroot** - Path to the static GUI files found in `./build/staticroot`\n- **tsd.storage.hbase.zk_quorum** - If HBase and Zookeeper are not running on the same machine, specify the host and port here.\n\nWith the config file written, you can start a tsd with the command:\n\n``` python\n./build/tsdb tsd\n```\n\nAlternatively, you can also use the following commands to create a temporary directory and pass in only command line flags:\n\n``` python\ntsdtmp=${TMPDIR-'/tmp'}/tsd  # For best performance, make sure\nmkdir -p \"$tsdtmp\"       # your temporary directory uses tmpfs\n./build/tsdb tsd --port=4242 --staticroot=build/staticroot --cachedir=\"$tsdtmp\" --zkquorum=myhost:2181\n```\n\nAt this point you can access the TSD's web interface through [http://127.0.0.1:4242](http://127.0.0.1:4242) (if it's running on your local machine).\n\nNote\n\nThe **Cache Directory** stores temporary files generated when a graph is requested via the built-in GUI. These files should be purged periodically to free up space. OpenTSDB doesn't clean up after itself at this time but there is a script that should be run as a cron at least once a day located at `tools/clean_cache.sh`.\n\n## Upgrading from 1.x\n\nOpenTSDB 2.3 is fully backwards compatible with 1.x data. We've taken great pains to make sure you can download 2.3, compile, stop your old TSD and start the new one. Your existing tools will read and write to the TSD without a problem. 2.3 introduces two new tables to HBase schema for storing meta-data. From the directory where you downloaded the source (or the tools directory if installed with the Debian package), execute:\n\n``` python\nenv COMPRESSION=NONE HBASE_HOME=path/to/hbase-0.94.X ./src/upgrade_1to2.sh\n```\n\nwhere `COMPRESSION` is the same as your existing production table compression format.\n\nWhile you can start a 2.3 TSD with the same command line options as a 1.0 TSD, we highly recommend that you create a configuration file based on the config included at `./src/opentsdb.conf`. Or if you install from a package, you'll want to edit the included default config. The config file includes many more options than are accessible via command line and the file is shared with CLI tools. See [*Configuration*](user_guide/configuration) for details.\n\nYou do not have to upgrade all of your TSDs to 2.3 at the same time. Some users upgrade their read-only TSDs first to gain access to the full HTTP API and test the new features. Later on you can upgrade the write-only TSDs at leisure. You can also perform a rolling upgrade without issues. Simply stop traffic to one TSD, upgrade it, restore traffic, and continue on until you have upgraded all of your TSDs.\n\nIf you do perform a rolling upgrade where you have multiple TSDs, heed the following warning:\n\nWarning\n\nDo not write **Annotations** or **Data point with Millisecond Timestamps** while you run a mixture of 1.x and 2.x. Because these data are stored in the same rows as regular data points, they can affect compactions and queries.\n\nBefore upgrading to 2.x, you may want to upgrade all of your TSDs to OpenTSDB 1.2. This release is fully forwards compatible in that it will ignore annotations and millisecond timestamps and operate as expected. With 1.2 running, if you accidentally record an annotation or millisecond data point, your 1.2 TSDs will operate normally.\n\n## Upgrading from 2.x to a Later 2.x\n\nIn general, upgrading within a single major release branch is simply a matter of updating the binaries or package and restarting a TSD. Within a branch we'll maintain settings, APIs and schema. However new features may be added with each minor version that include new configuration settings with useful defaults.\n\nNote\n\nThe exception so far has been the introduction of salted rows in 2.2.0. Disabled by default, using this feature requires creating a new HBase table with a new set of pre-splits and modifying the configuration of every TSD to use the new table with salting enabled. The schema for salted and unsalted tables is incompatible so if users have a lot of data in a previous table, it may be best to leave a few TSDs running to query against the old table and new TSDs to write to and read from the new salted table. For smaller amounts of data, the [*scan*](user_guide/cli/scan) tool can be used to export and re-import your data.\n\nNote\n\nLikewise with 2.3, the introduction of new backends (Bigtable or Cassandra) requires setting up new storage tables and migrating data.\n\n## Downgrading\n\nBecause we've worked hard to maintain backwards compatibility, you can turn off a 2.x TSD and restart your old 1.x TSD. The only exceptions are if you have written annotations or milliseconds as you saw in the warning above. In these cases you must downgrade to 1.2 or later. You may also delete the `tsdb-tree` and `tsdb-meta` tables if you so desire.\n\nDowngrades within a major version are idempotent.\n\nWarning\n\nIf you wrote data using a salted table or changed the UID widths for metrics, tag keys or tag values then you cannot downgrade. Create a new table and export the data from the old table, then re-write the data to the new table using the older TSD version.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/installation.html](http://opentsdb.net/docs/build/html/installation.html)"
- name: JSON Serializer
  id: api_http/serializers/json
  summary: The default OpenTSDB serializer parses and returns JSON formatted data
  description: "# JSON Serializer\n\nThe default OpenTSDB serializer parses and returns JSON formatted data. Below you'll find details about the serializer and request parameters that affect only the the JSON serializer. If the serializer has extra parameters for a specific endpoint, they'll be listed below.\n\n## Serializer Name\n\n`json`\n\n## Serializer Options\n\nThe following options are supported via query string:\n\n| Parameter | Data Type | Required | Description                                                               | Default | Example        |\n|-----------|-----------|----------|---------------------------------------------------------------------------|---------|----------------|\n| jsonp     | String    | Optional | Wraps the response in a JavaScript function name passed to the parameter. | `empty` | jsonp=callback |\n\n## JSONP\n\nThe JSON formatter can wrap responses in a JavaScript function using the `jsonp` query string parameter. Supply the name of the function you wish to use and the result will be wrapped.\n\n### Example Request\n\n``` python\nhttp://localhost:4242/api/version?jsonp=callback\n```\n\n### Example Response\n\n``` javascript\ncallback({\n  \"timestamp\": \"1362712695\",\n  \"host\": \"DF81QBM1\",\n  \"repo\": \"/c/temp/a/opentsdb/build\",\n  \"full_revision\": \"11c5eefd79f0c800b703ebd29c10e7f924c01572\",\n  \"short_revision\": \"11c5eef\",\n  \"user\": \"df81qbm1_/clarsen\",\n  \"repo_status\": \"MODIFIED\",\n  \"version\": \"2.0.0\"\n})\n```\n\n## api/query\n\nThe JSON serializer allows some query string parameters that modify the output but have no effect on the data retrieved.\n\n| Name   | Data Type | Required | Description                                                                                                                                              | Default | Example     |\n|--------|-----------|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------|---------|-------------|\n| arrays | Boolean   | Optional | Returns the data points formatted as an array of arrays instead of a map of key/value pairs. Each array consists of the timestamp followed by the value. | false   | arrays=true |\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_http/serializers/json.html](http://opentsdb.net/docs/build/html/api_http/serializers/json.html)"
- name: Load Balancing with Varnish
  id: user_guide/utilities/varnish
  summary: Varnish is a powerful HTTP load balancer (reverse proxy), which is also very good at caching
  description: "# Load Balancing with Varnish\n\n[Varnish](https://www.varnish-cache.org/) is a powerful HTTP load balancer (reverse proxy), which is also very good at caching. When running multiple TSDs, Varnish comes in handy to distribute the HTTP traffic across the TSDs. Bear in mind that write traffic doesn't use the HTTP protocol by default, and as such you can only use Varnish for read queries. Using Varnish will help you easily scale the amount of read capacity of your TSD cluster.\n\nThe following is a sample Varnish configuration recommended for use with OpenTSDB. It uses a slightly custom load balancing strategy to achieve optimal cache hit rate at the TSD level. This configuration requires at least Varnish 2.1.0 to run, but using Varnish 3.0 or above is strongly recommended.\n\nThis sample configuration is for 2 backends, named `foo` and `bar`. You need to substitute at least the host names.\n\n``` python\n# VCL configuration for OpenTSDB.\n\nbackend foo {\n  .host = \"foo\";\n  .port = \"4242\";\n  .probe = {\n    .url = \"/version\";\n    .interval = 30s;\n    .timeout = 10s;\n    .window = 5;\n    .threshold = 3;\n  }\n}\n\nbackend bar {\n  .host = \"bar\";\n  .port = \"4242\";\n  .probe = {\n    .url = \"/version\";\n    .interval = 30s;\n    .timeout = 10s;\n    .window = 5;\n    .threshold = 3;\n  }\n}\n\n# The `client' director will select a backend based on `client.identity'.\n# It's normally used to implement session stickiness but here we abuse it\n# to try to send pairs of requests to the same TSD, in order to achieve a\n# higher cache hit rate.  The UI sends queries first with a \"&json\" at the\n# end, in order to get meta-data back about the results, and then it sends\n# the same query again with \"&png\".  If the second query goes to a different\n# TSD, then that TSD will have to fetch the data from HBase again.  Whereas\n# if it goes to the same TSD that served the \"&json\" query, it'll hit the\n# cache of that TSD and produce the PNG directly without using HBase.\n#\n# Note that we cannot use the `hash' director here, because otherwise Varnish\n# would hash both the \"&json\" and the \"&png\" requests identically, and it\n# would thus serve a cached JSON response to a \"&png\" request.\ndirector tsd client {\n  { .backend = foo; .weight = 100; }\n  { .backend = bar; .weight = 100; }\n}\n\nsub vcl_recv {\n  set req.backend = tsd;\n  # Make sure we hit the same backend based on the URL requested,\n  # but ignore some parameters before hashing the URL.\n  set client.identity = regsuball(req.url, \"&(o|ignore|png|json|html|y2?range|y2?label|y2?log|key|nokey)\\b(=[^&]*)?\", \"\");\n}\n\nsub vcl_hash {\n  # Remove the `ignore' parameter from the URL we hash, so that two\n  # identical requests modulo that parameter will hit Varnish's cache.\n  hash_data(regsuball(req.url, \"&ignore\\b(=[^&]*)?\", \"\"));\n  if (req.http.host) {\n    hash_data(req.http.host);\n  } else {\n    hash_data(server.ip);\n  }\n  return (hash);\n}\n```\n\nOn many Linux distros (including Debian and Ubuntu), you need to put the configuration above in `/etc/varnish/default.vcl`. We also recommend tweaking the command-line parameters of `varnishd` in order to use a memory-backed cache of about 1GB if you can afford it. On Debian/Ubuntu systems, this is done by editing `/etc/default/varnish` to make sure that `-s`` ``malloc,1G` is passed to `varnishd`.\n\nRead more about Varnish:\n\n- [The VCL configuration language](http://www.varnish-cache.org/docs/trunk/reference/vcl.html)\n- [Health checking backends](http://www.varnish-cache.org/trac/wiki/BackendPolling)\n- [Tweaking the load balancing strategy](http://www.varnish-cache.org/trac/wiki/LoadBalancing)\n\nNote\n\nif you're using Varnish 2.x (which is not recommended as we would strongly encourage you to migrate to 3.x) you have to replace each function call `hash_data(foo);` to set `req.hash`` ``+=`` ``foo;` in the VCL configuration above.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/utilities/varnish.html](http://opentsdb.net/docs/build/html/user_guide/utilities/varnish.html)"
- name: Logging
  id: user_guide/logging
  summary: OpenTSDB uses the SLF4J abstraction layer along with Logback for logging flexibility
  description: "# Logging\n\nOpenTSDB uses the [SLF4J](http://www.slf4j.org/) abstraction layer along with [Logback](http://logback.qos.ch/) for logging flexibility. Configuration is performed via an XML file and there are many different formatting, level and destination options.\n\n## Levels\n\nEvery log message is accompanied by a descriptive severity level. Levels employed by OpenTSDB include:\n\n- **ERROR** - Something failed, be it invalid data, a failed connection or a bug in the code. You should pay attention to these and figure out what caused the error. Check with the user group for assistance.\n- **WARN** - These are often caused by bad user data or something else that was wrong but not a critical error. Look for warnings if you are not receiving the results you expect when using OpenTSDB.\n- **INFO** - Informational messages are notifications of expected or normal behavior. They can be useful during troubleshooting. Most logging appenders should be set to `INFO`.\n- **DEBUG** - If you require further troubleshooting you can enable `DEBUG` logging that will give much greater detail about what OpenTSDB is doing under the hood. Be careful enabling this level as it can return a vast amount of data.\n- **OFF** - To drop any logging messages from a class, simply set the level to `OFF`.\n\n## Configuration\n\nA file called `logback.xml` is included in the `/src` directory and copied for distributions. On startup, OpenTSDB will search the class path for this file and if found, load the configuration. The default config from GIT will log INFO level events to console and store the 1,024 latest messages in a round-robin buffer to be accessed from the GUI. However by default, it won't log to disk. Packages built from GIT have file logging enabled by default. As of 2.2, all queries can be logged to a separate file for parsing and automating. This log is disabled by default but can be enabled by setting the proper log level.\n\n### Appenders\n\nAppenders are destinations where logging information is sent. Typically logging configs send results to the console and a file. Optionally you can send logs to Syslog, email, sockets, databases and more. Each appender section defines a destination, a format and an optional trigger. Read about appenders in the [Logback Manual](http://logback.qos.ch/manual/appenders.html).\n\n### Loggers\n\nLoggers determine what data and what level of data is routed to the appenders. Loggers can match a particular Java class namespace and affect all messages emitted from that space. The default OpenTSDB config explicitly lists some loggers for Zookeeper, AsyncHBase and the Async libraries to set their levels to `INFO` so as to avoid chatty outputs that are not relevant most of the time. If you enable a plugin and start seeing a lot of messages that you don't care about, add a logger entry to suppress the messages.\n\n**Query Log** To enable the Query log, find the following section:\n\n``` xml\n<logger name=\"QueryLog\" level=\"OFF\" additivity=\"false\">\n  <appender-ref ref=\"QUERY_LOG\"/>\n</logger>\n```\n\nand set the `level` to `INFO`.\n\n**Log File** To enable the main log file, find the following section:\n\n``` xml\n<!--<appender-ref ref=\"FILE\"/>-->\n```\n\nand remove the comments so it appears as `<appender-ref`` ``ref=\"FILE\"/>`.\n\n### Root\n\nThe root section is the catch-all logger that determines that default logging level for all messages that don't match an explicit logger. It also handles routing to the different appenders.\n\n### Log to Rotating File\n\n``` xml\n<appender name=\"FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n  <file>/var/log/opentsdb/opentsdb.log</file>\n  <append>true</append>\n\n  <rollingPolicy class=\"ch.qos.logback.core.rolling.FixedWindowRollingPolicy\">\n  <fileNamePattern>/var/log/opentsdb/opentsdb.log.%i</fileNamePattern>\n  <minIndex>1</minIndex>\n  <maxIndex>3</maxIndex>\n  </rollingPolicy>\n\n  <triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\">\n  <maxFileSize>128MB</maxFileSize>\n  </triggeringPolicy>\n\n  <!-- encoders are assigned the type\n     ch.qos.logback.classic.encoder.PatternLayoutEncoder by default -->\n  <encoder>\n  <pattern>%d{HH:mm:ss.SSS} %-5level [%logger{0}.%M] - %msg%n</pattern>\n  </encoder>\n</appender>\n```\n\nThis appender will write to a log file called `/var/log/opentsdb/opentsdb.log`. When the file reaches 128MB in size, it will rotate the log to `opentsdb.log.1` and start a new `opentsdb.log` file. Once the new log fills up, it bumps `.1` to `.2`, `.log` to `.1` and creates a new one. It repeats this until there are four log files in total. The next time the log fills up, the last log is deleted. This way you are gauranteed to only use up to 512MB of disk space. Many other appenders are available so see what fits your needs the best.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/logging.html](http://opentsdb.net/docs/build/html/user_guide/logging.html)"
- name: Metadata
  id: user_guide/metadata
  summary: The primary purpose of OpenTSDB is to store timeseries data points and allow for various operations on that data
  description: "# Metadata\n\nThe primary purpose of OpenTSDB is to store timeseries data points and allow for various operations on that data. However it helps to know what kind of data is stored and provide some context when working with the information. OpenTSDB's metadata is data about the data points. Much of it is user configurable to provide tie-ins with external tools such as search engines or issue tracking systems. This chapter describes various metadata available and what it's used for.\n\n## UIDMeta\n\nEvery data point stored in OpenTSDB has at least three UIDs associated with it. There will always be a `metric` and one or more tag pairs consisting of a `tagk` or tag name, and a `tagv` or tag value. When a new name for one of these UIDs comes into the system, a Unique ID is assigned so that there is always a UID name and numeric identifier pair.\n\nEach UID may also have a metadata entry recorded in the `tsdb-uid` table. Data available for each UID includes immutable fields such as the `uid`, `type`, `name` and `created` timestamp that reflects the time when the UID was first assigned. Additionally some fields may be edited such as the `description`, `notes`, `displayName` and a set of `custom` key/value pairs to record extra information. For details on the fields, see the [*/api/uid/uidmeta*](../api_http/uid/uidmeta) endpoint.\n\nWhenever a new UIDMeta object is created or modified, it will be pushed to the Search plugin if a plugin has been configured and loaded. For information about UID values, see [*UIDs and TSUIDs*](uids).\n\n## TSMeta\n\nEach timeseries in OpenTSDB is uniquely identified by the combination of it's metric UID and tag name/value UIDs, creating a TSUID as per [*UIDs and TSUIDs*](uids). When a new timeseries is received, a TSMeta object can be recorded in the `tsdb-uid` table in a row identified by the TSUID. The meta object includes some immutable fields such as the `tsuid`, `metric`, `tags`, `lastReceived` and `created` timestamp that reflects the time when the TSMeta was first received. Additionally some fields can be edited such as a `description`, `notes` and others. See [*/api/uid/tsmeta*](../api_http/uid/tsmeta) for details.\n\n## Enabling Metadata\n\nIf you want to use metadata in your OpenTSDB setup, you must explicitly enable real-time metadata tracking and/or use the CLI tools. There are multiple options for meta data generation due to impacts on performance, so before you enable any of these settings, please test the impact on your TSDs before enabling the settings in production.\n\nFour options are available, starting with the least impact to the most.\n\n- `tsd.core.meta.enable_realtime_uid` - When enabled, any time a new metric, tag name or tag value is assigned a UID, a UIDMeta object is generated and optionally sent to the configured search plugin. As UIDs are assigned fairly infrequently, this setting should not impact performance very much.\n- `tsd.core.meta.enable_tsuid_tracking` - When enabled, every time a data point is recorded, a `1` is written to the `tsdb-meta` table with the timestamp of the given data point. Enabling this setting will generate twice the number of *puts* to storage and may require a greater amount of memory heap. For example a single TSD should be able to acheive 6,000 data points per second with about 2GB of heap.\n- `tsd.core.meta.enable_tsuid_incrementing` - When this setting is enabled, every data point written will increment a counter in the `tsdb-meta` table corresponding to the time series the data point belongs to. As every data points spawns an increment request, this can generate a much larger load in a TSD and chew up heap space pretty quickly so only enable this if you can spread the load across multiple TSDs or your writes are fairly small. Enabling incrementing will override the `tsd.core.meta.enable_tsuid_tracking` setting. For example a single TSD should be able to acheive 3,000 data points per second with about 6GB of heap.\n- `tsd.core.meta.enable_realtime_ts` - When enabled, any time a new time series arrives, a TSMeta object will be created and optionally sent to a configured search plugin. This option will also enabled the `tsd.core.meta.enable_tsuid_incrementing` setting even if it's explicitly set to `false` in the config. If you often push new time series to your TSDs, this option may incur a fair amount of overhead and require some garbage collection tuning. If you do not often push new time series, you should be able to enable this setting without a problem, but watch the memory usage of your TSDs.\n\nWarning\n\nWatch your JVM heap usage when enabling any of the real-time meta data settings.\n\nFor situations where a TSD crashes before metadata can be written to storage or if you do not enable real-time tracking, you can periodically use the `uid` CLI tool and the `metasync` sub command to generate missing UIDMeta and TSMeta objects. See [*uid*](cli/uid) for information.\n\n## Annotations\n\nAnother form of metadata is the *annotation*. Annotations are simple objects associated with a timestamp and, optionally, a timeseries. Annotations are meant to be a very basic means of recording an event. They are not intended as an event management or issue tracking system. Rather they can be used to link a timeseries to such an external system.\n\nEvery annotation is associated with a start timestamp. This determines where the note is stored in the backend and may be the start of an event with a beginning and end, or just used to record a note at a specific point in time. Optionally an end timestamp can be set if the note represents a time span, such as an issue that was resolved some time after the start.\n\nAdditionally, an annotation is defined by a TSUID. If the TSUID field is set to a valid TSUID, the annotation will be stored, and associated, along with the data points for the timeseries defined by the ID. This means that when creating a query for data points, any annotations stored within the requested timespan will be retrieved and optionally returned to the user. These annotations are considered \"local\".\n\nIf the TSUID is empty, the annotation is considered a \"global\" notation, something associated with all timeseries in the system. When querying, the user can specify that global annotations be fetched for the timespan of the query. These notes will then be returned along with \"local\" annotations.\n\nAnnotations should have a very brief *description*, limited to 25 characters or so since the note may appear on a graph. If the requested timespan has many annotations, the graph can become clogged with notes. User interfaces can then let the user select an annotation to retrieve greater detail. This detail may include lengthy \"notes\" and/or a custom map of key/value pairs.\n\nUsers can add, edit and delete annotations via the Http API at `../api_http/annotation`.\n\nAn example GnuPlot graph with annotation markers appears below. Notice how only the `description` field appears in a box with a blue line recording the `start_time`. Only the `start_time` appears on the graph.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/metadata.html](http://opentsdb.net/docs/build/html/user_guide/metadata.html)"
- name: mkmetric
  id: user_guide/cli/mkmetric
  summary: mkmetric is a shortcut to the uid`` ``assign`` ``metrics`` ``<metric> command where you can provide multiple metric names in a single call and UIDs will be assigned or retrieved
  description: "# mkmetric\n\nmkmetric is a shortcut to the `uid`` ``assign`` ``metrics`` ``<metric>` command where you can provide multiple metric names in a single call and UIDs will be assigned or retrieved. If any of the metrics already exist, the assigned UID will be returned.\n\n## Parameters\n\n``` bash\nmkmetric metric [metrics]\n```\n\nSimply supply one or more space separate metric names in the call.\n\nExample\n\n``` bash\nmkmetric sys.cpu.user sys.cpu.nice sys.cpu.idle\n```\n\n## Response\n\nThe response is the literal \"metrics\" followed by the name of the metric and a Java formatted byte array representing the UID assigned or retrieved for each metric, one per line.\n\nExample\n\n``` bash\nmetrics sys.cpu.user: [0, 0, -58]\nmetrics sys.cpu.nice: [0, 0, -57]\nmetrics sys.cpu.idle: [0, 0, -59]\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/cli/mkmetric.html](http://opentsdb.net/docs/build/html/user_guide/cli/mkmetric.html)"
- name: Plugins
  id: development/plugins
  summary: OpenTSDB implements a very simple plugin model to extend the application
  description: "# Plugins\n\nOpenTSDB implements a very simple plugin model to extend the application. Plugins use the *service* and *service provider* facilities built into Java 1.6 that allows for dynamically loading JAR files and instantiating plugin implementations after OpenTSDB has been started. While not as flexible as many framework implementations, all we need to do is load a plugin on startup, initialize the implementation, and start passing data to or through it.\n\nTo create a plugin, all you have to do is extend one of the *abstract* plugin classes, write a service description/manifest, compile, drop your JAR (along with any dependencies needed) into the OpenTSDB plugin folder, edit the TSD config and restart. That's all there is to it. No fancy frameworks, no worrying about loading and unloading at strange times, etc.\n\n## Manifest\n\nA plugin JAR requires a manifest with a special *services* folder and file to enable the [ServiceLoader](http://docs.oracle.com/javase/6/docs/api/java/util/ServiceLoader.html) to load it properly. Here are the steps for creating the proper files:\n\n> - Create a `META-INF` directory under your `src` directory. Some IDEs can automatically generate this\n>\n> - Within the `META-INF` directory, create a file named `MANIFEST.MF`. Again some IDEs can generate this automatically.\n>\n> - Edit the `MANIFEST.MF` file and add:\n>\n>   ``` python\n>   Manifest-Version: 1.0\n>   ```\n>\n>   making sure to end with a blank line. You can add more manifest information if you like. This is the bare minimum to satisfy plugin requirements.\n>\n> - Create a `services` directory under `META-INF`\n>\n> - Within `services` create a file with the canonical class name of the abstract plugin class you are implementing. E.g. if you implement `net.opentsdb.search.SearchPlugin`, use that for the name of the file.\n>\n> - Edit the new file and put the canonical name of each class that implements the abstract interface on a new line of the file. E.g. if your implementation is called `net.opentsdb.search.ElasticSearch`, put that on a line. Some quick notes about this file:\n>\n>   - You can put comments in the service implementation file. The comment character is the `#`, just like a Java properties file. E.g.:\n>\n>     ``` python\n>     # ElasticSearch plugin written by John Doe\n>     # that sends data over HTTP to a number of ElasticSearch servers\n>     net.opentsdb.search.ElasticSearch\n>     ```\n>\n>   - You can have more than one implementation of the same abstract class in one JAR and in this file. NOTE: If you have widely different implementations, start a different project and JAR. E.g. if you implement a search plugin for ElasticSearch and another for Solr, put Solr in a different project. However if you have two implementations that are very similar but slightly different, you can create one project. For example you could write an ElasticSearch plugin that uses HTTP for a protocol and another that uses Thrift. In that case, you could have a file like:\n>\n>     ``` python\n>     # ElasticSearch HTTP\n>     net.opentsdb.search.ElasticSearchHTTP\n>     # ElasticSearch Thrift\n>     net.opentsdb.search.ElasticSearchThrift\n>     ```\n>\n> - Now compile your JAR and make sure to include the manifest file. Each IDE handles this differently. If you're going command line, try this:\n>\n>   ``` python\n>   jar cvmf <path to MANIFEST.MF> <plugin jar name> <list of class files>\n>   ```\n>\n>   Where the `<list`` ``of`` ``class`` ``files>` includes the services file that you created above. E.g.:\n>\n>   ``` python\n>   jar cvmf META-INF/MANIFEST.MF searchplugin.jar ../bin/net/opentsdb/search/myplugin.class META-INF/services/net.opentsdb.search.SearchPlugin\n>   ```\n\n## Startup Plugins\n\nStartup Plugins can be used to perform additional initialization steps during the OpenTSDB startup process.\n\nThere are four hooks available, and they are called in this order:  \n- Constructor\n- Initialize\n- Ready\n- Shutdown\n\n### Constructor\n\nIn the constructor for your plugin, you should initialize your plugin and make any external connections required here. For example, to connect to a service discovery tool such as Etcd or Curator.\n\n### Initialize\n\nThe Initialize hook is called once OpenTSDB has fully read the configuration options, both from the file, and the command line. This is called prior to creating the TSDB object so you can modify the configuration at this time.\n\nThis hook could be used to register OpenTSDB with a service discovery mechanism or look up the location of an HBase cluster dynamically and populate the connfiguration. You could potentially create HBase tables if they do not exist at this time.\n\nNote\n\nYou will want to make sure you set the status to PENDING or some other non-ready state in your service discovery system when this is called. TSDB has not been initialized yet at this point.\n\n### Ready\n\nThis hook is called once OpenTSDB has been fully initialized and is ready to serve traffic. This hook could be used to set the status to READY in a service discovery system, change the state of in a load balancer or perform other tasks which require a fully functioning OpenTSDB instance.\n\n### Shutdown\n\nThis hook is called when OpenTSDB is performing shutdown tasks. No work should be done here which requires a functioning and connected OpenTSDB instance. You could use this to update the status of this node within your service discovery mechanism.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/development/plugins.html](http://opentsdb.net/docs/build/html/development/plugins.html)"
- name: Plugins
  id: user_guide/plugins
  summary: OpenTSDB 2.0 introduced a plugin framework, allowing varous contributors to quickly and easily customize their TSDs
  description: "# Plugins\n\nOpenTSDB 2.0 introduced a plugin framework, allowing varous contributors to quickly and easily customize their TSDs. This document gives you an overview of the plugin system and will link to some available implementations.\n\n## General\n\nPlugins are loaded at run time by a TSD or command line utility. Once the program or daemon is running, plugin configurations cannot be changed. You must restart the program for changes to take effect.\n\nPlugins are JAR files that must be downloaded to a directory accessible by OpenTSDB. Once a directory is created, it must be specified in the `opentsdb.conf` config file via the `tsd.core.plugin_path` property. If the plugin has dependency JARs that were not compiled into the plugin and are not located in the standard class path, they must be copied to this plugin directory as well.\n\nOnce the JARs are in place, they must be selected in the configuration file for the type of plugin specified. Usually this will be the fully qualified Java class name such as \"net.opentsdb.search.ElasticSearch\". Each plugin should have an \"enabled\" property as well that must be set to `true` for the plugin to be loaded. Plugins may also have configuration settings that must be added to the `opentsdb.conf` file before they can operate properly. See your plugin's documentation. See [*Configuration*](configuration) for details.\n\nWhen starting a TSD or CLI tool, a number of errors may prevent a successful launch due to plugin issues. If something happens you should see an exception in the logs related to a plugin. Some things to troubleshoot include:\n\n- Make sure the `tsd.core.plugin_path` is configured\n- Check that the path is readable for the user OpenTSDB is running under, i.e. check permissions\n- Check for typos in the config file. Case matters for plugin names.\n- The plugin may not have access to the dependencies it needs. If it has dependencies that are not included with OpenTSDB or packaged into it's own JAR you need to drop the dependencies in the plugin path.\n- The plugin may be missing configuration settings required for it to be initialized. Read the docs and see if anything is missing.\n\nNote\n\nYou should always test a new plugin in a development or QA environment before enabling them in production. Plugins may adversely affect write or read performance so be sure to do some load testing to avoid taking down your TSDs and losing data.\n\n### Logging\n\nPlugins and their dependencies can be pretty chatty so you may want to tweak your Logback settings to reduce the number of messages.\n\n## Serializers\n\nThe HTTP API provides a plugin interface for serializing and deserializing data in formats other than the default JSON formats. These plugins do not require a plugin name or enable flag in the configuration file. Instead simply drop the plugin in the plugin directory and it will be loaded when the TSD is launched. More than one serializer plugin can be loaded on startup. Serializer plugins may require configuration properties, so check the documentation before using them.\n\n### Plugins\n\nNo implementations, aside from the default, at this time.\n\n## Startup and Service Discovery\n\nOpenTSDB is sometimes used within environments where additional initialization or registration is desired beyond what OpenTSDB typically can do out of the box. Startup plugins can be enabled which will be called when OpenTSDB is initializing, when it is ready to serve traffic, and when it is being shutdown. The `tsd.startup.plugin` property can be used to specify the plugin class and `tsd.startup.enable` will instruct OpenTSDB to attempt to load the startup plugin.\n\nNote\n\nAdded in 2.3.0\n\n### Plugins\n\n- [Identity Plugin](https://github.com/inst-tech/opentsdb-discoveryplugins/blob/master/src/main/java/io/tsdb/opentsdb/discoveryplugins/IdentityPlugin.java) - An example plugin which does nothing but can be used as a starting point for future Startup Plugins and can be used to test the registration mechanism.\n- [Apache Curator](https://github.com/inst-tech/opentsdb-discoveryplugins/blob/master/src/main/java/io/tsdb/opentsdb/discoveryplugins/CuratorPlugin.java) - A beta plugin which can be used to register OpenTSDB in Zookeeper using Apache Curator's Service Discovery mechanism\n\n## Search\n\nOpenTSDB can emit meta data and annotations to a search engine for complex querying. A single search plugin can be enabled for a TSD to push data or execute queries. The `tsd.search.plugin` property lets you select a search plugin and `tsd.search.enable` will start sending data and queries. Search plugins will be loaded by TSDs and select command line tools such as the UID Manager tool.\n\n### Plugins\n\n- [Elastic Search](https://github.com/manolama/opentsdb-elasticsearch) - A beta plugin that connects to an Elastic Search cluster\n\n## Real Time Publishing\n\nEvery data point received by a TSD can be sent to another destination for real time processing. One plugin for this type may be enabled at a time. The `tsd.rtpublisher.plugin` property lets you select a plugin and `tsd.rtpublisher.enable` will start sending data.\n\n### Plugins\n\n- [RabbitMQ](https://github.com/manolama/opentsdb-rtpub-rabbitmq) - A proof-of-concept plugin to publish to a RabbitMQ cluster by metric name\n- [Skyline](https://github.com/gutefrage/OpenTsdbSkylinePublisher) - A proof-of-concept plugin to publish to an Etsy Skyline processor\n\n## RPC\n\nNatively, OpenTSDB supports ingesting data points via Telnet or HTTP. The RPC plugin interface allows users to implement and choose alternative protocols such as Protobufs, Thrift, Memcache or any other means of storing information. More than one plugin can be loaded at a time via the `tsd.rpc.plugins` or tsd.http.rpc.plugins\\` configuration property. Simply list the class name of any RPC plugins you wish to load, separated by a comma if you have more than one. RPC plugins are only initialized when running a TSD.\n\n### Plugins\n\nNo implementations at this time.\n\n## Storage Exception Handler\n\nIf a write to the underlying storage layer fails for any reason, an exception is raised. When this happens, if a a storage exception handler plugin is enabled, the data points that couldn't be written can be retried at a later date by spooling to disk or passing to a messaging system. (v2.2)\n\n### Plugins\n\nNo implementations at this time.\n\n## HTTP RPC Plugin\n\nThis is an interface used to implement additional HTTP API endpoints for OpenTSDB. (v2.2)\n\n### Plugins\n\nNo implementations at this time.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/plugins.html](http://opentsdb.net/docs/build/html/user_guide/plugins.html)"
- name: put
  id: api_telnet/put
  summary: Attempts to write a data point to storage
  description: "# put\n\nAttempts to write a data point to storage. Note that UTF-8 characters may not be handled properly by the Telnet style API so use the [*/api/put*](../api_http/put) method instead or use the Java API directly.\n\nNote\n\nBecause the socket is read and written to asynchronously, responses may be garbled. It's best to treat this similar to a UDP socket in that you may not always know if the data made it in. If you require truly synchronous writes with guarantees of the data making it to storage, please use the HTTP or Java APIs.\n\n## Request\n\nThe command format is:\n\n``` python\nput <metric> <timestamp> <value> <tagk_1>=<tagv_1>[ <tagk_n>=<tagv_n>]\n```\n\nNote:\n\n- Because fields are space delimited, metrics and tag values may not contain spaces.\n- The timestamp must be a positive Unix epoch timestamp. E.g. `1479496100` to represent `Fri,`` ``18`` ``Nov`` ``2016`` ``19:08:20`` ``GMT`\n- The value must be a number. It may be an integer (maximum and minimum values of Java's `long` data type), a floating point value or scientific notation (in the format `[-]<#>.<#>[e|E][-]<#>`).\n- At least one tag pair must be present. Additional tag pairs can be added with spaces in between.\n\n### Examples\n\n``` python\nput sys.if.bytes.out 1479496100 1.3E3 host=web01 interface=eth0\nput sys.procs.running 1479496100 42 host=web01\n```\n\n## Response\n\nA successful request will not return a response. Only on error will the socket return a line of data. Some examples appear below:\n\n### Example Requests and Responses\n\n``` python\nput\nput: illegal argument: not enough arguments (need least 4, got 1)\n```\n\n``` python\nput metric.foo notatime 42 host=web01\nput: invalid value: Invalid character 'n' in notatime\n```\n\nThe following will be returned if `tsd.core.auto_create_metrics` are disabled.\n\n``` python\nput new.metric 1479496160 1.3e3 host=web01\nput: unknown metric: No such name for 'metrics': 'new.metric'\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_telnet/put.html](http://opentsdb.net/docs/build/html/api_telnet/put.html)"
- name: query
  id: user_guide/cli/query
  summary: The query command line tool is meant to be a quick debugging tool for extracting data from OpenTSDB
  description: "# query\n\nThe query command line tool is meant to be a quick debugging tool for extracting data from OpenTSDB. The HTTP API will usually be much quicker when querying data as it incorprates caches and open connections to storage. Results are printed to stdout in a text format with one data point per line.\n\nNote that a query may return data points before and after the timespan requested. These are used in downsampling and graphing.\n\n## Parameters\n\n``` bash\nquery [Gnuplot opts] START-DATE [END-DATE] <aggregator> [rate] [counter,max,reset] [downsample N FUNC] <metric> [<tagk=tagv>] [...<tagk=tagv>] [...queries]\n```\n\n| Name               | Data Type         | Description                                                                                                                                                                                                             | Default            | Example                      |\n|--------------------|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|------------------------------|\n| Gnuplot opts       | Strings           | Optional values used to generate Gnuplot scripts and graphs. Note that the actual graph PNG will not be generated, only the files (written to the temp directory)                                                       |                    | +wxh=1286x836                |\n| START-DATE         | String or Integer | Starting time for the query. This may be an absolute or relative time. See [*Dates and Times*](../query/dates) for details                                                                                              |                    | 1h-ago                       |\n| END-DATE           | String or Integer | Optional end time for the query. If not provided, the current time is used. This may be an absolute or relative time. See [*Dates and Times*](../query/dates) for details                                               | Current timestamp  | 2014/01/01-00:00:00          |\n| aggregator         | String            | A function to use when multiple timeseries are included in the results                                                                                                                                                  |                    | sum                          |\n| rate               | String            | The literal `rate` if the timeseries represents a counter and the results should be returned as delta per second                                                                                                        |                    | rate                         |\n| counter            | String            | Optional literal `counter` that indicates the underlying data is a monotonically increasong counter that may roll over                                                                                                  |                    | counter                      |\n| max                | Integer           | A positive integer representing the maximum value for the counter                                                                                                                                                       | Java Long.MaxValue | 65535                        |\n| resetValue         | Integer           | An optional value that, when exceeded, will cause the aggregator to return a 0 instead of the calculated rate. Useful when data sources are frequently reset to avoid spurious spikes.                                  |                    | 65000                        |\n| downsample N FUNC  | String            | Optional downsampling specifier to group data into larger time spans and reduce the amount of data returned. Format is the literal `downsample` followed by a timespan in milliseconds and an aggregation function name |                    | downsample 300000 avg        |\n| metric             | String            | Required name of a metric to query for                                                                                                                                                                                  |                    | sys.cpu.user                 |\n| tagk=tagv          | String            | Optional pairs of tag names and tag values                                                                                                                                                                              |                    | host=web01                   |\n| additional queries | String            | Optional additional queries to execute. Each query must follow the same format starting with an aggregator. All queries share the same start and end times.                                                             |                    | sum tsd.hbase.rpcs type=scan |\n\nFor more details on querying, please see [*Querying or Reading Data*](../query/index).\n\nExample:\n\n``` bash\nquery 1h-ago now sum tsd.hbase.rpcs type=put sum tsd.hbase.rpcs type=scan\n```\n\n## Output Format\n\nData is printed to stdout with one data point per line. If one or more Gnuplot options were specified, then scripts and data files for each query will be written to the configured temporary directory.\n\n> \\<metric\\> \\<timestamp\\> \\<value\\> {\\<tagk=tagv\\>\\[,..\\<tagkN=tagvN\\>\\]}\n\nWhere:\n\n> - **metric** Is the name of the metric queried\n> - **timestamp** Is the absolute timestamp of the data point in seconds or milliseconds\n> - **value** Is the data point value\n> - **tagk=tagv** Is a list of common tag name and value pairs for all timeseries represented in the query\n\nExample:\n\n``` bash\ntsd.hbase.rpcs 1393376401000 28067146491 {type=put, fqdn=tsdb-data-1}\ntsd.hbase.rpcs 1393376461000 28067526510 {type=put, fqdn=tsdb-data-1}\ntsd.hbase.rpcs 1393376521000 28067826659 {type=put, fqdn=tsdb-data-1}\ntsd.hbase.rpcs 1393376581000 28068126093 {type=put, fqdn=tsdb-data-1}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/cli/query.html](http://opentsdb.net/docs/build/html/user_guide/cli/query.html)"
- name: Query Details and Stats
  id: user_guide/query/stats
  summary: With version 2.2 of OpenTSDB a number of details are now available around queries as we focus on improving flexibility and performance
  description: "# Query Details and Stats\n\nWith version 2.2 of OpenTSDB a number of details are now available around queries as we focus on improving flexibility and performance. Query details include who made the request (via headers and socket), what the response was (HTTP status codes and/or exceptions) and timing around the various processes the TSD takes.\n\nEach HTTP query can include some of these details such as the original query and the timing information using the `showSummary` and `showQuery` parameters. Other details can be found in the `/api/stats/query` output including headers, status and exceptions. And full details (minus the actual result data) can be logged to disk via the logging config. This page details the various query sections and the information found therein.\n\n## Query\n\nThis section is a serialization of the query given by the user. In the logs and stats page this will be the full query with timing and global options. When returned with the query results, only the sub query (metric and filters) are returned with the associated result set for identification purposes (e.g. if you request the same metric twice with two different aggregators, you need to know which data set corresponds to which aggregator).\n\nFor the fields and what they mean, see [*/api/query*](../../api_http/query/index). Some notes about the fields:\n\n- The `tags` map should have the same number of entries as the `filters` array has `group_by` entries. This is due to backwards compatibility with 2.1 and 1.0. Old style queries are converted into filtered queries and function the same way.\n- A number of extra fields may be shown here with their default values such as `null`.\n- You can copy and paste the query into a POST client to execute and find out what data was returned.\n\n### Example\n\n``` javascript\n{\n    \"start\": \"1455531250181\",\n    \"end\": null,\n    \"timezone\": null,\n    \"options\": null,\n    \"padding\": false,\n    \"queries\": [{\n        \"aggregator\": \"zimsum\",\n        \"metric\": \"tsd.connectionmgr.bytes.written\",\n        \"tsuids\": null,\n        \"downsample\": \"1m-avg\",\n        \"rate\": true,\n        \"filters\": [{\n            \"tagk\": \"colo\",\n            \"filter\": \"*\",\n            \"group_by\": true,\n            \"type\": \"wildcard\"\n        }, {\n            \"tagk\": \"env\",\n            \"filter\": \"prod\",\n            \"group_by\": true,\n            \"type\": \"literal_or\"\n        }, {\n            \"tagk\": \"role\",\n            \"filter\": \"frontend\",\n            \"group_by\": true,\n            \"type\": \"literal_or\"\n        }],\n        \"rateOptions\": {\n            \"counter\": true,\n            \"counterMax\": 9223372036854775807,\n            \"resetValue\": 1,\n            \"dropResets\": false\n        },\n        \"tags\": {\n            \"role\": \"literal_or(frontend)\",\n            \"env\": \"literal_or(prod)\",\n            \"colo\": \"wildcard(*)\"\n        }\n    }, {\n        \"aggregator\": \"zimsum\",\n        \"metric\": \"tsd.hbase.rpcs.cumulative_bytes_received\",\n        \"tsuids\": null,\n        \"downsample\": \"1m-avg\",\n        \"rate\": true,\n        \"filters\": [{\n            \"tagk\": \"colo\",\n            \"filter\": \"*\",\n            \"group_by\": true,\n            \"type\": \"wildcard\"\n        }, {\n            \"tagk\": \"env\",\n            \"filter\": \"prod\",\n            \"group_by\": true,\n            \"type\": \"literal_or\"\n        }, {\n            \"tagk\": \"role\",\n            \"filter\": \"frontend\",\n            \"group_by\": true,\n            \"type\": \"literal_or\"\n        }],\n        \"rateOptions\": {\n            \"counter\": true,\n            \"counterMax\": 9223372036854775807,\n            \"resetValue\": 1,\n            \"dropResets\": false\n        },\n        \"tags\": {\n            \"role\": \"literal_or(frontend)\",\n            \"env\": \"literal_or(prod)\",\n            \"colo\": \"wildcard(*)\"\n        }\n    }],\n    \"delete\": false,\n    \"noAnnotations\": false,\n    \"globalAnnotations\": false,\n    \"showTSUIDs\": false,\n    \"msResolution\": false,\n    \"showQuery\": false,\n    \"showStats\": false,\n    \"showSummary\": false\n}\n```\n\n## Exception\n\nIf the query failed, this field will include the message string and the first line of the stack trace for pinpointing. If the query was successful, this field will be null.\n\n### Example\n\n``` javascript\n\"exception\": \"No such name for 'metrics': 'nosuchmetric' net.opentsdb.uid.UniqueId$1GetIdCB.call(UniqueId.java:315)\"\n```\n\n## User\n\nFor future use, this field can be used to extract user information from queries and help debug who is using a TSD the most. It's fairly easy to modify the TSD code to extract the user from an HTTP header.\n\n## RequestHeaders\n\nThis is a map of the headers sent with the HTTP request. In a mediocre effort at security, the `Cookie` header field is obfuscated with asterisks in the case that it contains user identifiable or secure information. Each request is different so lookup the headers in the HTTP RFCs or your web browser or clients documentation.\n\n### Example\n\n``` javascript\n\"requestHeaders\": {\n  \"Accept-Language\": \"en-US,en;q=0.8\",\n  \"Host\": \"tsdhost:4242\",\n  \"Content-Length\": \"440\",\n  \"Referer\": \"http://tsdhost:8080/dashboard/db/tsdfrontend\",\n  \"Accept-Encoding\": \"gzip, deflate\",\n  \"X-Forwarded-For\": \"192.168.0.2\",\n  \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.109 Safari/537.36\",\n  \"Origin\": \"http://tsdhost:8080\",\n  \"Content-Type\": \"application/json;charset=UTF-8\",\n  \"Accept\": \"application/json, text/plain, */*\"\n}\n```\n\n## HttpResponse\n\nThis field contains the numeric HTTP response code and a textual representation of that code.\n\n### Example\n\n``` javascript\n\"httpResponse\": {\n    \"code\": 200,\n    \"reasonPhrase\": \"OK\"\n}\n```\n\n## Other Fields\n\nThe output for log files and the stats page include other fields with single values as listed below:\n\n| Metric                  | Type           | Description                                                                                                                                                                                 |\n|-------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| executed                | Counter        | If the same query was executed multiple times (same times, same agent, etc) then this integer counter will increment. Use this to find out when a client may want to start caching results. |\n| numRunningQueries       | Gauge          | How many queries were executing at the time the query was made (note that for the stats page this will always be up-to-date)                                                                |\n| queryStartTimestamp     | Timestamp (ms) | The timestamp (Unix epoch in milliseconds) when the query was received and started processing.                                                                                              |\n| queryCompletedTimestamp | Timestamp (ms) | The timestamp (Unix epoch in milliseconds) when the query was finished and sent to the client.                                                                                              |\n| sentToClient            | boolean        | Whether or not the query was successfully sent to the client. It may be blocked due to a socket exception or full write buffer.                                                             |\n\n## Stats\n\nA number of statistics are available around each query and more will be added over time. Various levels of detail are measured including:\n\n- **Global** - Metrics pertaining to the entire query including max and average timings of each sub query.\n- **Per-Sub Query** - Metrics pertaining to a single sub query (if multiple are present) including max and average timings of scanner.\n- **Per-Scanner** - Metrics around each individual scanner (useful when salting is enabled)\n\nGlobal stats are printed to the standard log, stats page. The full global, sub query and scanner details are available in the query log and via the query API when `showSummary` is present. Timing stats at a lower level are aggregated into max and average values at the upper level. Counters at each lower level are also aggregated at each upper level so you'll see the same counter metrics at each level. A table of stats and sections appears below.\n\nNote\n\nAll timings in the table below are in milliseconds. Also note that times can be inflated by JVM GCs so make sure to enable GC logging if something seems off.\n\n| Metric                 | Type    | Section                | Description                                                                                                                                                                                                                                                                                                                      |\n|------------------------|---------|------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| compactionTime         | Float   | Scanner                | Cumulative time spent running each row through the compaction code to create a single column and manage duplicate values.                                                                                                                                                                                                        |\n| hbaseTime              | Float   | Scanner                | Cumulative time spent waiting on HBase to return data. (Includes AsyncHBase deserialization time).                                                                                                                                                                                                                               |\n| scannerId              | String  | Scanner                | Details about the scanner including the table, start and end keys as well as filters used.                                                                                                                                                                                                                                       |\n| scannerTime            | Float   | Scanner                | The total time from initialization of the scanner to when the scanner completed and closed.                                                                                                                                                                                                                                      |\n| scannerUidToStringTime | Float   | Scanner                | Cumulative time spent resolving UIDs from row keys to strings for use with regex and wildcard filters. If neither filter is used this value should be zero.                                                                                                                                                                      |\n| successfulScan         | Integer | Scanner, Query, Global | How many scanners completed successfully. Per query, this should be equal to the number of salting buckets, or `1` if salting is disabled.                                                                                                                                                                                       |\n| uidPairsResolved       | Integer | Scanner                | Total number of row key UIDs resolved to tag values when a regex or wildcard filter is used. If neither filter is used this value should be zero.                                                                                                                                                                                |\n| aggregationTime        | Float   | Query                  | Cumulative time spent aggregating data points including downsampling, multi-series aggregation and rate calculations.                                                                                                                                                                                                            |\n| groupByTime            | Float   | Query                  | Cumulative time spent sorting scanner results into groups for future aggregation.                                                                                                                                                                                                                                                |\n| queryScanTime          | Float   | Query                  | Total time spent waiting on the scanners to return results. This includes the `groupByTime`.                                                                                                                                                                                                                                     |\n| saltScannerMergeTime   | Float   | Query                  | Total time spent merging the salt scanner results into a single set prior to group by operations.                                                                                                                                                                                                                                |\n| serializationTime      | Float   | Query                  | Total time spent serializing the query results. This includes `aggregationTime` and `uidToStringTime`.                                                                                                                                                                                                                           |\n| uidToStringTime        | Float   | Query                  | Cumulative time spent resolving UIDs to strings for serialization.                                                                                                                                                                                                                                                               |\n| emittedDPs             | Integer | Query, Global          | The total number of data points serialized in the output. Note that this may include NaNs or Nulls if the query specified such.                                                                                                                                                                                                  |\n| queryIndex             | Integer | Query                  | The index of the sub query in the original user supplied query list.                                                                                                                                                                                                                                                             |\n| processingPreWriteTime | Float   | Global                 | Total time spent processing, fetching data and serializing results for the query until it is written over the wire. This value is sent in the API summary results and used as an estimate of the total time spent processing by the TSD. However it does not include the amount of time it took to send the value over the wire. |\n| totalTime              | Float   | Global                 | Total time spent on the query including writing to the socket. This is only found in the log files and stats API.                                                                                                                                                                                                                |\n\n### Example\n\n``` javascript\n{\n    \"statsSummary\": {\n        \"avgAggregationTime\": 3.784976,\n        \"avgHBaseTime\": 8.530751,\n        \"avgQueryScanTime\": 10.964149,\n        \"avgScannerTime\": 8.588306,\n        \"avgScannerUidToStringTime\": 0.0,\n        \"avgSerializationTime\": 3.809661,\n        \"emittedDPs\": 1256,\n        \"maxAggregationTime\": 3.759478,\n        \"maxHBaseTime\": 9.904215,\n        \"maxQueryScanTime\": 10.320964,\n        \"maxScannerUidtoStringTime\": 0.0,\n        \"maxSerializationTime\": 3.779712,\n        \"maxUidToStringTime\": 0.197926,\n        \"processingPreWriteTime\": 20.170205,\n        \"queryIdx_00\": {\n            \"aggregationTime\": 3.784976,\n            \"avgHBaseTime\": 8.849337,\n            \"avgScannerTime\": 8.908597,\n            \"avgScannerUidToStringTime\": 0.0,\n            \"emittedDPs\": 628,\n            \"groupByTime\": 0.0,\n            \"maxHBaseTime\": 9.904215,\n            \"maxScannerUidtoStringTime\": 0.0,\n            \"queryIndex\": 0,\n            \"queryScanTime\": 10.964149,\n            \"saltScannerMergeTime\": 0.128234,\n            \"scannerStats\": {\n                \"scannerIdx_00\": {\n                    \"compactionTime\": 0.048703,\n                    \"hbaseTime\": 8.844783,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[0, 0, 2, 88, 86, -63, -25, -16], stop_key=[0, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.899045,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_01\": {\n                    \"compactionTime\": 0.066892,\n                    \"hbaseTime\": 8.240165,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[1, 0, 2, 88, 86, -63, -25, -16], stop_key=[1, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.314855,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_02\": {\n                    \"compactionTime\": 0.01298,\n                    \"hbaseTime\": 8.462203,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[2, 0, 2, 88, 86, -63, -25, -16], stop_key=[2, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.478315,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_03\": {\n                    \"compactionTime\": 0.036998,\n                    \"hbaseTime\": 9.862741,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[3, 0, 2, 88, 86, -63, -25, -16], stop_key=[3, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 9.904215,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_04\": {\n                    \"compactionTime\": 0.058698,\n                    \"hbaseTime\": 9.523481,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[4, 0, 2, 88, 86, -63, -25, -16], stop_key=[4, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 9.587324,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_05\": {\n                    \"compactionTime\": 0.041017,\n                    \"hbaseTime\": 9.757787,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[5, 0, 2, 88, 86, -63, -25, -16], stop_key=[5, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 9.802395,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_06\": {\n                    \"compactionTime\": 0.062371,\n                    \"hbaseTime\": 9.332585,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[6, 0, 2, 88, 86, -63, -25, -16], stop_key=[6, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 9.40264,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_07\": {\n                    \"compactionTime\": 0.063974,\n                    \"hbaseTime\": 8.195105,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[7, 0, 2, 88, 86, -63, -25, -16], stop_key=[7, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.265713,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_08\": {\n                    \"compactionTime\": 0.062196,\n                    \"hbaseTime\": 8.21871,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[8, 0, 2, 88, 86, -63, -25, -16], stop_key=[8, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.287582,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_09\": {\n                    \"compactionTime\": 0.051666,\n                    \"hbaseTime\": 7.790636,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[9, 0, 2, 88, 86, -63, -25, -16], stop_key=[9, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 7.849597,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_10\": {\n                    \"compactionTime\": 0.036429,\n                    \"hbaseTime\": 7.6472,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[10, 0, 2, 88, 86, -63, -25, -16], stop_key=[10, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 7.689386,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_11\": {\n                    \"compactionTime\": 0.044493,\n                    \"hbaseTime\": 7.897932,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[11, 0, 2, 88, 86, -63, -25, -16], stop_key=[11, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 7.94793,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_12\": {\n                    \"compactionTime\": 0.025362,\n                    \"hbaseTime\": 9.30409,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[12, 0, 2, 88, 86, -63, -25, -16], stop_key=[12, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 9.332411,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_13\": {\n                    \"compactionTime\": 0.057429,\n                    \"hbaseTime\": 9.215958,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[13, 0, 2, 88, 86, -63, -25, -16], stop_key=[13, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 9.278104,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_14\": {\n                    \"compactionTime\": 0.102855,\n                    \"hbaseTime\": 9.598685,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[14, 0, 2, 88, 86, -63, -25, -16], stop_key=[14, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 9.712258,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_15\": {\n                    \"compactionTime\": 0.0727,\n                    \"hbaseTime\": 9.273193,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[15, 0, 2, 88, 86, -63, -25, -16], stop_key=[15, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 9.35403,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_16\": {\n                    \"compactionTime\": 0.025867,\n                    \"hbaseTime\": 9.011146,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[16, 0, 2, 88, 86, -63, -25, -16], stop_key=[16, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 9.039663,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_17\": {\n                    \"compactionTime\": 0.066071,\n                    \"hbaseTime\": 9.175692,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[17, 0, 2, 88, 86, -63, -25, -16], stop_key=[17, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 9.24738,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_18\": {\n                    \"compactionTime\": 0.090249,\n                    \"hbaseTime\": 8.730833,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[18, 0, 2, 88, 86, -63, -25, -16], stop_key=[18, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.831461,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_19\": {\n                    \"compactionTime\": 0.039327,\n                    \"hbaseTime\": 8.903825,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[19, 0, 2, 88, 86, -63, -25, -16], stop_key=[19, 0, 2, 88, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.947639,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                }\n            },\n            \"serializationTime\": 3.809661,\n            \"successfulScan\": 20,\n            \"uidPairsResolved\": 0,\n            \"uidToStringTime\": 0.197926\n        },\n        \"queryIdx_01\": {\n            \"aggregationTime\": 3.73398,\n            \"avgHBaseTime\": 8.212164,\n            \"avgScannerTime\": 8.268015,\n            \"avgScannerUidToStringTime\": 0.0,\n            \"emittedDPs\": 628,\n            \"groupByTime\": 0.0,\n            \"maxHBaseTime\": 8.986041,\n            \"maxScannerUidtoStringTime\": 0.0,\n            \"queryIndex\": 1,\n            \"queryScanTime\": 9.67778,\n            \"saltScannerMergeTime\": 0.095797,\n            \"scannerStats\": {\n                \"scannerIdx_00\": {\n                    \"compactionTime\": 0.054894,\n                    \"hbaseTime\": 8.708179,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[0, 0, 2, 76, 86, -63, -25, -16], stop_key=[0, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.770252,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_01\": {\n                    \"compactionTime\": 0.055956,\n                    \"hbaseTime\": 8.666615,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[1, 0, 2, 76, 86, -63, -25, -16], stop_key=[1, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.730629,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_02\": {\n                    \"compactionTime\": 0.011224,\n                    \"hbaseTime\": 8.474637,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[2, 0, 2, 76, 86, -63, -25, -16], stop_key=[2, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.487582,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_03\": {\n                    \"compactionTime\": 0.081926,\n                    \"hbaseTime\": 8.894951,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[3, 0, 2, 76, 86, -63, -25, -16], stop_key=[3, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.986041,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_04\": {\n                    \"compactionTime\": 0.01882,\n                    \"hbaseTime\": 8.209866,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[4, 0, 2, 76, 86, -63, -25, -16], stop_key=[4, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.231502,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_05\": {\n                    \"compactionTime\": 0.056902,\n                    \"hbaseTime\": 8.709846,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[5, 0, 2, 76, 86, -63, -25, -16], stop_key=[5, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.772216,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_06\": {\n                    \"compactionTime\": 0.131424,\n                    \"hbaseTime\": 8.033916,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[6, 0, 2, 76, 86, -63, -25, -16], stop_key=[6, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.181117,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_07\": {\n                    \"compactionTime\": 0.022517,\n                    \"hbaseTime\": 8.006976,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[7, 0, 2, 76, 86, -63, -25, -16], stop_key=[7, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.032073,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_08\": {\n                    \"compactionTime\": 0.011527,\n                    \"hbaseTime\": 8.591358,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[8, 0, 2, 76, 86, -63, -25, -16], stop_key=[8, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.604491,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_09\": {\n                    \"compactionTime\": 0.162222,\n                    \"hbaseTime\": 8.25452,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[9, 0, 2, 76, 86, -63, -25, -16], stop_key=[9, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.435525,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_10\": {\n                    \"compactionTime\": 0.033886,\n                    \"hbaseTime\": 7.973254,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[10, 0, 2, 76, 86, -63, -25, -16], stop_key=[10, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.011236,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_11\": {\n                    \"compactionTime\": 0.039491,\n                    \"hbaseTime\": 7.959601,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[11, 0, 2, 76, 86, -63, -25, -16], stop_key=[11, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.003249,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_12\": {\n                    \"compactionTime\": 0.107793,\n                    \"hbaseTime\": 8.177353,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[12, 0, 2, 76, 86, -63, -25, -16], stop_key=[12, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.298284,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_13\": {\n                    \"compactionTime\": 0.020697,\n                    \"hbaseTime\": 8.124243,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[13, 0, 2, 76, 86, -63, -25, -16], stop_key=[13, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.147879,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_14\": {\n                    \"compactionTime\": 0.033261,\n                    \"hbaseTime\": 8.145149,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[14, 0, 2, 76, 86, -63, -25, -16], stop_key=[14, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.182331,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_15\": {\n                    \"compactionTime\": 0.057804,\n                    \"hbaseTime\": 8.17854,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[15, 0, 2, 76, 86, -63, -25, -16], stop_key=[15, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.243458,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_16\": {\n                    \"compactionTime\": 0.01212,\n                    \"hbaseTime\": 8.070582,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[16, 0, 2, 76, 86, -63, -25, -16], stop_key=[16, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 8.084813,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_17\": {\n                    \"compactionTime\": 0.036777,\n                    \"hbaseTime\": 7.919167,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[17, 0, 2, 76, 86, -63, -25, -16], stop_key=[17, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 7.959645,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_18\": {\n                    \"compactionTime\": 0.048097,\n                    \"hbaseTime\": 7.87351,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[18, 0, 2, 76, 86, -63, -25, -16], stop_key=[18, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 7.926318,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                },\n                \"scannerIdx_19\": {\n                    \"compactionTime\": 0.0,\n                    \"hbaseTime\": 7.271033,\n                    \"scannerId\": \"Scanner(table=\\\"tsdb\\\", start_key=[19, 0, 2, 76, 86, -63, -25, -16], stop_key=[19, 0, 2, 76, 86, -62, 4, 16], columns={\\\"t\\\"}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\\\"(?s)^.{8}(?:.{7})*\\\\Q\\u0000\\u0000\\u0005\\\\E(?:\\\\Q\\u0000\\u0000\\u00006\\\\E)(?:.{7})*$\\\", ISO-8859-1), scanner_id=0x0000000000000000)\",\n                    \"scannerTime\": 7.271664,\n                    \"scannerUidToStringTime\": 0.0,\n                    \"successfulScan\": 1,\n                    \"uidPairsResolved\": 0\n                }\n            },\n            \"serializationTime\": 3.749764,\n            \"successfulScan\": 20,\n            \"uidPairsResolved\": 0,\n            \"uidToStringTime\": 0.162088\n        },\n        \"successfulScan\": 40,\n        \"uidPairsResolved\": 0\n    }\n}\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/query/stats.html](http://opentsdb.net/docs/build/html/user_guide/query/stats.html)"
- name: Query Examples
  id: user_guide/query/examples
  summary: The following is a list of example queries using an example data set
  description: "# Query Examples\n\nThe following is a list of example queries using an example data set. We'll illustrate a number of common query types that may be encountered so you can get an understanding of how the query system works. Each time series in the example set has only a single data point stored and the UIDs have been truncated to a single byte to make it easier to read. The example queries are all *Metric* queries from the HTTP API and only show the `m=` component. See [*/api/query*](../../api_http/query/index) for details. If you are using a CLI tool, the query format will differ slightly so read the documentation for the particular command.\n\n## Sample Data\n\n**Time Series**\n\n| TS# | Metric         | Tags                  | TSUID      |\n|-----|----------------|-----------------------|------------|\n| 1   | sys.cpu.system | dc=dal host=web01     | 0102040101 |\n| 2   | sys.cpu.system | dc=dal host=web02     | 0102040102 |\n| 3   | sys.cpu.system | dc=dal host=web03     | 0102040103 |\n| 4   | sys.cpu.system | host=web01            | 010101     |\n| 5   | sys.cpu.system | host=web01 owner=jdoe | 0101010306 |\n| 6   | sys.cpu.system | dc=lax host=web01     | 0102050101 |\n| 7   | sys.cpu.system | dc=lax host=web02     | 0102050102 |\n| 8   | sys.cpu.user   | dc=dal host=web01     | 0202040101 |\n| 9   | sys.cpu.user   | dc=dal host=web02     | 0202040102 |\n\n**UIDs**\n\n| Name        | UID |\n|-------------|-----|\n| **Metrics** |     |\n| cpu.system  | 01  |\n| cpu.user    | 02  |\n| **Tagks**   |     |\n| host        | 01  |\n| dc          | 02  |\n| owner       | 03  |\n| **Tagvs**   |     |\n| web01       | 01  |\n| web02       | 02  |\n| web03       | 03  |\n| dal         | 04  |\n| lax         | 05  |\n| doe         | 06  |\n\nWarning\n\nThis isn't necesarily the best way to setup your metrics and tags, rather it's meant to be illustrative of how the query system works. In particular, TS \\#4 and 5, while legitimate timeseries, may screw up your queries unless you know how they work. In general, try to maintain the same number and type of tags for each timeseries.\n\n## Under the Hood\n\nYou may want to read up on how OpenTSDB stores timeseries data here: [*Storage*](../backends/index). Otherwise, remember that each row in storage has a unique key formatted:\n\n``` python\n<metricID><normalized_timestamp><tagkID1><tagvID1>[...<tagkIDN><tagvIDN>]\n```\n\nThe data table above would be stored as:\n\n``` python\n01<ts>0101\n01<ts>01010306\n01<ts>02040101\n01<ts>02040102\n01<ts>02040103\n01<ts>02050101\n01<ts>02050102\n02<ts>02040101\n02<ts>02040102\n```\n\nWhen you query OpenTSDB, here's what happens under the hood.\n\n- The query is parsed and verified to make sure that the format is correct and that the metrics, tag names and tag values exist. If a single metric, tag name or value doesn't exist in the system, it will kick back an error.\n- Then it sets up a scanner for the underlying storage system.\n  - If the query doesn't have any tags or tag values, then it will grab any rows of data that match `<metricID><timestamp>`, so if you have a ton of time series for a particular metric, this could be many, many rows.\n  - If the query does have one or more tags defined, then it will still scan all of the rows matching `<metricID><timestamp>`, but also perform a regex to return only the rows that contain the requested tag.\n- Once all of the data has been returned, OpenTSDB organizes it into groups, if required\n- If downsampling was requested, each individual time series is down sampled into smaller time spans using the proper aggregator\n- Then each group of data is aggregated using the specific aggregation function\n- If the `rate` flag was detected, each aggregate will then be adjusted to get the rate.\n- Results are returned to the caller\n\n## Query 1 - All Time Series for a Metric\n\n``` python\nm=sum:cpu.system\n```\n\nThis is the simplest query to make. TSDB will setup a scanner to fetch all data points for the metric UID `01` between *\\<start\\>* and *\\<end\\>*. The result will be the a single dataset with time series \\#1 through \\#7 summed together. If you have thousands of unique tag combinations for a given metric, they will all be added together into one series.\n\n``` javascript\n[\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {},\n    \"aggregated_tags\": [\n      \"host\"\n    ],\n    \"tsuids\": [\n      \"010101\",\n      \"0101010306\",\n      \"0102050101\",\n      \"0102040101\",\n      \"0102040102\",\n      \"0102040103\",\n      \"0102050102\"\n    ],\n    \"dps\": {\n      \"1346846400\": 130.29999923706055\n    }\n  }\n]\n```\n\n## Query 2 - Filter on a Tag\n\nUsually aggregating all of the time series for a metric isn't particularly useful. Instead we can drill down a little by filtering for time series that contain a specific tagk/tagv pair combination. Simply put the pair in curly braces:\n\n``` python\nm=sum:cpu.system{host=web01}\n```\n\nThis will return an aggregate of time series \\#1, \\#4, \\#5 and \\#6 since they're the only series that include `host=web01`.\n\n``` javascript\n[\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"host\": \"web01\"\n    },\n    \"aggregated_tags\": [],\n    \"tsuids\": [\n      \"010101\",\n      \"0101010306\",\n      \"0102040101\",\n      \"0102050101\"\n    ],\n    \"dps\": {\n      \"1346846400\": 63.59999942779541\n    }\n  }\n]\n```\n\n## Query 3 - Specific Time Series\n\nWhat if you want a specific timeseries? You have to include every tag and coresponding value.\n\n``` python\nm=sum:cpu.system{host=web01,dc=lax}\n```\n\nThis will return the data from timeseries \\#6 only.\n\n``` javascript\n[\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"dc\": \"lax\",\n      \"host\": \"web01\"\n    },\n    \"aggregated_tags\": [],\n    \"tsuids\": [\n      \"0102050101\"\n    ],\n    \"dps\": {\n      \"1346846400\": 15.199999809265137\n    }\n  }\n]\n```\n\nWarning\n\nThis is where a tagging scheme will stand or fall. Let's say you want to get just the data from timeseries \\#4. With the current system, you are unable to. You would send in query \\#2 `m=sum:cpu.system{host=web01}` thinking that it will return just the data from \\#4, but as we saw, you'll get the aggregate results for \\#1, \\#4, \\#5 and \\#6. To prevent such an occurance, you would need to add another tag to \\#4 that differentiates it from other timeseries in the group. Or if you've already commited, you can use TSUID queries.\n\n## Query 4 - TSUID Query\n\nIf you know the exact TSUID of the timeseries that you want to retrieve, you can simply pass it in like so:\n\n``` python\ntsuids=sum:0102040102\n```\n\nThe results will be the data points that you requested.\n\n``` javascript\n[\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"dc\": \"lax\",\n      \"host\": \"web01\"\n    },\n    \"aggregated_tags\": [],\n    \"tsuids\": [\n      \"0102050101\"\n    ],\n    \"dps\": {\n      \"1346846400\": 15.199999809265137\n    }\n  }\n]\n```\n\n## Query 5 - Multi-TSUID Query\n\nYou can also aggregate multiple TSUIDs in the same query, provided they share the same metric. If you attempt to aggregate multiple metrics, the API will issue an error.\n\n``` python\ntsuids=sum:0102040101,0102050101\n```\n\n``` javascript\n[\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"host\": \"web01\"\n    },\n    \"aggregated_tags\": [\n      \"dc\"\n    ],\n    \"tsuids\": [\n      \"0102040101\",\n      \"0102050101\"\n    ],\n    \"dps\": {\n      \"1346846400\": 33.19999980926514\n    }\n  }\n]\n```\n\n## Query 6 - Grouping\n\n``` python\nm=sum:cpu.system{host=*}\n```\n\nThe `*` (asterisk) is a grouping operator that will return a data set for each unique value of the tag name given. Every timeseries that includes the given metric and the given tag name, regardless of other tags or values, will be included in the results. After the individual timeseries results are grouped, they'll be aggregated and returned.\n\nIn this example, we will have 3 groups returned:\n\n| Group | Time Series Included |\n|-------|----------------------|\n| web01 | \\#1, \\#4, \\#5, \\#6   |\n| web02 | \\#2, \\#7             |\n| web03 | \\#3                  |\n\nTSDB found 7 total timeseries that included the \"host\" tag. There were 3 unique values for that tag (web01, web02, and web03).\n\n``` javascript\n[\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"host\": \"web01\"\n    },\n    \"aggregated_tags\": [],\n    \"tsuids\": [\n      \"010101\",\n      \"0101010306\",\n      \"0102040101\",\n      \"0102050101\"\n    ],\n    \"dps\": {\n      \"1346846400\": 63.59999942779541\n    }\n  },\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"host\": \"web02\"\n    },\n    \"aggregated_tags\": [\n      \"dc\"\n    ],\n    \"tsuids\": [\n      \"0102040102\",\n      \"0102050102\"\n    ],\n    \"dps\": {\n      \"1346846400\": 24.199999809265137\n    }\n  },\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"dc\": \"dal\",\n      \"host\": \"web03\"\n    },\n    \"aggregated_tags\": [],\n    \"tsuids\": [\n      \"0102040103\"\n    ],\n    \"dps\": {\n      \"1346846400\": 42.5\n    }\n  }\n]\n```\n\n## Query 7 - Group and Filter\n\nNote that the in example \\#2, the `web01` group included the odd-ball timeseries \\#4 and \\#5. We can filter those out by specifying a second tag ala:\n\n``` python\nm=sum:cpu.nice{host=*,dc=dal}\n```\n\nNow we'll only get results for \\#1 - \\#3, but we lose the `dc=lax` values.\n\n``` javascript\n[\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"dc\": \"dal\",\n      \"host\": \"web01\"\n    },\n    \"aggregated_tags\": [],\n    \"tsuids\": [\n      \"0102040101\"\n    ],\n    \"dps\": {\n      \"1346846400\": 18\n    }\n  },\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"dc\": \"dal\",\n      \"host\": \"web02\"\n    },\n    \"aggregated_tags\": [],\n    \"tsuids\": [\n      \"0102040102\"\n    ],\n    \"dps\": {\n      \"1346846400\": 9\n    }\n  },\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"dc\": \"dal\",\n      \"host\": \"web03\"\n    },\n    \"aggregated_tags\": [],\n    \"tsuids\": [\n      \"0102040103\"\n    ],\n    \"dps\": {\n      \"1346846400\": 42.5\n    }\n  }\n]\n```\n\n## Query 8 - Grouping With OR\n\nThe `*` operator is greedy and will return *all* values that are assigned to a tag name. If you only want a few tag values, you can use the `|` (pipe) operator instead.\n\n``` python\nm=sum:cpu.nice{host=web01|web02}\n```\n\nThis will find all of the timeseries that include \"host\" values for \"web01\" OR \"web02\", then group them by value, similar to the `*` operator. Our groups, this time, will look like this:\n\n| Group | Time Series Included |\n|-------|----------------------|\n| web01 | \\#1, \\#4, \\#5, \\#6   |\n| web02 | \\#2, \\#7             |\n\n``` javascript\n[\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"host\": \"web01\"\n    },\n    \"aggregated_tags\": [],\n    \"tsuids\": [\n      \"010101\",\n      \"0101010306\",\n      \"0102040101\",\n      \"0102050101\"\n    ],\n    \"dps\": {\n      \"1346846400\": 63.59999942779541\n    }\n  },\n  {\n    \"metric\": \"cpu.system\",\n    \"tags\": {\n      \"host\": \"web02\"\n    },\n    \"aggregated_tags\": [\n      \"dc\"\n    ],\n    \"tsuids\": [\n      \"0102040102\",\n      \"0102050102\"\n    ],\n    \"dps\": {\n      \"1346846400\": 24.199999809265137\n    }\n  }\n]\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/query/examples.html](http://opentsdb.net/docs/build/html/user_guide/query/examples.html)"
- name: Querying or Reading Data
  id: user_guide/query/index
  summary: OpenTSDB offers a number of means to extract data such as CLI tools, an HTTP API and as a GnuPlot graph
  description: "# Querying or Reading Data\n\nOpenTSDB offers a number of means to extract data such as CLI tools, an HTTP API and as a GnuPlot graph. Querying with OpenTSDB's tag based system can be a bit tricky so read through this document and checkout the following pages for deeper information. Example queries on this page follow the HTTP API format.\n\n- [Dates and Times](dates)\n- [Filters](filters)\n- [Understanding Metrics and Time Series](timeseries)\n- [Aggregators](aggregators)\n- [Query Examples](examples)\n- [Query Details and Stats](stats)\n\n## Query Components\n\nOpenTSDB's query language is fairly simple but flexible. Each query has the following components:\n\n| Parameter            | Date Type         | Required | Description                                                                                                                                       | Example        |\n|----------------------|-------------------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------|----------------|\n| Start Time           | String or Integer | Yes      | Starting time for the query. This may be an absolute or relative time. See [*Dates and Times*](dates) for details                                 | 24h-ago        |\n| End Time             | String or Integer | No       | An end time for the query. If the end time is not supplied, the current time on the TSD will be used. See [*Dates and Times*](dates) for details. | 1h-ago         |\n| Metric               | String            | Yes      | The full name of a metric in the system. Must be the complete name. Case sensitive                                                                | sys.cpu.user   |\n| Aggregation Function | String            | Yes      | A mathematical function to use in combining multiple time series                                                                                  | sum            |\n| Tags                 | String            | No       | An optional set of tags for filtering or grouping                                                                                                 | host=\\*,dc=lax |\n| Downsampler          | String            | No       | An optional interval and function to reduce the number of data points returned                                                                    | 1h-avg         |\n| Rate                 | String            | No       | An optional flag to calculate the rate of change for the result                                                                                   | rate           |\n\n## Times\n\nAbsolute time stamps are supported in human readable format or Unix style integers. Relative times may be used for refreshing dashboards. Currently, all queries are able to cover a single time span. In the future we hope to provide an offset query parameter that would allow for aggregations or graphing of a metric over different time periods, such as comparing last week to 1 year ago. See [*Dates and Times*](dates) for details on what is permissible.\n\nWhile OpenTSDB can store data with millisecond resolution, most queries will return the data with second resolution to provide backwards compatibility for existing tools. Unless a down sampling algorithm has been specified with a query, the data will automatically be down sampled to 1 second using the same aggregation function specified in a query. This way, if multiple data points are stored for a given second, they will be aggregated and returned in a normal query correctly.\n\nTo extract data with millisecond resolution, use the `/api/query` endpoint and specify the `msResolution` (ms\\` is also okay, but not recommended) JSON parameter or query string flag and it will bypass down sampling (unless specified) and return all timestamps in Unix epoch millisecond resolution. Also, the `scan` commandline utility will return the timestamp as written in storage.\n\n## Tags\n\nEvery time series is comprised of a metric and one or more tag name/value pairs. Since tags are optional in queries, if you request only the metric name, then every metric with any number or value of tags will be returned in the aggregated results. For example, if we have a stored data set:\n\n``` python\nsys.cpu.user host=webserver01,cpu=0  1356998400  1\nsys.cpu.user host=webserver01,cpu=1  1356998400  4\nsys.cpu.user host=webserver02,cpu=0  1356998400  2\nsys.cpu.user host=webserver02,cpu=1  1356998400  1\n```\n\nand simply craft a query `start=1356998400&m=sum:sys.cpu.user`, we will get a value of `8` at `1356998400` that incorporates all 4 time series.\n\nIf we want to aggregate the results for a specific group, we can filter on the `host` tag. The query `start=1356998400&m=sum:sys.cpu.user{host=webserver01}` will return a value of `5`, incorporating only the time series where `host=webserver01`. To drill down to a specific time series, you must include all of the tags for the series, e.g. `start=1356998400&m=sum:sys.cpu.user{host=webserver01,cpu=0}` will return `1`.\n\nNote\n\nInconsistent tags can cause unexpected results when querying. See `../writing` for details.\n\n## Grouping\n\nA query can also aggregate time series with multiple tags into groups based on a tag value. Two special characters can be passed to the right of the equals symbol in a query:\n\n- **\\*** - The asterisk will return a separate result for each unique tag value\n- **\\|** - The pipe will return a separate result *only* for the exact tag values specified\n\nLet's take the following data set as an example:\n\n``` python\nsys.cpu.user host=webserver01,cpu=0  1356998400  1\nsys.cpu.user host=webserver01,cpu=1  1356998400  4\nsys.cpu.user host=webserver02,cpu=0  1356998400  2\nsys.cpu.user host=webserver02,cpu=1  1356998400  1\nsys.cpu.user host=webserver03,cpu=0  1356998400  5\nsys.cpu.user host=webserver03,cpu=1  1356998400  3\n```\n\nIf we want to query for the average CPU time across each server we can craft a query like `start=1356998400&m=avg:sys.cpu.user{host=*}`. This will give us three results:\n\n1.  The aggregated average for `sys.cpu.user`` ``host=webserver01,cpu=0` and `sys.cpu.user`` ``host=webserver01,cpu=1`\n2.  The aggregated average for `sys.cpu.user`` ``host=webserver02,cpu=0` and `sys.cpu.user`` ``host=webserver02,cpu=1`\n3.  The aggregated average for `sys.cpu.user`` ``host=webserver03,cpu=0` and `sys.cpu.user`` ``host=webserver03,cpu=1`\n\nHowever if we have many web servers in the system, this could create a ton of results. To filter on only the hosts we want you can use the pipe operator to select a subset of time series. For example `start=1356998400&m=avg:sys.cpu.user{host=webserver01|webserver03}` will return results only for `webserver01` and `webserver03`.\n\nWith version 2.2 you can enable or disable grouping per tag filter. Additional filters are also available including wildcards and regular expressions.\n\n## Explicit Tags\n\nAs of 2.3 and later, if you know all of the tag keys for a given metric query latency can be improved greatly by using the `explicitTags` feature and making sure `tsd.query.enable_fuzzy_filter` is enabled in the config. A special filter is given to HBase that enables skipping ahead to rows that we need for the query instead of iterating over every row key and comparing a regular expression.\n\nFor example, using the data set above, if we only care about metrics where `host=webserver02` and there are hundreds of hosts, you can craft a query such as `start=1356998400&m=avg:explicit_tags:sys.cpu.user{host=webserver02,cpu=*}`. Note that you must specify every tag included in the time series for this to work and you can decide whether or not to group by the additional tags.\n\n## Aggregation\n\nA powerful feature of OpenTSDB is the ability to perform on-the-fly aggregations of multiple time series into a single set of data points. The original data is always available in storage but we can quickly extract the data in meaningful ways. Aggregation functions are means of merging two or more data points for a single time stamp into a single value. See [*Aggregators*](aggregators) for details.\n\n## Interpolation\n\nWhen performing an aggregation, what happens if the time stamps of the data points for each time series fail to line up? Say we record the temperature every 5 minutes in different regions around the world. A sensor in Paris may send a temperature of `27c` at `1356998400`. Then a sensor in San Francisco may send a value of `18c` at `1356998430`, 30 seconds later. Antarctica may report `-29c` at `1356998529`. If we run a query requesting the average temperature, we want all of the data points averaged together into a single point. This is where **interpolation** comes into play. See [*Aggregators*](aggregators) for details.\n\n## Downsampling\n\nOpenTSDB can ingest a large amount of data, even a data point every second for a given time series. Thus queries may return a large number of data points. Accessing the results of a query with a large number of points from the API can eat up bandwidth. High frequencies of data can easily overwhelm Javascript graphing libraries, hence the choice to use GnuPlot. Graphs created by the GUI can be difficult to read, resulting in thick lines such as the graph below:\n\nDown sampling can be used at query time to reduce the number of data points returned so that you can extract better information from a graph or pass less data over a connection. Down sampling requires an **aggregation** function and a **time interval**. The aggregation function is used to compute a new data point across all of the data points in the specified interval with the proper mathematical function. For example, if the aggregation `sum` is used, then all of the data points within the interval will be summed together into a single value. If `avg` is chosen, then the average of all data points within the interval will be returned.\n\nIntervals are specified by a number and a unit of time. For example, `30m` will aggregate data points every 30 minutes. `1h` will aggregate across an hour. See [*Dates and Times*](dates) for valid relative time units. Do not add the `-ago` to a down sampling query.\n\nUsing down sampling we can cleanup the previous graph to arrive at something much more useful:\n\nAs of 2.1, downsampled timestamps are normalized based on the remainder of the original data point timestamp divided by the downsampling interval in milliseconds, i.e. the modulus. In Java the code is `timestamp`` ``-`` ``(timestamp`` ``%`` ``interval_ms)`. For example, given a timestamp of `1388550980000`, or `1/1/2014`` ``04:36:20`` ``UTC` and an hourly interval that equates to 3600000 milliseconds, the resulting timestamp will be rounded to `1388548800000`. All data points between 4 and 5 UTC will wind up in the 4 AM bucket. If you query for a day's worth of data downsampling on 1 hour, you will receive 24 data points (assuming there is data for all 24 hours).\n\nNormalization works very well for common queries such as a day's worth of data downsampled to 1 minute or 1 hour. However if you try to downsample on an odd interval, such as 36 minutes, then the timestamps may look a little strange due to the nature of the modulus calculation. Given an interval of 36 minutes and our example above, the interval would be `2160000` milliseconds and the resulting timestamp `1388549520` or `04:12:00`` ``UTC`. All data points between `04:12` and `04:48` would wind up in a single bucket. Also note that OpenTSDB cannot currently normalize on non-UTC times and it cannot normalize on weekly or monthly boundaries.\n\nWith version 2.2 a downsampling query can emit a `NaN` or `null` when a downsample bucket is missing a value for all of the series involved. Because OpenTSDB does not allow for storing literal NaNs at this time, nor does it impose specific intervals on storage, this can be used to mimic systems that do such as RRDs.\n\nNote\n\nPrevious to 2.1, timestamps were not normalized. The buckets were calculated based on the starting time of the first data point retreived for each series, then the series went through interpolation. This means a graph may show varying gaps between values and return more values than expected.\n\n## Rate\n\nA number of data sources return values as constantly incrementing counters. One example is a web site hit counter. When you start a web server, it may have a hit counter of 0. After five minutes the value may be 1,024. After another five minutes it may be 2,048. The graph for a counter will be a somewhat straight line angling up to the right and isn't always very useful. OpenTSDB provides the **rate** key word that calculates the rate of change in values over time. This will transform counters into lines with spikes to show you when activity occurred and can be much more useful.\n\nThe rate is the first derivative of the values. It's defined as (v2 - v1) / (t2 - t1). Therefore you will get the rate of change per second. Currently the rate of change between millisecond values defaults to a per second calculation.\n\nOpenTSDB 2.0 provides support for special monotonically increasing counter data handling including the ability to set a \"rollover\" value and suppress anomalous fluctuations. When the `counterMax` value is specified in a query, if a data point approaches this value and the point after is less than the previous, the max value will be used to calculate an accurate rate given the two points. For example, if we were recording an integer counter on 2 bytes, the maximum value would be 65,535. If the value at `t0` is `64000` and the value at `t1` is `1000`, the resulting rate per second would be calculated as `-63000`. However we know that it's likely the counter rolled over so we can set the max to `65535` and now the calculation will be `65535`` ``-`` ``t0`` ``+`` ``t1` to give us `2535`.\n\nSystems that track data in counters often revert to 0 when restarted. When that happens and we could get a spurious result when using the max counter feature. For example, if the counter has reached `2000` at `t0` and someone reboots the server, the next value may be `500` at `t1`. If we set our max to `65535` the result would be `65535`` ``-`` ``2000`` ``+`` ``500` to give us `64035`. If the normal rate is a few points per second, this particular spike, with `30s` between points, would create a rate spike of `2,134.5`! To avoid this, we can set the `resetValue` which will, when the rate exceeds this value, return a data point of `0` so as to avoid spikes in either direction. For the example above, if we know that our rate almost never exceeds 100, we could configure a `resetValue` of `100` and when the data point above is calculated, it will return `0` instead of `2,134.5`. The default value of 0 means the reset value will be ignored, no rates will be suppressed.\n\n## Order of operations\n\nUnderstanding the order of operations is important. When returning query results the following is the order in which processing takes place:\n\n1.  Grouping\n2.  Down Sampling\n3.  Interpolation\n4.  Aggregation\n5.  Rate Calculation\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/query/index.html](http://opentsdb.net/docs/build/html/user_guide/query/index.html)"
- name: Quick Start
  id: user_guide/quickstart
  summary: Once you have a TSD up and running (after following the Installation guide) you can follow the steps below to get some data into OpenTSDB
  description: "# Quick Start\n\nOnce you have a TSD up and running (after following the [*Installation*](../installation) guide) you can follow the steps below to get some data into OpenTSDB. After you have some data stored, pull up the GUI and try generating some graphs.\n\n## Create Your First Metrics\n\nMetrics need to be registered before you can start storing data points for them. This helps to avoid ingesting unwanted data and catch typos. You can enable auto-metric creation via configuration. To register one or more metrics, call the `mkmetric` CLI:\n\n``` python\n./tsdb mkmetric mysql.bytes_received mysql.bytes_sent\n```\n\nThis will create 2 metrics: `mysql.bytes_received` and `mysql.bytes_sent`\n\nNew tags, on the other hand, are automatically registered whenever they're used for the first time. Right now OpenTSDB only allows you to have up to 2^24 = 16,777,216 different metrics, 16,777,216 different tag names and 16,777,216 different tag values. This is because each one of those is assigned a UID on 3 bytes. Metric names, tag names and tag values have their own UID spaces, which is why you can have 16,777,216 of each kind. The size of each space is configurable but there is no knob that exposes this configuration parameter right now. So bear in mind that using user ID or event ID as a tag value will not work right now if you have a large site.\n\n## Start Collecting Data\n\nSo now that we have our 2 metrics, we can start sending data to the TSD. Let's write a little shell script to collect some data off of MySQL and send it to the TSD:\n\n``` python\ncat >mysql-collector.sh <<\\EOF\n#!/bin/bash\nset -e\nwhile true; do\n  mysql -u USER -pPASS --batch -N --execute \"SHOW STATUS LIKE 'bytes%'\" \\\n  | awk -F\"\\t\" -v now=`date +%s` -v host=`hostname` \\\n  '{ print \"put mysql.\" tolower($1) \" \" now \" \" $2 \" host=\" host }'\n  sleep 15\ndone | nc -w 30 host.name.of.tsd PORT\nEOF\nchmod +x mysql-collector.sh\nnohup ./mysql-collector.sh &\n```\n\nEvery 15 seconds, the script will collect 2 data points from MySQL and send them to the TSD. You can use a smaller sleep interval for greater granularity.\n\nWhat does the script do? If you're not a big fan of shell and awk scripting, it may not be obvious how this works. But it's simple. The `set`` ``-e` command simply instructs bash to exit with an error if any of the commands fail. This simplifies error handling. The script then enters an infinite loop. In this loop, we query MySQL to retrieve 2 of its status variables:\n\n``` python\n$ mysql -u USER -pPASS --execute \"SHOW STATUS LIKE 'bytes%'\"\n+----------------+-------+\n| Variable_name  | Value |\n+----------------+-------+\n| Bytes_received | 133   |\n| Bytes_sent   | 190   |\n+----------------+-------+\n```\n\nThe `--batch`` ``-N` flags ask the mysql command to remove the human friendly fluff so we don't have to filter it out ourselves. Then the output is piped to awk, which is told to split fields on tabs `-F\"\\t\"` because with the `--batch` flag that's what mysql will use. We also create a couple of variables, one named `` now` ``` ``and`` ``initialize`` ``it`` ``to`` ``the`` ``current`` ``timestamp,`` ``the`` ``other`` ``named`` ```` ``host` ```` ``and`` ``set`` ``to`` ``the`` ``hostname`` ``of`` ``the`` ``local`` ``machine.`` ``Then,`` ``for`` ``every`` ``line,`` ``we`` ``print`` ``put`` ```` ``mysql. ```, followed by the lower-case form of the first word, then by a space, then by the current timestamp, then by the second word (the value), another space, and finally `host=` and the current hostname. Rinse and repeat every 15 seconds. The `-w`` ``30` parameter given to `nc` simply sets a timeout on the connection to the TSD. Bear in mind, this is just an example, in practice you can use tcollector's MySQL collector.\n\nIf you don't have a MySQL server to monitor, you can try this instead to collect basic load metrics from your Linux servers:\n\n``` python\ncat >loadavg-collector.sh <<\\EOF\n#!/bin/bash\nset -e\nwhile true; do\n  awk -v now=`date +%s` -v host=`hostname` \\\n  '{ print \"put proc.loadavg.1m \" now \" \" $1 \" host=\" host;\n  print \"put proc.loadavg.5m \" now \" \" $2 \" host=\" host }' /proc/loadavg\n  sleep 15\ndone | nc -w 30 host.name.of.tsd PORT\nEOF\nchmod +x loadavg-collector.sh\nnohup ./loadavg-collector.sh &\n```\n\nThis will store a reading of the 1-minute and 5-minute load average of your server in OpenTSDB by sending simple \"telnet-style commands\" to the TSD:\n\n``` python\nput proc.loadavg.1m 1288946927 0.36 host=foo\nput proc.loadavg.5m 1288946927 0.62 host=foo\nput proc.loadavg.1m 1288946942 0.43 host=foo\nput proc.loadavg.5m 1288946942 0.62 host=foo\n```\n\n## Batch Imports\n\nLet's imagine that you have a cron job that crunches gigabytes of application logs every day or every hour to extract profiling data. For instance, you could be logging the time taken to process a request and your cron job would compute an average for every 30 second window. Maybe you're particularly interested in 2 types of requests handled by your application, so you'll compute separate averages for those requests, and an another average for every other request type. So your cron job may produce an output file that looks like this:\n\n``` python\n1288900000 42 foo\n1288900000 51 bar\n1288900000 69 other\n1288900030 40 foo\n1288900030 59 bar\n1288900030 80 other\n```\n\nThe first column is a timestamp, the second is the average latency for that 30 second window, and the third is the type of request we're talking about. If you run your cron job on a day worth of logs, you'll end up with 8640 such lines. In order to import those into OpenTSDB, you need to adjust your cron job slightly to produce its output in the following format:\n\n``` python\nmyservice.latency.avg 1288900000 42 reqtype=foo\nmyservice.latency.avg 1288900000 51 reqtype=bar\nmyservice.latency.avg 1288900000 69 reqtype=other\nmyservice.latency.avg 1288900030 40 reqtype=foo\nmyservice.latency.avg 1288900030 59 reqtype=bar\nmyservice.latency.avg 1288900030 80 reqtype=other\n```\n\nNotice we're simply associating each data point with the name of a metric (myservice.latency.avg) and naming the tag that represents the request type. If each server has its own logs and you process them separately, you may want to add another tag to each line like the `host=foo` tag we saw in the previous section. This way you'll be able to plot the latency of each server individually, in addition to your average latency across the board and/or per request type. In order to import a data file in the format above (metric timestamp value tags) simply run the following command:\n\n``` python\n./tsdb import your-file\n```\n\nIf your data file is large, consider gzip'ing it first. This can be as simple as piping the output of your cron job to `gzip`` ``-9`` ``>output.gz` instead of writing directly to a file. The import command is able to read gzip'ed files and it greatly helps performance for large batch imports.\n\n## Self Monitoring\n\nEach TSD exports some stats about itself through the simple stats command. You can collect those stats and feed them back to the TSD every few seconds. First, create the necessary metrics:\n\n``` python\necho stats | nc -w 1 localhost 4242 \\\n| awk '{ print $1 }' | sort -u \\\n| xargs ./tsdb mkmetric\n```\n\nThis requests the stats from the TSD (assuming it's running on the local host and listening to port 4242), extract the names of the metrics from the stats and assigns them UIDs. Then you can use this simple script to collect stats and store them in OpenTSDB:\n\n``` python\n#!/bin/bash\nINTERVAL=15\nwhile :; do\n  echo stats || exit\n  sleep $INTERVAL\ndone | nc -w 30 localhost $1 \\\n  | sed 's/^/put /' \\\n  | nc -w 30 localhost $1\n```\n\nThis way you will collect and store stats from the TSD every 15 seconds.\n\n## Create a Graph\n\nOnce you've written some data using any of the methods above, you can now try to create a graph using that data. Pull up the GUI in your favorite browser. If you're running your TSD on the localhost, simply visit [http://127.0.0.1:4242](http://127.0.0.1:4242).\n\nFirst, pick one of the metrics and put that in the `Metric` box. For example, `proc.loadavg.1m`. As you type, you should see auto-complete lines pop up and you can click on any of them.\n\nThen click the `From` box at the top and a date-picker pop-up should appear. Select any time from yesterday and click on another box. At this point you should see \"Loading graph..\" very briefly followed by the actual graph. If the graph is empty, it may not have found the most recent data points so click the `(now)` link and the page should refresh.\n\nThis initial graph will aggregate all of the time series for the metric you selected. Try limiting your query to a specific host by adding `host` as a value in the left-hand box next to the `Tags` label (if it isn't already there) and add the specific host name (e.g. `foo`) in the right-hand box. After clicking in another box you should see the graph re-draw with different information.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/quickstart.html](http://opentsdb.net/docs/build/html/user_guide/quickstart.html)"
- name: rollup
  id: api_telnet/rollup
  summary: Attempts to write a rolled up and/or pre-aggregated data point to storage
  description: "# rollup\n\nAttempts to write a rolled up and/or pre-aggregated data point to storage. Note that UTF-8 characters may not be handled properly by the Telnet style API so use the [*/api/rollup*](../api_http/rollup) method instead or use the Java API directly. Also see the `../user_guide/rollup` documentation for more information. This endpoint behaves in a similar manner to the [*put*](put) API.\n\n## Request\n\nThe command format is:\n\n``` python\nrollup <rollup spec> <metric> <timestamp> <value> <tagk_1>=<tagv_1>[ <tagk_n>=<tagv_n>]\n```\n\nIn this case the rollup spec is one of:\n\n- `<interval>:<aggregator>` for a *raw* or *non-pre-aggregated* **rollup** over the interval.\n- `<group_by_aggregator>` for a *raw* **pre-aggregated** value that has not been rolled up over time.\n- `<interval>:<aggregator>:<group_by_aggregator>` for a *rolled up* *pre-aggregated* value.\n\nNote:\n\n- Because fields are space delimited, metrics and tag values may not contain spaces.\n- The timestamp must be a positive Unix epoch timestamp. E.g. `1479496100` to represent `Fri,`` ``18`` ``Nov`` ``2016`` ``19:08:20`` ``GMT`\n- The value must be a number. It may be an integer (maximum and minimum values of Java's `long` data type), a floating point value or scientific notation (in the format `[-]<#>.<#>[e|E][-]<#>`).\n- At least one tag pair must be present. Additional tag pairs can be added with spaces in between.\n\n### Examples\n\n``` python\nrollup 1h:SUM sys.if.bytes.out 1479412800 1.3E3 host=web01 interface=eth0\nrollup SUM sys.procs.running 1479496100 42 colo=lga\nrollup 1h:SUM:SUM sys.procs.running 1479412800 24 colo=lga\n```\n\n## Response\n\nA successful request will not return a response. Only on error will the socket return a line of data. Some examples appear below:\n\n### Example Requests and Responses\n\n``` python\nrollup\nrollup: illegal argument: not enough arguments (need least 5, got 1)\n```\n\n``` python\nrollup SUM metric.foo notatime 42 host=web01\nrollup: invalid value: Invalid character 'n' in notatime\n```\n\nThe following will be returned if `tsd.core.auto_create_metrics` are disabled.\n\n``` python\nrollup SUM new.metric 1479496160 1.3e3 host=web01\nrollup: unknown metric: No such name for 'metrics': 'new.metric'\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_telnet/rollup.html](http://opentsdb.net/docs/build/html/api_telnet/rollup.html)"
- name: Rollup And Pre-Aggregates
  id: user_guide/rollups
  summary: While TSDB is designed to store original, full resolution data as long as there is space, queries for wide time ranges or over many tag combinations can be quite painful
  description: "# Rollup And Pre-Aggregates\n\nWhile TSDB is designed to store original, full resolution data as long as there is space, queries for wide time ranges or over many tag combinations can be quite painful. Such queries can take a long time to complete or, in the worst case, kill TSDs with out-of-memory exceptions. As of OpenTSDB 2.4, a set of new APIs allow for storing and querying lower resolution data to answer such queries much quicker. This page will give you an overview of what rollups and pre-aggregates are, how they work in TSDB and how best to use them. Look in the API's section for specific implementation details.\n\nNote\n\nOpenTSDB does not itself calculate and store rollup or pre-aggregated data. There are multiple ways to compute the results but they all have benefits and drawbacks depending on the scale and accuracy requirements. See the [Generating Rollups and Pre-Aggregates](#generating) section discussing how to create this data.\n\n## Example Data\n\nTo help describe the lower resolution data, lets look at some full resolution (also known as *raw* data) example data. The first table defines the time series with a short-cut identifier.\n\n| Series ID | Metric              | Tag 1      | Tag 2    | Tag 3          |\n|-----------|---------------------|------------|----------|----------------|\n| ts1       | system.if.bytes.out | host=web01 | colo=lga | interface=eth0 |\n| ts2       | system.if.bytes.out | host=web02 | colo=lga | interface=eth0 |\n| ts3       | system.if.bytes.out | host=web03 | colo=sjc | interface=eth0 |\n| ts4       | system.if.bytes.out | host=web04 | colo=sjc | interface=eth0 |\n\nNotice that they all have the same `metric` and `interface` tag, but different `host` and `colo` tags.\n\nNext for some data written at 15 minute intervals:\n\n| Series ID | 12:00 | 12:15 | 12:30 | 12:45 | 13:00 | 13:15 | 13:30 | 13:45 |\n|-----------|-------|-------|-------|-------|-------|-------|-------|-------|\n| ts1       | 1     | 4     | -3    | 8     | 2     | -4    | 5     | 2     |\n| ts2       | 7     | 2     | 8     | -9    | 4     |       | 1     | 1     |\n| ts3       | 9     | 3     | -2    | -1    | 6     | 3     | 8     | 2     |\n| ts4       |       | 2     | 5     | 2     | 8     | 5     | -4    | 7     |\n\nNotice that some data points are missing. With those data sets, lets look at rollups first.\n\n## Rollups\n\nA \"rollup\" is defined, in OpenTSDB, as a **single** time series aggregated over time. It may also be called a \"time-based aggregation\". Rollups help to solve the problem of looking at wide time spans. For example, if you write a data point every 60 seconds and query for one year of data, a time series would return more than 525k individual data points. Graphing that many points could be pretty messy. Instead you may want to look at lower resolution data, say 1 hour data where you only have around 8k values to plot. Then you can identify anomalies and drill down for finer resolution data.\n\nIf you have already used OpenTSDB to query data, you are likely familiar with **downsamplers** that aggregate each time series into a smaller, or lower resolution, value. A rollup is essentially the result of a downsampler stored in the system and called up at will. Each rollup (or downsampler) requires two pieces of information:\n\n- **Interval** - How much time is \"rolled\" up into the new value. For example, `1h` for one hour of data or `1d` for a day of data.\n- **Aggregation Function** - What arithmetic was performed on the underlying values to arrive at the new value. E.g. `sum` to add all of the values or `max` to store the largest.\n\nWarning\n\nWhen storing rollups, it's best to avoid functions such as **average**, **median** or **deviation**. When performing further downsampling or grouping aggregations, such values become meaningless. Instead it's much better to always store the **sum** and **count** from which, at least, the **average** can be computed at query time. For more information, see the section below.\n\nThe timestamp of a rolled-up data point should snap to the top of the rollup interval. E.g. if the rollup interval is `1h` then it contains 1 hour of data and should snap to the top of the hour. (As all timestamps are written in Unix Epoch format, defined as the UTC timezone, this would be the start of an hour UTC time).\n\n### Rollup Example\n\nGiven the series above, lets store the `sum` and `count` with an interval of `1h`.\n\n| Series ID | 12:00 | 13:00 |\n|-----------|-------|-------|\n| ts1 SUM   | 10    | 5     |\n| ts1 COUNT | 4     | 4     |\n| ts2 SUM   | 8     | 6     |\n| ts2 COUNT | 4     | 3     |\n| ts3 SUM   | 9     | 19    |\n| ts3 COUNT | 4     | 4     |\n| ts4 SUM   | 9     | 16    |\n| ts4 COUNT | 3     | 4     |\n\nNotice that all timestamps align to the top of the hour regardless of when the first data point in the interval \"bucket\" appears. Also notice that if a data point is not present for an interval, the count is lower.\n\nIn general, you should aim to compute and store the `MAX`, `MIN`, `SUM` and `COUNT` for each time series when storing rollups.\n\n### Averaging Rollup Example\n\nWhen rollups are enabled and you request a downsampler with the `avg` function from OpenTSDB, the TSD will scan storage for `SUM` and `COUNT` values. Then while iterating over the data it will accurately compute the average.\n\nThe timestamps for count and sum values must match. However, if the expected count value for a sum is missing, the sum will be kicked out of the results. Take the following example set from above where we're now missing a count data point in `ts2`.\n\n| Series ID | 12:00 | 13:00 |\n|-----------|-------|-------|\n| ts1 SUM   | 10    | 5     |\n| ts1 COUNT | 4     | 4     |\n| ts2 SUM   | 8     | 6     |\n| ts2 COUNT | 4     |       |\n\nThe resulting `avg` for a `2h` downsampling query would look like this:\n\n| Series ID | 12:00 |\n|-----------|-------|\n| ts1 AVG   | 1.875 |\n| ts2 AVG   | 2     |\n\n## Pre-Aggregates\n\nWhile rollups help with wide time span queries, you can still run into query performance issues with small ranges if the metric has high cardinality (i.e. the unique number of time series for the given metric). In the example above, we have 4 web servers. But lets say that we have 10,000 servers. Fetching the sum or average of interface traffic may be fairly slow. If users are often fetching the group by (or some think of it as the spatial aggregate) of large sets like this then it makes sense to store the aggregate and query that instead, fetching *much* less data.\n\nUnlike rollups, pre-aggregates require only one extra piece of information:\n\n- **Aggregation Function** - What arithmetic was performed on the underlying values to arrive at the new value. E.g. `sum` to add all of the time series or `max` to store the largest.\n\nIn OpenTSDB, pre-aggregates are differentiated from other time series with a special tag. The default tag key is `_aggregate` (configurable via `tsd.rollups.agg_tag_key`). The **aggregation function** used to generate the data is then stored in the tag value in upper-case. Lets look at an example:\n\n### Pre-Aggregate Example\n\nGiven the example set at the top, we may want to look at the total interface traffic by colo (data center). In that case, we can aggregate by `SUM` and `COUNT` similarly to the rollups. The result would be four **new** time series with meta data like:\n\n| Series ID | Metric              | Tag 1    | Tag 2             |\n|-----------|---------------------|----------|-------------------|\n| ts1'      | system.if.bytes.out | colo=lga | \\_aggregate=SUM   |\n| ts2'      | system.if.bytes.out | colo=lga | \\_aggregate=COUNT |\n| ts3'      | system.if.bytes.out | colo=sjc | \\_aggregate=SUM   |\n| ts4'      | system.if.bytes.out | colo=sjc | \\_aggregate=SUM   |\n\nNotice that these time series have dropped the tags for `host` and `interface`. That's because, during aggregation, multiple, different values of the `host` and `interface` have been wrapped up into this new series so it no longer makes sense to have them as tags. Also note that we injected the new `_aggregate` tag in the stored data. Queries can now access this data by specifying an `_aggregate` value.\n\nNote\n\nWith rollups enabled, if you plan to use pre-aggregates, you may want to help differentiate raw data from pre-aggregates by having TSDB automatically inject `_aggregate=RAW`. Just configure the `tsd.rollups.tag_raw` setting to true.\n\nNow for the resulting data:\n\n| Series ID | 12:00 | 12:15 | 12:30 | 12:45 | 13:00 | 13:15 | 13:30 | 13:45 |\n|-----------|-------|-------|-------|-------|-------|-------|-------|-------|\n| ts1'      | 8     | 6     | 5     | -1    | 6     | -4    | 6     | 3     |\n| ts2'      | 2     | 2     | 2     | 2     | 2     | 1     | 2     | 2     |\n| ts3'      | 9     | 5     | 3     | 1     | 14    | 8     | 4     | 9     |\n| ts4'      | 1     | 2     | 2     | 2     | 2     | 2     | 2     | 2     |\n\nSince we're performing a group by aggregation (grouping by `colo`) we have a value for each timestamp from the original data set. We are *not* downsampling or performing a rollup in this situation.\n\nWarning\n\nAs with rollups, when writing pre-aggregates, it's best to avoid functions such as **average**, **median** or **deviation**. Just store **sum** and **count**\n\n## Rolled-up Pre-Aggregates\n\nWhile pre-aggregates certainly help with high-cardinality metrics, users may still want to ask for wide time spans but run into slow queries. Thankfully you can roll up a pre-aggregate in the same way as raw data. Just generate the pre-aggregate, then roll it up using the information above.\n\n## Generating Rollups and Pre-Aggregates\n\nCurrently the TSDs do not generate the rollup or pre-aggregated data for you. The primary reason for this is that OpenTSDB is meant to handle huge amounts of time series data so individual TSDs are focused on getting their data into storage as quickly as possible.\n\n### Problems\n\nBecause of the (essentially) stateless nature of the TSDs, they likely won't have the full set of data available to perform pre-aggregates. E.g., our sample `ts1` data may be written to `TSD_A` while `ts2` is written to `TSD_B`. Neither can perform a proper group-by without reading the data out of storage. We also don't know at what time we should perform the pre-aggregation. We could wait for 1 minute and pre-aggregate the data but miss anything that came in after that minute. Or we could wait an hour and queries over the pre-aggregates won't have data for the last hour. And what happens if data comes in much later?\n\nAdditionally for rollups, depending on how users write data to TSDs, for `ts1`, we may receive the `12:15` data point on `TSD_A` but the `12:30` value arrives on `TSD_B` so neither has the data required for the full hour. Time windowing constraints also apply to rollups.\n\n### Solutions\n\nUsing rollups and pre-aggregates require some analysis and a choice between various trade-offs. Since some OpenTSDB users already have means in place for calculating this kind of data, we simply provide the API to store and query. However here are some tips on how to compute these on your own.\n\n**Batch Processing**\n\nOne method that is commonly used by other time series databases is to read the data out of the database after some delay, calculate the pre-aggs and rollups, then write them. This is the easiest way of solving the problem and works well at small scales. However there are still a number of issues:\n\n- As data grows, queries for generating the rollups grow as well to the point where the query load impacts write and user query performance. OpenTSDB runs into this same problem when data compactions are enabled under HBase.\n- Also as data grows, more data means the batch processing time takes longer and must be sharded across multiple workers which can be a pain to coordinate and troubleshoot.\n- Late or historical data may not be rolled up unless some means of tracking is in place to trigger a new batch on old data.\n\nSome methods of improving batch processing include:\n\n- Reading from replicated systems, e.g. if you setup HBase replication, you could have users query the master system and aggregations read from the replicated store.\n- Read from alternate stores. One example is to mirror all data to another store such as HDFS and run batch jobs against that data.\n\n**Queueing on TSDs**\n\nAnother option that some databases use is to queue all of the data in memory in the process and write the results after a configured time window has passed. But because TSDs are stateless and generally users put a load balancer in front of their TSDs, a single TSD may not get the full picture of the rollup or pre-agg to be calculated (as we mentioned above). For this method to work, upstream collectors would have to route all of the data required for a calculation to a specific TSD. It's not a difficult task but the problems faced include:\n\n- Having enough RAM or disk space to spool the data locally on for each TSD.\n- If a TSD process dies, you'll either loose the data for the aggregation or it must be bootstrapped from storage.\n- Whenever the aggregation calculations are taking place, overall write throughput of the raw data can be affected.\n- You still have the late/historical data issue.\n- Since TSDB is JVM based, keeping all of that data in RAM and then running GC will hurt. A lot. (spooling to disk is better, but then you'll hit IO issues)\n\nIn general, queueing on a writer is a bad idea. Avoid the pain.\n\n**Stream Processing**\n\nA better way of dealing with rollups and pre-aggregates is to route the data into a stream processing system where it can be processed in near-real-time and written to the TSDs. It's similar to the *Queuing on TSDs* option but using one of the myriad stream processing frameworks (Storm, Flink, Spark, etc.) to handle message routing and in-memory storage. Then you simply write some code to compute the aggregates and spit the data out after a window has passed.\n\nThis is the solution used by many next-generation monitoring solutions such as that at Yahoo!. Yahoo is working to open source their stream processing system for others who need monitoring at massive scales and it plugs neatly into TSDB.\n\nWhile stream processing is better you still have problems to deal with such as:\n\n- Enough resources for the stream workers to do their job.\n- A dead stream worker requires bootstrapping from storage.\n- Late/historical data must be handled.\n\n**Share**\n\nIf you have working code for calculating aggregations, please share with the OpenTSDB group. If your solution is open-source we may be able to incorporate it in the OpenTSDB ecosystem.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/rollups.html](http://opentsdb.net/docs/build/html/user_guide/rollups.html)"
- name: scan
  id: user_guide/cli/scan
  summary: The scan command is useful for debugging and exporting data points
  description: "# scan\n\nThe scan command is useful for debugging and exporting data points. Provide a start time, optional end time and one or more queries and the response will be raw byte data from storage or data points in a text format acceptable for use with the **import** command. Scan also provides a rudimentary means of deleting data. The scan command accepts common CLI arguments. Data is emitted to standard out.\n\nNote that while queries require an aggregator, it is effectively ignored. If a query encompasses many time series, the scan output may be extremely large so be careful when crafting queries.\n\n## Parameters\n\n``` bash\nscan [--delete|--import] START-DATE [END-DATE] query [queries...]\n```\n\n| Name       | Data Type         | Description                                                                                                                                                               | Default           | Example                     |\n|------------|-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|-----------------------------|\n| --delete   | Flag              | Optional flag that deletes data in any row that matches the query. See warning below.                                                                                     | Not set           | --delete                    |\n| --import   | flag              | Optional flag that outputs results in a text format useful for importing or storing as a backup.                                                                          | Not set           | --import                    |\n| START-DATE | String or Integer | Starting time for the query. This may be an absolute or relative time. See [*Dates and Times*](../query/dates) for details                                                |                   | 1h-ago                      |\n| END-DATE   | String or Integer | Optional end time for the query. If not provided, the current time is used. This may be an absolute or relative time. See [*Dates and Times*](../query/dates) for details | Current timestamp | 2014/01/01-00:00:00         |\n| query      | String            | One or more command line queries                                                                                                                                          |                   | sum tsd.hbase.rpcs type=put |\n\nExample:\n\n``` bash\nscan --import 1h-ago now sum tsd.hbase.rpcs type=put sum tsd.hbase.rpcs type=scan\n```\n\nWarning\n\nIf you include the `--delete` flag, **ALL** data in 'any' row that matches on the query will be deleted permanently. Rows are separated on 1 hour boundaries so that if you issued a scan command with a start and end time that covered 10 minutes within a single hour, the entire hour of data will be deleted.\n\nDeletions will also delete any Annotations or non-TSDB related data in a row.\n\nNote\n\nThe scan command returns data on row boundaries (1 hour) so results may include data previous to and after the specified start and end times.\n\n## Raw Output\n\nThe default output for `scan` is a raw dump of the rows and columns that match the given queries. This is useful in debugging situations such as data point collisions or encoding issues. As the output includes raw byte arrays and the format changes slightly depending on the data, it is not easily machine paresable.\n\nRow keys, column qualifiers and column values are emitted as Java byte arrays. These are surrounded by square brackets and individual bytes are represented as signed integers (as Java does not have native unsigned ints). Row keys are printed first followed by a new line. Then each column is printed on it's own row and is indented with two spaces to indicate it belongs to the previous row. If a compacted column is found, the raw data and number of compacted values is printed followed by a new line. Each compacted data point is printed on it's own indented line. Annotations are also emitted in raw mode.\n\nThe various formats are listed below. The `\\t` expression represents a tab. `space` indicates a space character.\n\n### Row Key Format\n\n``` bash\n[<row key>] <metric name> <row timestamp> (<datetime>) <tag/value pairs>\n```\n\nWhere:\n\n> - **row key** Is the raw byte array of the row key\n> - **metric name** Is the decoded name of the metric the row represents\n> - **row timestamp** Is the base timestamp of the row in seconds (on 1 hour boundaries)\n> - **datetime** Is the system default formatted human readable timestamp\n> - **tag/value pairs** Are the tags associated with the time series\n\nExample:\n\n``` bash\n[0, 0, 1, 80, -30, 39, 0, 0, 0, 1, 0, 0, 1] sys.cpu.user 1356998400 (Mon Dec 31 19:00:00 EST 2012) {host=web01}\n```\n\n### Single Data Point Column Format\n\n``` bash\n<two spaces>[<qualifier>]\\t[<value>]\\t<offset>\\t<l|f>\\t<timestamp>\\t(<datetime>)\n```\n\nWhere:\n\n> - **qualifier** Is the raw byte array of the column qualifier\n> - **value** Is the raw byte array of the column value\n> - **offset** Is the number of seconds or milliseconds (based on timestamp) of offset from the row base timestamp\n> - **l\\|f** Is either `l` to indicate the value is an Integer (Java Long) or `f` for a floating point value.\n> - **timestamp** Is the absolute timestamp of the data point in seconds or milliseconds\n> - **datetime** Is the system default formatted human readable timestamp\n\nExample:\n\n``` bash\n[0, 17]     [0, 17] [1, 1]  1     l     1356998401    (Mon Dec 31 19:00:01 EST 2012)\n```\n\n### Compacted Column Format\n\n``` bash\n<two spaces>[<qualifier>]\\t[<value>] = <number of datapoints> values:\n```\n\nWhere:\n\n> - **qualifier** Is the raw byte array of the column qualifier\n> - **value** Is the raw byte array of the column value\n> - **number of datapoints** Is the number of data points in the compacted column\n\nExample:\n\n``` bash\n[-16, 0, 0, 7, -16, 0, 2, 7, -16, 0, 1, 7]  [0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 6, 0] = 3 values:\n```\n\nEach data point within the compacted column follows the same format as a single column with the addition of two spaces of indentation.\n\n### Annotation Column Format\n\n``` bash\n<two spaces>[<qualifier>]\\t[<value>]\\t<offset>\\t<JSON\\>\\t<timestamp\\>\\t(<datetime>)\n```\n\nWhere:\n\n> - **qualifier** Is the raw byte array of the column qualifier\n> - **value** Is the raw byte array of the column value\n> - **offset** Is the number of seconds or milliseconds (based on timestamp) of offset from the row base timestamp\n> - **JSON** Is the decoded JSON data stored in the column\n> - **timestamp** Is the absolute timestamp of the data point in seconds or milliseconds\n> - **datetime** Is the system default formatted human readable timestamp\n\nExample:\n\n``` bash\n[1, 0, 0]   [123, 34...]  0     {\"tsuid\":\"000001000001000001\",\"startTime\":1356998400,\"endTime\":0,\"description\":\"Annotation on seconds\",\"notes\":\"\",\"custom\":null}    1356998416000   (Mon Dec 31 19:00:16 EST 2012)\n```\n\n## Import Format\n\nThe import format is the same as a Telnet style `put` command.\n\n``` bash\n<metric> <timestamp> <value> <tagk=tagv>[...<tagk=tagv>]\n```\n\nWhere:\n\n> - **metric** Is the name of the metric as a string\n> - **timestamp** Is the absolute timestamp of the data point in seconds or milliseconds\n> - **value** Is the value of the data point\n> - **tagk=tagv** Are tag name/value pairs separated by spaces\n\nExample:\n\n``` bash\nsys.cpu.user 1356998400 42 host=web01 cpu=0\nsys.cpu.user 1356998401 24 host=web01 cpu=0\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/cli/scan.html](http://opentsdb.net/docs/build/html/user_guide/cli/scan.html)"
- name: search
  id: user_guide/cli/search
  summary: The search command allows for searching OpenTSDB to reteive a list of time series or associated meta data
  description: "# search\n\nNote\n\nAvailable in 2.1\n\nThe search command allows for searching OpenTSDB to reteive a list of time series or associated meta data. Search does not return actual data points or time series objects stored in the data table. Use the query tools to access that data. Currently only the `lookup` command is implemented.\n\n## Lookup\n\nLookup queries use either the meta data table or the main data table to determine what time series are associated with a given metric, tag name, tag value, tag pair or combination thereof. For example, if you want to know what metrics are available for a tag pair `host=web01` you can execute a lookup to find out.\n\nNote\n\nBy default lookups are performed against the `tsdb-meta` table. You must enable real-time meta data creation or perform a `metasync` using the `uid` command in order to retreive data from a lookup. Alternatively you can lookup against the raw data table but this can take a very long time depending on how much data is in your system.\n\n### Command Format\n\n``` bash\nsearch lookup <query>\n```\n\n### Parameters\n\n| Name             | Data Type | Description                                                                                                                             | Default | Example              |\n|------------------|-----------|-----------------------------------------------------------------------------------------------------------------------------------------|---------|----------------------|\n| query            | String    | One or more command line queries similar to a data CLI query. See the query section below.                                              |         | tsd.hbase.rpcs type= |\n| --use_data_table | Flag      | Optional flag that will cause the lookup to run against the main `tsdb-data` table. *NOTE:* This can take a very long time to complete. | Not set | --use_data_table     |\n\n### Query Format\n\nFor details on crafting a query, see [*/api/search/lookup*](../../api_http/search/lookup). The CLI query is similar to an API query but spaces are used as separators instead of commas and curly braces are not used.\n\n``` bash\n[<metric>] [[tagk]=[tagv]] ...[[tagk]=[tagv]]\n```\n\nAt least one metric, tagk or tagv is required.\n\n### Example Command\n\n``` bash\nsearch lookup tsd.hbase.rpcs type=\n```\n\n### Output\n\nDuring a lookup, the results will be printed to standard out. Note that if you have logging enabled, messages may be interspersed with the results. Set the logging level to WARN or ERROR in the `logback.xml` configuration to supress these warnings. You may want to run the lookup in the background and capture standard out to a file, particularly when running lookups against the data table as these may take a long time to complete.\n\n``` bash\n<tsuid> <metric name> <tag/value pairs>\n```\n\nWhere:\n\n> - **tsuid** Is the hex encoded UID of the time series\n> - **metric name** Is the decoded name of the metric the row represents\n> - **tag/value pairs** Are the tags associated with the time series\n\n### Example Response\n\n``` bash\n0023E3000002017358000006017438 tsd.hbase.rpcs type=openScanner host=tsdb-1.mysite.com\n```\n\nNote\n\nDuring scanning, if the UID for a metric, tag name or tag value cannot be resolved to a name, an exception will be thrown.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/cli/search.html](http://opentsdb.net/docs/build/html/user_guide/cli/search.html)"
- name: stats
  id: api_telnet/stats
  summary: This command is similar to the HTTP /api/stats endpoint in that it will return a list of the TSD stats, one per line, in the put format
  description: "# stats\n\nThis command is similar to the HTTP [*/api/stats*](../api_http/stats/index) endpoint in that it will return a list of the TSD stats, one per line, in the `put` format. This command does not modify TSD in any way.\n\n## Request\n\nThe command format is:\n\n``` python\nstats\n```\n\n## Response\n\nA set of time series with data about the running TSD.\n\n### Example\n\n``` python\ntsd.hbase.rpcs 1479600574 0 type=increment host=web01\ntsd.hbase.rpcs 1479600574 0 type=delete host=web01\ntsd.hbase.rpcs 1479600574 1 type=get host=web01\ntsd.hbase.rpcs 1479600574 0 type=put host=web01\ntsd.hbase.rpcs 1479600574 0 type=append host=web01\ntsd.hbase.rpcs 1479600574 0 type=rowLock host=web01\ntsd.hbase.rpcs 1479600574 0 type=openScanner host=web01\ntsd.hbase.rpcs 1479600574 0 type=scan host=web01\ntsd.hbase.rpcs.batched 1479600574 0 host=web01\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_telnet/stats.html](http://opentsdb.net/docs/build/html/api_telnet/stats.html)"
- name: Stats
  id: user_guide/stats
  summary: OpenTSDB offers a number of metrics about its performance, accessible via various API endpoints
  description: "# Stats\n\nOpenTSDB offers a number of metrics about its performance, accessible via various API endpoints. The main stats are accessible from the GUI via the \"Stats\" tab, from the Http API at `/api/stats` or the legacy API at `/stats`. The Telnet style API also supports the \"stats\" command for fetching over CLI. These can easily be published right back into OpenTSDB at any interval you like.\n\nAdditional stats available include JVM information, storage details (e.g. per-region-client HBase stats) and executed query details. See [*/api/stats*](../api_http/stats/index) for more details about the other endpoints.\n\nAll metrics from the main stats endpoint include a `host` tag that includes the name of the host where the TSD is running. If the `tsd.stats.canonical` configuration flag is set, this will change to `fqdn` and the TSD will try to resolve its host name to return the fully qualified domain name. Currently all stats are integer values. Each request for stats will fetch statistics in real time so the timestamp will reflect the current time on the TSD host.\n\nNote\n\nThe `/api/stats` endpoint is a good place to execute a health check for your TSD as it will execute a query to storage for fetching UID stats. If the TSD is unable to reach the backing store, the API will return an exception.\n\n[TABLE]\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/stats.html](http://opentsdb.net/docs/build/html/user_guide/stats.html)"
- name: Storage
  id: user_guide/backends/index
  summary: OpenTSDB currently supports Apache HBase as its main storage backend
  description: "# Storage\n\nOpenTSDB currently supports Apache HBase as its main storage backend. As of version 2.3, OpenTSDB also works with Google's Bigtable in the cloud (fitting as OpenTSDB is descended from a monitoring system at Google and HBase is descended from HBase). Select the HBase link below to learn about the storage schema or Bigtable to find the configs and setup for use in the cloud.\n\n- [HBase Schema](hbase)\n  - [Data Table Schema](hbase#data-table-schema)\n  - [UID Table Schema](hbase#uid-table-schema)\n  - [Meta Table Schema](hbase#meta-table-schema)\n  - [Tree Table Schema](hbase#tree-table-schema)\n  - [Rollup Tables Schema](hbase#rollup-tables-schema)\n- [Bigtable](bigtable)\n  - [Setup](bigtable#setup)\n  - [Configuration](bigtable#configuration)\n- [Cassandra](cassandra)\n  - [Setup](cassandra#setup)\n  - [Configuration](cassandra#configuration)\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/backends/index.html](http://opentsdb.net/docs/build/html/user_guide/backends/index.html)"
- name: TCollector
  id: user_guide/utilities/tcollector
  summary: tcollector is a client-side process that gathers data from local collectors and pushes the data to OpenTSDB
  description: "# TCollector\n\n[tcollector](https://github.com/OpenTSDB/tcollector/) is a client-side process that gathers data from local collectors and pushes the data to OpenTSDB. You run it on all your hosts, and it does the work of sending each host's data to the TSD.\n\nOpenTSDB is designed to make it easy to collect and write data to it. It has a simple protocol, simple enough for even a shell script to start sending data. However, to do so reliably and consistently is a bit harder. What do you do when your TSD server is down? How do you make sure your collectors stay running? This is where tcollector comes in.\n\nTcollector does several things for you:\n\n- Runs all of your data collectors and gathers their data\n- Does all of the connection management work of sending data to the TSD\n- You don't have to embed all of this code in every collector you write\n- Does de-duplication of repeated values\n- Handles all of the wire protocol work for you, as well as future enhancements\n\n## Deduplication\n\nTypically you want to gather data about everything in your system. This generates a lot of datapoints, the majority of which don't change very often over time (if ever). However, you want fine-grained resolution when they do change. Tcollector remembers the last value and timestamp that was sent for all of the time series for all of the collectors it manages. If the value doesn't change between sample intervals, it suppresses sending that datapoint. Once the value does change (or 10 minutes have passed), it sends the last suppressed value and timestamp, plus the current value and timestamp. In this way all of your graphs and such are correct. Deduplication typically reduces the number of datapoints TSD needs to collect by a large fraction. This reduces network load and storage in the backend. A future OpenTSDB release however will improve on the storage format by using RLE (among other things), making it essentially free to store repeated values.\n\n## Collecting lots of metrics with tcollector\n\nCollectors in tcollector can be written in any language. They just need to be executable and output the data to stdout. Tcollector will handle the rest. The collectors are placed in the `collectors` directory. Tcollector iterates over every directory named with a number in that directory and runs all the collectors in each directory. If you name the directory `60`, then tcollector will try to run every collector in that directory every 60 seconds. The shortest supported interval is 15 seconds, you should use a long-running collector in the 0 folder for intervals shorter than 15 seconds. TCollector will sleep for 15 seconds after each time it runs the collectors so intervals of 15 seconds are the only actually supported intervals. For example, this would allow you to run a collector every 15, 30, 45, 60, 75, or 90 seconds, but not 80 or 55 seconds. Use the directory `0` for any collectors that are long-lived and run continuously. Tcollector will read their output and respawn them if they die. Generally you want to write long-lived collectors since that has less overhead. OpenTSDB is designed to have lots of datapoints for each metric (for most metrics we send datapoints every 15 seconds).\n\nIf there any non-numeric named directories in the `collectors` directory, then they are ignored. We've included a `lib` and `etc` directory for library and config data used by all collectors.\n\n## Installation of tcollector\n\nYou need to clone tcollector from GitHub:\n\n``` python\ngit clone git://github.com/OpenTSDB/tcollector.git\n```\n\nand edit 'tcollector/startstop' script to set following variable: `TSD_HOST=dns.name.of.tsd`\n\nTo avoid having to run `mkmetric` for every metric that tcollector tracks you can to start TSD with the `--auto-metric` flag. This is useful to get started quickly, but it's not recommended to keep this flag in the long term, to avoid accidental metric creation.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/utilities/tcollector.html](http://opentsdb.net/docs/build/html/user_guide/utilities/tcollector.html)"
- name: Telnet Style API
  id: api_telnet/index
  summary: The original way of interacting with OpenTSDB was through a Telnet style API
  description: "# Telnet Style API\n\nThe original way of interacting with OpenTSDB was through a Telnet style API. A user or application simply had to open a socket to the TSD and start sending ASCII string commands and expect a response. This documentation lists the various commands provided by OpenTSDB.\n\nEach command must be sent as a series of strings with a **new line** character terminating the request.\n\nNote\n\nConnections will be closed after a period of inactivity, typically 5 minutes.\n\nIf a command is sent to the API that is not supported or recognized, a response similar to the following will be shown:\n\n``` python\nunknown command: nosuchcommand.  Try `help'.\n```\n\nAt any time the connection can be closed by issuing the `exit` command.\n\n- [put](put)\n- [rollup](rollup)\n- [stats](stats)\n- [version](version)\n- [help](help)\n- [dropcaches](dropcaches)\n- [diediedie](diediedie)\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_telnet/index.html](http://opentsdb.net/docs/build/html/api_telnet/index.html)"
- name: Trees
  id: user_guide/trees
  summary: Along with metadata, OpenTSDB 2.0 introduces the concept of trees, a hierarchical method of organizing timeseries into an easily navigable structure that can be browsed similar to a file system on a computer
  description: "# Trees\n\nAlong with metadata, OpenTSDB 2.0 introduces the concept of **trees**, a hierarchical method of organizing timeseries into an easily navigable structure that can be browsed similar to a file system on a computer. Users can define a number of trees with various rule sets that organize TSMeta objects into a tree structure. Then users can browse the resulting tree via an HTTP API endpoint. See [*/api/tree*](../api_http/tree/index) for details.\n\n## Tree Terminology\n\n- **Branch** - Each branch is one node of a tree. It contains a list of child branches and leaves as well as a list of parent branches.\n- **Leaf** - The end of a branch and represents a unique timeseries. The leaf will contain a TSUID value that can be used to generate a TSD query. A branch can, and likely will, have multiple leaves\n- **Root** - The root branch is the start of the tree and all branches reach out from this root. It has a depth of 0.\n- **Depth** - Each time a branch is added to another branch, the depth increases\n- **Strict Matching** - When enabled, a timeseries must match a rule in every level of the rule set. If one or more levels fail to match, the timeseries will not be included in the tree.\n- **Path** - The name and level of each branch above the current branch in the hierarchy.\n\n## Branch\n\nEach node of a tree is recorded as a *branch* object. Each branch contains information such as:\n\n- **Branch ID** - The ID of the branch. This is a hexadecimal value described below.\n- **Display Name** - A name for the branch, parsed from a TSMeta object by the tree rule set.\n- **Depth** - How deep within the hierarchy the branch resides.\n- **Path** - The depth and name of each parent branch (includes the local branch).\n- **Branches** - Child branches one depth level below this branch.\n- **Leaves** - Leaves that belong to this branch.\n\nNavigating a tree starts at the **root** branch which always has an ID that matches the ID of the tree the branch belongs to. The root should have one or more child branches that can be used to navigate down one level of the tree. Each child can be used to navigate to their children and so on. The root does not have any parent branches and is always at a depth of 0. If a tree has just been defined or enabled, it may not have a root branch yet, and by extension, there won't be any child branches.\n\nEach branch will often have a list of child branches. However if a branch is at the end of a path, it may not have any child branches, but it should have a list of leaves.\n\n### Branch IDs and Paths\n\nBranch IDs are hexadecimal encoded byte arrays similar to TSUIDs but with a different format. Branch IDs always start with the ID of the tree encoded on 2 bytes. Root branches have a branch ID equal to the tree ID. Thus the root for tree `1` would have a branch ID of `0001`.\n\nEach child branch has a `DisplayName` value and the hash of this value is used to generate a 32 bit integer ID for the branch. The hash function used is the Java `java.lang.String` hash function. The 4 bytes of the integer value are then encoded to 8 hexadecimal characters. For example, if we have a display name of `sys` for a branch, the hash returned will be 102093. The TSD will convert that value to hexadecimal `0001BECD`.\n\nA branch ID is composed of the tree ID concatenated with the ID of each parent above the current branch, concatenated with the ID of the current branch. Thus, if our child branch `sys` is a child of the root, we would have a branch ID of `00010001BECD`.\n\nLets say there is a branch with a display name of `cpu` off of the `sys` child branch. `cpu` returns a hash of 98728 which converts to `000181A8` in hex. The ID of this child would be `00010001BECD000181A8`.\n\nIDs are created this way primarily due to the method of branch and leaf storage but also as a way to navigate back up a tree from a branch anywhere in the tree structure. This can be particularly useful if you know the end branch of a path and want to move back up one level or more. Unfortunately a deep tree can create very long branch IDs, but a well designed tree really shouldn't be more than 5 to 10 levels deep. Most URI requests should support branches up to 100 levels deep before the URI character constraints are reached.\n\n## Leaves\n\nA unique timeseries is represented as a *leaf* on the tree. A leaf can appear on any branch in the structure, including the root. But they will usually appear at the end of a series of branches in a branch that has one or more leaves but no child branches. Each leaf contains the TSUID for the timeseries to be used in a query as well as the metric and tag name/values. It also contains a *display name* that is parsed from the rule set but may not be identical to any of the metric, tag names or tag values.\n\nIdeally a timeseries will only appear once on a tree. But if the TSMeta object for a timeseries, OR the UIDMeta for a metric or tag is modified, it may be processed a second time and a second leaf added. This can happen particularly in situations where a tree has a *custom* rule on the metric, tag name or tag value where the TSMeta has been processed then a user adds a custom field that matches the rule set. In these situations it is recommended to enable *strict matching* on the tree so that the timeseries will not show up until the custom data has been added.\n\n## Rules\n\nEach tree is dynamically built from a set of rules defined by the user. A rule set must contain at least one rule and usually will have more than one. Each set has multiple *levels* that determine the order of rule processing. Rules located at level 0 are processed first, then rules at level 1, and so on until all of the rules have been applied to a given timeseries. Each level in the rule set may have multiple rules to handle situations where metrics and tags may not have been planned out ahead of time or some arbitrary data may have snuck in. If multiple rules are stored in a level, the first one with a successful match will be applied and the others ignored. These rules are also ordered by the *order* field so that a rule with order 0 is processed first, then a rule with order 1 and so on. In logs and when using the test endpoint, rules are usually given IDs in the format of \"\\[\\<treeId\\>:\\<level\\>:\\<order\\>:\\<type\\>\\]\" such as \"\\[1:0:1:0\\]\" indicates the rule for tree 1, at level 0, order 1 of the type `METRIC`.\n\n### Rule Types\n\nEach rule acts on a single component of the timeseries data. Currently available types include:\n\n| Type          | ID  | Description                                                                                                                                                                                                          |\n|---------------|-----|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| METRIC        | 0   | Processes the name of the metric associated with the timeseries                                                                                                                                                      |\n| METRIC_CUSTOM | 1   | Searches the metric metadata custom tag list for the given secondary name. If matched, the value associated with the tag name will be processed.                                                                     |\n| TAGK          | 2   | Searches the list of tagks for the given name. If matched, the tagv value associated with the tag name will be processed                                                                                             |\n| TAGK_CUSTOM   | 3   | Searches the list of tagks for the given name. If matched, the tagk metadata custom tag list is searched for the given secondary name. If that matches, the value associated with the custom name will be processed. |\n| TAGV_CUSTOM   | 4   | Searches the list of tagvs for the given name. If matched, the tagv metadata custom tag list is searched for the given secondary name. If that matches, the value associated with the custom name will be processed. |\n\n### Rule Config\n\nA single rule can either process a regex, a separator, or none. If a regex and a separator are defined for a rule, only the regex will be processed and the separator ignored.\n\nAll changes to a rule are validated to confirm that proper fields are filled out so that the rule can process data. The following fields must be filled out for each rule type:\n\n| Type          | field | customField |\n|---------------|-------|-------------|\n| Metric        |       |             |\n| Metric_Custom | X     | X           |\n| TagK          | X     |             |\n| TagK_Custom   | X     | X           |\n| TagV_Custom   | X     | X           |\n\n### Display Formatter\n\nOccasionally the data extracted from a tag or metric may not be very descriptive. For example, an application may output a timeseries with a tag pair such as \"port=80\" or \"port=443\". With a standard rule that matched on the tagk value \"port\", we would have two branches with the names \"80\" and \"443\". The uninitiated may not know what these numbers mean. Thus users can define a token based formatter that will alter the output of the branch to display useful information. For example, we could declare a formatter of \"{tag_name}: {value}\" and the branches will now display \"port: 80\" and \"port: 443\".\n\nTokens are case sensitive and must appear only one time per formatter. They must also appear exactly as deliniated in the table below:\n\n| Token      | Description                                                                                                                                                                             | Applicable Rule Type                          |\n|------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n| {ovalue}   | Original value processed by the rule. For example, if the rule uses a regex to extract a portion of the value but you do not want the extracted value, you could use the original here. | All                                           |\n| {value}    | The processed value. If a rule has an extracted regex group or the value was split by a separator, this represents the value after that processing has occured.                         | All                                           |\n| {tag_name} | The name of the tagk or custom tag associated with the value.                                                                                                                           | METRIC_CUSTOM, TAGK_CUSTOM, TAGV_CUSTOM, TAGK |\n| {tsuid}    | the TSUID of the timeseries                                                                                                                                                             | All                                           |\n\n### Regex Rules\n\nIn some situations, you may want to extract only a component of a metric, tag or custom value to use for grouping. For example, if you have computers in mutiple data centers with fully qualified domain names that incorporate the name of the DC, but not all metrics include a DC tag, you could use a regex to extract the DC for grouping.\n\nThe `regex` rule parameter must be set with a valid regular expression that includes one or more extraction operators, i.e. the parentheses. If the regex matches on the value provided, the extracted data will be used to build the branch or leaf. If more than one extractions are provided in the regex, you can use the `regex_group_index` parameter to choose which extracted value to use. The index is 0 based and defaults to 0, so if you want to choose the output of the second extraction, you would set this index to 1. If the regex does not match on the value or the extraction fails to return a valid string, the rule will be considered a no match.\n\nFor example, if we have a host tagk with a tagv of `web01.nyc.mysite.com`, we could use a regex similar to `.*\\.(.*)\\..*\\..*` to extract the \"nyc\" portion of the FQDN and group all of the servers in the \"nyc\" data center under the \"nyc\" branch.\n\n### Separator Rules\n\nThe metrics for a number of systems are generally strings with a separator, such as a period, to deliniate components of the metric. For example, \"sys.cpu.0.user\". To build a useful tree, you can use a separator rule that will break apart the string based on a character sequence and create a branch or leaf from each individual value. Setting the separator to \".\" for the previous example would yield three branches \"sys\", \"cpu\", \"0\" and one leaf \"user\".\n\n### Order of Precedence\n\nEach rule can only process a regex, a separator, or neither. If the rule has both a \"regex\" and \"separator\" value in their respective fields, only the \"regex\" will be executed on the timeseries. The \"separator\" will be ignored. If neither \"regex\" or \"separator\" are defined, then when the rule's \"field\" is matched, the entire value for that field will be processed into a branch or leaf.\n\n## Tree Building\n\nFirst, you must create the `tsdb-tree` table in HBase if you haven't already done so. If you enable tree processing and the table does not exist, the TSDs will not start.\n\nA tree can be built in two ways. The `tsd.core.tree.enable_processing` configuration setting enables real-time tree creation. Whenever a new TSMeta object is created or edited by a user, the TSMeta will be passed through every configured and enabled tree. The resulting branch will be recorded to storage. If a collision occurs or the TSUID failed to match on any rules, a warning will be logged and if the tree options configured, may be recorded to storage.\n\nAlternatively you can periodically synchronize all TSMeta objects via the CLI `uid` tool. This will scan through the `tsdb-uid` table and pass each discovered TSMeta object through configured and enabled trees. See [*uid*](cli/uid) for details.\n\nNote\n\nFor real-time tree building you need to enable the `tsd.core.meta.enable_tracking` setting as well so that TSMeta objects are created when a timeseries is received.\n\nThe general process for creating and building a tree is as follows:\n\n1.  Create a new tree via the HTTP API\n2.  Assign one or more rules to the tree via the HTTP API\n3.  Test the rules with some TSMeta objects via the HTTP API\n4.  After veryfing the branches would appear correctly, set the tree's `enable` flag to `true`\n5.  Run the `uid` tool with the `treesync` sub command to synchronize existing TSMeta objects in the tree\n\nNote\n\nWhen you create a new tree, it will be disabled by default so TSMeta objects will not be processed through the rule set. This is so you have time to configure the rule set and test it to verify that the tree would be built as you expect it to.\n\n### Rule Processing Order\n\nA tree will usually have more than one rule in order for the resulting tree to be useful. As noted above, rules are organized into levels and orders. A TSMeta is processed through the rule set starting at level 0 and order 0. Processing proceedes through the rules on a level in increasing order. After the first rule on a level that successfully matches on the TSMeta data, processing skips to the next level. This means that rules on a level are effectively [\\`\\`](#id1)or\\`\\`ed. If level 0 has rules at order 0, 1, 2 and 3, and the TSMeta matches on the rule with an order of 1, the rules with order 2 and 3 will be skipped.\n\nWhen editing rules, it may happen that some levels or orders are skipped or left empty. In these situations, processing simply skips the empty locations. You should do your best to keep things organized properly but the rule processor is a little forgiving.\n\n### Strict Matching\n\nAll TSMeta objects are processed through every tree. If you only want a single, monolithic tree to organize all of your OpenTSDB timeseries, this isn't a problem. But if you want to create a number of trees for specific subsets of information, you may want to exclude some timeseries entries from creating leaves. The `strictMatch` flag on a tree helps to filter out timeseries that belong on one tree but not another. With strict matching enabled, a timeseries must match a rule on every level (that has one or more rules) in the rule set in order for it to be included in the tree. If the meta fails to match on any of the levels with rules, it will be recorded as a not matched entry and no leaf will be generated.\n\nBy default strict matching is disabled so that as many timeseries as possible can be captured in a tree. If you change this setting on a tree, you may want to delete the existing branches and run a re-sync.\n\n## Collisions\n\nDue to the flexibility of rule sets and the wide variety of metric, tag name and value naming, it is almost inevitable that two different TSMeta entries would try to create the same leaf on a tree. Each branch can only have one leaf with a given display name. For example, if a branch has a leaf named `user` with a tsuid of `010101` but the tree tries to add a new leaf named `user` with a tsuid of `020202`, the new leaf will not be added to the tree. Instead, a *collision* entry will be recorded for the tree to say that tsuid `0202020` collided with an existing leaf for tsuid `010101`. The HTTP API can then be used to query the collision list to see if a particular TSUID did not appear in the tree due to a collision.\n\n## Not Matched\n\nWhen *strict matching* is enabled for a tree, a TSMeta must match on a rule on every level of the rule set in order to be added to the tree. If one or more levels fail to match, the TSUID will not be added. Similar to *collisions*, a not matched entry will be recorded for every TSUID that failed to be written to the tree. The entry will contain the TSUID and a brief message about which rule and level failed to match.\n\n## Examples\n\nAssume that our TSD has the following timeseries stored:\n\n| TS# | Metric          | Tags                                 | TSUID      |\n|-----|-----------------|--------------------------------------|------------|\n| 1   | cpu.system      | dc=dal, host=web01.dal.mysite.com    | 0102040101 |\n| 2   | cpu.system      | dc=dal, host=web02.dal.mysite.com    | 0102040102 |\n| 3   | cpu.system      | dc=dal, host=web03.dal.mysite.com    | 0102040103 |\n| 4   | app.connections | host=web01.dal.mysite.com            | 010101     |\n| 5   | app.errors      | host=web01.dal.mysite.com, owner=doe | 0101010306 |\n| 6   | cpu.system      | dc=lax, host=web01.lax.mysite.com    | 0102050101 |\n| 7   | cpu.system      | dc=lax, host=web02.lax.mysite.com    | 0102050102 |\n| 8   | cpu.user        | dc=dal, host=web01.dal.mysite.com    | 0202040101 |\n| 9   | cpu.user        | dc=dal, host=web02.dal.mysite.com    | 0202040102 |\n\nNote that for this example we won't be using any custom value rules so we don't need to show the TSMeta objects, but assume these values populate a TSMeta. Also, the TSUIDs are truncated with 1 byte per UID for illustration purposes.\n\nNow let's setup a tree with `strictMatching` disabled and the following rules:\n\n| Level | Order | Rule Type | Field (value) | Regex                   | Separator |\n|-------|-------|-----------|---------------|-------------------------|-----------|\n| 0     | 0     | TagK      | dc            |                         |           |\n| 0     | 1     | TagK      | host          | .\\*\\\\(.\\*)\\\\mysite\\\\com |           |\n| 1     | 0     | TagK      | host          |                         | \\\\.       |\n| 2     | 0     | Metric    |               |                         | \\\\.       |\n\nThe goal for this set of rules is to order our timeseres by data center, then host, then by metric. Our company may have thousands of servers around the world so it doesn't make sense to display all of them in one branch of the tree, rather we want to group them by data center and let users drill down as needed.\n\nIn our example data, we had some old timeseries that didn't have a `dc` tag name. However the `host` tag does have a fully qualified domain name with the data center name embedded. Thus the first level of our rule set has two rules. The first will look for a `dc` tag, and if found, it will use that tag's value and the second rule is skipped. If the `dc` tag does not exist, then the second rule will scan the `host` tag's value and attempt to extract the data center name from the FQDN. The second level has one rule and that is used to group on the value of the `host` tag so that all metrics belonging to that host can be displayed in branches beneath it. The final level has the metric rule that includes a separator to further group the timeseries by the data contained. Since we have multiple CPU and application metrics, all deliniated by a period, it makes sense to add a separator at this point.\n\n### Result\n\nThe resulting tree would look like this:\n\n- dal\n  - web01.dal.mysite.com\n    - app\n      - connections (tsuid=010101)\n      - errors (tsuid=0101010306)\n    - cpu\n      - system (tsuid=0102040101)\n      - user (tsuid=0202040101)\n  - web02.dal.mysite.com\n    - cpu\n      - system (tsuid=0102040102)\n      - user (tsuid=0202040102)\n  - web03.dal.mysite.com\n    - cpu\n      - system (tsuid=0102040103)\n- lax\n  - web01.lax.mysite.com\n    - cpu\n      - system (tsuid=0102050101)\n  - web02.lax.mysite.com\n    - cpu\n      - system (tsuid=0102050102)\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/trees.html](http://opentsdb.net/docs/build/html/user_guide/trees.html)"
- name: Troubleshooting
  id: user_guide/troubleshooting
  summary: This page lists common issues encountered by users of OpenTSDB along with various troubleshooting steps
  description: "# Troubleshooting\n\nThis page lists common issues encountered by users of OpenTSDB along with various troubleshooting steps. If you run into an issue, please check the [OpenTSDB Google Group](https://groups.google.com/forum/#!forum/opentsdb) or the [Github Issues](https://github.com/OpenTSDB/opentsdb/issues). If you can't find an answer, please include your operating system, TSD version and HBase version in your question.\n\n## OpenTSDB compactions trigger large .tmp files and region server crashes in HBase\n\nThis can be caused if you use millisecond timestamps and write thousands of data points for a single metric in a single hour. In this case, the column qualifier and row key can grow larger than the configured `hfile.index.block.max.size`. In this situation we recommend that you disable TSD compaction code. In the future we will support appends which will allow for compacted columns with small qualifiers.\n\n## TSDs are slow to respond after region splits or over long run times\n\nDuring region splits or region migrations, OpenTSDB's AsyncHBase client will buffer RPCs in memory and attempt to flush them once the regions are back online. Each region has a 10,000 RPC buffer by default and if many regions are down then the RPCs can eventually fill up the TSD heap and cause long garbage collection pauses. If this happens, you can either increase your heap to accommodate more region splits or decrease the NSRE queue size by modifying the `hbase.nsre.high_watermark` config parameter in AsyncHBase 1.7 and OpenTSDB 2.2.\n\n## TSDs are stuck in GC or crashing due to Out of Memory Exceptions\n\nThere are a number of potential causes for this problem including:\n\n- Multiple NSREs from HBase - See the section above about TSDs being slow to respond.\n- Too many writes - If the rate of writes to TSD is high, queues can build up in AsyncHBase (see above) or in the compaction queue. If this is the case, check HBase performance and try disabling compactions.\n- Large queries - A very large query with many time series or for a long range can cause the TSD to OOM. Try reducing query size or break large queries up into smaller chunks.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/troubleshooting.html](http://opentsdb.net/docs/build/html/user_guide/troubleshooting.html)"
- name: tsd
  id: user_guide/cli/tsd
  summary: The TSD command launches the OpenTSDB daemon in the foreground so that it can accept connections over TCP and HTTP
  description: "# tsd\n\nThe TSD command launches the OpenTSDB daemon in the foreground so that it can accept connections over TCP and HTTP. If successful, you should see a number of messages then:\n\n``` python\n2014-02-26 18:33:02,472 INFO  [main] TSDMain: Ready to serve on 0.0.0.0:4242\n```\n\nThe daemon will continue to run until killed via a Telnet or HTTP command is sent to tell it to stop. If an error occurred, such as failure to connect to Zookeeper or the inability to bind to the proper interface and port, an error will be logged and the daemon will exit.\n\nNote that the daemon does not fork and run in the background.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/cli/tsd.html](http://opentsdb.net/docs/build/html/user_guide/cli/tsd.html)"
- name: tsddrain.py
  id: user_guide/utilities/tsddrain
  summary: This is a simple utility for consuming data points from collectors while a TSD, HBase or HDFS is underoing maintenance
  description: "# tsddrain.py\n\nThis is a simple utility for consuming data points from collectors while a TSD, HBase or HDFS is underoing maintenance. The script should be run on the same port as a TSD and accepts data in the `put` Telnet style. Data points are then written directly to disk in a format that can be used with the [*import*](../cli/import) command once HBase is back up.\n\n## Parameters\n\n``` bash\ntsddrain.py <port> <directory>\n```\n\n| Name      | Data Type | Description                                                                                                                                   | Default | Example       |\n|-----------|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------|---------|---------------|\n| port      | Integer   | The TCP port to listen on                                                                                                                     |         | 4242          |\n| directory | String    | Path to a directory where data files should be written. A file is created for each client with the IP address of the client as the file name, |         | /opt/temptsd/ |\n\nExample\n\n``` bash\n./tsddrain.py 4242 /opt/temptsd/\n```\n\n## Results\n\nOn succesfully binding to the default IPv4 address `0.0.0.0` and port it will simply print out the line below and start writing. When you're ready to resume using a TSD, simply kill the process.\n\n``` bash\nUse Ctrl-C to stop me.\n```\n\nWarning\n\nTsddrain does not accept HTTP input at this time.\n\nWarning\n\nTest throughput on your systems to make sure it handles the load properly. Since it writes each point to disk immediately this can result in a huge disk IO load so very large OpenTSDB installations may require a larger number of drains than TSDs.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/utilities/tsddrain.html](http://opentsdb.net/docs/build/html/user_guide/utilities/tsddrain.html)"
- name: uid
  id: user_guide/cli/uid
  summary: The UID utility provides various functions to search or modify information in the tsdb-uid table
  description: "# uid\n\nThe UID utility provides various functions to search or modify information in the `tsdb-uid` table. This includes UID assignments for metrics, tag names and tag values as well as UID meta data, timeseries meta data and tree definitions or data.\n\nUse the UID utility with the command line:\n\n``` python\nuid <subcommands> [arguments]\n```\n\n## Common CLI Parameters\n\nParameters specific to the UID utility include:\n\n## Lookup\n\nThe lookup command is the default for `uid` used to lookup the UID assigned to a name or the name assinged to a UID for a given type.\n\n### Command Format\n\n``` python\n<kind> <name>\n<kind> <UID>\n```\n\n### Example Command\n\n``` python\n./tsdb uid tagk host\n```\n\n### Example Response\n\n``` python\ntagk host: [0, 0, 1]\n```\n\n## grep\n\nThe grep sub command performs a regular expression search for the given UID type and returns a list of all UID names that match the expression. Fields required for the grep command include:\n\n| Name       | Data Type | Description                                                                   | Default | Example      |\n|------------|-----------|-------------------------------------------------------------------------------|---------|--------------|\n| kind       | String    | The type of the UID to search for. Must be one of `metrics`, `tagk` or `tagv` |         | tagk         |\n| expression | String    | The regex expression to search with                                           |         | disk.\\*write |\n\n### Command Format\n\n``` python\ngrep <kind> '<expression>'\n```\n\n### Example Command\n\n``` python\n./tsdb uid grep metrics 'disk.*write'\n```\n\n### Example Response\n\n``` python\nmetrics iostat.disk.msec_write: [0, 3, -67]\nmetrics iostat.disk.write_merged: [0, 3, -69]\nmetrics iostat.disk.write_requests: [0, 3, -70]\nmetrics iostat.disk.write_sectors: [0, 3, -68]\n```\n\n## assign\n\nThis sub command is used to assign IDs to new unique names for metrics, tag names or tag values. Supply a list of one or more values to assign UIDs and the list of assignments will be returned.\n\n| Name | Data Type | Description                                                                                 | Example |\n|------|-----------|---------------------------------------------------------------------------------------------|---------|\n| kind | String    | The type of the UID the names represent. Must be one of `metrics`, `tagk` or `tagv`         | tagk    |\n| name | String    | One or more names to assign UIDs to. Names must not be in quotes and cannot contain spaces. | owner   |\n\n### Command Format\n\n``` python\nassign <kind> <name> [<name>...]\n```\n\n### Example Command\n\n``` python\n./tsdb uid assign metrics disk.d0 disk.d1 disk.d2 disk.d3\n```\n\n### Example Response\n\n## rename\n\nChanges the name of an already assigned UID. If the UID of the given type does not exist, an error will be returned.\n\nNote\n\nAfter changing a UID name you must flush the cache (see [*/api/dropcaches*](../../api_http/dropcaches)) or restart all TSDs for the change to take effect. TSDs do not periodically reload UID maps.\n\n| Name    | Data Type | Description                                                                        | Example      |\n|---------|-----------|------------------------------------------------------------------------------------|--------------|\n| kind    | String    | The type of the UID the name represent. Must be one of `metrics`, `tagk` or `tagv` | tagk         |\n| name    | String    | The existing UID name                                                              | owner        |\n| newname | String    | The new name UID name                                                              | server_owner |\n\n### Command Format\n\n``` python\nrename <kind> <name> <newname>\n```\n\n### Example Command\n\n``` python\n./tsdb uid rename metrics disk.d0 disk.d0.bytes_read\n```\n\n## delete\n\nRemoves the mapping of the UID from the `tsdb-uid` table. Make sure all sources are no longer writing data using the UID and that sufficient time has passed so that users would not query for data that used the UIDs.\n\nNote\n\nAfter deleting a UID, it may still remain in the caches of running TSD servers. Make sure to flush their caches after deleting an entry.\n\nWarning\n\nDeleting a UID will not delete the underlying data associated with the UIDs (we're working on that). For metrics this is safe, it won't affect queries. But for tag names and values, if a query scans over data containing the old UID, the query will fail with an exception because it can no longer find the name mapping.\n\n| Name | Data Type | Description                                                                        | Example |\n|------|-----------|------------------------------------------------------------------------------------|---------|\n| kind | String    | The type of the UID the name represent. Must be one of `metrics`, `tagk` or `tagv` | tagk    |\n| name | String    | The existing UID name                                                              | owner   |\n\n### Command Format\n\n``` python\ndelete <kind> <name>\n```\n\n### Example Command\n\n``` python\n./tsdb uid delete disk.d0\n```\n\n## fsck\n\nThe UID FSCK command will scan the entire UID table for errors pertaining to name and UID mappings. By default, the run will scan every column in the table and log any errors that were found. With version 2.1 it is possible to fix errors in the table by passing the \"fix\" flag. UIDMeta objects are skipped during scanning. Possible errors include:\n\n| Error                                                                                          | Description                                                                                                                                                                                                                                                                                  | Fix                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n|------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Max ID for metrics is 42 but only 41 entries were found. Maybe 1 IDs were deleted?             | This indicates one or more UIDs were not used for mapping entries. If a UID was deleted, this message is normal. If UIDs were not deleted, this can indicate wasted UIDs due to auto-assignments by TSDs where data was coming in too fast. Try assigning UIDs up-front as much as possible. | No fix necessary                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| We found an ID of 42 for metrics but the max ID is only 41! Future IDs may be double-assigned! | If this happens it is usually due to a corruption and indicates the max ID row was not updated properly.                                                                                                                                                                                     | Set the max ID row to the largest detected value                                                                                                                                                                                                                                                                                                                                                                                                              |\n| Invalid maximum ID for metrics: should be on 8 bytes                                           | Indicates a corruption in the max ID row.                                                                                                                                                                                                                                                    | No fix yet.                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| Forward metrics mapping is missing reverse mapping: foo -\\> 000001                             | This may occur if a TSD crashes before the reverse map is written and would only prevent queries from executing against time series using the UID as they would not be able to lookukp the name.                                                                                             | The fix is to restore the missing reverse map.                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Forward metrics mapping bar -\\> 000001 is different than reverse mapping: 000001 -\\> foo       | The reverse map points to a different name than the forward map and this should rarely happen. It will be paired with another message.                                                                                                                                                       | Depends on the second message                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| Inconsistent forward metrics mapping bar -\\> 000001 vs bar -\\> foo / foo -\\> 000001            | With a forward/reverse miss-match, it is possible that a UID was assigned to multiple names for the same type. If this occurs, then data for two different names has been written to the same time series and that data is effectively corrupt.                                              | The fix is to delete the forward maps for all names that map to the same UID. Then the UID is given a new name that is a dot seperated concatenation of the previous names with an \"fsck\" prefix. E.g. in the example above we would have a new name of \"fsck.bar.foo\". This name may be used to access data from the corrupt time series. The next time data is written for the errant names, new UIDs will be assigned to each and new time series created. |\n| Duplicate forward metrics mapping bar -\\> 000002 and null -\\> foo                              | In this case the UID was not used more than once but the reverse mapping was incorrect.                                                                                                                                                                                                      | The reverse map will be restored, in this case: 000002 -\\> bar                                                                                                                                                                                                                                                                                                                                                                                                |\n| Reverse metrics mapping is missing forward mapping: bar -\\> 000002                             | A reverse map was found without a forward map. The UID may have been deleted.                                                                                                                                                                                                                | Remove the reverse map                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Inconsistent reverse metrics mapping 000003 -\\> foo vs 000001 -\\> foo / foo -\\> 000001         | If an orphaned reverse map points to a resolved forward map, this error occurs.                                                                                                                                                                                                              | Remove the reverse map                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n\n**Options**\n\n- fix - Attempts to fix errors per the table above\n- delete_unknown - Removes any columns in the UID table that do not belong to OpenTSDB\n\n### Command Format\n\n``` python\nfsck [fix] [delete_unknown]\n```\n\n### Example Command\n\n``` python\n./tsdb uid fsck fix\n```\n\n### Example Response\n\n``` python\nINFO  [main] UidManager: ----------------------------------\nINFO  [main] UidManager: -  Running fsck in FIX mode  -\nINFO  [main] UidManager: -    Remove Unknowns: false  -\nINFO  [main] UidManager: ----------------------------------\nINFO  [main] UidManager: Maximum ID for metrics: 2\nINFO  [main] UidManager: Maximum ID for tagk: 4\nINFO  [main] UidManager: Maximum ID for tagv: 2\nERROR [main] UidManager: Forward tagk mapping is missing reverse mapping: bar -> 000004\nINFO  [main] UidManager: FIX: Restoring tagk reverse mapping: 000004 -> bar\nERROR [main] UidManager: Inconsistent reverse tagk mapping 000003 -> bar vs 000004 -> bar / bar -> 000004\nINFO  [main] UidManager: FIX: Removed tagk reverse mapping: 000003 -> bar\nERROR [main] UidManager: tagk: Found 2 errors.\nINFO  [main] UidManager: 17 KVs analyzed in 334ms (~50 KV/s)\nWARN  [main] UidManager: 2 errors found.\n```\n\n## metasync\n\nThis command will run through the entire data table, scanning each row of timeseries data and generate missing TSMeta objects and UIDMeta objects or update the created timestamps for each object type if necessary. Use this command after enabling meta tracking with existing data or if you suspect that some timeseries may not have been indexed properly. The command will also push new or updated meta entries to a search engine if a plugin has been configured. If existing meta is corrupted, meaning the TSD is unable to deserialize the object, it will be replaced with a new entry.\n\nIt is safe to run this command at any time as it will not destroy or overwrite valid data. (Unless you modify columns directly in HBase in a manner inconsistent with the meta data formats). The utility will split the data table into chunks processed by multiple threads so the more cores in your processor, the faster the command will complete.\n\n### Command Format\n\n``` python\nmetasync\n```\n\n### Example Command\n\n``` python\n./tsdb uid metasync\n```\n\n## metapurge\n\nThis sub command will mark all TSMeta and UIDMeta objects for deletion in the UID table. This is useful for downgrading from 2.0 to a 1.x version or simply flushing all meta data and starting over with a `metasync`.\n\n### Command Format\n\n``` python\nmetapurge\n```\n\n### Example Command\n\n``` python\n./tsdb uid metapurge\n```\n\n## treesync\n\nRuns through the list of TSMeta objects in the UID table and processes each through all configured and enabled trees to compile branches. This command may be run at any time and will not affect existing objects.\n\n### Command Format\n\n``` python\ntreesync\n```\n\n### Example Command\n\n``` python\n./tsdb uid treesync\n```\n\n## treepurge\n\nRemoves all branches, collision, not matched data and optionally the tree definition itself for a given tree. Parameters include:\n\n| Name       | Data Type | Description                                                                            | Example    |\n|------------|-----------|----------------------------------------------------------------------------------------|------------|\n| id         | Integer   | ID of the tree to purge                                                                | 1          |\n| definition | Flag      | Add this literal after the ID to delete the definition of the tree as well as the data | definition |\n\n### Command Format\n\n``` python\ntreepurge <id> [definition]\n```\n\n### Example Command\n\n``` python\n./tsdb uid treepurge 1\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/cli/uid.html](http://opentsdb.net/docs/build/html/user_guide/cli/uid.html)"
- name: UIDs and TSUIDs
  id: user_guide/uids
  summary: In OpenTSDB, when you write a timeseries data point, it is always associated with a metric and at least one tag name/value pair
  description: "# UIDs and TSUIDs\n\nIn OpenTSDB, when you write a timeseries data point, it is always associated with a metric and at least one tag name/value pair. Each metric, tag name and tag value is assigned a unique identifier (UID) the first time it is encountered or when explicitly assigned via the API or a CLI tool. The combination of metric and tag name/value pairs create a timeseries UID or TSUID.\n\n## UID\n\nTypes of UID objects include:\n\n- **metric** - A metric such as `sys.cpu.0` or `trades.per.second`\n- **tagk** - A tag name such as `host` or `symbol`. This is always the \"key\" (the first value) in a tag key/value pair.\n- **tagv** - A tag value such as `web01` or `goog`. This is always the \"value\" (the second value) in a tag key/value pair.\n\n### Assignment\n\nThe UID is a positive integer that is unique to the name of the UID object and it's type. Within the storage system there is a counter that is incremented for each `metric`, `tagk` and `tagv`. When you create a new `tsdb-uid` table, this counter is set to 0 for each type. So if you put a new data point with a metric of `sys.cpu.0` and a tag pair of `host=web01` you will have 3 new UID objects, each with a UID of 1.\n\nUIDs are assigned automatically for new `tagk` and `tagv` objects when data points are written to a TSD. `metric` objects also receive new UIDs but only if the *auto metric* setting has been configured to `true`. Otherwise data points with new metrics are rejected. The UIDs are looked up in a cached map for every incoming data point. If the lookup fails, then the TSD will attempt to assign a new UID.\n\n### Storage\n\nBy default, UIDs are encoded on 3 bytes in storage, giving a maximum unique ID of 16,777,215 for each UID type. This is done to reduce the amount of space taken up in storage and to reduce the memory footprint of a TSD. For the vast majority of users, 16 million unique metrics, 16 million unique tag names and 16 million unique tag values should be enough. But if you do need more of a particular type, you can modify the OpenTSDB source code and recompile with 4 bytes or more. As of version 2.2 you can override the UID size via the config file.\n\nWarning\n\nIf you do adjust the byte encoding number, you must start with a fresh `tsdb` and fresh `tsdb-uid` table, otherwise the results will be unexpected. If you have data in an existing setup, you must export it, drop all tables, create them from scratch and re-import the data.\n\n### Display\n\nUIDs can be displayed in a few ways. The most common method is via the HTTP API where the 3 bytes of UID data are encoded as a hexadecimal string. For example, the UID of `1` would be written in binary as `000000000000000000000001`. As an array of unsigned byte values, you could imagine it as `[0,`` ``0,`` ``1]`. Encoded as a hex string, the value would be `000001` where the string is padded with 0s for each byte. The UID of 255 would result in a hex value of `0000FF` (or as a byte array, `[0,`` ``0,`` ``255]`. To convert between a decimal UID to a hex, use any kind of hex conversion tool you prefer and put 0s in front of the resulting value until you have a total of 6 characters. To convert from a hex UID to decimal, simply drop any 0s from the front, then use a tool to convert the hex string to a decimal.\n\nIn some CLI tools and log files, a UID may be displayed as an array of signed bytes (thanks to Java) such as the above example of `[0,`` ``0,`` ``1]` or `[0,`` ``0,`` ``-28]`. To convert from this signed array to an an array of unsigned bytes, then to hex. For example, `-28` would be binary `10011100` which results in a decimal value of `156` and a hex value of `9C`.\n\n### Modification\n\nUIDs can be renamed or deleted. Renaming can be accomplished via the CLI and is generally safe but will affect EVERY time series that includes the renamed ID. E.g. if we have a series `sys.cpu.user`` ``host=web01` and another `apache.requests`` ``host=web01` and rename the `web01` tag value to `web01.mysite.org`, then both series will now reflect the new host name and all queries referring to the old name must be updated.. If a data point comes in that has the previous string, a new UID will be assigned.\n\nDeleting UIDs can be tricky as of version 2.2. Deleting a metric is safe in that users may no longer query for the data and it won't show up in calls to the suggest API. However deleting a tag name or value can cause queries to fail. E.g. if you have time series for the metric `sys.cpu.user` with hosts `web01`, `web02`, `web03`, etc. and you delete the UID for `web02`, any query that would scan over data that includes the series `sys.cpu.user`` ``host=web02` will throw an exception to the user because the data remains in storage. We highly recommend you run an FSCK with a query to repair such issues.\n\n### Why UIDs?\n\nThis question is asked often enough it's worth laying out the reasons here. Looking up or assigning a UID takes up precious cycles in the TSD so folks wonder if it wouldn't be faster to use the raw name of the metric or computer a hash. Indeed, from a write perspective it would be slightly faster, but there are a number of drawbacks that become apparent.\n\n## Raw Names\n\nSince OpenTSDB uses HBase as the storage layer, you could use strings as the row key. Following the current schema, you may have a row key that looked like `sys.cpu.0.user`` ``1292148000`` ``host=websv01.lga.mysite.com`` ``owner=operations`. Ordering would be similar to the existing schema, but now you're using up 70 bytes of storage each hour instead of 19. Additionally, the row key must be written and returned with every query to HBase, so you're increasing your network usage as well. So resorting to UIDs can help save space.\n\n## Hashes\n\nAnother idea is to simply bump up the UIDs to 4 bytes then calculate a hash on the strings and store the hash with forward and reverse maps as we currently do. This would certainly reduce the amount of time it takes to assign a UID, but there are a few problems. First, you will encounter collisions where different names return the same hash. You could try different algorithms and even try increasing the hash to 8 bytes, but you'll always have the issue of colliding hashes. Second, you are now adding a hash calculation to every data put since it would have to determine the hash, then lookup the hash in the UID table to see if it's been mapped yet. Right now, each data point only performs the lookup. Third, you can't pre-split your HBase regions as easily. If you know you will have roughly 800 metrics in your system (the tags are irrelevant for this purpose), you can pre-split your HBase table to evenly distribute those 800 metrics and increase your initial write performance.\n\n## TSUIDs\n\nWhen a data point is written to OpenTSDB, the row key is formatted as `<metric_UID><timestamp><tagk1_UID><tagv1_UID>[...<tagkN_UID><tagvN_UID>]`. By simply dropping the timestamp from the row key, we have a long array of UIDs that combined, form a unique timeseries ID. Encoding the bytes as a hex string will give us a useful TSUID that can be passed around various API calls. Thus from our UID example above where each metric, tag name and value has a UID of 1, our TSUID, encoded as a hexadecimal string, would be `000001000001000001`.\n\nWhile this TSUID format may be long and ugly, particularly with all of the 0s for early UIDs, there are a few reasons why this is useful:\n\n- If you know the width of each UID (by default 3 bytes as stated above), then you can easily parse the UID for each metric, tag name and value from the UID string.\n- Assigning a unique numeric ID for each timeseries creates issues with lock contention and/or synchronization issues where a timeseries may be missed if the UID could not be incremented.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/uids.html](http://opentsdb.net/docs/build/html/user_guide/uids.html)"
- name: Understanding Metrics and Time Series
  id: user_guide/query/timeseries
  summary: OpenTSDB is a time series database
  description: "# Understanding Metrics and Time Series\n\nOpenTSDB is a time series database. A time series is a series of numeric data points of some particular metric over time. Each time series consists of a metric plus one or more tags associated with this metric (we'll cover tags in a bit). A metric is any particular piece of data (e.g. hits to an Apache hosted file) that you wish to track over time.\n\nOpenTSDB is also a data plotting system. OpenTSDB plots things a bit differently than other systems. We'll discuss plotting in more detail below, but for now it's important to know that for OpenTSDB, the basis of any given plot is the metric. It takes that metric, finds all of the time series for the time range you select, aggregates those times series together (e.g. by summing them up) and plots the result. The plotting mechanism is very flexible and powerful and you can do much more than this, but for now let's talk about the key to the time series, which is the metric.\n\nIn OpenTSDB, a metric is named with a string, like `http.hits`. To be able to store all the different values for all the places where this metric exists, you tag the data with one or more tags when you send them to the TSD. TSD stores the timestamp, the value, and the tags. When you want to retrieve this data, TSD retrieves all of the values for the time span you supply, optionally with a tag filter you supply, aggregates all these values together how you want, and plots a graph of this value over time.\n\nThere's a bunch of things in here that we've introduced so far. To help you understand how things work, I'll start with a typical example. Let's say you have a bunch of web servers and you want to track two things: hits to the web server and load average of the system. Let's make up metric names to express this. For load average, let's call it `proc.loadavg.1min` (since on Linux you can easily get this data by reading `/proc/loadavg`). For many web servers, there is a way to ask the web server for a counter expressing the number of hits to the server since it started. This is a convenient counter upon which to use for a metric we'll call `http.hits`. I chose these two examples for two reasons. One, we'll get to see how OpenTSDB easily handles both counters (values that increase over time, except when they get reset by a restart/reboot or overflow) and how it handles normal values that go up and down, like load average. A great advantage of OpenTSDB is that you don't need to do any rate calculation of your counters. It will do it all for you. The second reason is that we can also show you how you can plot two different metrics with different scales on the same graph, which is a great way to correlate different metrics.\n\n## Your first datapoints\n\nWithout going into too much detail on how collectors send data to the TSD , you write a collector that periodically sends the current value of these datapoints for each server to the TSD. So the TSD can aggregate the data from multiple hosts, you tag each value with a \"host\" tag. So, if you have web servers A, B, C, etc, they each periodically send something like this to the TSD:\n\n``` python\nput http.hits 1234567890 34877 host=A\nput proc.loadavg.1min 1234567890 1.35 host=A\n```\n\nHere \"1234567890\" is the current epoch time (date +%s) in seconds. The next number is the value of the metric at this time. This is data from host A, so it's tagged with `host=A`. Data from host B would be tagged with `host=B`, and so forth. Over time, you'll get a bunch of time series stored in OpenTSDB.\n\n## Your first plot\n\nNow, let's revisit what we talked about here at the beginning. A time series is a series of datapoints of some particular metric (and its tags) over time. For this example, each host is sending two time series to the TSD. If you had 3 boxes each sending these two time series, TSD would be collecting and storing 6 time series. Now that you have the data, let's start plotting.\n\nTo plot HTTP hits, you just go to the UI and enter `http.hits` as your metric name, and enter the time range. Check the \"Rate\" button since this particular metric is a rate counter, and voil?, you have a plot of the rate of HTTP hits to your web servers over time.\n\n## Aggregators\n\nThe default for the UI is to aggregate each time series for each host by adding them together (sum). What this means is, TSD is taking the three time series with this metric (host=A, B and C) and adding their values together to come up with the total hits by all web servers at a given time . Note you don't need to send your datapoints at exactly the same time, the TSD will figure it out. So, if each of your hosts was serving 1000 hits per second each at some point in time, the graph would show 3000. What if you wanted to show about how many hits each web server was serving? Two ways. If you just care about the average that each web server was serving, just change the Aggregator method from sum to avg. You can also try the others (max, min) to see the maximum or minimum value. More aggregation functions are in the works (percentiles, etc.). This is done on a per-interval basis , so if at some point in time one of your webservers was serving 50 QPS and the others were serving 100 and later a different webserver was serving 50 QPS and the others were serving 100, for these two points the Min would be 50. In other words it doesn't figure out which time series was the total minimum and just show you that host plot. The other way to see how many hits each web server is serving? This is where we look at the tag fields.\n\n## Downsampling\n\nTo reduce the number of datapoints returned, you can specify a downsampling interval and method, such as 1h-avg or 1d-sum. This is also useful (such as when using the max and min) to find best and worst-case datapoints over a given period. Downsampling is most useful to make the graphing phase less intensive and more readable, especially when graphing more datapoints than screen pixels.\n\n## Tag Filters\n\nIn the UI you'll see that the TSD has filled one or more \"Tags\", the first one is host. What TSD is saying here that for this time range it sees that the data was tagged with a host tag. You can filter the graph so that it just plots the value of one host. If you fill in A in the host row, you'll just plot the values over time of host A. If you want to give a list of hosts to plot, fill in the list of hosts separated by the pipe symbol, e.g. A\\|B. This will give you two plots instead of one, one for A and one for B. Finally, you can also specify the special character [\\*](#id1), which means to plot a line for every host.\n\n## Adding More Metrics\n\nSo, now you have a plot of your web hits. How does that correlate against load average? On this same graph, click the \"+\" tab to add a new metric to this existing graph. Enter proc.loadavg.1min as your metric and click \"Right Axis\" so the Y axis is scaled separately and its labels on the right. Make sure \"Rate\" is unchecked, since load average is not a counter metric. Voil?! Now you can see how changes in the rate of web hits affects your system's load average.\n\n## Getting Fancy\n\nImagine each if your servers actually ran two webservers, say, one for static content and one for dynamic content. Rather than create another metric, just tag the http.hits metric with the server instance. Have your collector send stuff like:\n\n`put`` ``http.hits`` ``1234567890`` ``34877`` ``host=A`` ``webserver=static`` ``put`` ``http.hits`` ``1234567890`` ``4357`` ``host=A`` ``webserver=dynamic`` ``put`` ``proc.loadavg.1min`` ``1234567890`` ``1.35`` ``host=A`\n\nWhy do this instead of creating another metric? Well, what if sometimes you care about plotting total HTTP hits and sometimes you care about breaking out static vs. dynamic hits? With a tag, it's easy. With this new tag, you'll see a webserver tag appear in the UI when plotting this metric. You can leave it blank and it will aggregate up both values into one plot (according to your Aggregator setting) and you can see the total hits, or you can do webserver=\\* to break out how much each of your static and dynamic instances are collectively doing across your web servers. You can even go deeper and specify webserver=\\* and host=\\* to see the full breakdown.\n\n## Guidelines When to Create Metrics\n\nRight now, you cannot combine two metrics into one plot line. This means you want a metric to be the biggest possible aggregation point. If you want to drill down to specifics within a metric, use tags.\n\n## Tags vs. Metrics\n\nThe metric should be a specific thing, like \"Ethernet packets\" but not be broken out into a particular instance of a thing. Generally you don't want to collect a metric like net.bytes.eth0, net.bytes.eth1, etc. Collect net.bytes and tag eth0 datapoints with iface=eth0, etc. Don't bother creating separate \"in\" and \"out\" metrics, either. Add the tag direction=in or direction=out. This way you can easily see the total network activity for a given box without having to plot a bunch of metrics. This still gives you the flexibility to drill down and just show activity for a particular interface, or just a particular direction.\n\n## Counters and Rates\n\nIf something is a counter, or is naturally something that is a rate, don't convert it to a rate before sending it to the TSD. There's two main reasons for this. First, doing your own rate calculation, reset/overflow handling, etc. is silly, since TSD can do it for you. You also don't have to worry about getting the units-per-second calculation correct based on a slightly inaccurate or changing sample interval. Secondly, if something happens where you lose a datapoint or more, if you are sending the current counter value then you won't lose data, just resolution of that data. The golden rule in TSD is, if your source data is a counter (some counter out of /proc or SNMP), keep it that way. Don't convert it. If you're writing your own collector (say, one that counts how often a particular error message appears in a tail -f of a log), don't reset your counter every sample interval. Let TSD to do the work for you.\n\n## Tags are your Friend\n\nIn anything above a small environment, you probably have clusters or groups of machines doing the same thing. Over time these change, though. That's OK. Just use a tag when you send the data to TSD to pass this cluster info along. Add something like cluster=webserver to all the datapoints being sent from each of your webservers, and cluster=db for all your databases, etc.\n\nNow when you plot CPU activity for your webserver cluster, you see all of them aggregated into one plot. Then let's say you add a webserver or even change it from a webserver to a database. All you have to do is make sure the right tag gets sent when its role changes, and now that box's CPU activity gets counted toward the right cluster. What's more, all of your historical data is still correct! This is the true power of OpenTSDB. Not only do you never lose resolution of your datapoints over time like RRD-based systems, but historical data doesn't get lost as your boxes shift around. You also don't have to put a bunch of cluster or grouping awareness logic into your dashboards.\n\n## Precisions on Metrics and Tags\n\nThe maximum number of tags allowed on a data point is defined by a constant (Const.MAX_NUM_TAGS), which at time of writing is 8. Metric names, tag names and tag values have to be made of alpha numeric characters, dash \"-\", underscore \"\\_\", period \".\", and forward slash \"/\", as is enforced by the package-private function Tags.validateString.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/query/timeseries.html](http://opentsdb.net/docs/build/html/user_guide/query/timeseries.html)"
- name: User Guide
  id: user_guide/index
  summary: These pages serve as a user and administration guide
  description: "# User Guide\n\nThese pages serve as a user and administration guide. We highly recommend that you start with the `writing` and [*Querying or Reading Data*](query/index) sections to understand how OpenTSDB handles its core purpose of storing and serving time series information. Then follow up with the [*Quick Start*](quickstart) section to play around with getting some data into your OpenTSDB instance. Finally follow up with the other pages for details on the other features of OpenTSDB.\n\n- [Configuration](configuration)\n- [Writing Data](writing/index)\n- [Querying or Reading Data](query/index)\n- [Rollup And Pre-Aggregates](rollups)\n- [UIDs and TSUIDs](uids)\n- [Metadata](metadata)\n- [Trees](trees)\n- [GUI](guis/index)\n- [Plugins](plugins)\n- [Stats](stats)\n- [Definitions](definitions)\n- [Storage](backends/index)\n- [CLI Tools](cli/index)\n- [Utilities](utilities/index)\n- [Logging](logging)\n- [Troubleshooting](troubleshooting)\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/index.html](http://opentsdb.net/docs/build/html/user_guide/index.html)"
- name: Utilities
  id: user_guide/utilities/index
  summary: This page lists some of the utilities or projects included with OpenTSDB or maintained by the OpenTSDB group
  description: "# Utilities\n\nThis page lists some of the utilities or projects included with OpenTSDB or maintained by the OpenTSDB group. Additional utilities, such as front-ends, clients and publishers can be found on the [*Additional Resources*](../../resources) page or via a simple Google search.\n\n- [TCollector](tcollector)\n- [Collectors bundled with `tcollector`](tcollector#collectors-bundled-with-tcollector)\n- [clean_cache.sh](clean_cache)\n- [tsddrain.py](tsddrain)\n- [Load Balancing with Varnish](varnish)\n- [Alerting with Nagios](nagios)\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/utilities/index.html](http://opentsdb.net/docs/build/html/user_guide/utilities/index.html)"
- name: version
  id: api_telnet/version
  summary: This command is similar to the HTTP /api/version endpoint in that it will return information about the currently running version of OpenTSDB
  description: "# version\n\nThis command is similar to the HTTP [*/api/version*](../api_http/version) endpoint in that it will return information about the currently running version of OpenTSDB. This command does not modify TSD in any way.\n\n## Request\n\nThe command format is:\n\n``` python\nversion\n```\n\n## Response\n\nA set of lines with version information.\n\n### Example\n\n``` python\nnet.opentsdb.tools BuildData built at revision a7a0980 (MODIFIED)\nBuilt on 2016/11/03 19:35:50 +0000 by clarsen@tsdvm:/Users/clarsen/Documents/opentsdb/opentsdb_dev\n```\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/api_telnet/version.html](http://opentsdb.net/docs/build/html/api_telnet/version.html)"
- name: What's New
  id: new
  summary: OpenTSDB has a thriving community who contributed and requested a number of new features
  description: "# What's New\n\nOpenTSDB has a thriving community who contributed and requested a number of new features.\n\n## 3.X (Planned)\n\nWhile 3.0 is still a ways off, we'll be pushing some of the new features into a new branch of the repo. Some are in progress and other features are planned. If you have any features that you want to see, let us know.\n\n- Distributed Queries - Based on the great work of Turn on [Splicer](https://github.com/turn/splicer) we have a distributed query layer to split queries amongst multiple TSDs for greater throughput.\n- Query Caching - Improve queries with time-sharded caching of results.\n- Improved Expressions - Perform group by, downsampling and arithmetic modifications in any order. Potentially support UDFs as well.\n- Anomaly Processing/Forecasting - Integrate with modeling libraries (such as [EGADs](https://github.com/yahoo/egads)) for deeper time series analysis.\n\n## 2.4 (Planned)\n\n- Rollup/Pre-Aggregates - Support for storing and querying time-based rolled up data and/or pre-aggregated values.\n- Distributed Percentile - Store histograms (or sketches) for calculating proper percentiles over multiple sources.\n\n## 2.3\n\n- Expressions - Query time computations using time series data. For example, dividing one metric by another.\n- Graphite Style Functions - Additional filtering and mutation of data at query time using Graphite style functions.\n- Calendar Based Downsampling - The ability to align downsampled data on Gregorian calendar boundaries.\n- Bigtable Support - Run TSDB in the cloud using Google's hosted Bigtable service.\n- Cassandra Support - Support for running OpenTSDB on legacy Cassandra clusters.\n- Write Filters - Block or allow time series or UID assignments based on plugins or whitelists.\n- New Aggregators - None for returning raw data. First and Last to return the first or last data points during downsampling.\n- Meta Data Cache Plugin - A new API for caching meta data to improve query performance.\n- Startup Plugins - APIs to help with service discovery on TSD startup.\n- Example Java API usage classes.\n\n## 2.2\n\n- Appends - Support writing all data points for an hour in a single column. This saves the need for TSD compactions and reduces network traffic at query time.\n- Salting - Enables greater distribution of writes for high cardinality metrics as well as asynchronous scanning for improved query speed. (Non backwards compatible)\n- Random Metric UIDs - Enables better distribution of writes when creating new metrics\n- Storage Exception Plugin - Enables various handling of data points when HBase is unavailable\n- Secure AsyncHBase - Access HBase clusters requiring Kerberos or simple authentication along with optional encryption.\n- Fill Policy - Enable emitting NaNs or Nulls via the JSON query endpoint when data points are \"missing\"\n- Count and Percentiles - New aggregator functions\n- More Stats - Gives greater insight into query performance via the query stats endpoint and new stats for threads, region clients and the JVM\n- Annotations - Scan for multiple annotations only via the /api/annotations endpoint\n- Query Filters - New filters for flexibility including case (in)sensitive literals, wildcards and regular expressions.\n- Override Tag Widths - You can now override tag widths in the config instead of having to recompile the code.\n- Compaction Tuning - New parameters allow for tuning the TSD compaction process.\n- Delete Data And UIDs - Allow for deleting data at query time as well as removing UIDs from the system.\n- Synchronous Writing - The HTTP Put API now supports synchronous writing to make sure data is flushed to HBase.\n- Query Stats - Query details are now logged that include timing statistics. A new endpoint also shows running and completed queries.\n\n## 2.1\n\n- Downsampling - Timestamps are now aligned on modulus boundaries, reducing the need to interpolation across series.\n- Last Data Point API - Query for the last data point for specific time series within a certain time window\n- Duplicates - Handle duplicate data points at query time or during FSCK\n- FSCK - An updated FSCK utility that iterates over the main data table, finding and fixing errors\n- Read/Write Modes - Block assigning UIDs on individual TSDs for backup clusters\n- UID Cache - Preload portions of the UID table on startup to improve writes\n\n## 2.0\n\n- Lock-less UID Assignment - Drastically improves write speed when storing new metrics, tag names, or values\n- Restful API - Provides access to all of OpenTSDB's features as well as offering new options, defaulting to JSON\n- Cross Origin Resource Sharing - For the API so you can make AJAX calls easily\n- Store Data Via HTTP - Write data points over HTTP as an alternative to Telnet\n- Configuration File - A key/value file shared by the TSD and command line tools\n- Pluggable Serializers - Enable different inputs and outputs for the API\n- Annotations - Record meta data about specific time series or data points\n- Meta Data - Record meta data for each time series, metrics, tag names, or values\n- Trees - Flatten metric and tag combinations into a single name for navigation or usage with different tools\n- Search Plugins - Send meta data to search engines to delve into your data and figure out what's in your database\n- Real-Time Publishing Plugin - Send data to external systems as they arrive to your TSD\n- Ingest Plugins - Accept data points in different formats\n- Millisecond Resolution - Optionally store data with millisecond precision\n- Variable Length Encoding - Use less storage space for smaller integer values\n- Non-Interpolating Aggregation Functions - For situations where you require raw data\n- Rate Counter Calculations - Handle roll-over and anomaly supression\n- Additional Statistics - Including the number of UIDs assigned and available\n\nThank you to everyone who has contributed to 2.3. Help us out by sharing your ideas and code at [GitHub](https://github.com/OpenTSDB)\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/new.html](http://opentsdb.net/docs/build/html/new.html)"
- name: Writing Data
  id: user_guide/writing/index
  summary: You may want to jump right in and start throwing data into your TSD, but to really take advantage of OpenTSDB's power and flexibility, you may want to pause and think about your naming schema
  description: "# Writing Data\n\nYou may want to jump right in and start throwing data into your TSD, but to really take advantage of OpenTSDB's power and flexibility, you may want to pause and think about your naming schema. After you've done that, you can proceed to pushing data over the Telnet or HTTP APIs, or use an existing tool with OpenTSDB support such as 'tcollector'.\n\n## Naming Schema\n\nMany metrics administrators are used to supplying a single name for their time series. For example, systems administrators used to RRD-style systems may name their time series `webserver01.sys.cpu.0.user`. The name tells us that the time series is recording the amount of time in user space for cpu `0` on `webserver01`. This works great if you want to retrieve just the user time for that cpu core on that particular web server later on.\n\nBut what if the web server has 64 cores and you want to get the average time across all of them? Some systems allow you to specify a wild card such as `webserver01.sys.cpu.*.user` that would read all 64 files and aggregate the results. Alternatively, you could record a new time series called `webserver01.sys.cpu.user.all` that represents the same aggregate but you must now write '64 + 1' different time series. What if you had a thousand web servers and you wanted the average cpu time for all of your servers? You could craft a wild card query like `*.sys.cpu.*.user` and the system would open all 64,000 files, aggregate the results and return the data. Or you setup a process to pre-aggregate the data and write it to `webservers.sys.cpu.user.all`.\n\nOpenTSDB handles things a bit differently by introducing the idea of 'tags'. Each time series still has a 'metric' name, but it's much more generic, something that can be shared by many unique time series. Instead, the uniqueness comes from a combination of tag key/value pairs that allows for flexible queries with very fast aggregations.\n\nNote\n\nEvery time series in OpenTSDB must have at least one tag.\n\nTake the previous example where the metric was `webserver01.sys.cpu.0.user`. In OpenTSDB, this may become `sys.cpu.user`` ``host=webserver01,`` ``cpu=0`. Now if we want the data for an individual core, we can craft a query like `sum:sys.cpu.user{host=webserver01,cpu=42}`. If we want all of the cores, we simply drop the cpu tag and ask for `sum:sys.cpu.user{host=webserver01}`. This will give us the aggregated results for all 64 cores. If we want the results for all 1,000 servers, we simply request `sum:sys.cpu.user`. The underlying data schema will store all of the `sys.cpu.user` time series next to each other so that aggregating the individual values is very fast and efficient. OpenTSDB was designed to make these aggregate queries as fast as possible since most users start out at a high level, then drill down for detailed information.\n\n### Aggregations\n\nWhile the tagging system is flexible, some problems can arise if you don't understand the querying side of OpenTSDB, hence the need for some forethought. Take the example query above: `sum:sys.cpu.user{host=webserver01}`. We recorded 64 unique time series for `webserver01`, one time series for each of the CPU cores. When we issued that query, all of the time series for metric `sys.cpu.user` with the tag `host=webserver01` were retrieved, averaged, and returned as one series of numbers. Let's say the resulting average was `50` for timestamp `1356998400`. Now we were migrating from another system to OpenTSDB and had a process that pre-aggregated all 64 cores so that we could quickly get the average value and simply wrote a new time series `sys.cpu.user`` ``host=webserver01`. If we run the same query, we'll get a value of `100` at `1356998400`. What happened? OpenTSDB aggregated all 64 time series *and* the pre-aggregated time series to get to that 100. In storage, we would have something like this:\n\n``` python\nsys.cpu.user host=webserver01    1356998400  50\nsys.cpu.user host=webserver01,cpu=0  1356998400  1\nsys.cpu.user host=webserver01,cpu=1  1356998400  0\nsys.cpu.user host=webserver01,cpu=2  1356998400  2\nsys.cpu.user host=webserver01,cpu=3  1356998400  0\n...\nsys.cpu.user host=webserver01,cpu=63 1356998400  1\n```\n\nOpenTSDB will *automatically* aggregate *all* of the time series for the metric in a query if no tags are given. If one or more tags are defined, the aggregate will 'include all' time series that match on that tag, regardless of other tags. With the query `sum:sys.cpu.user{host=webserver01}`, we would include `sys.cpu.user`` ``host=webserver01,cpu=0` as well as `sys.cpu.user`` ``host=webserver01,cpu=0,manufacturer=Intel`, `sys.cpu.user`` ``host=webserver01,foo=bar` and `sys.cpu.user`` ``host=webserver01,cpu=0,datacenter=lax,department=ops`. The moral of this example is: *be careful with your naming schema*.\n\n### Time Series Cardinality\n\nA critical aspect of any naming schema is to consider the cardinality of your time series. Cardinality is defined as the number of unique items in a set. In OpenTSDB's case, this means the number of items associated with a metric, i.e. all of the possible tag name and value combinations, as well as the number of unique metric names, tag names and tag values. Cardinality is important for two reasons outlined below.\n\n**Limited Unique IDs (UIDs)**\n\nThere is a limited number of unique IDs to assign for each metric, tag name and tag value. By default there are just over 16 million possible IDs per type. If, for example, you ran a very popular web service and tried to track the IP address of clients as a tag, e.g. `web.app.hits`` ``clientip=38.26.34.10`, you may quickly run into the UID assignment limit as there are over 4 billion possible IP version 4 addresses. Additionally, this approach would lead to creating a very sparse time series as the user at address `38.26.34.10` may only use your app sporadically, or perhaps never again from that specific address.\n\nThe UID limit is usually not an issue, however. A tag value is assigned a UID that is completely disassociated from its tag name. If you use numeric identifiers for tag values, the number is assigned a UID once and can be used with many tag names. For example, if we assign a UID to the number `2`, we could store timeseries with the tag pairs `cpu=2`, `interface=2`, `hdd=2` and `fan=2` while consuming only 1 tag value UID (`2`) and 4 tag name UIDs (`cpu`, `interface`, `hdd` and `fan`).\n\nIf you think that the UID limit may impact you, first think about the queries that you want to execute. If we look at the `web.app.hits` example above, you probably only care about the total number of hits to your service and rarely need to drill down to a specific IP address. In that case, you may want to store the IP address as an annotation. That way you could still benefit from low cardinality but if you need to, you could search the results for that particular IP using external scripts. (Note: Support for annotation queries is expected in a *future* version of OpenTSDB.)\n\nIf you desperately need more than 16 million values, you can increase the number of bytes that OpenTSDB uses to encode UIDs from 3 bytes up to a maximum of 8 bytes. This change would require modifying the value in source code, recompiling, deploying your customized code to all TSDs which will access this data, and maintaining this customization across all future patches and releases.\n\nWarning\n\nIt is possible that your situation requires this value to be increased. If you choose to modify this value, you must start with fresh data and a new UID table. Any data written with a TSD expecting 3-byte UID encoding will be incompatible with this change, so ensure that all of your TSDs are running the same modified code and that any data you have stored in OpenTSDB prior to making this change has been exported to a location where it can be manipulated by external tools. See the `TSDB.java` file for the values to change.\n\n**Query Speed**\n\nCardinality also affects query speed a great deal, so consider the queries you will be performing frequently and optimize your naming schema for those. OpenTSDB creates a new row per time series per hour. If we have the time series `sys.cpu.user`` ``host=webserver01,cpu=0` with data written every second for 1 day, that would result in 86400 rows of data. However if we have 8 possible CPU cores for that host, now we have 691200 rows of data. This looks good because we can get easily a sum or average of CPU usage across all cores by issuing a query like `start=1d-ago&m=avg:sys.cpu.user{host=webserver01}`.\n\nHowever what if we have 20,000 hosts, each with 8 cores? Now we will have 3.8 million rows per day due to a high cardinality of host values. Queries for the average core usage on host `webserver01` will be slower as it must pick out 691200 rows out of 3.8 million.\n\nThe benefits of this schema are that you have very deep granularity in your data, e.g., storing usage metrics on a per-core basis. You can also easily craft a query to get the average usage across all cores an all hosts: `start=1d-ago&m=avg:sys.cpu.user`. However queries against that particular metric will take longer as there are more rows to sift through.\n\nHere are some common means of dealing with cardinality:\n\n**Pre-Aggregate** - In the example above with `sys.cpu.user`, you generally care about the average usage on the host, not the usage per core. While the data collector may send a separate value per core with the tagging schema above, the collector could also send one extra data point such as `sys.cpu.user.avg`` ``host=webserver01`. Now you have a completely separate timeseries that would only have 24 rows per day and with 20K hosts, only 480K rows to sift through. Queries will be much more responsive for the per-host average and you still have per-core data to drill down to separately.\n\n**Shift to Metric** - What if you really only care about the metrics for a particular host and don't need to aggregate across hosts? In that case you can shift the hostname to the metric. Our previous example becomes `sys.cpu.user.websvr01`` ``cpu=0`. Queries against this schema are very fast as there would only be 192 rows per day for the metric. However to aggregate across hosts you would have to execute multiple queries and aggregate outside of OpenTSDB. (Future work will include this capability).\n\n### Naming Conclusion\n\nWhen you design your naming schema, keep these suggestions in mind:\n\n- Be consistent with your naming to reduce duplication. Always use the same case for metrics, tag names and values.\n- Use the same number and type of tags for each metric. E.g. don't store `my.metric`` ``host=foo` and `my.metric`` ``datacenter=lga`.\n- Think about the most common queries you'll be executing and optimize your schema for those queries\n- Think about how you may want to drill down when querying\n- Don't use too many tags, keep it to a fairly small number, usually up to 4 or 5 tags (By default, OpenTSDB supports a maximum of 8 tags).\n\n## Data Specification\n\nEvery time series data point requires the following data:\n\n- metric - A generic name for the time series such as `sys.cpu.user`, `stock.quote` or `env.probe.temp`.\n- timestamp - A Unix/POSIX Epoch timestamp in seconds or milliseconds defined as the number of seconds that have elapsed since January 1st, 1970 at 00:00:00 UTC time. Only positive timestamps are supported at this time.\n- value - A numeric value to store at the given timestamp for the time series. This may be an integer or a floating point value.\n- tag(s) - A key/value pair consisting of a `tagk` (the key) and a `tagv` (the value). Each data point must have at least one tag.\n\n### Timestamps\n\nData can be written to OpenTSDB with second or millisecond resolution. Timestamps must be integers and be no longer than 13 digits (See first \\[NOTE\\] below). Millisecond timestamps must be of the format `1364410924250` where the final three digits represent the milliseconds. Applications that generate timestamps with more than 13 digits (i.e., greater than millisecond resolution) must be rounded to a maximum of 13 digits before submitting or an error will be generated.\n\nTimestamps with second resolution are stored on 2 bytes while millisecond resolution are stored on 4. Thus if you do not need millisecond resolution or all of your data points are on 1 second boundaries, we recommend that you submit timestamps with 10 digits for second resolution so that you can save on storage space. It's also a good idea to avoid mixing second and millisecond timestamps for a given time series. Doing so will slow down queries as iteration across mixed timestamps takes longer than if you only record one type or the other. OpenTSDB will store whatever you give it.\n\nNote\n\nWhen writing to the telnet interface, timestamps may optionally be written in the form `1364410924.250`, where three digits representing the milliseconds are placed after a period. Timestamps sent to the `/api/put` endpoint over HTTP *must* be integers and may not have periods. Data with millisecond resolution can only be extracted via the `/api/query` endpoint or CLI command at this time. See `query/index` for details.\n\nNote\n\nProviding millisecond resolution does not necessarily mean that OpenTSDB supports write speeds of 1 data point per millisecond over many time series. While a single TSD may be able to handle a few thousand writes per second, that would only cover a few time series if you're trying to store a point every millisecond. Instead OpenTSDB aims to provide greater measurement accuracy and you should generally avoid recording data at such a speed, particularly for long running time series.\n\n### Metrics and Tags\n\nThe following rules apply to metric and tag values:\n\n- Strings are case sensitive, i.e. \"Sys.Cpu.User\" will be stored separately from \"sys.cpu.user\"\n- Spaces are not allowed\n- Only the following characters are allowed: `a` to `z`, `A` to `Z`, `0` to `9`, `-`, `_`, `.`, `/` or Unicode letters (as per the specification)\n\nMetric and tags are not limited in length, though you should try to keep the values fairly short.\n\n### Integer Values\n\nIf the value from a `put` command is parsed without a decimal point (`.`), it will be treated as a signed integer. Integers are stored, unsigned, with variable length encoding so that a data point may take as little as 1 byte of space or up to 8 bytes. This means a data point can have a minimum value of -9,223,372,036,854,775,808 and a maximum value of 9,223,372,036,854,775,807 (inclusive). Integers cannot have commas or any character other than digits and the dash (for negative values). For example, in order to store the maximum value, it must be provided in the form `9223372036854775807`.\n\n### Floating Point Values\n\nIf the value from a `put` command is parsed with a decimal point (`.`) it will be treated as a floating point value. Currently all floating point values are stored on 4 bytes, single-precision, with support for 8 bytes planned for a future release. Floats are stored in IEEE 754 floating-point \"single format\" with positive and negative value support. Infinity and Not-a-Number values are not supported and will throw an error if supplied to a TSD. See [Wikipedia](https://en.wikipedia.org/wiki/IEEE_floating_point) and the [Java Documentation](http://docs.oracle.com/javase/specs/jls/se7/html/jls-4.html#jls-4.2.3) for details.\n\nNote\n\nBecause OpenTSDB only supports floating point values, it is not suitable for storing measurements that require exact values like currency. This is why, when storing a value like `15.2` the database may return `15.199999809265137`.\n\n### Ordering\n\nUnlike other solutions, OpenTSDB allows for writing data for a given time series in any order you want. This enables significant flexibility in writing data to a TSD, allowing for populating current data from your systems, then importing historical data at a later time.\n\n### Duplicate Data Points\n\nWriting data points in OpenTSDB is generally idempotent within an hour of the original write. This means you can write the value `42` at timestamp `1356998400` and then write `42` again for the same time and nothing bad will happen. However if you have compactions enabled to reduce storage consumption and write the same data point after the row of data has been compacted, an exception may be returned when you query over that row. If you attempt to write two different values with the same timestamp, a duplicate data point exception may be thrown during query time. This is due to a difference in encoding integers on 1, 2, 4 or 8 bytes and floating point numbers. If the first value was an integer and the second a floating point, the duplicate error will always be thrown. However if both values were floats or they were both integers that could be encoded on the same length, then the original value may be overwritten if a compaction has not occurred on the row.\n\nIn most situations, if a duplicate data point is written it is usually an indication that something went wrong with the data source such as a process restarting unexpectedly or a bug in a script. OpenTSDB will fail \"safe\" by throwing an exception when you query over a row with one or more duplicates so you can down the issue.\n\nWith OpenTSDB 2.1 you can enable last-write-wins by setting the `tsd.storage.fix_duplicates` configuration value to `true`. With this flag enabled, at query time, the most recent value recorded will be returned instead of throwing an exception. A warning will also be written to the log file noting a duplicate was found. If compaction is also enabled, then the original compacted value will be overwritten with the latest value.\n\n## Input Methods\n\nThere are currently three main methods to get data into OpenTSDB: Telnet API, HTTP API and batch import from a file. Alternatively you can use a tool that provides OpenTSDB support, or if you're extremely adventurous, use the Java library.\n\nWarning\n\nDon't try to write directly to the underlying storage system, e.g. HBase. Just don't. It'll get messy quickly.\n\nNote\n\nIf the `tsd.mode` is set to `ro` instead of `rw`, the TSD will not accept data points through RPC calls. Telnet style calls will throw an exception and calls to the HTTP endpoint will return a 404 error. However it is still possible to write via the JAVA API when the mode is set to read only.\n\n### Telnet\n\nThe easiest way to get started with OpenTSDB is to open up a terminal or telnet client, connect to your TSD and issue a `put` command and hit 'enter'. If you are writing a program, simply open a socket, print the string command with a new line and send the packet. The telnet command format is:\n\n``` python\nput <metric> <timestamp> <value> <tagk1=tagv1[ tagk2=tagv2 ...tagkN=tagvN]>\n```\n\nFor example:\n\n``` python\nput sys.cpu.user 1356998400 42.5 host=webserver01 cpu=0\n```\n\nEach `put` can only send a single data point. Don't forget the newline character, e.g. `\\n` at the end of your command.\n\nNote\n\nThe Telnet method of writing is discouraged as it doesn't provide a way of determining which data points failed to write due to formatting or storage errors. Instead use the HTTP API.\n\n### Http API\n\nAs of version 2.0, data can be sent over HTTP in formats supported by 'Serializer' plugins. Multiple, un-related data points can be sent in a single HTTP POST request to save bandwidth. See the `../api_http/put` for details.\n\n### Batch Import\n\nIf you are importing data from another system or you need to backfill historical data, you can use the `import` CLI utility. See `cli/import` for details.\n\n## Write Performance\n\nOpenTSDB can scale to writing millions of data points per 'second' on commodity servers with regular spinning hard drives. However users who fire up a VM with HBase in stand-alone mode and try to slam millions of data points at a brand new TSD are disappointed when they can only write data in the hundreds of points per second. Here's what you need to do to scale for brand new installs or testing and for expanding existing systems.\n\n### UID Assignment\n\nThe first sticking point folks run into is ''uid assignment''. Every string for a metric, tag key and tag value must be assigned a UID before the data point can be stored. For example, the metric `sys.cpu.user` may be assigned a UID of `000001` the first time it is encountered by a TSD. This assignment takes a fair amount of time as it must fetch an available UID, write a UID to name mapping and a name to UID mapping, then use the UID to write the data point's row key. The UID will be stored in the TSD's cache so that the next time the same metric comes through, it can find the UID very quickly.\n\nTherefore, we recommend that you 'pre-assign' UID to as many metrics, tag keys and tag values as you can. If you have designed a naming schema as recommended above, you'll know most of the values to assign. You can use the CLI tools `cli/mkmetric`, `cli/uid` or the HTTP API `../api_http/uid/index` to perform pre-assignments. Any time you are about to send a bunch of new metrics or tags to a running OpenTSDB cluster, try to pre-assign or the TSDs will bog down a bit when they get the new data.\n\nNote\n\nIf you restart a TSD, it will have to lookup the UID for every metric and tag so performance will be a little slow until the cache is filled.\n\n### Random Metric UID Assignment\n\nWith 2.2 you can randomly assign UIDs to metrics for better region server write distribution. Because metric UIDs are located at the start of the row key, if a new set of busy metric are created, all writes for those metric will be on the same server until the region splits. With random UID generation enabled, the new metrics will be distributed across the key space and likely to wind up in different regions on different servers.\n\nRandom metric generation can be enabled or disabled at any time by modifying the `tsd.core.uid.random_metrics` flag and data is backwards compatible all the way back to OpenTSDB 1.0. However it is recommended that you pre-split your TSDB data table according to the full metric UID space. E.g. if you use the default UID size in OpenTSDB, UIDs are 3 bytes wide, thus you can have 16,777,215 values. If you already have data in your TSDB table and choose to enable random UIDs, you may want to create new regions.\n\nWhen generating random IDs, TSDB will try up to 10 times to assign a UID without a collision. Thus as the number of assigned metrics increases so too will the number of collisions and the likely hood that a data point may be dropped due to retries. If you enable random IDs and keep adding more metrics then you may want to increase the number of bytes on metric UIDs. Note that the UID change is not backwards compatible so you have to create a new table and migrate your old data.\n\n### Salting\n\nIn 2.2 salting is supported to greatly increase write distribution across region servers. When enabled, a configured number of bytes are prepended to each row key. Each metric and combination of tags is then hashed into one \"bucket\", the ID of which is written to the salt bytes. Distribution is improved particularly for high-cardinality metrics (those with a large number of tag combinations) as the time series are split across the configured bucket count, thus routed to different regions and different servers. For example, without salting, a metric with 1 million series will be written to a single region on a single server. With salting enabled and a bucket size of 20, the series will be split across 20 regions (and 20 servers if the cluster has that many hosts) where each region has 50,000 series.\n\nWarning\n\nBecause salting modifies the storage format, you cannot enable or disable salting at whim. If you have existing data, you must start a new data table and migrate data from the old table into the new one. Salted data cannot be read from previous versions of OpenTSDB.\n\nTo enable salting you must modify the config file parameter `tsd.storage.salt.width` and optionally `tsd.storage.salt.buckets`. We recommend setting the salt width to `1` and determine the number of buckets based on a factor of the number of region servers in your cluster. Note that at query time, the TSD will fire `tsd.storage.salt.buckets` number of scanners to fetch data. The proper number of salt buckets must be determined through experimentation as at some point query performance may suffer due to having too many scanners open and collating the results. In the future the salt width and buckets may be configurable but we didn't want folks changing settings on accident and losing data.\n\n### Appends\n\nAlso in 2.2, writing to HBase columns via appends is now supported. This can improve both read and write performance in that TSDs will no longer maintain a queue of rows to compact at the end of each hour, thus preventing a massive read and re-write operation in HBase. However due to the way appends operate in HBase, an increase in CPU utilization, store file size and HDFS traffic will occur on the region servers. Make sure to monitor your HBase servers closely.\n\nAt read time, only one column is returned per row similar to post-TSD-compaction rows. However note that if the `tsd.storage.repair_appends` is enabled, then when a column has duplicates or out of order data, it will be re-written to HBase. Also columns with many duplicates or ordering issues may slow queries as they must be resolved before answering the caller.\n\nAppends can be enabled and disabled at any time. However versions of OpenTSDB prior to 2.2 will skip over appended values.\n\n### Pre-Split HBase Regions\n\nFor brand new installs you will see much better performance if you pre-split the regions in HBase regardless of if you're testing on a stand-alone server or running a full cluster. HBase regions handle a defined range of row keys and are essentially a single file. When you create the `tsdb` table and start writing data for the first time, all of those data points are being sent to this one file on one server. As a region fills up, HBase will automatically split it into different files and move it to other servers in the cluster, but when this happens, the TSDs cannot write to the region and must buffer the data points. Therefore, if you can pre-allocate a number of regions before you start writing, the TSDs can send data to multiple files or servers and you'll be taking advantage of the linear scalability immediately.\n\nThe simplest way to pre-split your `tsdb` table regions is to estimate the number of unique metric names you'll be recording. If you have designed a naming schema, you should have a pretty good idea. Let's say that we will track 4,000 metrics in our system. That's not to say 4,000 time series, as we're not counting the tags yet, just the metric names such as \"sys.cpu.user\". Data points are written in row keys where the metric's UID comprises the first bytes, 3 bytes by default. The first metric will be assigned a UID of `000001` as a hex encoded value. The 4,000th metric will have a UID of `000FA0` in hex. You can use these as the start and end keys in the script from the [HBase Book](http://hbase.apache.org/book/perf.writing.html) to split your table into any number of regions. 256 regions may be a good place to start depending on how many time series share each metric.\n\nTODO - include scripts for pre-splitting.\n\nThe simple split method above assumes that you have roughly an equal number of time series per metric (i.e. a fairly consistent cardinality). E.g. the metric with a UID of `000001` may have 200 time series and `000FA0` has about 150. If you have a wide range of time series per metric, e.g. `000001` has 10,000 time series while `000FA0` only has 2, you may need to develop a more complex splitting algorithm.\n\nBut don't worry too much about splitting. As stated above, HBase will automatically split regions for you so over time, the data will be distributed fairly evenly.\n\n### Distributed HBase\n\nHBase will run in stand-alone mode where it will use the local file system for storing files. It will still use multiple regions and perform as well as the underlying disk or raid array will let it. You'll definitely want a RAID array under HBase so that if a drive fails, you can replace it without losing data. This kind of setup is fine for testing or very small installations and you should be able to get into the low thousands of data points per second.\n\nHowever if you want serious throughput and scalability you have to setup a Hadoop and HBase cluster with multiple servers. In a distributed setup HDFS manages region files, automatically distributing copies to different servers for fault tolerance. HBase assigns regions to different servers and OpenTSDB's client will send data points to the specific server where they will be stored. You're now spreading operations amongst multiple servers, increasing performance and storage. If you need even more throughput or storage, just add nodes or disks.\n\nThere are a number of ways to setup a Hadoop/HBase cluster and a ton of various tuning tweaks to make, so Google around and ask user groups for advice. Some general recommendations include:\n\n- Dedicate a pair of high memory, low disk space servers for the Name Node. Set them up for high availability using something like Heartbeat and Pacemaker.\n- Setup Zookeeper on at least 3 servers for fault tolerance. They must have a lot of RAM and a fairly fast disk for log writing. On small clusters, these can run on the Name node servers.\n- JBOD for the HDFS data nodes\n- HBase region servers can be collocated with the HDFS data nodes\n- At least 1 gbps links between servers, 10 gbps preferable.\n- Keep the cluster in a single data center\n\n### Multiple TSDs\n\nA single TSD can handle thousands of writes per second. But if you have many sources it's best to scale by running multiple TSDs and using a load balancer (such as Varnish or DNS round robin) to distribute the writes. Many users colocate TSDs on their HBase region servers when the cluster is dedicated to OpenTSDB.\n\n### Persistent Connections\n\nEnable keep-alives in the TSDs and make sure that any applications you are using to send time series data keep their connections open instead of opening and closing for every write. See `configuration` for details.\n\n### Disable Meta Data and Real Time Publishing\n\nOpenTSDB 2.0 introduced meta data for tracking the kinds of data in the system. When tracking is enabled, a counter is incremented for every data point written and new UIDs or time series will generate meta data. The data may be pushed to a search engine or passed through tree generation code. These processes require greater memory in the TSD and may affect throughput. Tracking is disabled by default so test it out before enabling the feature.\n\n2.0 also introduced a real-time publishing plugin where incoming data points can be emitted to another destination immediately after they're queued for storage. This is disabled by default so test any plugins you are interested in before deploying in production.\n\n© 2010–2016 The OpenTSDB Authors  \nLicensed under the GNU LGPLv2.1+ and GPLv3+ licenses.  \n[http://opentsdb.net/docs/build/html/user_guide/writing/index.html](http://opentsdb.net/docs/build/html/user_guide/writing/index.html)"
